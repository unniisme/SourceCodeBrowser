[
 {
  "docstring": "Returns the size of the cut between two sets of nodes.\n\nA *cut* is a partition of the nodes of a graph into two sets. The\n*cut size* is the sum of the weights of the edges \"between\" the two\nsets of nodes.\n\nParameters\n----------\nG : NetworkX graph\n\nS : collection\n    A collection of nodes in `G`.\n\nT : collection\n    A collection of nodes in `G`. If not specified, this is taken to\n    be the set complement of `S`.\n\nweight : object\n    Edge attribute key to use as weight. If not specified, edges\n    have weight one.\n\nReturns\n-------\nnumber\n    Total weight of all edges from nodes in set `S` to nodes in\n    set `T` (and, in the case of directed graphs, all edges from\n    nodes in `T` to nodes in `S`).\n\nExamples\n--------\nIn the graph with two cliques joined by a single edges, the natural\nbipartition of the graph into two blocks, one for each clique,\nyields a cut of weight one::\n\n    >>> G = nx.barbell_graph(3, 0)\n    >>> S = {0, 1, 2}\n    >>> T = {3, 4, 5}\n    >>> nx.cut_size(G, S, T)\n    1\n\nEach parallel edge in a multigraph is counted when determining the\ncut size::\n\n    >>> G = nx.MultiGraph([\"ab\", \"ab\"])\n    >>> S = {\"a\"}\n    >>> T = {\"b\"}\n    >>> nx.cut_size(G, S, T)\n    2\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef cut_size(G, S, T=None, weight=None):\n    edges = nx.edge_boundary(G, S, T, data=weight, default=1)\n    if G.is_directed():\n        edges = chain(edges, nx.edge_boundary(G, T, S, data=weight, default=1))\n    return sum((weight for u, v, weight in edges))"
 },
 {
  "docstring": "Returns the volume of a set of nodes.\n\nThe *volume* of a set *S* is the sum of the (out-)degrees of nodes\nin *S* (taking into account parallel edges in multigraphs). [1]\n\nParameters\n----------\nG : NetworkX graph\n\nS : collection\n    A collection of nodes in `G`.\n\nweight : object\n    Edge attribute key to use as weight. If not specified, edges\n    have weight one.\n\nReturns\n-------\nnumber\n    The volume of the set of nodes represented by `S` in the graph\n    `G`.\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef volume(G, S, weight=None):\n    degree = G.out_degree if G.is_directed() else G.degree\n    return sum((d for v, d in degree(S, weight=weight)))"
 },
 {
  "docstring": "Returns the normalized size of the cut between two sets of nodes.\n\nThe *normalized cut size* is the cut size times the sum of the\nreciprocal sizes of the volumes of the two sets. [1]\n\nParameters\n----------\nG : NetworkX graph\n\nS : collection\n    A collection of nodes in `G`.\n\nT : collection\n    A collection of nodes in `G`.\n\nweight : object\n    Edge attribute key to use as weight. If not specified, edges\n    have weight one.\n\nReturns\n-------\nnumber\n    The normalized cut size between the two sets `S` and `T`.\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef normalized_cut_size(G, S, T=None, weight=None):\n    if T is None:\n        T = set(G) - set(S)\n    num_cut_edges = cut_size(G, S, T=T, weight=weight)\n    volume_S = volume(G, S, weight=weight)\n    volume_T = volume(G, T, weight=weight)\n    return num_cut_edges * (1 / volume_S + 1 / volume_T)"
 },
 {
  "docstring": "Returns the conductance of two sets of nodes.\n\nThe *conductance* is the quotient of the cut size and the smaller of\nthe volumes of the two sets. [1]\n\nParameters\n----------\nG : NetworkX graph\n\nS : collection\n    A collection of nodes in `G`.\n\nT : collection\n    A collection of nodes in `G`.\n\nweight : object\n    Edge attribute key to use as weight. If not specified, edges\n    have weight one.\n\nReturns\n-------\nnumber\n    The conductance between the two sets `S` and `T`.\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef conductance(G, S, T=None, weight=None):\n    if T is None:\n        T = set(G) - set(S)\n    num_cut_edges = cut_size(G, S, T, weight=weight)\n    volume_S = volume(G, S, weight=weight)\n    volume_T = volume(G, T, weight=weight)\n    return num_cut_edges / min(volume_S, volume_T)"
 },
 {
  "docstring": "Returns the edge expansion between two node sets.\n\nThe *edge expansion* is the quotient of the cut size and the smaller\nof the cardinalities of the two sets. [1]\n\nParameters\n----------\nG : NetworkX graph\n\nS : collection\n    A collection of nodes in `G`.\n\nT : collection\n    A collection of nodes in `G`.\n\nweight : object\n    Edge attribute key to use as weight. If not specified, edges\n    have weight one.\n\nReturns\n-------\nnumber\n    The edge expansion between the two sets `S` and `T`.\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef edge_expansion(G, S, T=None, weight=None):\n    if T is None:\n        T = set(G) - set(S)\n    num_cut_edges = cut_size(G, S, T=T, weight=weight)\n    return num_cut_edges / min(len(S), len(T))"
 },
 {
  "docstring": "Returns the mixing expansion between two node sets.\n\nThe *mixing expansion* is the quotient of the cut size and twice the\nnumber of edges in the graph. [1]\n\nParameters\n----------\nG : NetworkX graph\n\nS : collection\n    A collection of nodes in `G`.\n\nT : collection\n    A collection of nodes in `G`.\n\nweight : object\n    Edge attribute key to use as weight. If not specified, edges\n    have weight one.\n\nReturns\n-------\nnumber\n    The mixing expansion between the two sets `S` and `T`.\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef mixing_expansion(G, S, T=None, weight=None):\n    num_cut_edges = cut_size(G, S, T=T, weight=weight)\n    num_total_edges = G.number_of_edges()\n    return num_cut_edges / (2 * num_total_edges)"
 },
 {
  "docstring": "Returns the node expansion of the set `S`.\n\nThe *node expansion* is the quotient of the size of the node\nboundary of *S* and the cardinality of *S*. [1]\n\nParameters\n----------\nG : NetworkX graph\n\nS : collection\n    A collection of nodes in `G`.\n\nReturns\n-------\nnumber\n    The node expansion of the set `S`.\n\n",
  "code": "@nx._dispatch\ndef node_expansion(G, S):\n    neighborhood = set(chain.from_iterable((G.neighbors(v) for v in S)))\n    return len(neighborhood) / len(S)"
 },
 {
  "docstring": "Returns the boundary expansion of the set `S`.\n\nThe *boundary expansion* is the quotient of the size\nof the node boundary and the cardinality of *S*. [1]\n\nParameters\n----------\nG : NetworkX graph\n\nS : collection\n    A collection of nodes in `G`.\n\nReturns\n-------\nnumber\n    The boundary expansion of the set `S`.\n\n",
  "code": "@nx._dispatch\ndef boundary_expansion(G, S):\n    return len(nx.node_boundary(G, S)) / len(S)"
 },
 {
  "docstring": "Return the lowest common ancestor of all pairs or the provided pairs\n\nParameters\n----------\nG : NetworkX directed graph\n\npairs : iterable of pairs of nodes, optional (default: all pairs)\n    The pairs of nodes of interest.\n    If None, will find the LCA of all pairs of nodes.\n\nYields\n------\n((node1, node2), lca) : 2-tuple\n    Where lca is least common ancestor of node1 and node2.\n    Note that for the default case, the order of the node pair is not considered,\n    e.g. you will not get both ``(a, b)`` and ``(b, a)``\n\nRaises\n------\nNetworkXPointlessConcept\n    If `G` is null.\nNetworkXError\n    If `G` is not a DAG.\n\nExamples\n--------\nThe default behavior is to yield the lowest common ancestor for all\npossible combinations of nodes in `G`, including self-pairings:\n\n>>> G = nx.DiGraph([(0, 1), (0, 3), (1, 2)])\n>>> dict(nx.all_pairs_lowest_common_ancestor(G))\n{(0, 0): 0, (0, 1): 0, (0, 3): 0, (0, 2): 0, (1, 1): 1, (1, 3): 0, (1, 2): 1, (3, 3): 3, (3, 2): 0, (2, 2): 2}\n\nThe pairs argument can be used to limit the output to only the\nspecified node pairings:\n\n>>> dict(nx.all_pairs_lowest_common_ancestor(G, pairs=[(1, 2), (2, 3)]))\n{(1, 2): 1, (2, 3): 0}\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef all_pairs_lowest_common_ancestor(G, pairs=None):\n    if not nx.is_directed_acyclic_graph(G):\n        raise nx.NetworkXError('LCA only defined on directed acyclic graphs.')\n    if len(G) == 0:\n        raise nx.NetworkXPointlessConcept('LCA meaningless on null graphs.')\n    if pairs is None:\n        pairs = combinations_with_replacement(G, 2)\n    else:\n        pairs = dict.fromkeys(pairs)\n        nodeset = set(G)\n        for pair in pairs:\n            if set(pair) - nodeset:\n                raise nx.NodeNotFound(f'Node(s) {set(pair) - nodeset} from pair {pair} not in G.')\n\n    def generate_lca_from_pairs(G, pairs):\n        ancestor_cache = {}\n        for v, w in pairs:\n            if v not in ancestor_cache:\n                ancestor_cache[v] = nx.ancestors(G, v)\n                ancestor_cache[v].add(v)\n            if w not in ancestor_cache:\n                ancestor_cache[w] = nx.ancestors(G, w)\n                ancestor_cache[w].add(w)\n            common_ancestors = ancestor_cache[v] & ancestor_cache[w]\n            if common_ancestors:\n                common_ancestor = next(iter(common_ancestors))\n                while True:\n                    successor = None\n                    for lower_ancestor in G.successors(common_ancestor):\n                        if lower_ancestor in common_ancestors:\n                            successor = lower_ancestor\n                            break\n                    if successor is None:\n                        break\n                    common_ancestor = successor\n                yield ((v, w), common_ancestor)\n    return generate_lca_from_pairs(G, pairs)"
 },
 {
  "docstring": "Compute the lowest common ancestor of the given pair of nodes.\n\nParameters\n----------\nG : NetworkX directed graph\n\nnode1, node2 : nodes in the graph.\n\ndefault : object\n    Returned if no common ancestor between `node1` and `node2`\n\nReturns\n-------\nThe lowest common ancestor of node1 and node2,\nor default if they have no common ancestors.\n\nExamples\n--------\n>>> G = nx.DiGraph()\n>>> nx.add_path(G, (0, 1, 2, 3))\n>>> nx.add_path(G, (0, 4, 3))\n>>> nx.lowest_common_ancestor(G, 2, 4)\n0\n\nSee Also\n--------\nall_pairs_lowest_common_ancestor",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef lowest_common_ancestor(G, node1, node2, default=None):\n    ans = list(all_pairs_lowest_common_ancestor(G, pairs=[(node1, node2)]))\n    if ans:\n        assert len(ans) == 1\n        return ans[0][1]\n    return default"
 },
 {
  "docstring": "Yield the lowest common ancestor for sets of pairs in a tree.\n\nParameters\n----------\nG : NetworkX directed graph (must be a tree)\n\nroot : node, optional (default: None)\n    The root of the subtree to operate on.\n    If None, assume the entire graph has exactly one source and use that.\n\npairs : iterable or iterator of pairs of nodes, optional (default: None)\n    The pairs of interest. If None, Defaults to all pairs of nodes\n    under `root` that have a lowest common ancestor.\n\nReturns\n-------\nlcas : generator of tuples `((u, v), lca)` where `u` and `v` are nodes\n    in `pairs` and `lca` is their lowest common ancestor.\n\nExamples\n--------\n>>> import pprint\n>>> G = nx.DiGraph([(1, 3), (2, 4), (1, 2)])\n>>> pprint.pprint(dict(nx.tree_all_pairs_lowest_common_ancestor(G)))\n{(1, 1): 1,\n (2, 1): 1,\n (2, 2): 2,\n (3, 1): 1,\n (3, 2): 1,\n (3, 3): 3,\n (3, 4): 1,\n (4, 1): 1,\n (4, 2): 2,\n (4, 4): 4}\n\nWe can also use `pairs` argument to specify the pairs of nodes for which we\nwant to compute lowest common ancestors. Here is an example:\n\n>>> dict(nx.tree_all_pairs_lowest_common_ancestor(G, pairs=[(1, 4), (2, 3)]))\n{(2, 3): 1, (1, 4): 1}\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef tree_all_pairs_lowest_common_ancestor(G, root=None, pairs=None):\n    if len(G) == 0:\n        raise nx.NetworkXPointlessConcept('LCA meaningless on null graphs.')\n    if pairs is not None:\n        pair_dict = defaultdict(set)\n        if not isinstance(pairs, Mapping | Set):\n            pairs = set(pairs)\n        for u, v in pairs:\n            for n in (u, v):\n                if n not in G:\n                    msg = f'The node {str(n)} is not in the digraph.'\n                    raise nx.NodeNotFound(msg)\n            pair_dict[u].add(v)\n            pair_dict[v].add(u)\n    if root is None:\n        for n, deg in G.in_degree:\n            if deg == 0:\n                if root is not None:\n                    msg = 'No root specified and tree has multiple sources.'\n                    raise nx.NetworkXError(msg)\n                root = n\n            elif deg > 1 and len(G.pred[n]) > 1:\n                msg = 'Tree LCA only defined on trees; use DAG routine.'\n                raise nx.NetworkXError(msg)\n    if root is None:\n        raise nx.NetworkXError('Graph contains a cycle.')\n    uf = UnionFind()\n    ancestors = {}\n    for node in G:\n        ancestors[node] = uf[node]\n    colors = defaultdict(bool)\n    for node in nx.dfs_postorder_nodes(G, root):\n        colors[node] = True\n        for v in pair_dict[node] if pairs is not None else G:\n            if colors[v]:\n                if pairs is not None and (node, v) in pairs:\n                    yield ((node, v), ancestors[uf[v]])\n                if pairs is None or (v, node) in pairs:\n                    yield ((v, node), ancestors[uf[v]])\n        if node != root:\n            parent = arbitrary_element(G.pred[node])\n            uf.union(parent, node)\n            ancestors[uf[parent]] = parent"
 },
 {
  "docstring": "Find an asteroidal triple in the given graph.\n\nAn asteroidal triple is a triple of non-adjacent vertices such that\nthere exists a path between any two of them which avoids the closed\nneighborhood of the third. It checks all independent triples of vertices\nand whether they are an asteroidal triple or not. This is done with the\nhelp of a data structure called a component structure.\nA component structure encodes information about which vertices belongs to\nthe same connected component when the closed neighborhood of a given vertex\nis removed from the graph. The algorithm used to check is the trivial\none, outlined in [1]_, which has a runtime of\n:math:`O(|V||\\overline{E} + |V||E|)`, where the second term is the\ncreation of the component structure.\n\nParameters\n----------\nG : NetworkX Graph\n    The graph to check whether is AT-free or not\n\nReturns\n-------\nlist or None\n    An asteroidal triple is returned as a list of nodes. If no asteroidal\n    triple exists, i.e. the graph is AT-free, then None is returned.\n    The returned value depends on the certificate parameter. The default\n    option is a bool which is True if the graph is AT-free, i.e. the\n    given graph contains no asteroidal triples, and False otherwise, i.e.\n    if the graph contains at least one asteroidal triple.\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef find_asteroidal_triple(G):\n    V = set(G.nodes)\n    if len(V) < 6:\n        return None\n    component_structure = create_component_structure(G)\n    E_complement = set(nx.complement(G).edges)\n    for e in E_complement:\n        u = e[0]\n        v = e[1]\n        u_neighborhood = set(G[u]).union([u])\n        v_neighborhood = set(G[v]).union([v])\n        union_of_neighborhoods = u_neighborhood.union(v_neighborhood)\n        for w in V - union_of_neighborhoods:\n            if component_structure[u][v] == component_structure[u][w] and component_structure[v][u] == component_structure[v][w] and (component_structure[w][u] == component_structure[w][v]):\n                return [u, v, w]\n    return None"
 },
 {
  "docstring": "Check if a graph is AT-free.\n\nThe method uses the `find_asteroidal_triple` method to recognize\nan AT-free graph. If no asteroidal triple is found the graph is\nAT-free and True is returned. If at least one asteroidal triple is\nfound the graph is not AT-free and False is returned.\n\nParameters\n----------\nG : NetworkX Graph\n    The graph to check whether is AT-free or not.\n\nReturns\n-------\nbool\n    True if G is AT-free and False otherwise.\n\nExamples\n--------\n>>> G = nx.Graph([(0, 1), (0, 2), (1, 2), (1, 3), (1, 4), (4, 5)])\n>>> nx.is_at_free(G)\nTrue\n\n>>> G = nx.cycle_graph(6)\n>>> nx.is_at_free(G)\nFalse",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef is_at_free(G):\n    return find_asteroidal_triple(G) is None"
 },
 {
  "docstring": "Create component structure for G.\n\nA *component structure* is an `nxn` array, denoted `c`, where `n` is\nthe number of vertices,  where each row and column corresponds to a vertex.\n\n.. math::\n    c_{uv} = \\begin{cases} 0, if v \\in N[u] \\\\\n        k, if v \\in component k of G \\setminus N[u] \\end{cases}\n\nWhere `k` is an arbitrary label for each component. The structure is used\nto simplify the detection of asteroidal triples.\n\nParameters\n----------\nG : NetworkX Graph\n    Undirected, simple graph.\n\nReturns\n-------\ncomponent_structure : dictionary\n    A dictionary of dictionaries, keyed by pairs of vertices.",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef create_component_structure(G):\n    V = set(G.nodes)\n    component_structure = {}\n    for v in V:\n        label = 0\n        closed_neighborhood = set(G[v]).union({v})\n        row_dict = {}\n        for u in closed_neighborhood:\n            row_dict[u] = 0\n        G_reduced = G.subgraph(set(G.nodes) - closed_neighborhood)\n        for cc in nx.connected_components(G_reduced):\n            label += 1\n            for u in cc:\n                row_dict[u] = label\n        component_structure[v] = row_dict\n    return component_structure"
 },
 {
  "docstring": "Returns the edge boundary of `nbunch1`.\n\nThe *edge boundary* of a set *S* with respect to a set *T* is the\nset of edges (*u*, *v*) such that *u* is in *S* and *v* is in *T*.\nIf *T* is not specified, it is assumed to be the set of all nodes\nnot in *S*.\n\nParameters\n----------\nG : NetworkX graph\n\nnbunch1 : iterable\n    Iterable of nodes in the graph representing the set of nodes\n    whose edge boundary will be returned. (This is the set *S* from\n    the definition above.)\n\nnbunch2 : iterable\n    Iterable of nodes representing the target (or \"exterior\") set of\n    nodes. (This is the set *T* from the definition above.) If not\n    specified, this is assumed to be the set of all nodes in `G`\n    not in `nbunch1`.\n\nkeys : bool\n    This parameter has the same meaning as in\n    :meth:`MultiGraph.edges`.\n\ndata : bool or object\n    This parameter has the same meaning as in\n    :meth:`MultiGraph.edges`.\n\ndefault : object\n    This parameter has the same meaning as in\n    :meth:`MultiGraph.edges`.\n\nReturns\n-------\niterator\n    An iterator over the edges in the boundary of `nbunch1` with\n    respect to `nbunch2`. If `keys`, `data`, or `default`\n    are specified and `G` is a multigraph, then edges are returned\n    with keys and/or data, as in :meth:`MultiGraph.edges`.\n\nExamples\n--------\n>>> G = nx.wheel_graph(6)\n\nWhen nbunch2=None:\n\n>>> list(nx.edge_boundary(G, (1, 3)))\n[(1, 0), (1, 2), (1, 5), (3, 0), (3, 2), (3, 4)]\n\nWhen nbunch2 is given:\n\n>>> list(nx.edge_boundary(G, (1, 3), (2, 0)))\n[(1, 0), (1, 2), (3, 0), (3, 2)]\n\n",
  "code": "@nx._dispatch(edge_attrs={'data': 'default'}, preserve_edge_attrs='data')\ndef edge_boundary(G, nbunch1, nbunch2=None, data=False, keys=False, default=None):\n    nset1 = {n for n in nbunch1 if n in G}\n    if G.is_multigraph():\n        edges = G.edges(nset1, data=data, keys=keys, default=default)\n    else:\n        edges = G.edges(nset1, data=data, default=default)\n    if nbunch2 is None:\n        return (e for e in edges if (e[0] in nset1) ^ (e[1] in nset1))\n    nset2 = set(nbunch2)\n    return (e for e in edges if e[0] in nset1 and e[1] in nset2 or (e[1] in nset1 and e[0] in nset2))"
 },
 {
  "docstring": "Returns the node boundary of `nbunch1`.\n\nThe *node boundary* of a set *S* with respect to a set *T* is the\nset of nodes *v* in *T* such that for some *u* in *S*, there is an\nedge joining *u* to *v*. If *T* is not specified, it is assumed to\nbe the set of all nodes not in *S*.\n\nParameters\n----------\nG : NetworkX graph\n\nnbunch1 : iterable\n    Iterable of nodes in the graph representing the set of nodes\n    whose node boundary will be returned. (This is the set *S* from\n    the definition above.)\n\nnbunch2 : iterable\n    Iterable of nodes representing the target (or \"exterior\") set of\n    nodes. (This is the set *T* from the definition above.) If not\n    specified, this is assumed to be the set of all nodes in `G`\n    not in `nbunch1`.\n\nReturns\n-------\nset\n    The node boundary of `nbunch1` with respect to `nbunch2`.\n\nExamples\n--------\n>>> G = nx.wheel_graph(6)\n\nWhen nbunch2=None:\n\n>>> list(nx.node_boundary(G, (3, 4)))\n[0, 2, 5]\n\nWhen nbunch2 is given:\n\n>>> list(nx.node_boundary(G, (3, 4), (0, 1, 5)))\n[0, 5]\n\n",
  "code": "@nx._dispatch\ndef node_boundary(G, nbunch1, nbunch2=None):\n    nset1 = {n for n in nbunch1 if n in G}\n    bdy = set(chain.from_iterable((G[v] for v in nset1))) - nset1\n    if nbunch2 is not None:\n        bdy &= set(nbunch2)\n    return bdy"
 },
 {
  "docstring": "Generate all bridges in a graph.\n\nA *bridge* in a graph is an edge whose removal causes the number of\nconnected components of the graph to increase.  Equivalently, a bridge is an\nedge that does not belong to any cycle. Bridges are also known as cut-edges,\nisthmuses, or cut arcs.\n\nParameters\n----------\nG : undirected graph\n\nroot : node (optional)\n   A node in the graph `G`. If specified, only the bridges in the\n   connected component containing this node will be returned.\n\nYields\n------\ne : edge\n   An edge in the graph whose removal disconnects the graph (or\n   causes the number of connected components to increase).\n\nRaises\n------\nNodeNotFound\n   If `root` is not in the graph `G`.\n\nNetworkXNotImplemented\n    If `G` is a directed graph.\n\nExamples\n--------\nThe barbell graph with parameter zero has a single bridge:\n\n>>> G = nx.barbell_graph(10, 0)\n>>> list(nx.bridges(G))\n[(9, 10)]\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch\ndef bridges(G, root=None):\n    multigraph = G.is_multigraph()\n    H = nx.Graph(G) if multigraph else G\n    chains = nx.chain_decomposition(H, root=root)\n    chain_edges = set(chain.from_iterable(chains))\n    H_copy = H.copy()\n    if root is not None:\n        H = H.subgraph(nx.node_connected_component(H, root)).copy()\n    for u, v in H.edges():\n        if (u, v) not in chain_edges and (v, u) not in chain_edges:\n            if multigraph and len(G[u][v]) > 1:\n                continue\n            yield (u, v)"
 },
 {
  "docstring": "Decide whether a graph has any bridges.\n\nA *bridge* in a graph is an edge whose removal causes the number of\nconnected components of the graph to increase.\n\nParameters\n----------\nG : undirected graph\n\nroot : node (optional)\n   A node in the graph `G`. If specified, only the bridges in the\n   connected component containing this node will be considered.\n\nReturns\n-------\nbool\n   Whether the graph (or the connected component containing `root`)\n   has any bridges.\n\nRaises\n------\nNodeNotFound\n   If `root` is not in the graph `G`.\n\nNetworkXNotImplemented\n    If `G` is a directed graph.\n\nExamples\n--------\nThe barbell graph with parameter zero has a single bridge::\n\n    >>> G = nx.barbell_graph(10, 0)\n    >>> nx.has_bridges(G)\n    True\n\nOn the other hand, the cycle graph has no bridges::\n\n    >>> G = nx.cycle_graph(5)\n    >>> nx.has_bridges(G)\n    False\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch\ndef has_bridges(G, root=None):\n    try:\n        next(bridges(G, root=root))\n    except StopIteration:\n        return False\n    else:\n        return True"
 },
 {
  "docstring": "Iterate over local bridges of `G` optionally computing the span\n\nA *local bridge* is an edge whose endpoints have no common neighbors.\nThat is, the edge is not part of a triangle in the graph.\n\nThe *span* of a *local bridge* is the shortest path length between\nthe endpoints if the local bridge is removed.\n\nParameters\n----------\nG : undirected graph\n\nwith_span : bool\n    If True, yield a 3-tuple `(u, v, span)`\n\nweight : function, string or None (default: None)\n    If function, used to compute edge weights for the span.\n    If string, the edge data attribute used in calculating span.\n    If None, all edges have weight 1.\n\nYields\n------\ne : edge\n    The local bridges as an edge 2-tuple of nodes `(u, v)` or\n    as a 3-tuple `(u, v, span)` when `with_span is True`.\n\nRaises\n------\nNetworkXNotImplemented\n    If `G` is a directed graph or multigraph.\n\nExamples\n--------\nA cycle graph has every edge a local bridge with span N-1.\n\n   >>> G = nx.cycle_graph(9)\n   >>> (0, 8, 8) in set(nx.local_bridges(G))\n   True",
  "code": "@not_implemented_for('multigraph')\n@not_implemented_for('directed')\n@nx._dispatch(edge_attrs='weight')\ndef local_bridges(G, with_span=True, weight=None):\n    if with_span is not True:\n        for u, v in G.edges:\n            if not set(G[u]) & set(G[v]):\n                yield (u, v)\n    else:\n        wt = nx.weighted._weight_function(G, weight)\n        for u, v in G.edges:\n            if not set(G[u]) & set(G[v]):\n                enodes = {u, v}\n\n                def hide_edge(n, nbr, d):\n                    if n not in enodes or nbr not in enodes:\n                        return wt(n, nbr, d)\n                    return None\n                try:\n                    span = nx.shortest_path_length(G, u, v, weight=hide_edge)\n                    yield (u, v, span)\n                except nx.NetworkXNoPath:\n                    yield (u, v, float('inf'))"
 },
 {
  "docstring": "Returns the chain decomposition of a graph.\n\nThe *chain decomposition* of a graph with respect a depth-first\nsearch tree is a set of cycles or paths derived from the set of\nfundamental cycles of the tree in the following manner. Consider\neach fundamental cycle with respect to the given tree, represented\nas a list of edges beginning with the nontree edge oriented away\nfrom the root of the tree. For each fundamental cycle, if it\noverlaps with any previous fundamental cycle, just take the initial\nnon-overlapping segment, which is a path instead of a cycle. Each\ncycle or path is called a *chain*. For more information, see [1]_.\n\nParameters\n----------\nG : undirected graph\n\nroot : node (optional)\n   A node in the graph `G`. If specified, only the chain\n   decomposition for the connected component containing this node\n   will be returned. This node indicates the root of the depth-first\n   search tree.\n\nYields\n------\nchain : list\n   A list of edges representing a chain. There is no guarantee on\n   the orientation of the edges in each chain (for example, if a\n   chain includes the edge joining nodes 1 and 2, the chain may\n   include either (1, 2) or (2, 1)).\n\nRaises\n------\nNodeNotFound\n   If `root` is not in the graph `G`.\n\nExamples\n--------\n>>> G = nx.Graph([(0, 1), (1, 4), (3, 4), (3, 5), (4, 5)])\n>>> list(nx.chain_decomposition(G))\n[[(4, 5), (5, 3), (3, 4)]]\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef chain_decomposition(G, root=None):\n\n    def _dfs_cycle_forest(G, root=None):\n        \"\"\"Builds a directed graph composed of cycles from the given graph.\n\n        `G` is an undirected simple graph. `root` is a node in the graph\n        from which the depth-first search is started.\n\n        This function returns both the depth-first search cycle graph\n        (as a :class:`~networkx.DiGraph`) and the list of nodes in\n        depth-first preorder. The depth-first search cycle graph is a\n        directed graph whose edges are the edges of `G` oriented toward\n        the root if the edge is a tree edge and away from the root if\n        the edge is a non-tree edge. If `root` is not specified, this\n        performs a depth-first search on each connected component of `G`\n        and returns a directed forest instead.\n\n        If `root` is not in the graph, this raises :exc:`KeyError`.\n\n        \"\"\"\n        H = nx.DiGraph()\n        nodes = []\n        for u, v, d in nx.dfs_labeled_edges(G, source=root):\n            if d == 'forward':\n                if u == v:\n                    H.add_node(v, parent=None)\n                    nodes.append(v)\n                else:\n                    H.add_node(v, parent=u)\n                    H.add_edge(v, u, nontree=False)\n                    nodes.append(v)\n            elif d == 'nontree' and v not in H[u]:\n                H.add_edge(v, u, nontree=True)\n            else:\n                pass\n        return (H, nodes)\n\n    def _build_chain(G, u, v, visited):\n        \"\"\"Generate the chain starting from the given nontree edge.\n\n        `G` is a DFS cycle graph as constructed by\n        :func:`_dfs_cycle_graph`. The edge (`u`, `v`) is a nontree edge\n        that begins a chain. `visited` is a set representing the nodes\n        in `G` that have already been visited.\n\n        This function yields the edges in an initial segment of the\n        fundamental cycle of `G` starting with the nontree edge (`u`,\n        `v`) that includes all the edges up until the first node that\n        appears in `visited`. The tree edges are given by the 'parent'\n        node attribute. The `visited` set is updated to add each node in\n        an edge yielded by this function.\n\n        \"\"\"\n        while v not in visited:\n            yield (u, v)\n            visited.add(v)\n            u, v = (v, G.nodes[v]['parent'])\n        yield (u, v)\n    if root is not None and root not in G:\n        raise nx.NodeNotFound(f'Root node {root} is not in graph')\n    H, nodes = _dfs_cycle_forest(G, root)\n    visited = set()\n    for u in nodes:\n        visited.add(u)\n        edges = ((u, v) for u, v, d in H.out_edges(u, data='nontree') if d)\n        for u, v in edges:\n            chain = list(_build_chain(H, u, v, visited))\n            yield chain"
 },
 {
  "docstring": "Builds a directed graph composed of cycles from the given graph.\n\n`G` is an undirected simple graph. `root` is a node in the graph\nfrom which the depth-first search is started.\n\nThis function returns both the depth-first search cycle graph\n(as a :class:`~networkx.DiGraph`) and the list of nodes in\ndepth-first preorder. The depth-first search cycle graph is a\ndirected graph whose edges are the edges of `G` oriented toward\nthe root if the edge is a tree edge and away from the root if\nthe edge is a non-tree edge. If `root` is not specified, this\nperforms a depth-first search on each connected component of `G`\nand returns a directed forest instead.\n\nIf `root` is not in the graph, this raises :exc:`KeyError`.",
  "code": "def _dfs_cycle_forest(G, root=None):\n    H = nx.DiGraph()\n    nodes = []\n    for u, v, d in nx.dfs_labeled_edges(G, source=root):\n        if d == 'forward':\n            if u == v:\n                H.add_node(v, parent=None)\n                nodes.append(v)\n            else:\n                H.add_node(v, parent=u)\n                H.add_edge(v, u, nontree=False)\n                nodes.append(v)\n        elif d == 'nontree' and v not in H[u]:\n            H.add_edge(v, u, nontree=True)\n        else:\n            pass\n    return (H, nodes)"
 },
 {
  "docstring": "Generate the chain starting from the given nontree edge.\n\n`G` is a DFS cycle graph as constructed by\n:func:`_dfs_cycle_graph`. The edge (`u`, `v`) is a nontree edge\nthat begins a chain. `visited` is a set representing the nodes\nin `G` that have already been visited.\n\nThis function yields the edges in an initial segment of the\nfundamental cycle of `G` starting with the nontree edge (`u`,\n`v`) that includes all the edges up until the first node that\nappears in `visited`. The tree edges are given by the 'parent'\nnode attribute. The `visited` set is updated to add each node in\nan edge yielded by this function.",
  "code": "def _build_chain(G, u, v, visited):\n    while v not in visited:\n        yield (u, v)\n        visited.add(v)\n        u, v = (v, G.nodes[v]['parent'])\n    yield (u, v)"
 },
 {
  "docstring": "Checks whether G is a chordal graph.\n\nA graph is chordal if every cycle of length at least 4 has a chord\n(an edge joining two nodes not adjacent in the cycle).\n\nParameters\n----------\nG : graph\n  A NetworkX graph.\n\nReturns\n-------\nchordal : bool\n  True if G is a chordal graph and False otherwise.\n\nRaises\n------\nNetworkXNotImplemented\n    The algorithm does not support DiGraph, MultiGraph and MultiDiGraph.\n\nExamples\n--------\n>>> e = [\n...     (1, 2),\n...     (1, 3),\n...     (2, 3),\n...     (2, 4),\n...     (3, 4),\n...     (3, 5),\n...     (3, 6),\n...     (4, 5),\n...     (4, 6),\n...     (5, 6),\n... ]\n>>> G = nx.Graph(e)\n>>> nx.is_chordal(G)\nTrue\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef is_chordal(G):\n    if len(G.nodes) <= 3:\n        return True\n    return len(_find_chordality_breaker(G)) == 0"
 },
 {
  "docstring": "Returns the set of induced nodes in the path from s to t.\n\nParameters\n----------\nG : graph\n  A chordal NetworkX graph\ns : node\n    Source node to look for induced nodes\nt : node\n    Destination node to look for induced nodes\ntreewidth_bound: float\n    Maximum treewidth acceptable for the graph H. The search\n    for induced nodes will end as soon as the treewidth_bound is exceeded.\n\nReturns\n-------\ninduced_nodes : Set of nodes\n    The set of induced nodes in the path from s to t in G\n\nRaises\n------\nNetworkXError\n    The algorithm does not support DiGraph, MultiGraph and MultiDiGraph.\n    If the input graph is an instance of one of these classes, a\n    :exc:`NetworkXError` is raised.\n    The algorithm can only be applied to chordal graphs. If the input\n    graph is found to be non-chordal, a :exc:`NetworkXError` is raised.\n\nExamples\n--------\n>>> G = nx.Graph()\n>>> G = nx.generators.classic.path_graph(10)\n>>> induced_nodes = nx.find_induced_nodes(G, 1, 9, 2)\n>>> sorted(induced_nodes)\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n",
  "code": "@nx._dispatch\ndef find_induced_nodes(G, s, t, treewidth_bound=sys.maxsize):\n    if not is_chordal(G):\n        raise nx.NetworkXError('Input graph is not chordal.')\n    H = nx.Graph(G)\n    H.add_edge(s, t)\n    induced_nodes = set()\n    triplet = _find_chordality_breaker(H, s, treewidth_bound)\n    while triplet:\n        u, v, w = triplet\n        induced_nodes.update(triplet)\n        for n in triplet:\n            if n != s:\n                H.add_edge(s, n)\n        triplet = _find_chordality_breaker(H, s, treewidth_bound)\n    if induced_nodes:\n        induced_nodes.add(t)\n        for u in G[s]:\n            if len(induced_nodes & set(G[u])) == 2:\n                induced_nodes.add(u)\n                break\n    return induced_nodes"
 },
 {
  "docstring": "Returns all maximal cliques of a chordal graph.\n\nThe algorithm breaks the graph in connected components and performs a\nmaximum cardinality search in each component to get the cliques.\n\nParameters\n----------\nG : graph\n  A NetworkX graph\n\nYields\n------\nfrozenset of nodes\n    Maximal cliques, each of which is a frozenset of\n    nodes in `G`. The order of cliques is arbitrary.\n\nRaises\n------\nNetworkXError\n    The algorithm does not support DiGraph, MultiGraph and MultiDiGraph.\n    The algorithm can only be applied to chordal graphs. If the input\n    graph is found to be non-chordal, a :exc:`NetworkXError` is raised.\n\nExamples\n--------\n>>> e = [\n...     (1, 2),\n...     (1, 3),\n...     (2, 3),\n...     (2, 4),\n...     (3, 4),\n...     (3, 5),\n...     (3, 6),\n...     (4, 5),\n...     (4, 6),\n...     (5, 6),\n...     (7, 8),\n... ]\n>>> G = nx.Graph(e)\n>>> G.add_node(9)\n>>> cliques = [c for c in chordal_graph_cliques(G)]\n>>> cliques[0]\nfrozenset({1, 2, 3})",
  "code": "@nx._dispatch\ndef chordal_graph_cliques(G):\n    for C in (G.subgraph(c).copy() for c in connected_components(G)):\n        if C.number_of_nodes() == 1:\n            if nx.number_of_selfloops(C) > 0:\n                raise nx.NetworkXError('Input graph is not chordal.')\n            yield frozenset(C.nodes())\n        else:\n            unnumbered = set(C.nodes())\n            v = arbitrary_element(C)\n            unnumbered.remove(v)\n            numbered = {v}\n            clique_wanna_be = {v}\n            while unnumbered:\n                v = _max_cardinality_node(C, unnumbered, numbered)\n                unnumbered.remove(v)\n                numbered.add(v)\n                new_clique_wanna_be = set(C.neighbors(v)) & numbered\n                sg = C.subgraph(clique_wanna_be)\n                if _is_complete_graph(sg):\n                    new_clique_wanna_be.add(v)\n                    if not new_clique_wanna_be >= clique_wanna_be:\n                        yield frozenset(clique_wanna_be)\n                    clique_wanna_be = new_clique_wanna_be\n                else:\n                    raise nx.NetworkXError('Input graph is not chordal.')\n            yield frozenset(clique_wanna_be)"
 },
 {
  "docstring": "Returns the treewidth of the chordal graph G.\n\nParameters\n----------\nG : graph\n  A NetworkX graph\n\nReturns\n-------\ntreewidth : int\n    The size of the largest clique in the graph minus one.\n\nRaises\n------\nNetworkXError\n    The algorithm does not support DiGraph, MultiGraph and MultiDiGraph.\n    The algorithm can only be applied to chordal graphs. If the input\n    graph is found to be non-chordal, a :exc:`NetworkXError` is raised.\n\nExamples\n--------\n>>> e = [\n...     (1, 2),\n...     (1, 3),\n...     (2, 3),\n...     (2, 4),\n...     (3, 4),\n...     (3, 5),\n...     (3, 6),\n...     (4, 5),\n...     (4, 6),\n...     (5, 6),\n...     (7, 8),\n... ]\n>>> G = nx.Graph(e)\n>>> G.add_node(9)\n>>> nx.chordal_graph_treewidth(G)\n3\n\n",
  "code": "@nx._dispatch\ndef chordal_graph_treewidth(G):\n    if not is_chordal(G):\n        raise nx.NetworkXError('Input graph is not chordal.')\n    max_clique = -1\n    for clique in nx.chordal_graph_cliques(G):\n        max_clique = max(max_clique, len(clique))\n    return max_clique - 1"
 },
 {
  "docstring": "Returns True if G is a complete graph.",
  "code": "def _is_complete_graph(G):\n    if nx.number_of_selfloops(G) > 0:\n        raise nx.NetworkXError('Self loop found in _is_complete_graph()')\n    n = G.number_of_nodes()\n    if n < 2:\n        return True\n    e = G.number_of_edges()\n    max_edges = n * (n - 1) / 2\n    return e == max_edges"
 },
 {
  "docstring": "Given a non-complete graph G, returns a missing edge.",
  "code": "def _find_missing_edge(G):\n    nodes = set(G)\n    for u in G:\n        missing = nodes - set(list(G[u].keys()) + [u])\n        if missing:\n            return (u, missing.pop())"
 },
 {
  "docstring": "Returns a the node in choices that has more connections in G\nto nodes in wanna_connect.",
  "code": "def _max_cardinality_node(G, choices, wanna_connect):\n    max_number = -1\n    for x in choices:\n        number = len([y for y in G[x] if y in wanna_connect])\n        if number > max_number:\n            max_number = number\n            max_cardinality_node = x\n    return max_cardinality_node"
 },
 {
  "docstring": "Given a graph G, starts a max cardinality search\n(starting from s if s is given and from an arbitrary node otherwise)\ntrying to find a non-chordal cycle.\n\nIf it does find one, it returns (u,v,w) where u,v,w are the three\nnodes that together with s are involved in the cycle.\n\nIt ignores any self loops.",
  "code": "def _find_chordality_breaker(G, s=None, treewidth_bound=sys.maxsize):\n    unnumbered = set(G)\n    if s is None:\n        s = arbitrary_element(G)\n    unnumbered.remove(s)\n    numbered = {s}\n    current_treewidth = -1\n    while unnumbered:\n        v = _max_cardinality_node(G, unnumbered, numbered)\n        unnumbered.remove(v)\n        numbered.add(v)\n        clique_wanna_be = set(G[v]) & numbered\n        sg = G.subgraph(clique_wanna_be)\n        if _is_complete_graph(sg):\n            current_treewidth = max(current_treewidth, len(clique_wanna_be))\n            if current_treewidth > treewidth_bound:\n                raise nx.NetworkXTreewidthBoundExceeded(f'treewidth_bound exceeded: {current_treewidth}')\n        else:\n            u, w = _find_missing_edge(sg)\n            return (u, v, w)\n    return ()"
 },
 {
  "docstring": "Return a copy of G completed to a chordal graph\n\nAdds edges to a copy of G to create a chordal graph. A graph G=(V,E) is\ncalled chordal if for each cycle with length bigger than 3, there exist\ntwo non-adjacent nodes connected by an edge (called a chord).\n\nParameters\n----------\nG : NetworkX graph\n    Undirected graph\n\nReturns\n-------\nH : NetworkX graph\n    The chordal enhancement of G\nalpha : Dictionary\n        The elimination ordering of nodes of G\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch\ndef complete_to_chordal_graph(G):\n    H = G.copy()\n    alpha = {node: 0 for node in H}\n    if nx.is_chordal(H):\n        return (H, alpha)\n    chords = set()\n    weight = {node: 0 for node in H.nodes()}\n    unnumbered_nodes = list(H.nodes())\n    for i in range(len(H.nodes()), 0, -1):\n        z = max(unnumbered_nodes, key=lambda node: weight[node])\n        unnumbered_nodes.remove(z)\n        alpha[z] = i\n        update_nodes = []\n        for y in unnumbered_nodes:\n            if G.has_edge(y, z):\n                update_nodes.append(y)\n            else:\n                y_weight = weight[y]\n                lower_nodes = [node for node in unnumbered_nodes if weight[node] < y_weight]\n                if nx.has_path(H.subgraph(lower_nodes + [z, y]), y, z):\n                    update_nodes.append(y)\n                    chords.add((z, y))\n        for node in update_nodes:\n            weight[node] += 1\n    H.add_edges_from(chords)\n    return (H, alpha)"
 },
 {
  "docstring": "Returns all cliques in an undirected graph.\n\nThis function returns an iterator over cliques, each of which is a\nlist of nodes. The iteration is ordered by cardinality of the\ncliques: first all cliques of size one, then all cliques of size\ntwo, etc.\n\nParameters\n----------\nG : NetworkX graph\n    An undirected graph.\n\nReturns\n-------\niterator\n    An iterator over cliques, each of which is a list of nodes in\n    `G`. The cliques are ordered according to size.\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch\ndef enumerate_all_cliques(G):\n    index = {}\n    nbrs = {}\n    for u in G:\n        index[u] = len(index)\n        nbrs[u] = {v for v in G[u] if v not in index}\n    queue = deque((([u], sorted(nbrs[u], key=index.__getitem__)) for u in G))\n    while queue:\n        base, cnbrs = map(list, queue.popleft())\n        yield base\n        for i, u in enumerate(cnbrs):\n            queue.append((chain(base, [u]), filter(nbrs[u].__contains__, islice(cnbrs, i + 1, None))))"
 },
 {
  "docstring": "Returns all maximal cliques in an undirected graph.\n\nFor each node *n*, a *maximal clique for n* is a largest complete\nsubgraph containing *n*. The largest maximal clique is sometimes\ncalled the *maximum clique*.\n\nThis function returns an iterator over cliques, each of which is a\nlist of nodes. It is an iterative implementation, so should not\nsuffer from recursion depth issues.\n\nThis function accepts a list of `nodes` and only the maximal cliques\ncontaining all of these `nodes` are returned. It can considerably speed up\nthe running time if some specific cliques are desired.\n\nParameters\n----------\nG : NetworkX graph\n    An undirected graph.\n\nnodes : list, optional (default=None)\n    If provided, only yield *maximal cliques* containing all nodes in `nodes`.\n    If `nodes` isn't a clique itself, a ValueError is raised.\n\nReturns\n-------\niterator\n    An iterator over maximal cliques, each of which is a list of\n    nodes in `G`. If `nodes` is provided, only the maximal cliques\n    containing all the nodes in `nodes` are returned. The order of\n    cliques is arbitrary.\n\nRaises\n------\nValueError\n    If `nodes` is not a clique.\n\nExamples\n--------\n>>> from pprint import pprint  # For nice dict formatting\n>>> G = nx.karate_club_graph()\n>>> sum(1 for c in nx.find_cliques(G))  # The number of maximal cliques in G\n36\n>>> max(nx.find_cliques(G), key=len)  # The largest maximal clique in G\n[0, 1, 2, 3, 13]\n\nThe size of the largest maximal clique is known as the *clique number* of\nthe graph, which can be found directly with:\n\n>>> max(len(c) for c in nx.find_cliques(G))\n5\n\nOne can also compute the number of maximal cliques in `G` that contain a given\nnode. The following produces a dictionary keyed by node whose\nvalues are the number of maximal cliques in `G` that contain the node:\n\n>>> pprint({n: sum(1 for c in nx.find_cliques(G) if n in c) for n in G})\n{0: 13,\n 1: 6,\n 2: 7,\n 3: 3,\n 4: 2,\n 5: 3,\n 6: 3,\n 7: 1,\n 8: 3,\n 9: 2,\n 10: 2,\n 11: 1,\n 12: 1,\n 13: 2,\n 14: 1,\n 15: 1,\n 16: 1,\n 17: 1,\n 18: 1,\n 19: 2,\n 20: 1,\n 21: 1,\n 22: 1,\n 23: 3,\n 24: 2,\n 25: 2,\n 26: 1,\n 27: 3,\n 28: 2,\n 29: 2,\n 30: 2,\n 31: 4,\n 32: 9,\n 33: 14}\n\nOr, similarly, the maximal cliques in `G` that contain a given node.\nFor example, the 4 maximal cliques that contain node 31:\n\n>>> [c for c in nx.find_cliques(G) if 31 in c]\n[[0, 31], [33, 32, 31], [33, 28, 31], [24, 25, 31]]\n\nSee Also\n--------\nfind_cliques_recursive\n    A recursive version of the same algorithm.\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch\ndef find_cliques(G, nodes=None):\n    if len(G) == 0:\n        return\n    adj = {u: {v for v in G[u] if v != u} for u in G}\n    Q = nodes[:] if nodes is not None else []\n    cand = set(G)\n    for node in Q:\n        if node not in cand:\n            raise ValueError(f'The given `nodes` {nodes} do not form a clique')\n        cand &= adj[node]\n    if not cand:\n        yield Q[:]\n        return\n    subg = cand.copy()\n    stack = []\n    Q.append(None)\n    u = max(subg, key=lambda u: len(cand & adj[u]))\n    ext_u = cand - adj[u]\n    try:\n        while True:\n            if ext_u:\n                q = ext_u.pop()\n                cand.remove(q)\n                Q[-1] = q\n                adj_q = adj[q]\n                subg_q = subg & adj_q\n                if not subg_q:\n                    yield Q[:]\n                else:\n                    cand_q = cand & adj_q\n                    if cand_q:\n                        stack.append((subg, cand, ext_u))\n                        Q.append(None)\n                        subg = subg_q\n                        cand = cand_q\n                        u = max(subg, key=lambda u: len(cand & adj[u]))\n                        ext_u = cand - adj[u]\n            else:\n                Q.pop()\n                subg, cand, ext_u = stack.pop()\n    except IndexError:\n        pass"
 },
 {
  "docstring": "Returns all maximal cliques in a graph.\n\nFor each node *v*, a *maximal clique for v* is a largest complete\nsubgraph containing *v*. The largest maximal clique is sometimes\ncalled the *maximum clique*.\n\nThis function returns an iterator over cliques, each of which is a\nlist of nodes. It is a recursive implementation, so may suffer from\nrecursion depth issues, but is included for pedagogical reasons.\nFor a non-recursive implementation, see :func:`find_cliques`.\n\nThis function accepts a list of `nodes` and only the maximal cliques\ncontaining all of these `nodes` are returned. It can considerably speed up\nthe running time if some specific cliques are desired.\n\nParameters\n----------\nG : NetworkX graph\n\nnodes : list, optional (default=None)\n    If provided, only yield *maximal cliques* containing all nodes in `nodes`.\n    If `nodes` isn't a clique itself, a ValueError is raised.\n\nReturns\n-------\niterator\n    An iterator over maximal cliques, each of which is a list of\n    nodes in `G`. If `nodes` is provided, only the maximal cliques\n    containing all the nodes in `nodes` are yielded. The order of\n    cliques is arbitrary.\n\nRaises\n------\nValueError\n    If `nodes` is not a clique.\n\nSee Also\n--------\nfind_cliques\n    An iterative version of the same algorithm. See docstring for examples.\n\n",
  "code": "@nx._dispatch\ndef find_cliques_recursive(G, nodes=None):\n    if len(G) == 0:\n        return iter([])\n    adj = {u: {v for v in G[u] if v != u} for u in G}\n    Q = nodes[:] if nodes is not None else []\n    cand_init = set(G)\n    for node in Q:\n        if node not in cand_init:\n            raise ValueError(f'The given `nodes` {nodes} do not form a clique')\n        cand_init &= adj[node]\n    if not cand_init:\n        return iter([Q])\n    subg_init = cand_init.copy()\n\n    def expand(subg, cand):\n        u = max(subg, key=lambda u: len(cand & adj[u]))\n        for q in cand - adj[u]:\n            cand.remove(q)\n            Q.append(q)\n            adj_q = adj[q]\n            subg_q = subg & adj_q\n            if not subg_q:\n                yield Q[:]\n            else:\n                cand_q = cand & adj_q\n                if cand_q:\n                    yield from expand(subg_q, cand_q)\n            Q.pop()\n    return expand(subg_init, cand_init)"
 },
 {
  "docstring": "Returns the maximal clique graph of the given graph.\n\nThe nodes of the maximal clique graph of `G` are the cliques of\n`G` and an edge joins two cliques if the cliques are not disjoint.\n\nParameters\n----------\nG : NetworkX graph\n\ncreate_using : NetworkX graph constructor, optional (default=nx.Graph)\n   Graph type to create. If graph instance, then cleared before populated.\n\nReturns\n-------\nNetworkX graph\n    A graph whose nodes are the cliques of `G` and whose edges\n    join two cliques if they are not disjoint.\n\n",
  "code": "@nx._dispatch\ndef make_max_clique_graph(G, create_using=None):\n    if create_using is None:\n        B = G.__class__()\n    else:\n        B = nx.empty_graph(0, create_using)\n    cliques = list(enumerate((set(c) for c in find_cliques(G))))\n    B.add_nodes_from((i for i, c in cliques))\n    clique_pairs = combinations(cliques, 2)\n    B.add_edges_from(((i, j) for (i, c1), (j, c2) in clique_pairs if c1 & c2))\n    return B"
 },
 {
  "docstring": "Returns the bipartite clique graph corresponding to `G`.\n\nIn the returned bipartite graph, the \"bottom\" nodes are the nodes of\n`G` and the \"top\" nodes represent the maximal cliques of `G`.\nThere is an edge from node *v* to clique *C* in the returned graph\nif and only if *v* is an element of *C*.\n\nParameters\n----------\nG : NetworkX graph\n    An undirected graph.\n\nfpos : bool\n    If True or not None, the returned graph will have an\n    additional attribute, `pos`, a dictionary mapping node to\n    position in the Euclidean plane.\n\ncreate_using : NetworkX graph constructor, optional (default=nx.Graph)\n   Graph type to create. If graph instance, then cleared before populated.\n\nReturns\n-------\nNetworkX graph\n    A bipartite graph whose \"bottom\" set is the nodes of the graph\n    `G`, whose \"top\" set is the cliques of `G`, and whose edges\n    join nodes of `G` to the cliques that contain them.\n\n    The nodes of the graph `G` have the node attribute\n    'bipartite' set to 1 and the nodes representing cliques\n    have the node attribute 'bipartite' set to 0, as is the\n    convention for bipartite graphs in NetworkX.",
  "code": "@nx._dispatch\ndef make_clique_bipartite(G, fpos=None, create_using=None, name=None):\n    B = nx.empty_graph(0, create_using)\n    B.clear()\n    B.add_nodes_from(G, bipartite=1)\n    for i, cl in enumerate(find_cliques(G)):\n        name = -i - 1\n        B.add_node(name, bipartite=0)\n        B.add_edges_from(((v, name) for v in cl))\n    return B"
 },
 {
  "docstring": "Returns the size of the largest maximal clique containing each given node.\n\nReturns a single or list depending on input nodes.\nAn optional list of cliques can be input if already computed.\n\nParameters\n----------\nG : NetworkX graph\n    An undirected graph.\n\ncliques : list, optional (default=None)\n    A list of cliques, each of which is itself a list of nodes.\n    If not specified, the list of all cliques will be computed\n    using :func:`find_cliques`.\n\nReturns\n-------\nint or dict\n    If `nodes` is a single node, returns the size of the\n    largest maximal clique in `G` containing that node.\n    Otherwise return a dict keyed by node to the size\n    of the largest maximal clique containing that node.\n\nSee Also\n--------\nfind_cliques\n    find_cliques yields the maximal cliques of G.\n    It accepts a `nodes` argument which restricts consideration to\n    maximal cliques containing all the given `nodes`.\n    The search for the cliques is optimized for `nodes`.",
  "code": "@nx._dispatch\ndef node_clique_number(G, nodes=None, cliques=None, separate_nodes=False):\n    if cliques is None:\n        if nodes is not None:\n            if nodes in G:\n                return max((len(c) for c in find_cliques(nx.ego_graph(G, nodes))))\n            return {n: max((len(c) for c in find_cliques(nx.ego_graph(G, n)))) for n in nodes}\n        cliques = list(find_cliques(G))\n    if nodes in G:\n        return max((len(c) for c in cliques if nodes in c))\n    size_for_n = defaultdict(int)\n    for c in cliques:\n        size_of_c = len(c)\n        for n in c:\n            if size_for_n[n] < size_of_c:\n                size_for_n[n] = size_of_c\n    if nodes is None:\n        return size_for_n\n    return {n: size_for_n[n] for n in nodes}"
 },
 {
  "docstring": "Returns the number of maximal cliques for each node.\n\nReturns a single or list depending on input nodes.\nOptional list of cliques can be input if already computed.",
  "code": "def number_of_cliques(G, nodes=None, cliques=None):\n    if cliques is None:\n        cliques = list(find_cliques(G))\n    if nodes is None:\n        nodes = list(G.nodes())\n    if not isinstance(nodes, list):\n        v = nodes\n        numcliq = len([1 for c in cliques if v in c])\n    else:\n        numcliq = {}\n        for v in nodes:\n            numcliq[v] = len([1 for c in cliques if v in c])\n    return numcliq"
 },
 {
  "docstring": "Find a maximum weight clique in G.\n\nA *clique* in a graph is a set of nodes such that every two distinct nodes\nare adjacent.  The *weight* of a clique is the sum of the weights of its\nnodes.  A *maximum weight clique* of graph G is a clique C in G such that\nno clique in G has weight greater than the weight of C.\n\nParameters\n----------\nG : NetworkX graph\n    Undirected graph\nweight : string or None, optional (default='weight')\n    The node attribute that holds the integer value used as a weight.\n    If None, then each node has weight 1.\n\nReturns\n-------\nclique : list\n    the nodes of a maximum weight clique\nweight : int\n    the weight of a maximum weight clique\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch(node_attrs='weight')\ndef max_weight_clique(G, weight='weight'):\n    mwc = MaxWeightClique(G, weight)\n    mwc.find_max_weight_clique()\n    return (mwc.incumbent_nodes, mwc.incumbent_weight)"
 },
 {
  "docstring": "Update the incumbent if the node set C has greater weight.\n\nC is assumed to be a clique.",
  "code": "def update_incumbent_if_improved(self, C, C_weight):\n    if C_weight > self.incumbent_weight:\n        self.incumbent_nodes = C[:]\n        self.incumbent_weight = C_weight"
 },
 {
  "docstring": "Greedily find an independent set of nodes from a set of\nnodes P.",
  "code": "def greedily_find_independent_set(self, P):\n    independent_set = []\n    P = P[:]\n    while P:\n        v = P[0]\n        independent_set.append(v)\n        P = [w for w in P if v != w and (not self.G.has_edge(v, w))]\n    return independent_set"
 },
 {
  "docstring": "Find a set of nodes to branch on.",
  "code": "def find_branching_nodes(self, P, target):\n    residual_wt = {v: self.node_weights[v] for v in P}\n    total_wt = 0\n    P = P[:]\n    while P:\n        independent_set = self.greedily_find_independent_set(P)\n        min_wt_in_class = min((residual_wt[v] for v in independent_set))\n        total_wt += min_wt_in_class\n        if total_wt > target:\n            break\n        for v in independent_set:\n            residual_wt[v] -= min_wt_in_class\n        P = [v for v in P if residual_wt[v] != 0]\n    return P"
 },
 {
  "docstring": "Look for the best clique that contains all the nodes in C and zero or\nmore of the nodes in P, backtracking if it can be shown that no such\nclique has greater weight than the incumbent.",
  "code": "def expand(self, C, C_weight, P):\n    self.update_incumbent_if_improved(C, C_weight)\n    branching_nodes = self.find_branching_nodes(P, self.incumbent_weight - C_weight)\n    while branching_nodes:\n        v = branching_nodes.pop()\n        P.remove(v)\n        new_C = C + [v]\n        new_C_weight = C_weight + self.node_weights[v]\n        new_P = [w for w in P if self.G.has_edge(v, w)]\n        self.expand(new_C, new_C_weight, new_P)"
 },
 {
  "docstring": "Find a maximum weight clique.",
  "code": "def find_max_weight_clique(self):\n    nodes = sorted(self.G.nodes(), key=lambda v: self.G.degree(v), reverse=True)\n    nodes = [v for v in nodes if self.node_weights[v] > 0]\n    self.expand([], 0, nodes)"
 },
 {
  "docstring": "Compute the number of triangles.\n\nFinds the number of triangles that include a node as one vertex.\n\nParameters\n----------\nG : graph\n   A networkx graph\n\nnodes : node, iterable of nodes, or None (default=None)\n    If a singleton node, return the number of triangles for that node.\n    If an iterable, compute the number of triangles for each of those nodes.\n    If `None` (the default) compute the number of triangles for all nodes in `G`.\n\nReturns\n-------\nout : dict or int\n   If `nodes` is a container of nodes, returns number of triangles keyed by node (dict).\n   If `nodes` is a specific node, returns number of triangles for the node (int).\n\nExamples\n--------\n>>> G = nx.complete_graph(5)\n>>> print(nx.triangles(G, 0))\n6\n>>> print(nx.triangles(G))\n{0: 6, 1: 6, 2: 6, 3: 6, 4: 6}\n>>> print(list(nx.triangles(G, [0, 1]).values()))\n[6, 6]\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch\ndef triangles(G, nodes=None):\n    if nodes is not None:\n        if nodes in G:\n            return next(_triangles_and_degree_iter(G, nodes))[2] // 2\n        return {v: t // 2 for v, d, t, _ in _triangles_and_degree_iter(G, nodes)}\n    later_neighbors = {}\n    for node, neighbors in G.adjacency():\n        later_neighbors[node] = {n for n in neighbors if n not in later_neighbors and n != node}\n    triangle_counts = Counter(dict.fromkeys(G, 0))\n    for node1, neighbors in later_neighbors.items():\n        for node2 in neighbors:\n            third_nodes = neighbors & later_neighbors[node2]\n            m = len(third_nodes)\n            triangle_counts[node1] += m\n            triangle_counts[node2] += m\n            triangle_counts.update(third_nodes)\n    return dict(triangle_counts)"
 },
 {
  "docstring": "Return an iterator of (node, degree, triangles, generalized degree).\n\nThis double counts triangles so you may want to divide by 2.\nSee degree(), triangles() and generalized_degree() for definitions\nand details.",
  "code": "@not_implemented_for('multigraph')\ndef _triangles_and_degree_iter(G, nodes=None):\n    if nodes is None:\n        nodes_nbrs = G.adj.items()\n    else:\n        nodes_nbrs = ((n, G[n]) for n in G.nbunch_iter(nodes))\n    for v, v_nbrs in nodes_nbrs:\n        vs = set(v_nbrs) - {v}\n        gen_degree = Counter((len(vs & set(G[w]) - {w}) for w in vs))\n        ntriangles = sum((k * val for k, val in gen_degree.items()))\n        yield (v, len(vs), ntriangles, gen_degree)"
 },
 {
  "docstring": "Return an iterator of (node, degree, weighted_triangles).\n\nUsed for weighted clustering.\nNote: this returns the geometric average weight of edges in the triangle.\nAlso, each triangle is counted twice (each direction).\nSo you may want to divide by 2.",
  "code": "@not_implemented_for('multigraph')\ndef _weighted_triangles_and_degree_iter(G, nodes=None, weight='weight'):\n    import numpy as np\n    if weight is None or G.number_of_edges() == 0:\n        max_weight = 1\n    else:\n        max_weight = max((d.get(weight, 1) for u, v, d in G.edges(data=True)))\n    if nodes is None:\n        nodes_nbrs = G.adj.items()\n    else:\n        nodes_nbrs = ((n, G[n]) for n in G.nbunch_iter(nodes))\n\n    def wt(u, v):\n        return G[u][v].get(weight, 1) / max_weight\n    for i, nbrs in nodes_nbrs:\n        inbrs = set(nbrs) - {i}\n        weighted_triangles = 0\n        seen = set()\n        for j in inbrs:\n            seen.add(j)\n            jnbrs = set(G[j]) - seen\n            wij = wt(i, j)\n            weighted_triangles += sum(np.cbrt([wij * wt(j, k) * wt(k, i) for k in inbrs & jnbrs]))\n        yield (i, len(inbrs), 2 * weighted_triangles)"
 },
 {
  "docstring": "Return an iterator of\n(node, total_degree, reciprocal_degree, directed_triangles).\n\nUsed for directed clustering.\nNote that unlike `_triangles_and_degree_iter()`, this function counts\ndirected triangles so does not count triangles twice.",
  "code": "@not_implemented_for('multigraph')\ndef _directed_triangles_and_degree_iter(G, nodes=None):\n    nodes_nbrs = ((n, G._pred[n], G._succ[n]) for n in G.nbunch_iter(nodes))\n    for i, preds, succs in nodes_nbrs:\n        ipreds = set(preds) - {i}\n        isuccs = set(succs) - {i}\n        directed_triangles = 0\n        for j in chain(ipreds, isuccs):\n            jpreds = set(G._pred[j]) - {j}\n            jsuccs = set(G._succ[j]) - {j}\n            directed_triangles += sum((1 for k in chain(ipreds & jpreds, ipreds & jsuccs, isuccs & jpreds, isuccs & jsuccs)))\n        dtotal = len(ipreds) + len(isuccs)\n        dbidirectional = len(ipreds & isuccs)\n        yield (i, dtotal, dbidirectional, directed_triangles)"
 },
 {
  "docstring": "Return an iterator of\n(node, total_degree, reciprocal_degree, directed_weighted_triangles).\n\nUsed for directed weighted clustering.\nNote that unlike `_weighted_triangles_and_degree_iter()`, this function counts\ndirected triangles so does not count triangles twice.",
  "code": "@not_implemented_for('multigraph')\ndef _directed_weighted_triangles_and_degree_iter(G, nodes=None, weight='weight'):\n    import numpy as np\n    if weight is None or G.number_of_edges() == 0:\n        max_weight = 1\n    else:\n        max_weight = max((d.get(weight, 1) for u, v, d in G.edges(data=True)))\n    nodes_nbrs = ((n, G._pred[n], G._succ[n]) for n in G.nbunch_iter(nodes))\n\n    def wt(u, v):\n        return G[u][v].get(weight, 1) / max_weight\n    for i, preds, succs in nodes_nbrs:\n        ipreds = set(preds) - {i}\n        isuccs = set(succs) - {i}\n        directed_triangles = 0\n        for j in ipreds:\n            jpreds = set(G._pred[j]) - {j}\n            jsuccs = set(G._succ[j]) - {j}\n            directed_triangles += sum(np.cbrt([wt(j, i) * wt(k, i) * wt(k, j) for k in ipreds & jpreds]))\n            directed_triangles += sum(np.cbrt([wt(j, i) * wt(k, i) * wt(j, k) for k in ipreds & jsuccs]))\n            directed_triangles += sum(np.cbrt([wt(j, i) * wt(i, k) * wt(k, j) for k in isuccs & jpreds]))\n            directed_triangles += sum(np.cbrt([wt(j, i) * wt(i, k) * wt(j, k) for k in isuccs & jsuccs]))\n        for j in isuccs:\n            jpreds = set(G._pred[j]) - {j}\n            jsuccs = set(G._succ[j]) - {j}\n            directed_triangles += sum(np.cbrt([wt(i, j) * wt(k, i) * wt(k, j) for k in ipreds & jpreds]))\n            directed_triangles += sum(np.cbrt([wt(i, j) * wt(k, i) * wt(j, k) for k in ipreds & jsuccs]))\n            directed_triangles += sum(np.cbrt([wt(i, j) * wt(i, k) * wt(k, j) for k in isuccs & jpreds]))\n            directed_triangles += sum(np.cbrt([wt(i, j) * wt(i, k) * wt(j, k) for k in isuccs & jsuccs]))\n        dtotal = len(ipreds) + len(isuccs)\n        dbidirectional = len(ipreds & isuccs)\n        yield (i, dtotal, dbidirectional, directed_triangles)"
 },
 {
  "docstring": "Compute the average clustering coefficient for the graph G.\n\nThe clustering coefficient for the graph is the average,\n\n.. math::\n\n   C = \\frac{1}{n}\\sum_{v \\in G} c_v,\n\nwhere :math:`n` is the number of nodes in `G`.\n\nParameters\n----------\nG : graph\n\nnodes : container of nodes, optional (default=all nodes in G)\n   Compute average clustering for nodes in this container.\n\nweight : string or None, optional (default=None)\n   The edge attribute that holds the numerical value used as a weight.\n   If None, then each edge has weight 1.\n\ncount_zeros : bool\n   If False include only the nodes with nonzero clustering in the average.\n\nReturns\n-------\navg : float\n   Average clustering\n\nExamples\n--------\n>>> G = nx.complete_graph(5)\n>>> print(nx.average_clustering(G))\n1.0\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef average_clustering(G, nodes=None, weight=None, count_zeros=True):\n    c = clustering(G, nodes, weight=weight).values()\n    if not count_zeros:\n        c = [v for v in c if abs(v) > 0]\n    return sum(c) / len(c)"
 },
 {
  "docstring": "Compute the clustering coefficient for nodes.\n\nFor unweighted graphs, the clustering of a node :math:`u`\nis the fraction of possible triangles through that node that exist,\n\n.. math::\n\n  c_u = \\frac{2 T(u)}{deg(u)(deg(u)-1)},\n\nwhere :math:`T(u)` is the number of triangles through node :math:`u` and\n:math:`deg(u)` is the degree of :math:`u`.\n\nFor weighted graphs, there are several ways to define clustering [1]_.\nthe one used here is defined\nas the geometric average of the subgraph edge weights [2]_,\n\n.. math::\n\n   c_u = \\frac{1}{deg(u)(deg(u)-1))}\n         \\sum_{vw} (\\hat{w}_{uv} \\hat{w}_{uw} \\hat{w}_{vw})^{1/3}.\n\nThe edge weights :math:`\\hat{w}_{uv}` are normalized by the maximum weight\nin the network :math:`\\hat{w}_{uv} = w_{uv}/\\max(w)`.\n\nThe value of :math:`c_u` is assigned to 0 if :math:`deg(u) < 2`.\n\nAdditionally, this weighted definition has been generalized to support negative edge weights [3]_.\n\nFor directed graphs, the clustering is similarly defined as the fraction\nof all possible directed triangles or geometric average of the subgraph\nedge weights for unweighted and weighted directed graph respectively [4]_.\n\n.. math::\n\n   c_u = \\frac{T(u)}{2(deg^{tot}(u)(deg^{tot}(u)-1) - 2deg^{\\leftrightarrow}(u))},\n\nwhere :math:`T(u)` is the number of directed triangles through node\n:math:`u`, :math:`deg^{tot}(u)` is the sum of in degree and out degree of\n:math:`u` and :math:`deg^{\\leftrightarrow}(u)` is the reciprocal degree of\n:math:`u`.\n\n\nParameters\n----------\nG : graph\n\nnodes : node, iterable of nodes, or None (default=None)\n    If a singleton node, return the number of triangles for that node.\n    If an iterable, compute the number of triangles for each of those nodes.\n    If `None` (the default) compute the number of triangles for all nodes in `G`.\n\nweight : string or None, optional (default=None)\n   The edge attribute that holds the numerical value used as a weight.\n   If None, then each edge has weight 1.\n\nReturns\n-------\nout : float, or dictionary\n   Clustering coefficient at specified nodes\n\nExamples\n--------\n>>> G = nx.complete_graph(5)\n>>> print(nx.clustering(G, 0))\n1.0\n>>> print(nx.clustering(G))\n{0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0}\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef clustering(G, nodes=None, weight=None):\n    if G.is_directed():\n        if weight is not None:\n            td_iter = _directed_weighted_triangles_and_degree_iter(G, nodes, weight)\n            clusterc = {v: 0 if t == 0 else t / ((dt * (dt - 1) - 2 * db) * 2) for v, dt, db, t in td_iter}\n        else:\n            td_iter = _directed_triangles_and_degree_iter(G, nodes)\n            clusterc = {v: 0 if t == 0 else t / ((dt * (dt - 1) - 2 * db) * 2) for v, dt, db, t in td_iter}\n    elif weight is not None:\n        td_iter = _weighted_triangles_and_degree_iter(G, nodes, weight)\n        clusterc = {v: 0 if t == 0 else t / (d * (d - 1)) for v, d, t in td_iter}\n    else:\n        td_iter = _triangles_and_degree_iter(G, nodes)\n        clusterc = {v: 0 if t == 0 else t / (d * (d - 1)) for v, d, t, _ in td_iter}\n    if nodes in G:\n        return clusterc[nodes]\n    return clusterc"
 },
 {
  "docstring": "Compute graph transitivity, the fraction of all possible triangles\npresent in G.\n\nPossible triangles are identified by the number of \"triads\"\n(two edges with a shared vertex).\n\nThe transitivity is\n\n.. math::\n\n    T = 3\\frac{\\#triangles}{\\#triads}.\n\nParameters\n----------\nG : graph\n\nReturns\n-------\nout : float\n   Transitivity\n\nExamples\n--------\n>>> G = nx.complete_graph(5)\n>>> print(nx.transitivity(G))\n1.0",
  "code": "@nx._dispatch\ndef transitivity(G):\n    triangles_contri = [(t, d * (d - 1)) for v, d, t, _ in _triangles_and_degree_iter(G)]\n    if len(triangles_contri) == 0:\n        return 0\n    triangles, contri = map(sum, zip(*triangles_contri))\n    return 0 if triangles == 0 else triangles / contri"
 },
 {
  "docstring": "Compute the squares clustering coefficient for nodes.\n\nFor each node return the fraction of possible squares that exist at\nthe node [1]_\n\n.. math::\n   C_4(v) = \\frac{ \\sum_{u=1}^{k_v}\n   \\sum_{w=u+1}^{k_v} q_v(u,w) }{ \\sum_{u=1}^{k_v}\n   \\sum_{w=u+1}^{k_v} [a_v(u,w) + q_v(u,w)]},\n\nwhere :math:`q_v(u,w)` are the number of common neighbors of :math:`u` and\n:math:`w` other than :math:`v` (ie squares), and :math:`a_v(u,w) = (k_u -\n(1+q_v(u,w)+\\theta_{uv})) + (k_w - (1+q_v(u,w)+\\theta_{uw}))`, where\n:math:`\\theta_{uw} = 1` if :math:`u` and :math:`w` are connected and 0\notherwise. [2]_\n\nParameters\n----------\nG : graph\n\nnodes : container of nodes, optional (default=all nodes in G)\n   Compute clustering for nodes in this container.\n\nReturns\n-------\nc4 : dictionary\n   A dictionary keyed by node with the square clustering coefficient value.\n\nExamples\n--------\n>>> G = nx.complete_graph(5)\n>>> print(nx.square_clustering(G, 0))\n1.0\n>>> print(nx.square_clustering(G))\n{0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0}\n\n",
  "code": "@nx._dispatch\ndef square_clustering(G, nodes=None):\n    if nodes is None:\n        node_iter = G\n    else:\n        node_iter = G.nbunch_iter(nodes)\n    clustering = {}\n    for v in node_iter:\n        clustering[v] = 0\n        potential = 0\n        for u, w in combinations(G[v], 2):\n            squares = len((set(G[u]) & set(G[w])) - {v})\n            clustering[v] += squares\n            degm = squares + 1\n            if w in G[u]:\n                degm += 1\n            potential += len(G[u]) - degm + (len(G[w]) - degm) + squares\n        if potential > 0:\n            clustering[v] /= potential\n    if nodes in G:\n        return clustering[nodes]\n    return clustering"
 },
 {
  "docstring": "Compute the generalized degree for nodes.\n\nFor each node, the generalized degree shows how many edges of given\ntriangle multiplicity the node is connected to. The triangle multiplicity\nof an edge is the number of triangles an edge participates in. The\ngeneralized degree of node :math:`i` can be written as a vector\n:math:`\\mathbf{k}_i=(k_i^{(0)}, \\dotsc, k_i^{(N-2)})` where\n:math:`k_i^{(j)}` is the number of edges attached to node :math:`i` that\nparticipate in :math:`j` triangles.\n\nParameters\n----------\nG : graph\n\nnodes : container of nodes, optional (default=all nodes in G)\n   Compute the generalized degree for nodes in this container.\n\nReturns\n-------\nout : Counter, or dictionary of Counters\n   Generalized degree of specified nodes. The Counter is keyed by edge\n   triangle multiplicity.\n\nExamples\n--------\n>>> G = nx.complete_graph(5)\n>>> print(nx.generalized_degree(G, 0))\nCounter({3: 4})\n>>> print(nx.generalized_degree(G))\n{0: Counter({3: 4}), 1: Counter({3: 4}), 2: Counter({3: 4}), 3: Counter({3: 4}), 4: Counter({3: 4})}\n\nTo recover the number of triangles attached to a node:\n\n>>> k1 = nx.generalized_degree(G, 0)\n>>> sum([k * v for k, v in k1.items()]) / 2 == nx.triangles(G, 0)\nTrue\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch\ndef generalized_degree(G, nodes=None):\n    if nodes in G:\n        return next(_triangles_and_degree_iter(G, nodes))[3]\n    return {v: gd for v, d, t, gd in _triangles_and_degree_iter(G, nodes)}"
 },
 {
  "docstring": "Returns communicability between all pairs of nodes in G.\n\nThe communicability between pairs of nodes in G is the sum of\nwalks of different lengths starting at node u and ending at node v.\n\nParameters\n----------\nG: graph\n\nReturns\n-------\ncomm: dictionary of dictionaries\n    Dictionary of dictionaries keyed by nodes with communicability\n    as the value.\n\nRaises\n------\nNetworkXError\n   If the graph is not undirected and simple.\n\nSee Also\n--------\ncommunicability_exp:\n   Communicability between all pairs of nodes in G  using spectral\n   decomposition.\ncommunicability_betweenness_centrality:\n   Communicability betweenness centrality for each node in G.\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef communicability(G):\n    import numpy as np\n    nodelist = list(G)\n    A = nx.to_numpy_array(G, nodelist)\n    A[A != 0.0] = 1\n    w, vec = np.linalg.eigh(A)\n    expw = np.exp(w)\n    mapping = dict(zip(nodelist, range(len(nodelist))))\n    c = {}\n    for u in G:\n        c[u] = {}\n        for v in G:\n            s = 0\n            p = mapping[u]\n            q = mapping[v]\n            for j in range(len(nodelist)):\n                s += vec[:, j][p] * vec[:, j][q] * expw[j]\n            c[u][v] = float(s)\n    return c"
 },
 {
  "docstring": "Returns communicability between all pairs of nodes in G.\n\nCommunicability between pair of node (u,v) of node in G is the sum of\nwalks of different lengths starting at node u and ending at node v.\n\nParameters\n----------\nG: graph\n\nReturns\n-------\ncomm: dictionary of dictionaries\n    Dictionary of dictionaries keyed by nodes with communicability\n    as the value.\n\nRaises\n------\nNetworkXError\n    If the graph is not undirected and simple.\n\nSee Also\n--------\ncommunicability:\n   Communicability between pairs of nodes in G.\ncommunicability_betweenness_centrality:\n   Communicability betweenness centrality for each node in G.\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef communicability_exp(G):\n    import scipy as sp\n    nodelist = list(G)\n    A = nx.to_numpy_array(G, nodelist)\n    A[A != 0.0] = 1\n    expA = sp.linalg.expm(A)\n    mapping = dict(zip(nodelist, range(len(nodelist))))\n    c = {}\n    for u in G:\n        c[u] = {}\n        for v in G:\n            c[u][v] = float(expA[mapping[u], mapping[v]])\n    return c"
 },
 {
  "docstring": "Returns the core number for each node.\n\nA k-core is a maximal subgraph that contains nodes of degree k or more.\n\nThe core number of a node is the largest value k of a k-core containing\nthat node.\n\nParameters\n----------\nG : NetworkX graph\n   An undirected or directed graph\n\nReturns\n-------\ncore_number : dictionary\n   A dictionary keyed by node to the core number.\n\nRaises\n------\nNetworkXNotImplemented\n    If `G` is a multigraph or contains self loops.\n\n",
  "code": "@nx.utils.not_implemented_for('multigraph')\n@nx._dispatch\ndef core_number(G):\n    if nx.number_of_selfloops(G) > 0:\n        msg = 'Input graph has self loops which is not permitted; Consider using G.remove_edges_from(nx.selfloop_edges(G)).'\n        raise nx.NetworkXNotImplemented(msg)\n    degrees = dict(G.degree())\n    nodes = sorted(degrees, key=degrees.get)\n    bin_boundaries = [0]\n    curr_degree = 0\n    for i, v in enumerate(nodes):\n        if degrees[v] > curr_degree:\n            bin_boundaries.extend([i] * (degrees[v] - curr_degree))\n            curr_degree = degrees[v]\n    node_pos = {v: pos for pos, v in enumerate(nodes)}\n    core = degrees\n    nbrs = {v: list(nx.all_neighbors(G, v)) for v in G}\n    for v in nodes:\n        for u in nbrs[v]:\n            if core[u] > core[v]:\n                nbrs[u].remove(v)\n                pos = node_pos[u]\n                bin_start = bin_boundaries[core[u]]\n                node_pos[u] = bin_start\n                node_pos[nodes[bin_start]] = pos\n                nodes[bin_start], nodes[pos] = (nodes[pos], nodes[bin_start])\n                bin_boundaries[core[u]] += 1\n                core[u] -= 1\n    return core"
 },
 {
  "docstring": "Returns the subgraph induced by nodes passing filter `k_filter`.\n\nParameters\n----------\nG : NetworkX graph\n   The graph or directed graph to process\nk_filter : filter function\n   This function filters the nodes chosen. It takes three inputs:\n   A node of G, the filter's cutoff, and the core dict of the graph.\n   The function should return a Boolean value.\nk : int, optional\n  The order of the core. If not specified use the max core number.\n  This value is used as the cutoff for the filter.\ncore : dict, optional\n  Precomputed core numbers keyed by node for the graph `G`.\n  If not specified, the core numbers will be computed from `G`.",
  "code": "def _core_subgraph(G, k_filter, k=None, core=None):\n    if core is None:\n        core = core_number(G)\n    if k is None:\n        k = max(core.values())\n    nodes = (v for v in core if k_filter(v, k, core))\n    return G.subgraph(nodes).copy()"
 },
 {
  "docstring": "Returns the k-core of G.\n\nA k-core is a maximal subgraph that contains nodes of degree `k` or more.\n\n.. deprecated:: 3.3\n   `k_core` will not accept `MultiGraph` objects in version 3.5.\n\nParameters\n----------\nG : NetworkX graph\n  A graph or directed graph\nk : int, optional\n  The order of the core. If not specified return the main core.\ncore_number : dictionary, optional\n  Precomputed core numbers for the graph G.\n\nReturns\n-------\nG : NetworkX graph\n  The k-core subgraph\n\nRaises\n------\nNetworkXNotImplemented\n  The k-core is not defined for multigraphs or graphs with self loops.\n\n",
  "code": "@nx._dispatch(preserve_all_attrs=True)\ndef k_core(G, k=None, core_number=None):\n    import warnings\n    if G.is_multigraph():\n        warnings.warn('\\n\\n`k_core` will not accept `MultiGraph` objects in version 3.5.\\nConvert it to an undirected graph instead, using::\\n\\n\\tG = nx.Graph(G)\\n', category=DeprecationWarning, stacklevel=5)\n\n    def k_filter(v, k, c):\n        return c[v] >= k\n    return _core_subgraph(G, k_filter, k, core_number)"
 },
 {
  "docstring": "Returns the k-shell of G.\n\nThe k-shell is the subgraph induced by nodes with core number k.\nThat is, nodes in the k-core that are not in the (k+1)-core.\n\n.. deprecated:: 3.3\n   `k_shell` will not accept `MultiGraph` objects in version 3.5.\n\nParameters\n----------\nG : NetworkX graph\n  A graph or directed graph.\nk : int, optional\n  The order of the shell. If not specified return the outer shell.\ncore_number : dictionary, optional\n  Precomputed core numbers for the graph G.\n\n\nReturns\n-------\nG : NetworkX graph\n   The k-shell subgraph\n\nRaises\n------\nNetworkXNotImplemented\n    The k-shell is not implemented for multigraphs or graphs with self loops.\n\n",
  "code": "@nx._dispatch(preserve_all_attrs=True)\ndef k_shell(G, k=None, core_number=None):\n    import warnings\n    if G.is_multigraph():\n        warnings.warn('\\n\\n`k_shell` will not accept `MultiGraph` objects in version 3.5.\\nConvert it to an undirected graph instead, using::\\n\\n\\tG = nx.Graph(G)\\n', category=DeprecationWarning, stacklevel=5)\n\n    def k_filter(v, k, c):\n        return c[v] == k\n    return _core_subgraph(G, k_filter, k, core_number)"
 },
 {
  "docstring": "Returns the k-crust of G.\n\nThe k-crust is the graph G with the edges of the k-core removed\nand isolated nodes found after the removal of edges are also removed.\n\n.. deprecated:: 3.3\n   `k_crust` will not accept `MultiGraph` objects in version 3.5.\n\nParameters\n----------\nG : NetworkX graph\n   A graph or directed graph.\nk : int, optional\n  The order of the shell. If not specified return the main crust.\ncore_number : dictionary, optional\n  Precomputed core numbers for the graph G.\n\nReturns\n-------\nG : NetworkX graph\n   The k-crust subgraph\n\nRaises\n------\nNetworkXNotImplemented\n    The k-crust is not implemented for multigraphs or graphs with self loops.\n\n",
  "code": "@nx._dispatch(preserve_all_attrs=True)\ndef k_crust(G, k=None, core_number=None):\n    import warnings\n    if G.is_multigraph():\n        warnings.warn('\\n\\n`k_crust` will not accept `MultiGraph` objects in version 3.5.\\nConvert it to an undirected graph instead, using::\\n\\n\\tG = nx.Graph(G)\\n', category=DeprecationWarning, stacklevel=5)\n    if core_number is None:\n        core_number = nx.core_number(G)\n    if k is None:\n        k = max(core_number.values()) - 1\n    nodes = (v for v in core_number if core_number[v] <= k)\n    return G.subgraph(nodes).copy()"
 },
 {
  "docstring": "Returns the k-corona of G.\n\nThe k-corona is the subgraph of nodes in the k-core which have\nexactly k neighbours in the k-core.\n\n.. deprecated:: 3.3\n   `k_corona` will not accept `MultiGraph` objects in version 3.5.\n\nParameters\n----------\nG : NetworkX graph\n   A graph or directed graph\nk : int\n   The order of the corona.\ncore_number : dictionary, optional\n   Precomputed core numbers for the graph G.\n\nReturns\n-------\nG : NetworkX graph\n   The k-corona subgraph\n\nRaises\n------\nNetworkXNotImplemented\n    The k-corona is not defined for multigraphs or graphs with self loops.\n\n",
  "code": "@nx._dispatch(preserve_all_attrs=True)\ndef k_corona(G, k, core_number=None):\n    import warnings\n    if G.is_multigraph():\n        warnings.warn('\\n\\n`k_corona` will not accept `MultiGraph` objects in version 3.5.\\nConvert it to an undirected graph instead, using::\\n\\n\\tG = nx.Graph(G)\\n', category=DeprecationWarning, stacklevel=5)\n\n    def func(v, k, c):\n        return c[v] == k and k == sum((1 for w in G[v] if c[w] >= k))\n    return _core_subgraph(G, func, k, core_number)"
 },
 {
  "docstring": "Returns the k-truss of `G`.\n\nThe k-truss is the maximal induced subgraph of `G` which contains at least\nthree vertices where every edge is incident to at least `k-2` triangles.\n\nParameters\n----------\nG : NetworkX graph\n  An undirected graph\nk : int\n  The order of the truss\n\nReturns\n-------\nH : NetworkX graph\n  The k-truss subgraph\n\nRaises\n------\nNetworkXNotImplemented\n  If `G` is a multigraph or directed graph or if it contains self loops.\n\n",
  "code": "@nx.utils.not_implemented_for('directed')\n@nx.utils.not_implemented_for('multigraph')\n@nx._dispatch(preserve_all_attrs=True)\ndef k_truss(G, k):\n    if nx.number_of_selfloops(G) > 0:\n        msg = 'Input graph has self loops which is not permitted; Consider using G.remove_edges_from(nx.selfloop_edges(G)).'\n        raise nx.NetworkXNotImplemented(msg)\n    H = G.copy()\n    n_dropped = 1\n    while n_dropped > 0:\n        n_dropped = 0\n        to_drop = []\n        seen = set()\n        for u in H:\n            nbrs_u = set(H[u])\n            seen.add(u)\n            new_nbrs = [v for v in nbrs_u if v not in seen]\n            for v in new_nbrs:\n                if len(nbrs_u & set(H[v])) < k - 2:\n                    to_drop.append((u, v))\n        H.remove_edges_from(to_drop)\n        n_dropped = len(to_drop)\n        H.remove_nodes_from(list(nx.isolates(H)))\n    return H"
 },
 {
  "docstring": "Returns the layer of each vertex in an onion decomposition of the graph.\n\nThe onion decomposition refines the k-core decomposition by providing\ninformation on the internal organization of each k-shell. It is usually\nused alongside the `core numbers`.\n\nParameters\n----------\nG : NetworkX graph\n    An undirected graph without self loops.\n\nReturns\n-------\nod_layers : dictionary\n    A dictionary keyed by node to the onion layer. The layers are\n    contiguous integers starting at 1.\n\nRaises\n------\nNetworkXNotImplemented\n    If `G` is a multigraph or directed graph or if it contains self loops.\n\nExamples\n--------\n>>> degrees = [0, 1, 2, 2, 2, 2, 3]\n>>> H = nx.havel_hakimi_graph(degrees)\n>>> H.degree\nDegreeView({0: 1, 1: 2, 2: 2, 3: 2, 4: 2, 5: 3, 6: 0})\n>>> nx.onion_layers(H)\n{6: 1, 0: 2, 4: 3, 1: 4, 2: 4, 3: 4, 5: 4}\n\nSee Also\n--------\ncore_number\n\n",
  "code": "@nx.utils.not_implemented_for('multigraph')\n@nx.utils.not_implemented_for('directed')\n@nx._dispatch\ndef onion_layers(G):\n    if nx.number_of_selfloops(G) > 0:\n        msg = 'Input graph contains self loops which is not permitted; Consider using G.remove_edges_from(nx.selfloop_edges(G)).'\n        raise nx.NetworkXNotImplemented(msg)\n    od_layers = {}\n    neighbors = {v: list(nx.all_neighbors(G, v)) for v in G}\n    degrees = dict(G.degree())\n    current_core = 1\n    current_layer = 1\n    isolated_nodes = list(nx.isolates(G))\n    if len(isolated_nodes) > 0:\n        for v in isolated_nodes:\n            od_layers[v] = current_layer\n            degrees.pop(v)\n        current_layer = 2\n    while len(degrees) > 0:\n        nodes = sorted(degrees, key=degrees.get)\n        min_degree = degrees[nodes[0]]\n        if min_degree > current_core:\n            current_core = min_degree\n        this_layer = []\n        for n in nodes:\n            if degrees[n] > current_core:\n                break\n            this_layer.append(n)\n        for v in this_layer:\n            od_layers[v] = current_layer\n            for n in neighbors[v]:\n                neighbors[n].remove(v)\n                degrees[n] = degrees[n] - 1\n            degrees.pop(v)\n        current_layer = current_layer + 1\n    return od_layers"
 },
 {
  "docstring": "Returns the min cardinality edge cover of the graph as a set of edges.\n\nA smallest edge cover can be found in polynomial time by finding\na maximum matching and extending it greedily so that all nodes\nare covered. This function follows that process. A maximum matching\nalgorithm can be specified for the first step of the algorithm.\nThe resulting set may return a set with one 2-tuple for each edge,\n(the usual case) or with both 2-tuples `(u, v)` and `(v, u)` for\neach edge. The latter is only done when a bipartite matching algorithm\nis specified as `matching_algorithm`.\n\nParameters\n----------\nG : NetworkX graph\n    An undirected graph.\n\nmatching_algorithm : function\n    A function that returns a maximum cardinality matching for `G`.\n    The function must take one input, the graph `G`, and return\n    either a set of edges (with only one direction for the pair of nodes)\n    or a dictionary mapping each node to its mate. If not specified,\n    :func:`~networkx.algorithms.matching.max_weight_matching` is used.\n    Common bipartite matching functions include\n    :func:`~networkx.algorithms.bipartite.matching.hopcroft_karp_matching`\n    or\n    :func:`~networkx.algorithms.bipartite.matching.eppstein_matching`.\n\nReturns\n-------\nmin_cover : set\n\n    A set of the edges in a minimum edge cover in the form of tuples.\n    It contains only one of the equivalent 2-tuples `(u, v)` and `(v, u)`\n    for each edge. If a bipartite method is used to compute the matching,\n    the returned set contains both the 2-tuples `(u, v)` and `(v, u)`\n    for each edge of a minimum edge cover.\n\nExamples\n--------\n>>> G = nx.Graph([(0, 1), (0, 2), (0, 3), (1, 2), (1, 3)])\n>>> sorted(nx.min_edge_cover(G))\n[(2, 1), (3, 0)]\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef min_edge_cover(G, matching_algorithm=None):\n    if len(G) == 0:\n        return set()\n    if nx.number_of_isolates(G) > 0:\n        raise nx.NetworkXException('Graph has a node with no edge incident on it, so no edge cover exists.')\n    if matching_algorithm is None:\n        matching_algorithm = partial(nx.max_weight_matching, maxcardinality=True)\n    maximum_matching = matching_algorithm(G)\n    try:\n        min_cover = set(maximum_matching.items())\n        bipartite_cover = True\n    except AttributeError:\n        min_cover = maximum_matching\n        bipartite_cover = False\n    uncovered_nodes = set(G) - {v for u, v in min_cover} - {u for u, v in min_cover}\n    for v in uncovered_nodes:\n        u = arbitrary_element(G[v])\n        min_cover.add((u, v))\n        if bipartite_cover:\n            min_cover.add((v, u))\n    return min_cover"
 },
 {
  "docstring": "Decides whether a set of edges is a valid edge cover of the graph.\n\nGiven a set of edges, whether it is an edge covering can\nbe decided if we just check whether all nodes of the graph\nhas an edge from the set, incident on it.\n\nParameters\n----------\nG : NetworkX graph\n    An undirected bipartite graph.\n\ncover : set\n    Set of edges to be checked.\n\nReturns\n-------\nbool\n    Whether the set of edges is a valid edge cover of the graph.\n\nExamples\n--------\n>>> G = nx.Graph([(0, 1), (0, 2), (0, 3), (1, 2), (1, 3)])\n>>> cover = {(2, 1), (3, 0)}\n>>> nx.is_edge_cover(G, cover)\nTrue\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch\ndef is_edge_cover(G, cover):\n    return set(G) <= set(chain.from_iterable(cover))"
 },
 {
  "docstring": "Returns a list of cycles which form a basis for cycles of G.\n\nA basis for cycles of a network is a minimal collection of\ncycles such that any cycle in the network can be written\nas a sum of cycles in the basis.  Here summation of cycles\nis defined as \"exclusive or\" of the edges. Cycle bases are\nuseful, e.g. when deriving equations for electric circuits\nusing Kirchhoff's Laws.\n\nParameters\n----------\nG : NetworkX Graph\nroot : node, optional\n   Specify starting node for basis.\n\nReturns\n-------\nA list of cycle lists.  Each cycle list is a list of nodes\nwhich forms a cycle (loop) in G.\n\nExamples\n--------\n>>> G = nx.Graph()\n>>> nx.add_cycle(G, [0, 1, 2, 3])\n>>> nx.add_cycle(G, [0, 3, 4, 5])\n>>> nx.cycle_basis(G, 0)\n[[3, 4, 5, 0], [1, 2, 3, 0]]\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef cycle_basis(G, root=None):\n    gnodes = dict.fromkeys(G)\n    cycles = []\n    while gnodes:\n        if root is None:\n            root = gnodes.popitem()[0]\n        stack = [root]\n        pred = {root: root}\n        used = {root: set()}\n        while stack:\n            z = stack.pop()\n            zused = used[z]\n            for nbr in G[z]:\n                if nbr not in used:\n                    pred[nbr] = z\n                    stack.append(nbr)\n                    used[nbr] = {z}\n                elif nbr == z:\n                    cycles.append([z])\n                elif nbr not in zused:\n                    pn = used[nbr]\n                    cycle = [nbr, z]\n                    p = pred[z]\n                    while p not in pn:\n                        cycle.append(p)\n                        p = pred[p]\n                    cycle.append(p)\n                    cycles.append(cycle)\n                    used[nbr].add(z)\n        for node in pred:\n            gnodes.pop(node, None)\n        root = None\n    return cycles"
 },
 {
  "docstring": "Find simple cycles (elementary circuits) of a graph.\n\nA `simple cycle`, or `elementary circuit`, is a closed path where\nno node appears twice.  In a directed graph, two simple cycles are distinct\nif they are not cyclic permutations of each other.  In an undirected graph,\ntwo simple cycles are distinct if they are not cyclic permutations of each\nother nor of the other's reversal.\n\nOptionally, the cycles are bounded in length.  In the unbounded case, we use\na nonrecursive, iterator/generator version of Johnson's algorithm [1]_.  In\nthe bounded case, we use a version of the algorithm of Gupta and\nSuzumura[2]_. There may be better algorithms for some cases [3]_ [4]_ [5]_.\n\nThe algorithms of Johnson, and Gupta and Suzumura, are enhanced by some\nwell-known preprocessing techniques.  When G is directed, we restrict our\nattention to strongly connected components of G, generate all simple cycles\ncontaining a certain node, remove that node, and further decompose the\nremainder into strongly connected components.  When G is undirected, we\nrestrict our attention to biconnected components, generate all simple cycles\ncontaining a particular edge, remove that edge, and further decompose the\nremainder into biconnected components.\n\nNote that multigraphs are supported by this function -- and in undirected\nmultigraphs, a pair of parallel edges is considered a cycle of length 2.\nLikewise, self-loops are considered to be cycles of length 1.  We define\ncycles as sequences of nodes; so the presence of loops and parallel edges\ndoes not change the number of simple cycles in a graph.\n\nParameters\n----------\nG : NetworkX DiGraph\n   A directed graph\n\nlength_bound : int or None, optional (default=None)\n   If length_bound is an int, generate all simple cycles of G with length at\n   most length_bound.  Otherwise, generate all simple cycles of G.\n\nYields\n------\nlist of nodes\n   Each cycle is represented by a list of nodes along the cycle.\n\nExamples\n--------\n>>> edges = [(0, 0), (0, 1), (0, 2), (1, 2), (2, 0), (2, 1), (2, 2)]\n>>> G = nx.DiGraph(edges)\n>>> sorted(nx.simple_cycles(G))\n[[0], [0, 1, 2], [0, 2], [1, 2], [2]]\n\nTo filter the cycles so that they don't include certain nodes or edges,\ncopy your graph and eliminate those nodes or edges before calling.\nFor example, to exclude self-loops from the above example:\n\n>>> H = G.copy()\n>>> H.remove_edges_from(nx.selfloop_edges(G))\n>>> sorted(nx.simple_cycles(H))\n[[0, 1, 2], [0, 2], [1, 2]]\n\n",
  "code": "@nx._dispatch\ndef simple_cycles(G, length_bound=None):\n    if length_bound is not None:\n        if length_bound == 0:\n            return\n        elif length_bound < 0:\n            raise ValueError('length bound must be non-negative')\n    directed = G.is_directed()\n    yield from ([v] for v, Gv in G.adj.items() if v in Gv)\n    if length_bound is not None and length_bound == 1:\n        return\n    if G.is_multigraph() and (not directed):\n        visited = set()\n        for u, Gu in G.adj.items():\n            multiplicity = ((v, len(Guv)) for v, Guv in Gu.items() if v in visited)\n            yield from ([u, v] for v, m in multiplicity if m > 1)\n            visited.add(u)\n    if directed:\n        G = nx.DiGraph(((u, v) for u, Gu in G.adj.items() for v in Gu if v != u))\n    else:\n        G = nx.Graph(((u, v) for u, Gu in G.adj.items() for v in Gu if v != u))\n    if length_bound is not None and length_bound == 2:\n        if directed:\n            visited = set()\n            for u, Gu in G.adj.items():\n                yield from ([v, u] for v in visited.intersection(Gu) if G.has_edge(v, u))\n                visited.add(u)\n        return\n    if directed:\n        yield from _directed_cycle_search(G, length_bound)\n    else:\n        yield from _undirected_cycle_search(G, length_bound)"
 },
 {
  "docstring": "A dispatch function for `simple_cycles` for directed graphs.\n\nWe generate all cycles of G through binary partition.\n\n    1. Pick a node v in G which belongs to at least one cycle\n        a. Generate all cycles of G which contain the node v.\n        b. Recursively generate all cycles of G \\ v.\n\nThis is accomplished through the following:\n\n    1. Compute the strongly connected components SCC of G.\n    2. Select and remove a biconnected component C from BCC.  Select a\n       non-tree edge (u, v) of a depth-first search of G[C].\n    3. For each simple cycle P containing v in G[C], yield P.\n    4. Add the biconnected components of G[C \\ v] to BCC.\n\nIf the parameter length_bound is not None, then step 3 will be limited to\nsimple cycles of length at most length_bound.\n\nParameters\n----------\nG : NetworkX DiGraph\n   A directed graph\n\nlength_bound : int or None\n   If length_bound is an int, generate all simple cycles of G with length at most length_bound.\n   Otherwise, generate all simple cycles of G.\n\nYields\n------\nlist of nodes\n   Each cycle is represented by a list of nodes along the cycle.",
  "code": "def _directed_cycle_search(G, length_bound):\n    scc = nx.strongly_connected_components\n    components = [c for c in scc(G) if len(c) >= 2]\n    while components:\n        c = components.pop()\n        Gc = G.subgraph(c)\n        v = next(iter(c))\n        if length_bound is None:\n            yield from _johnson_cycle_search(Gc, [v])\n        else:\n            yield from _bounded_cycle_search(Gc, [v], length_bound)\n        G.remove_node(v)\n        components.extend((c for c in scc(Gc) if len(c) >= 2))"
 },
 {
  "docstring": "A dispatch function for `simple_cycles` for undirected graphs.\n\nWe generate all cycles of G through binary partition.\n\n    1. Pick an edge (u, v) in G which belongs to at least one cycle\n        a. Generate all cycles of G which contain the edge (u, v)\n        b. Recursively generate all cycles of G \\ (u, v)\n\nThis is accomplished through the following:\n\n    1. Compute the biconnected components BCC of G.\n    2. Select and remove a biconnected component C from BCC.  Select a\n       non-tree edge (u, v) of a depth-first search of G[C].\n    3. For each (v -> u) path P remaining in G[C] \\ (u, v), yield P.\n    4. Add the biconnected components of G[C] \\ (u, v) to BCC.\n\nIf the parameter length_bound is not None, then step 3 will be limited to simple paths\nof length at most length_bound.\n\nParameters\n----------\nG : NetworkX Graph\n   An undirected graph\n\nlength_bound : int or None\n   If length_bound is an int, generate all simple cycles of G with length at most length_bound.\n   Otherwise, generate all simple cycles of G.\n\nYields\n------\nlist of nodes\n   Each cycle is represented by a list of nodes along the cycle.",
  "code": "def _undirected_cycle_search(G, length_bound):\n    bcc = nx.biconnected_components\n    components = [c for c in bcc(G) if len(c) >= 3]\n    while components:\n        c = components.pop()\n        Gc = G.subgraph(c)\n        uv = list(next(iter(Gc.edges)))\n        G.remove_edge(*uv)\n        if length_bound is None:\n            yield from _johnson_cycle_search(Gc, uv)\n        else:\n            yield from _bounded_cycle_search(Gc, uv, length_bound)\n        components.extend((c for c in bcc(Gc) if len(c) >= 3))"
 },
 {
  "docstring": "The main loop of the cycle-enumeration algorithm of Johnson.\n\nParameters\n----------\nG : NetworkX Graph or DiGraph\n   A graph\n\npath : list\n   A cycle prefix.  All cycles generated will begin with this prefix.\n\nYields\n------\nlist of nodes\n   Each cycle is represented by a list of nodes along the cycle.\n\n",
  "code": "def _johnson_cycle_search(G, path):\n    G = _NeighborhoodCache(G)\n    blocked = set(path)\n    B = defaultdict(set)\n    start = path[0]\n    stack = [iter(G[path[-1]])]\n    closed = [False]\n    while stack:\n        nbrs = stack[-1]\n        for w in nbrs:\n            if w == start:\n                yield path[:]\n                closed[-1] = True\n            elif w not in blocked:\n                path.append(w)\n                closed.append(False)\n                stack.append(iter(G[w]))\n                blocked.add(w)\n                break\n        else:\n            stack.pop()\n            v = path.pop()\n            if closed.pop():\n                if closed:\n                    closed[-1] = True\n                unblock_stack = {v}\n                while unblock_stack:\n                    u = unblock_stack.pop()\n                    if u in blocked:\n                        blocked.remove(u)\n                        unblock_stack.update(B[u])\n                        B[u].clear()\n            else:\n                for w in G[v]:\n                    B[w].add(v)"
 },
 {
  "docstring": "The main loop of the cycle-enumeration algorithm of Gupta and Suzumura.\n\nParameters\n----------\nG : NetworkX Graph or DiGraph\n   A graph\n\npath : list\n   A cycle prefix.  All cycles generated will begin with this prefix.\n\nlength_bound: int\n    A length bound.  All cycles generated will have length at most length_bound.\n\nYields\n------\nlist of nodes\n   Each cycle is represented by a list of nodes along the cycle.\n\n",
  "code": "def _bounded_cycle_search(G, path, length_bound):\n    G = _NeighborhoodCache(G)\n    lock = {v: 0 for v in path}\n    B = defaultdict(set)\n    start = path[0]\n    stack = [iter(G[path[-1]])]\n    blen = [length_bound]\n    while stack:\n        nbrs = stack[-1]\n        for w in nbrs:\n            if w == start:\n                yield path[:]\n                blen[-1] = 1\n            elif len(path) < lock.get(w, length_bound):\n                path.append(w)\n                blen.append(length_bound)\n                lock[w] = len(path)\n                stack.append(iter(G[w]))\n                break\n        else:\n            stack.pop()\n            v = path.pop()\n            bl = blen.pop()\n            if blen:\n                blen[-1] = min(blen[-1], bl)\n            if bl < length_bound:\n                relax_stack = [(bl, v)]\n                while relax_stack:\n                    bl, u = relax_stack.pop()\n                    if lock.get(u, length_bound) < length_bound - bl + 1:\n                        lock[u] = length_bound - bl + 1\n                        relax_stack.extend(((bl + 1, w) for w in B[u].difference(path)))\n            else:\n                for w in G[v]:\n                    B[w].add(v)"
 },
 {
  "docstring": "Find simple chordless cycles of a graph.\n\nA `simple cycle` is a closed path where no node appears twice.  In a simple\ncycle, a `chord` is an additional edge between two nodes in the cycle.  A\n`chordless cycle` is a simple cycle without chords.  Said differently, a\nchordless cycle is a cycle C in a graph G where the number of edges in the\ninduced graph G[C] is equal to the length of `C`.\n\nNote that some care must be taken in the case that G is not a simple graph\nnor a simple digraph.  Some authors limit the definition of chordless cycles\nto have a prescribed minimum length; we do not.\n\n    1. We interpret self-loops to be chordless cycles, except in multigraphs\n       with multiple loops in parallel.  Likewise, in a chordless cycle of\n       length greater than 1, there can be no nodes with self-loops.\n\n    2. We interpret directed two-cycles to be chordless cycles, except in\n       multi-digraphs when any edge in a two-cycle has a parallel copy.\n\n    3. We interpret parallel pairs of undirected edges as two-cycles, except\n       when a third (or more) parallel edge exists between the two nodes.\n\n    4. Generalizing the above, edges with parallel clones may not occur in\n       chordless cycles.\n\nIn a directed graph, two chordless cycles are distinct if they are not\ncyclic permutations of each other.  In an undirected graph, two chordless\ncycles are distinct if they are not cyclic permutations of each other nor of\nthe other's reversal.\n\nOptionally, the cycles are bounded in length.\n\nWe use an algorithm strongly inspired by that of Dias et al [1]_.  It has\nbeen modified in the following ways:\n\n    1. Recursion is avoided, per Python's limitations\n\n    2. The labeling function is not necessary, because the starting paths\n        are chosen (and deleted from the host graph) to prevent multiple\n        occurrences of the same path\n\n    3. The search is optionally bounded at a specified length\n\n    4. Support for directed graphs is provided by extending cycles along\n        forward edges, and blocking nodes along forward and reverse edges\n\n    5. Support for multigraphs is provided by omitting digons from the set\n        of forward edges\n\nParameters\n----------\nG : NetworkX DiGraph\n   A directed graph\n\nlength_bound : int or None, optional (default=None)\n   If length_bound is an int, generate all simple cycles of G with length at\n   most length_bound.  Otherwise, generate all simple cycles of G.\n\nYields\n------\nlist of nodes\n   Each cycle is represented by a list of nodes along the cycle.\n\nExamples\n--------\n>>> sorted(list(nx.chordless_cycles(nx.complete_graph(4))))\n[[1, 0, 2], [1, 0, 3], [2, 0, 3], [2, 1, 3]]\n\n",
  "code": "@nx._dispatch\ndef chordless_cycles(G, length_bound=None):\n    if length_bound is not None:\n        if length_bound == 0:\n            return\n        elif length_bound < 0:\n            raise ValueError('length bound must be non-negative')\n    directed = G.is_directed()\n    multigraph = G.is_multigraph()\n    if multigraph:\n        yield from ([v] for v, Gv in G.adj.items() if len(Gv.get(v, ())) == 1)\n    else:\n        yield from ([v] for v, Gv in G.adj.items() if v in Gv)\n    if length_bound is not None and length_bound == 1:\n        return\n    if directed:\n        F = nx.DiGraph(((u, v) for u, Gu in G.adj.items() if u not in Gu for v in Gu))\n        B = F.to_undirected(as_view=False)\n    else:\n        F = nx.Graph(((u, v) for u, Gu in G.adj.items() if u not in Gu for v in Gu))\n        B = None\n    if multigraph:\n        if not directed:\n            B = F.copy()\n            visited = set()\n        for u, Gu in G.adj.items():\n            if directed:\n                multiplicity = ((v, len(Guv)) for v, Guv in Gu.items())\n                for v, m in multiplicity:\n                    if m > 1:\n                        F.remove_edges_from(((u, v), (v, u)))\n            else:\n                multiplicity = ((v, len(Guv)) for v, Guv in Gu.items() if v in visited)\n                for v, m in multiplicity:\n                    if m == 2:\n                        yield [u, v]\n                    if m > 1:\n                        F.remove_edge(u, v)\n                visited.add(u)\n    if directed:\n        for u, Fu in F.adj.items():\n            digons = [[u, v] for v in Fu if F.has_edge(v, u)]\n            yield from digons\n            F.remove_edges_from(digons)\n            F.remove_edges_from((e[::-1] for e in digons))\n    if length_bound is not None and length_bound == 2:\n        return\n    if directed:\n        separate = nx.strongly_connected_components\n\n        def stems(C, v):\n            for u, w in product(C.pred[v], C.succ[v]):\n                if not G.has_edge(u, w):\n                    yield ([u, v, w], F.has_edge(w, u))\n    else:\n        separate = nx.biconnected_components\n\n        def stems(C, v):\n            yield from (([u, v, w], F.has_edge(w, u)) for u, w in combinations(C[v], 2))\n    components = [c for c in separate(F) if len(c) > 2]\n    while components:\n        c = components.pop()\n        v = next(iter(c))\n        Fc = F.subgraph(c)\n        Fcc = Bcc = None\n        for S, is_triangle in stems(Fc, v):\n            if is_triangle:\n                yield S\n            else:\n                if Fcc is None:\n                    Fcc = _NeighborhoodCache(Fc)\n                    Bcc = Fcc if B is None else _NeighborhoodCache(B.subgraph(c))\n                yield from _chordless_cycle_search(Fcc, Bcc, S, length_bound)\n        components.extend((c for c in separate(F.subgraph(c - {v})) if len(c) > 2))"
 },
 {
  "docstring": "The main loop for chordless cycle enumeration.\n\nThis algorithm is strongly inspired by that of Dias et al [1]_.  It has been\nmodified in the following ways:\n\n    1. Recursion is avoided, per Python's limitations\n\n    2. The labeling function is not necessary, because the starting paths\n        are chosen (and deleted from the host graph) to prevent multiple\n        occurrences of the same path\n\n    3. The search is optionally bounded at a specified length\n\n    4. Support for directed graphs is provided by extending cycles along\n        forward edges, and blocking nodes along forward and reverse edges\n\n    5. Support for multigraphs is provided by omitting digons from the set\n        of forward edges\n\nParameters\n----------\nF : _NeighborhoodCache\n   A graph of forward edges to follow in constructing cycles\n\nB : _NeighborhoodCache\n   A graph of blocking edges to prevent the production of chordless cycles\n\npath : list\n   A cycle prefix.  All cycles generated will begin with this prefix.\n\nlength_bound : int\n   A length bound.  All cycles generated will have length at most length_bound.\n\n\nYields\n------\nlist of nodes\n   Each cycle is represented by a list of nodes along the cycle.\n\n",
  "code": "def _chordless_cycle_search(F, B, path, length_bound):\n    blocked = defaultdict(int)\n    target = path[0]\n    blocked[path[1]] = 1\n    for w in path[1:]:\n        for v in B[w]:\n            blocked[v] += 1\n    stack = [iter(F[path[2]])]\n    while stack:\n        nbrs = stack[-1]\n        for w in nbrs:\n            if blocked[w] == 1 and (length_bound is None or len(path) < length_bound):\n                Fw = F[w]\n                if target in Fw:\n                    yield (path + [w])\n                else:\n                    Bw = B[w]\n                    if target in Bw:\n                        continue\n                    for v in Bw:\n                        blocked[v] += 1\n                    path.append(w)\n                    stack.append(iter(Fw))\n                    break\n        else:\n            stack.pop()\n            for v in B[path.pop()]:\n                blocked[v] -= 1"
 },
 {
  "docstring": "Find simple cycles (elementary circuits) of a directed graph.\n\nA `simple cycle`, or `elementary circuit`, is a closed path where\nno node appears twice. Two elementary circuits are distinct if they\nare not cyclic permutations of each other.\n\nThis version uses a recursive algorithm to build a list of cycles.\nYou should probably use the iterator version called simple_cycles().\nWarning: This recursive version uses lots of RAM!\nIt appears in NetworkX for pedagogical value.\n\nParameters\n----------\nG : NetworkX DiGraph\n   A directed graph\n\nReturns\n-------\nA list of cycles, where each cycle is represented by a list of nodes\nalong the cycle.\n\nExample:\n\n>>> edges = [(0, 0), (0, 1), (0, 2), (1, 2), (2, 0), (2, 1), (2, 2)]\n>>> G = nx.DiGraph(edges)\n>>> nx.recursive_simple_cycles(G)\n[[0], [2], [0, 1, 2], [0, 2], [1, 2]]\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef recursive_simple_cycles(G):\n\n    def _unblock(thisnode):\n        \"\"\"Recursively unblock and remove nodes from B[thisnode].\"\"\"\n        if blocked[thisnode]:\n            blocked[thisnode] = False\n            while B[thisnode]:\n                _unblock(B[thisnode].pop())\n\n    def circuit(thisnode, startnode, component):\n        closed = False\n        path.append(thisnode)\n        blocked[thisnode] = True\n        for nextnode in component[thisnode]:\n            if nextnode == startnode:\n                result.append(path[:])\n                closed = True\n            elif not blocked[nextnode]:\n                if circuit(nextnode, startnode, component):\n                    closed = True\n        if closed:\n            _unblock(thisnode)\n        else:\n            for nextnode in component[thisnode]:\n                if thisnode not in B[nextnode]:\n                    B[nextnode].append(thisnode)\n        path.pop()\n        return closed\n    path = []\n    blocked = defaultdict(bool)\n    B = defaultdict(list)\n    result = []\n    for v in G:\n        if G.has_edge(v, v):\n            result.append([v])\n            G.remove_edge(v, v)\n    ordering = dict(zip(G, range(len(G))))\n    for s in ordering:\n        subgraph = G.subgraph((node for node in G if ordering[node] >= ordering[s]))\n        strongcomp = nx.strongly_connected_components(subgraph)\n        mincomp = min(strongcomp, key=lambda ns: min((ordering[n] for n in ns)))\n        component = G.subgraph(mincomp)\n        if len(component) > 1:\n            startnode = min(component, key=ordering.__getitem__)\n            for node in component:\n                blocked[node] = False\n                B[node][:] = []\n            dummy = circuit(startnode, startnode, component)\n    return result"
 },
 {
  "docstring": "Returns a cycle found via depth-first traversal.\n\nThe cycle is a list of edges indicating the cyclic path.\nOrientation of directed edges is controlled by `orientation`.\n\nParameters\n----------\nG : graph\n    A directed/undirected graph/multigraph.\n\nsource : node, list of nodes\n    The node from which the traversal begins. If None, then a source\n    is chosen arbitrarily and repeatedly until all edges from each node in\n    the graph are searched.\n\norientation : None | 'original' | 'reverse' | 'ignore' (default: None)\n    For directed graphs and directed multigraphs, edge traversals need not\n    respect the original orientation of the edges.\n    When set to 'reverse' every edge is traversed in the reverse direction.\n    When set to 'ignore', every edge is treated as undirected.\n    When set to 'original', every edge is treated as directed.\n    In all three cases, the yielded edge tuples add a last entry to\n    indicate the direction in which that edge was traversed.\n    If orientation is None, the yielded edge has no direction indicated.\n    The direction is respected, but not reported.\n\nReturns\n-------\nedges : directed edges\n    A list of directed edges indicating the path taken for the loop.\n    If no cycle is found, then an exception is raised.\n    For graphs, an edge is of the form `(u, v)` where `u` and `v`\n    are the tail and head of the edge as determined by the traversal.\n    For multigraphs, an edge is of the form `(u, v, key)`, where `key` is\n    the key of the edge. When the graph is directed, then `u` and `v`\n    are always in the order of the actual directed edge.\n    If orientation is not None then the edge tuple is extended to include\n    the direction of traversal ('forward' or 'reverse') on that edge.\n\nRaises\n------\nNetworkXNoCycle\n    If no cycle was found.\n\nExamples\n--------\nIn this example, we construct a DAG and find, in the first call, that there\nare no directed cycles, and so an exception is raised. In the second call,\nwe ignore edge orientations and find that there is an undirected cycle.\nNote that the second call finds a directed cycle while effectively\ntraversing an undirected graph, and so, we found an \"undirected cycle\".\nThis means that this DAG structure does not form a directed tree (which\nis also known as a polytree).\n\n>>> G = nx.DiGraph([(0, 1), (0, 2), (1, 2)])\n>>> nx.find_cycle(G, orientation=\"original\")\nTraceback (most recent call last):\n    ...\nnetworkx.exception.NetworkXNoCycle: No cycle found.\n>>> list(nx.find_cycle(G, orientation=\"ignore\"))\n[(0, 1, 'forward'), (1, 2, 'forward'), (0, 2, 'reverse')]\n\nSee Also\n--------\nsimple_cycles",
  "code": "@nx._dispatch\ndef find_cycle(G, source=None, orientation=None):\n    if not G.is_directed() or orientation in (None, 'original'):\n\n        def tailhead(edge):\n            return edge[:2]\n    elif orientation == 'reverse':\n\n        def tailhead(edge):\n            return (edge[1], edge[0])\n    elif orientation == 'ignore':\n\n        def tailhead(edge):\n            if edge[-1] == 'reverse':\n                return (edge[1], edge[0])\n            return edge[:2]\n    explored = set()\n    cycle = []\n    final_node = None\n    for start_node in G.nbunch_iter(source):\n        if start_node in explored:\n            continue\n        edges = []\n        seen = {start_node}\n        active_nodes = {start_node}\n        previous_head = None\n        for edge in nx.edge_dfs(G, start_node, orientation):\n            tail, head = tailhead(edge)\n            if head in explored:\n                continue\n            if previous_head is not None and tail != previous_head:\n                while True:\n                    try:\n                        popped_edge = edges.pop()\n                    except IndexError:\n                        edges = []\n                        active_nodes = {tail}\n                        break\n                    else:\n                        popped_head = tailhead(popped_edge)[1]\n                        active_nodes.remove(popped_head)\n                    if edges:\n                        last_head = tailhead(edges[-1])[1]\n                        if tail == last_head:\n                            break\n            edges.append(edge)\n            if head in active_nodes:\n                cycle.extend(edges)\n                final_node = head\n                break\n            else:\n                seen.add(head)\n                active_nodes.add(head)\n                previous_head = head\n        if cycle:\n            break\n        else:\n            explored.update(seen)\n    else:\n        assert len(cycle) == 0\n        raise nx.exception.NetworkXNoCycle('No cycle found.')\n    for i, edge in enumerate(cycle):\n        tail, head = tailhead(edge)\n        if tail == final_node:\n            break\n    return cycle[i:]"
 },
 {
  "docstring": "Returns a minimum weight cycle basis for G\n\nMinimum weight means a cycle basis for which the total weight\n(length for unweighted graphs) of all the cycles is minimum.\n\nParameters\n----------\nG : NetworkX Graph\nweight: string\n    name of the edge attribute to use for edge weights\n\nReturns\n-------\nA list of cycle lists.  Each cycle list is a list of nodes\nwhich forms a cycle (loop) in G. Note that the nodes are not\nnecessarily returned in a order by which they appear in the cycle\n\nExamples\n--------\n>>> G = nx.Graph()\n>>> nx.add_cycle(G, [0, 1, 2, 3])\n>>> nx.add_cycle(G, [0, 3, 4, 5])\n>>> nx.minimum_cycle_basis(G)\n[[5, 4, 3, 0], [3, 2, 1, 0]]\n\nReferences:\n    [1] Kavitha, Telikepalli, et al. \"An O(m^2n) Algorithm for\n    Minimum Cycle Basis of Graphs.\"\n    http://link.springer.com/article/10.1007/s00453-007-9064-z\n    [2] de Pina, J. 1995. Applications of shortest path methods.\n    Ph.D. thesis, University of Amsterdam, Netherlands\n\nSee Also\n--------\nsimple_cycles, cycle_basis",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch(edge_attrs='weight')\ndef minimum_cycle_basis(G, weight=None):\n    return sum((_min_cycle_basis(G.subgraph(c), weight) for c in nx.connected_components(G)), [])"
 },
 {
  "docstring": "Computes the minimum weight cycle in G,\northogonal to the vector orth as per [p. 338, 1]\nUse (u, 1) to indicate the lifted copy of u (denoted u' in paper).",
  "code": "def _min_cycle(G, orth, weight):\n    Gi = nx.Graph()\n    for u, v, wt in G.edges(data=weight, default=1):\n        if (u, v) in orth or (v, u) in orth:\n            Gi.add_edges_from([(u, (v, 1)), ((u, 1), v)], Gi_weight=wt)\n        else:\n            Gi.add_edges_from([(u, v), ((u, 1), (v, 1))], Gi_weight=wt)\n    spl = nx.shortest_path_length\n    lift = {n: spl(Gi, source=n, target=(n, 1), weight='Gi_weight') for n in G}\n    start = min(lift, key=lift.get)\n    end = (start, 1)\n    min_path_i = nx.shortest_path(Gi, source=start, target=end, weight='Gi_weight')\n    min_path = [n if n in G else n[0] for n in min_path_i]\n    edgelist = list(pairwise(min_path))\n    edgeset = set()\n    for e in edgelist:\n        if e in edgeset:\n            edgeset.remove(e)\n        elif e[::-1] in edgeset:\n            edgeset.remove(e[::-1])\n        else:\n            edgeset.add(e)\n    min_edgelist = []\n    for e in edgelist:\n        if e in edgeset:\n            min_edgelist.append(e)\n            edgeset.remove(e)\n        elif e[::-1] in edgeset:\n            min_edgelist.append(e[::-1])\n            edgeset.remove(e[::-1])\n    return min_edgelist"
 },
 {
  "docstring": "Returns the girth of the graph.\n\nThe girth of a graph is the length of its shortest cycle, or infinity if\nthe graph is acyclic. The algorithm follows the description given on the\nWikipedia page [1]_, and runs in time O(mn) on a graph with m edges and n\nnodes.\n\nParameters\n----------\nG : NetworkX Graph\n\nReturns\n-------\nint or math.inf\n\nExamples\n--------\nAll examples below (except P_5) can easily be checked using Wikipedia,\nwhich has a page for each of these famous graphs.\n\n>>> nx.girth(nx.chvatal_graph())\n4\n>>> nx.girth(nx.tutte_graph())\n4\n>>> nx.girth(nx.petersen_graph())\n5\n>>> nx.girth(nx.heawood_graph())\n6\n>>> nx.girth(nx.pappus_graph())\n6\n>>> nx.girth(nx.path_graph(5))\ninf\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef girth(G):\n    girth = depth_limit = inf\n    tree_edge = nx.algorithms.traversal.breadth_first_search.TREE_EDGE\n    level_edge = nx.algorithms.traversal.breadth_first_search.LEVEL_EDGE\n    for n in G:\n        depth = {n: 0}\n        for u, v, label in nx.bfs_labeled_edges(G, n):\n            du = depth[u]\n            if du > depth_limit:\n                break\n            if label is tree_edge:\n                depth[v] = du + 1\n            else:\n                delta = label is level_edge\n                length = du + du + 2 - delta\n                if length < girth:\n                    girth = length\n                    depth_limit = du - delta\n    return girth"
 },
 {
  "docstring": "Recursively unblock and remove nodes from B[thisnode].",
  "code": "def _unblock(thisnode):\n    if blocked[thisnode]:\n        blocked[thisnode] = False\n        while B[thisnode]:\n            _unblock(B[thisnode].pop())"
 },
 {
  "docstring": "Returns all nodes reachable from `source` in `G`.\n\nParameters\n----------\nG : NetworkX Graph\nsource : node in `G`\n\nReturns\n-------\nset()\n    The descendants of `source` in `G`\n\nRaises\n------\nNetworkXError\n    If node `source` is not in `G`.\n\nExamples\n--------\n>>> DG = nx.path_graph(5, create_using=nx.DiGraph)\n>>> sorted(nx.descendants(DG, 2))\n[3, 4]\n\nThe `source` node is not a descendant of itself, but can be included manually:\n\n>>> sorted(nx.descendants(DG, 2) | {2})\n[2, 3, 4]\n\n",
  "code": "@nx._dispatch\ndef descendants(G, source):\n    return {child for parent, child in nx.bfs_edges(G, source)}"
 },
 {
  "docstring": "Returns all nodes having a path to `source` in `G`.\n\nParameters\n----------\nG : NetworkX Graph\nsource : node in `G`\n\nReturns\n-------\nset()\n    The ancestors of `source` in `G`\n\nRaises\n------\nNetworkXError\n    If node `source` is not in `G`.\n\nExamples\n--------\n>>> DG = nx.path_graph(5, create_using=nx.DiGraph)\n>>> sorted(nx.ancestors(DG, 2))\n[0, 1]\n\nThe `source` node is not an ancestor of itself, but can be included manually:\n\n>>> sorted(nx.ancestors(DG, 2) | {2})\n[0, 1, 2]\n\n",
  "code": "@nx._dispatch\ndef ancestors(G, source):\n    return {child for parent, child in nx.bfs_edges(G, source, reverse=True)}"
 },
 {
  "docstring": "Decides whether the directed graph has a cycle.",
  "code": "@nx._dispatch\ndef has_cycle(G):\n    try:\n        deque(topological_sort(G), maxlen=0)\n    except nx.NetworkXUnfeasible:\n        return True\n    else:\n        return False"
 },
 {
  "docstring": "Returns True if the graph `G` is a directed acyclic graph (DAG) or\nFalse if not.\n\nParameters\n----------\nG : NetworkX graph\n\nReturns\n-------\nbool\n    True if `G` is a DAG, False otherwise\n\nExamples\n--------\nUndirected graph::\n\n    >>> G = nx.Graph([(1, 2), (2, 3)])\n    >>> nx.is_directed_acyclic_graph(G)\n    False\n\nDirected graph with cycle::\n\n    >>> G = nx.DiGraph([(1, 2), (2, 3), (3, 1)])\n    >>> nx.is_directed_acyclic_graph(G)\n    False\n\nDirected acyclic graph::\n\n    >>> G = nx.DiGraph([(1, 2), (2, 3)])\n    >>> nx.is_directed_acyclic_graph(G)\n    True\n\n",
  "code": "@nx._dispatch\ndef is_directed_acyclic_graph(G):\n    return G.is_directed() and (not has_cycle(G))"
 },
 {
  "docstring": "Stratifies a DAG into generations.\n\nA topological generation is node collection in which ancestors of a node in each\ngeneration are guaranteed to be in a previous generation, and any descendants of\na node are guaranteed to be in a following generation. Nodes are guaranteed to\nbe in the earliest possible generation that they can belong to.\n\nParameters\n----------\nG : NetworkX digraph\n    A directed acyclic graph (DAG)\n\nYields\n------\nsets of nodes\n    Yields sets of nodes representing each generation.\n\nRaises\n------\nNetworkXError\n    Generations are defined for directed graphs only. If the graph\n    `G` is undirected, a :exc:`NetworkXError` is raised.\n\nNetworkXUnfeasible\n    If `G` is not a directed acyclic graph (DAG) no topological generations\n    exist and a :exc:`NetworkXUnfeasible` exception is raised.  This can also\n    be raised if `G` is changed while the returned iterator is being processed\n\nRuntimeError\n    If `G` is changed while the returned iterator is being processed.\n\nExamples\n--------\n>>> DG = nx.DiGraph([(2, 1), (3, 1)])\n>>> [sorted(generation) for generation in nx.topological_generations(DG)]\n[[2, 3], [1]]\n\n",
  "code": "@nx._dispatch\ndef topological_generations(G):\n    if not G.is_directed():\n        raise nx.NetworkXError('Topological sort not defined on undirected graphs.')\n    multigraph = G.is_multigraph()\n    indegree_map = {v: d for v, d in G.in_degree() if d > 0}\n    zero_indegree = [v for v, d in G.in_degree() if d == 0]\n    while zero_indegree:\n        this_generation = zero_indegree\n        zero_indegree = []\n        for node in this_generation:\n            if node not in G:\n                raise RuntimeError('Graph changed during iteration')\n            for child in G.neighbors(node):\n                try:\n                    indegree_map[child] -= len(G[node][child]) if multigraph else 1\n                except KeyError as err:\n                    raise RuntimeError('Graph changed during iteration') from err\n                if indegree_map[child] == 0:\n                    zero_indegree.append(child)\n                    del indegree_map[child]\n        yield this_generation\n    if indegree_map:\n        raise nx.NetworkXUnfeasible('Graph contains a cycle or graph changed during iteration')"
 },
 {
  "docstring": "Returns a generator of nodes in topologically sorted order.\n\nA topological sort is a nonunique permutation of the nodes of a\ndirected graph such that an edge from u to v implies that u\nappears before v in the topological sort order. This ordering is\nvalid only if the graph has no directed cycles.\n\nParameters\n----------\nG : NetworkX digraph\n    A directed acyclic graph (DAG)\n\nYields\n------\nnodes\n    Yields the nodes in topological sorted order.\n\nRaises\n------\nNetworkXError\n    Topological sort is defined for directed graphs only. If the graph `G`\n    is undirected, a :exc:`NetworkXError` is raised.\n\nNetworkXUnfeasible\n    If `G` is not a directed acyclic graph (DAG) no topological sort exists\n    and a :exc:`NetworkXUnfeasible` exception is raised.  This can also be\n    raised if `G` is changed while the returned iterator is being processed\n\nRuntimeError\n    If `G` is changed while the returned iterator is being processed.\n\nExamples\n--------\nTo get the reverse order of the topological sort:\n\n>>> DG = nx.DiGraph([(1, 2), (2, 3)])\n>>> list(reversed(list(nx.topological_sort(DG))))\n[3, 2, 1]\n\nIf your DiGraph naturally has the edges representing tasks/inputs\nand nodes representing people/processes that initiate tasks, then\ntopological_sort is not quite what you need. You will have to change\nthe tasks to nodes with dependence reflected by edges. The result is\na kind of topological sort of the edges. This can be done\nwith :func:`networkx.line_graph` as follows:\n\n>>> list(nx.topological_sort(nx.line_graph(DG)))\n[(1, 2), (2, 3)]\n\n",
  "code": "@nx._dispatch\ndef topological_sort(G):\n    for generation in nx.topological_generations(G):\n        yield from generation"
 },
 {
  "docstring": "Generate the nodes in the unique lexicographical topological sort order.\n\nGenerates a unique ordering of nodes by first sorting topologically (for which there are often\nmultiple valid orderings) and then additionally by sorting lexicographically.\n\nA topological sort arranges the nodes of a directed graph so that the\nupstream node of each directed edge precedes the downstream node.\nIt is always possible to find a solution for directed graphs that have no cycles.\nThere may be more than one valid solution.\n\nLexicographical sorting is just sorting alphabetically. It is used here to break ties in the\ntopological sort and to determine a single, unique ordering.  This can be useful in comparing\nsort results.\n\nThe lexicographical order can be customized by providing a function to the `key=` parameter.\nThe definition of the key function is the same as used in python's built-in `sort()`.\nThe function takes a single argument and returns a key to use for sorting purposes.\n\nLexicographical sorting can fail if the node names are un-sortable. See the example below.\nThe solution is to provide a function to the `key=` argument that returns sortable keys.\n\n\nParameters\n----------\nG : NetworkX digraph\n    A directed acyclic graph (DAG)\n\nkey : function, optional\n    A function of one argument that converts a node name to a comparison key.\n    It defines and resolves ambiguities in the sort order.  Defaults to the identity function.\n\nYields\n------\nnodes\n    Yields the nodes of G in lexicographical topological sort order.\n\nRaises\n------\nNetworkXError\n    Topological sort is defined for directed graphs only. If the graph `G`\n    is undirected, a :exc:`NetworkXError` is raised.\n\nNetworkXUnfeasible\n    If `G` is not a directed acyclic graph (DAG) no topological sort exists\n    and a :exc:`NetworkXUnfeasible` exception is raised.  This can also be\n    raised if `G` is changed while the returned iterator is being processed\n\nRuntimeError\n    If `G` is changed while the returned iterator is being processed.\n\nTypeError\n    Results from un-sortable node names.\n    Consider using `key=` parameter to resolve ambiguities in the sort order.\n\nExamples\n--------\n>>> DG = nx.DiGraph([(2, 1), (2, 5), (1, 3), (1, 4), (5, 4)])\n>>> list(nx.lexicographical_topological_sort(DG))\n[2, 1, 3, 5, 4]\n>>> list(nx.lexicographical_topological_sort(DG, key=lambda x: -x))\n[2, 5, 1, 4, 3]\n\nThe sort will fail for any graph with integer and string nodes. Comparison of integer to strings\nis not defined in python.  Is 3 greater or less than 'red'?\n\n>>> DG = nx.DiGraph([(1, 'red'), (3, 'red'), (1, 'green'), (2, 'blue')])\n>>> list(nx.lexicographical_topological_sort(DG))\nTraceback (most recent call last):\n...\nTypeError: '<' not supported between instances of 'str' and 'int'\n...\n\nIncomparable nodes can be resolved using a `key` function. This example function\nallows comparison of integers and strings by returning a tuple where the first\nelement is True for `str`, False otherwise. The second element is the node name.\nThis groups the strings and integers separately so they can be compared only among themselves.\n\n>>> key = lambda node: (isinstance(node, str), node)\n>>> list(nx.lexicographical_topological_sort(DG, key=key))\n[1, 2, 3, 'blue', 'green', 'red']\n\n",
  "code": "@nx._dispatch\ndef lexicographical_topological_sort(G, key=None):\n    if not G.is_directed():\n        msg = 'Topological sort not defined on undirected graphs.'\n        raise nx.NetworkXError(msg)\n    if key is None:\n\n        def key(node):\n            return node\n    nodeid_map = {n: i for i, n in enumerate(G)}\n\n    def create_tuple(node):\n        return (key(node), nodeid_map[node], node)\n    indegree_map = {v: d for v, d in G.in_degree() if d > 0}\n    zero_indegree = [create_tuple(v) for v, d in G.in_degree() if d == 0]\n    heapq.heapify(zero_indegree)\n    while zero_indegree:\n        _, _, node = heapq.heappop(zero_indegree)\n        if node not in G:\n            raise RuntimeError('Graph changed during iteration')\n        for _, child in G.edges(node):\n            try:\n                indegree_map[child] -= 1\n            except KeyError as err:\n                raise RuntimeError('Graph changed during iteration') from err\n            if indegree_map[child] == 0:\n                try:\n                    heapq.heappush(zero_indegree, create_tuple(child))\n                except TypeError as err:\n                    raise TypeError(f'{err}\\nConsider using `key=` parameter to resolve ambiguities in the sort order.')\n                del indegree_map[child]\n        yield node\n    if indegree_map:\n        msg = 'Graph contains a cycle or graph changed during iteration'\n        raise nx.NetworkXUnfeasible(msg)"
 },
 {
  "docstring": "Returns a generator of _all_ topological sorts of the directed graph G.\n\nA topological sort is a nonunique permutation of the nodes such that an\nedge from u to v implies that u appears before v in the topological sort\norder.\n\nParameters\n----------\nG : NetworkX DiGraph\n    A directed graph\n\nYields\n------\ntopological_sort_order : list\n    a list of nodes in `G`, representing one of the topological sort orders\n\nRaises\n------\nNetworkXNotImplemented\n    If `G` is not directed\nNetworkXUnfeasible\n    If `G` is not acyclic\n\nExamples\n--------\nTo enumerate all topological sorts of directed graph:\n\n>>> DG = nx.DiGraph([(1, 2), (2, 3), (2, 4)])\n>>> list(nx.all_topological_sorts(DG))\n[[1, 2, 4, 3], [1, 2, 3, 4]]\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef all_topological_sorts(G):\n    if not G.is_directed():\n        raise nx.NetworkXError('Topological sort not defined on undirected graphs.')\n    count = dict(G.in_degree())\n    D = deque([v for v, d in G.in_degree() if d == 0])\n    bases = []\n    current_sort = []\n    while True:\n        assert all((count[v] == 0 for v in D))\n        if len(current_sort) == len(G):\n            yield list(current_sort)\n            while len(current_sort) > 0:\n                assert len(bases) == len(current_sort)\n                q = current_sort.pop()\n                for _, j in G.out_edges(q):\n                    count[j] += 1\n                    assert count[j] >= 0\n                while len(D) > 0 and count[D[-1]] > 0:\n                    D.pop()\n                D.appendleft(q)\n                if D[-1] == bases[-1]:\n                    bases.pop()\n                else:\n                    break\n        else:\n            if len(D) == 0:\n                raise nx.NetworkXUnfeasible('Graph contains a cycle.')\n            q = D.pop()\n            for _, j in G.out_edges(q):\n                count[j] -= 1\n                assert count[j] >= 0\n                if count[j] == 0:\n                    D.append(j)\n            current_sort.append(q)\n            if len(bases) < len(current_sort):\n                bases.append(q)\n        if len(bases) == 0:\n            break"
 },
 {
  "docstring": "Returns True if `G` is aperiodic.\n\nA directed graph is aperiodic if there is no integer k > 1 that\ndivides the length of every cycle in the graph.\n\nParameters\n----------\nG : NetworkX DiGraph\n    A directed graph\n\nReturns\n-------\nbool\n    True if the graph is aperiodic False otherwise\n\nRaises\n------\nNetworkXError\n    If `G` is not directed\n\nExamples\n--------\nA graph consisting of one cycle, the length of which is 2. Therefore ``k = 2``\ndivides the length of every cycle in the graph and thus the graph\nis *not aperiodic*::\n\n    >>> DG = nx.DiGraph([(1, 2), (2, 1)])\n    >>> nx.is_aperiodic(DG)\n    False\n\nA graph consisting of two cycles: one of length 2 and the other of length 3.\nThe cycle lengths are coprime, so there is no single value of k where ``k > 1``\nthat divides each cycle length and therefore the graph is *aperiodic*::\n\n    >>> DG = nx.DiGraph([(1, 2), (2, 3), (3, 1), (1, 4), (4, 1)])\n    >>> nx.is_aperiodic(DG)\n    True\n\nA graph consisting of two cycles: one of length 2 and the other of length 4.\nThe lengths of the cycles share a common factor ``k = 2``, and therefore\nthe graph is *not aperiodic*::\n\n    >>> DG = nx.DiGraph([(1, 2), (2, 1), (3, 4), (4, 5), (5, 6), (6, 3)])\n    >>> nx.is_aperiodic(DG)\n    False\n\nAn acyclic graph, therefore the graph is *not aperiodic*::\n\n    >>> DG = nx.DiGraph([(1, 2), (2, 3)])\n    >>> nx.is_aperiodic(DG)\n    False\n\n",
  "code": "@nx._dispatch\ndef is_aperiodic(G):\n    if not G.is_directed():\n        raise nx.NetworkXError('is_aperiodic not defined for undirected graphs')\n    s = arbitrary_element(G)\n    levels = {s: 0}\n    this_level = [s]\n    g = 0\n    lev = 1\n    while this_level:\n        next_level = []\n        for u in this_level:\n            for v in G[u]:\n                if v in levels:\n                    g = gcd(g, levels[u] - levels[v] + 1)\n                else:\n                    next_level.append(v)\n                    levels[v] = lev\n        this_level = next_level\n        lev += 1\n    if len(levels) == len(G):\n        return g == 1\n    else:\n        return g == 1 and nx.is_aperiodic(G.subgraph(set(G) - set(levels)))"
 },
 {
  "docstring": "Returns transitive closure of a graph\n\nThe transitive closure of G = (V,E) is a graph G+ = (V,E+) such that\nfor all v, w in V there is an edge (v, w) in E+ if and only if there\nis a path from v to w in G.\n\nHandling of paths from v to v has some flexibility within this definition.\nA reflexive transitive closure creates a self-loop for the path\nfrom v to v of length 0. The usual transitive closure creates a\nself-loop only if a cycle exists (a path from v to v with length > 0).\nWe also allow an option for no self-loops.\n\nParameters\n----------\nG : NetworkX Graph\n    A directed/undirected graph/multigraph.\nreflexive : Bool or None, optional (default: False)\n    Determines when cycles create self-loops in the Transitive Closure.\n    If True, trivial cycles (length 0) create self-loops. The result\n    is a reflexive transitive closure of G.\n    If False (the default) non-trivial cycles create self-loops.\n    If None, self-loops are not created.\n\nReturns\n-------\nNetworkX graph\n    The transitive closure of `G`\n\nRaises\n------\nNetworkXError\n    If `reflexive` not in `{None, True, False}`\n\nExamples\n--------\nThe treatment of trivial (i.e. length 0) cycles is controlled by the\n`reflexive` parameter.\n\nTrivial (i.e. length 0) cycles do not create self-loops when\n``reflexive=False`` (the default)::\n\n    >>> DG = nx.DiGraph([(1, 2), (2, 3)])\n    >>> TC = nx.transitive_closure(DG, reflexive=False)\n    >>> TC.edges()\n    OutEdgeView([(1, 2), (1, 3), (2, 3)])\n\nHowever, nontrivial (i.e. length greater than 0) cycles create self-loops\nwhen ``reflexive=False`` (the default)::\n\n    >>> DG = nx.DiGraph([(1, 2), (2, 3), (3, 1)])\n    >>> TC = nx.transitive_closure(DG, reflexive=False)\n    >>> TC.edges()\n    OutEdgeView([(1, 2), (1, 3), (1, 1), (2, 3), (2, 1), (2, 2), (3, 1), (3, 2), (3, 3)])\n\nTrivial cycles (length 0) create self-loops when ``reflexive=True``::\n\n    >>> DG = nx.DiGraph([(1, 2), (2, 3)])\n    >>> TC = nx.transitive_closure(DG, reflexive=True)\n    >>> TC.edges()\n    OutEdgeView([(1, 2), (1, 1), (1, 3), (2, 3), (2, 2), (3, 3)])\n\nAnd the third option is not to create self-loops at all when ``reflexive=None``::\n\n    >>> DG = nx.DiGraph([(1, 2), (2, 3), (3, 1)])\n    >>> TC = nx.transitive_closure(DG, reflexive=None)\n    >>> TC.edges()\n    OutEdgeView([(1, 2), (1, 3), (2, 3), (2, 1), (3, 1), (3, 2)])\n\n",
  "code": "@nx._dispatch(preserve_all_attrs=True)\ndef transitive_closure(G, reflexive=False):\n    TC = G.copy()\n    if reflexive not in {None, True, False}:\n        raise nx.NetworkXError('Incorrect value for the parameter `reflexive`')\n    for v in G:\n        if reflexive is None:\n            TC.add_edges_from(((v, u) for u in nx.descendants(G, v) if u not in TC[v]))\n        elif reflexive is True:\n            TC.add_edges_from(((v, u) for u in nx.descendants(G, v) | {v} if u not in TC[v]))\n        elif reflexive is False:\n            TC.add_edges_from(((v, e[1]) for e in nx.edge_bfs(G, v) if e[1] not in TC[v]))\n    return TC"
 },
 {
  "docstring": "Returns the transitive closure of a directed acyclic graph.\n\nThis function is faster than the function `transitive_closure`, but fails\nif the graph has a cycle.\n\nThe transitive closure of G = (V,E) is a graph G+ = (V,E+) such that\nfor all v, w in V there is an edge (v, w) in E+ if and only if there\nis a non-null path from v to w in G.\n\nParameters\n----------\nG : NetworkX DiGraph\n    A directed acyclic graph (DAG)\n\ntopo_order: list or tuple, optional\n    A topological order for G (if None, the function will compute one)\n\nReturns\n-------\nNetworkX DiGraph\n    The transitive closure of `G`\n\nRaises\n------\nNetworkXNotImplemented\n    If `G` is not directed\nNetworkXUnfeasible\n    If `G` has a cycle\n\nExamples\n--------\n>>> DG = nx.DiGraph([(1, 2), (2, 3)])\n>>> TC = nx.transitive_closure_dag(DG)\n>>> TC.edges()\nOutEdgeView([(1, 2), (1, 3), (2, 3)])\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch(preserve_all_attrs=True)\ndef transitive_closure_dag(G, topo_order=None):\n    if topo_order is None:\n        topo_order = list(topological_sort(G))\n    TC = G.copy()\n    for v in reversed(topo_order):\n        TC.add_edges_from(((v, u) for u in nx.descendants_at_distance(TC, v, 2)))\n    return TC"
 },
 {
  "docstring": "Returns transitive reduction of a directed graph\n\nThe transitive reduction of G = (V,E) is a graph G- = (V,E-) such that\nfor all v,w in V there is an edge (v,w) in E- if and only if (v,w) is\nin E and there is no path from v to w in G with length greater than 1.\n\nParameters\n----------\nG : NetworkX DiGraph\n    A directed acyclic graph (DAG)\n\nReturns\n-------\nNetworkX DiGraph\n    The transitive reduction of `G`\n\nRaises\n------\nNetworkXError\n    If `G` is not a directed acyclic graph (DAG) transitive reduction is\n    not uniquely defined and a :exc:`NetworkXError` exception is raised.\n\nExamples\n--------\nTo perform transitive reduction on a DiGraph:\n\n>>> DG = nx.DiGraph([(1, 2), (2, 3), (1, 3)])\n>>> TR = nx.transitive_reduction(DG)\n>>> list(TR.edges)\n[(1, 2), (2, 3)]\n\nTo avoid unnecessary data copies, this implementation does not return a\nDiGraph with node/edge data.\nTo perform transitive reduction on a DiGraph and transfer node/edge data:\n\n>>> DG = nx.DiGraph()\n>>> DG.add_edges_from([(1, 2), (2, 3), (1, 3)], color='red')\n>>> TR = nx.transitive_reduction(DG)\n>>> TR.add_nodes_from(DG.nodes(data=True))\n>>> TR.add_edges_from((u, v, DG.edges[u, v]) for u, v in TR.edges)\n>>> list(TR.edges(data=True))\n[(1, 2, {'color': 'red'}), (2, 3, {'color': 'red'})]\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef transitive_reduction(G):\n    if not is_directed_acyclic_graph(G):\n        msg = 'Directed Acyclic Graph required for transitive_reduction'\n        raise nx.NetworkXError(msg)\n    TR = nx.DiGraph()\n    TR.add_nodes_from(G.nodes())\n    descendants = {}\n    check_count = dict(G.in_degree)\n    for u in G:\n        u_nbrs = set(G[u])\n        for v in G[u]:\n            if v in u_nbrs:\n                if v not in descendants:\n                    descendants[v] = {y for x, y in nx.dfs_edges(G, v)}\n                u_nbrs -= descendants[v]\n            check_count[v] -= 1\n            if check_count[v] == 0:\n                del descendants[v]\n        TR.add_edges_from(((u, v) for v in u_nbrs))\n    return TR"
 },
 {
  "docstring": "Generates antichains from a directed acyclic graph (DAG).\n\nAn antichain is a subset of a partially ordered set such that any\ntwo elements in the subset are incomparable.\n\nParameters\n----------\nG : NetworkX DiGraph\n    A directed acyclic graph (DAG)\n\ntopo_order: list or tuple, optional\n    A topological order for G (if None, the function will compute one)\n\nYields\n------\nantichain : list\n    a list of nodes in `G` representing an antichain\n\nRaises\n------\nNetworkXNotImplemented\n    If `G` is not directed\n\nNetworkXUnfeasible\n    If `G` contains a cycle\n\nExamples\n--------\n>>> DG = nx.DiGraph([(1, 2), (1, 3)])\n>>> list(nx.antichains(DG))\n[[], [3], [2], [2, 3], [1]]\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef antichains(G, topo_order=None):\n    if topo_order is None:\n        topo_order = list(nx.topological_sort(G))\n    TC = nx.transitive_closure_dag(G, topo_order)\n    antichains_stacks = [([], list(reversed(topo_order)))]\n    while antichains_stacks:\n        antichain, stack = antichains_stacks.pop()\n        yield antichain\n        while stack:\n            x = stack.pop()\n            new_antichain = antichain + [x]\n            new_stack = [t for t in stack if not (t in TC[x] or x in TC[t])]\n            antichains_stacks.append((new_antichain, new_stack))"
 },
 {
  "docstring": "Returns the longest path in a directed acyclic graph (DAG).\n\nIf `G` has edges with `weight` attribute the edge data are used as\nweight values.\n\nParameters\n----------\nG : NetworkX DiGraph\n    A directed acyclic graph (DAG)\n\nweight : str, optional\n    Edge data key to use for weight\n\ndefault_weight : int, optional\n    The weight of edges that do not have a weight attribute\n\ntopo_order: list or tuple, optional\n    A topological order for `G` (if None, the function will compute one)\n\nReturns\n-------\nlist\n    Longest path\n\nRaises\n------\nNetworkXNotImplemented\n    If `G` is not directed\n\nExamples\n--------\n>>> DG = nx.DiGraph([(0, 1, {'cost':1}), (1, 2, {'cost':1}), (0, 2, {'cost':42})])\n>>> list(nx.all_simple_paths(DG, 0, 2))\n[[0, 1, 2], [0, 2]]\n>>> nx.dag_longest_path(DG)\n[0, 1, 2]\n>>> nx.dag_longest_path(DG, weight=\"cost\")\n[0, 2]\n\nIn the case where multiple valid topological orderings exist, `topo_order`\ncan be used to specify a specific ordering:\n\n>>> DG = nx.DiGraph([(0, 1), (0, 2)])\n>>> sorted(nx.all_topological_sorts(DG))  # Valid topological orderings\n[[0, 1, 2], [0, 2, 1]]\n>>> nx.dag_longest_path(DG, topo_order=[0, 1, 2])\n[0, 1]\n>>> nx.dag_longest_path(DG, topo_order=[0, 2, 1])\n[0, 2]\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch(edge_attrs={'weight': 'default_weight'})\ndef dag_longest_path(G, weight='weight', default_weight=1, topo_order=None):\n    if not G:\n        return []\n    if topo_order is None:\n        topo_order = nx.topological_sort(G)\n    dist = {}\n    for v in topo_order:\n        us = [(dist[u][0] + (max(data.values(), key=lambda x: x.get(weight, default_weight)) if G.is_multigraph() else data).get(weight, default_weight), u) for u, data in G.pred[v].items()]\n        maxu = max(us, key=lambda x: x[0]) if us else (0, v)\n        dist[v] = maxu if maxu[0] >= 0 else (0, v)\n    u = None\n    v = max(dist, key=lambda x: dist[x][0])\n    path = []\n    while u != v:\n        path.append(v)\n        u = v\n        v = dist[v][1]\n    path.reverse()\n    return path"
 },
 {
  "docstring": "Returns the longest path length in a DAG\n\nParameters\n----------\nG : NetworkX DiGraph\n    A directed acyclic graph (DAG)\n\nweight : string, optional\n    Edge data key to use for weight\n\ndefault_weight : int, optional\n    The weight of edges that do not have a weight attribute\n\nReturns\n-------\nint\n    Longest path length\n\nRaises\n------\nNetworkXNotImplemented\n    If `G` is not directed\n\nExamples\n--------\n>>> DG = nx.DiGraph([(0, 1, {'cost':1}), (1, 2, {'cost':1}), (0, 2, {'cost':42})])\n>>> list(nx.all_simple_paths(DG, 0, 2))\n[[0, 1, 2], [0, 2]]\n>>> nx.dag_longest_path_length(DG)\n2\n>>> nx.dag_longest_path_length(DG, weight=\"cost\")\n42\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch(edge_attrs={'weight': 'default_weight'})\ndef dag_longest_path_length(G, weight='weight', default_weight=1):\n    path = nx.dag_longest_path(G, weight, default_weight)\n    path_length = 0\n    if G.is_multigraph():\n        for u, v in pairwise(path):\n            i = max(G[u][v], key=lambda x: G[u][v][x].get(weight, default_weight))\n            path_length += G[u][v][i].get(weight, default_weight)\n    else:\n        for u, v in pairwise(path):\n            path_length += G[u][v].get(weight, default_weight)\n    return path_length"
 },
 {
  "docstring": "Yields root-to-leaf paths in a directed acyclic graph.\n\n`G` must be a directed acyclic graph. If not, the behavior of this\nfunction is undefined. A \"root\" in this graph is a node of in-degree\nzero and a \"leaf\" a node of out-degree zero.\n\nWhen invoked, this function iterates over each path from any root to\nany leaf. A path is a list of nodes.",
  "code": "@nx._dispatch\ndef root_to_leaf_paths(G):\n    roots = (v for v, d in G.in_degree() if d == 0)\n    leaves = (v for v, d in G.out_degree() if d == 0)\n    all_paths = partial(nx.all_simple_paths, G)\n    return chaini(starmap(all_paths, product(roots, leaves)))"
 },
 {
  "docstring": "Returns a branching representing all (overlapping) paths from\nroot nodes to leaf nodes in the given directed acyclic graph.\n\nAs described in :mod:`networkx.algorithms.tree.recognition`, a\n*branching* is a directed forest in which each node has at most one\nparent. In other words, a branching is a disjoint union of\n*arborescences*. For this function, each node of in-degree zero in\n`G` becomes a root of one of the arborescences, and there will be\none leaf node for each distinct path from that root to a leaf node\nin `G`.\n\nEach node `v` in `G` with *k* parents becomes *k* distinct nodes in\nthe returned branching, one for each parent, and the sub-DAG rooted\nat `v` is duplicated for each copy. The algorithm then recurses on\nthe children of each copy of `v`.\n\nParameters\n----------\nG : NetworkX graph\n    A directed acyclic graph.\n\nReturns\n-------\nDiGraph\n    The branching in which there is a bijection between root-to-leaf\n    paths in `G` (in which multiple paths may share the same leaf)\n    and root-to-leaf paths in the branching (in which there is a\n    unique path from a root to a leaf).\n\n    Each node has an attribute 'source' whose value is the original\n    node to which this node corresponds. No other graph, node, or\n    edge attributes are copied into this new graph.\n\nRaises\n------\nNetworkXNotImplemented\n    If `G` is not directed, or if `G` is a multigraph.\n\nHasACycle\n    If `G` is not acyclic.\n\nExamples\n--------\nTo examine which nodes in the returned branching were produced by\nwhich original node in the directed acyclic graph, we can collect\nthe mapping from source node to new nodes into a dictionary. For\nexample, consider the directed diamond graph::\n\n    >>> from collections import defaultdict\n    >>> from operator import itemgetter\n    >>>\n    >>> G = nx.DiGraph(nx.utils.pairwise(\"abd\"))\n    >>> G.add_edges_from(nx.utils.pairwise(\"acd\"))\n    >>> B = nx.dag_to_branching(G)\n    >>>\n    >>> sources = defaultdict(set)\n    >>> for v, source in B.nodes(data=\"source\"):\n    ...     sources[source].add(v)\n    >>> len(sources[\"a\"])\n    1\n    >>> len(sources[\"d\"])\n    2\n\nTo copy node attributes from the original graph to the new graph,\nyou can use a dictionary like the one constructed in the above\nexample::\n\n    >>> for source, nodes in sources.items():\n    ...     for v in nodes:\n    ...         B.nodes[v].update(G.nodes[source])\n\n",
  "code": "@not_implemented_for('multigraph')\n@not_implemented_for('undirected')\n@nx._dispatch\ndef dag_to_branching(G):\n    if has_cycle(G):\n        msg = 'dag_to_branching is only defined for acyclic graphs'\n        raise nx.HasACycle(msg)\n    paths = root_to_leaf_paths(G)\n    B = nx.prefix_tree(paths)\n    B.remove_node(0)\n    B.remove_node(-1)\n    return B"
 },
 {
  "docstring": "Iterate through the graph to compute all v-structures.\n\nV-structures are triples in the directed graph where\ntwo parent nodes point to the same child and the two parent nodes\nare not adjacent.\n\nParameters\n----------\nG : graph\n    A networkx DiGraph.\n\nReturns\n-------\nvstructs : iterator of tuples\n    The v structures within the graph. Each v structure is a 3-tuple with the\n    parent, collider, and other parent.\n\nExamples\n--------\n>>> G = nx.DiGraph()\n>>> G.add_edges_from([(1, 2), (0, 5), (3, 1), (2, 4), (3, 1), (4, 5), (1, 5)])\n>>> sorted(nx.compute_v_structures(G))\n[(0, 5, 1), (0, 5, 4), (1, 5, 4)]\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef compute_v_structures(G):\n    for collider, preds in G.pred.items():\n        for common_parents in combinations(preds, r=2):\n            common_parents = sorted(common_parents)\n            yield (common_parents[0], collider, common_parents[1])"
 },
 {
  "docstring": "Compute requested extreme distance metric of undirected graph G\n\nComputation is based on smart lower and upper bounds, and in practice\nlinear in the number of nodes, rather than quadratic (except for some\nborder cases such as complete graphs or circle shaped graphs).\n\nParameters\n----------\nG : NetworkX graph\n   An undirected graph\n\ncompute : string denoting the requesting metric\n   \"diameter\" for the maximal eccentricity value,\n   \"radius\" for the minimal eccentricity value,\n   \"periphery\" for the set of nodes with eccentricity equal to the diameter,\n   \"center\" for the set of nodes with eccentricity equal to the radius,\n   \"eccentricities\" for the maximum distance from each node to all other nodes in G\n\nweight : string, function, or None\n    If this is a string, then edge weights will be accessed via the\n    edge attribute with this key (that is, the weight of the edge\n    joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\n    such edge attribute exists, the weight of the edge is assumed to\n    be one.\n\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly three\n    positional arguments: the two endpoints of an edge and the\n    dictionary of edge attributes for that edge. The function must\n    return a number.\n\n    If this is None, every edge has weight/distance/cost 1.\n\n    Weights stored as floating point values can lead to small round-off\n    errors in distances. Use integer weights to avoid this.\n\n    Weights should be positive, since they are distances.\n\nReturns\n-------\nvalue : value of the requested metric\n   int for \"diameter\" and \"radius\" or\n   list of nodes for \"center\" and \"periphery\" or\n   dictionary of eccentricity values keyed by node for \"eccentricities\"\n\nRaises\n------\nNetworkXError\n    If the graph consists of multiple components\nValueError\n    If `compute` is not one of \"diameter\", \"radius\", \"periphery\", \"center\", or \"eccentricities\".\n\n",
  "code": "def _extrema_bounding(G, compute='diameter', weight=None):\n    degrees = dict(G.degree())\n    minlowernode = max(degrees, key=degrees.get)\n    N = len(degrees)\n    high = False\n    ecc_lower = dict.fromkeys(G, 0)\n    ecc_upper = dict.fromkeys(G, N)\n    candidates = set(G)\n    minlower = N\n    maxlower = 0\n    minupper = N\n    maxupper = 0\n    while candidates:\n        if high:\n            current = maxuppernode\n        else:\n            current = minlowernode\n        high = not high\n        dist = nx.shortest_path_length(G, source=current, weight=weight)\n        if len(dist) != N:\n            msg = 'Cannot compute metric because graph is not connected.'\n            raise nx.NetworkXError(msg)\n        current_ecc = max(dist.values())\n        maxuppernode = None\n        minlowernode = None\n        for i in candidates:\n            d = dist[i]\n            ecc_lower[i] = low = max(ecc_lower[i], max(d, current_ecc - d))\n            ecc_upper[i] = upp = min(ecc_upper[i], current_ecc + d)\n            minlower = min(ecc_lower[i], minlower)\n            maxlower = max(ecc_lower[i], maxlower)\n            minupper = min(ecc_upper[i], minupper)\n            maxupper = max(ecc_upper[i], maxupper)\n        if compute == 'diameter':\n            ruled_out = {i for i in candidates if ecc_upper[i] <= maxlower and 2 * ecc_lower[i] >= maxupper}\n        elif compute == 'radius':\n            ruled_out = {i for i in candidates if ecc_lower[i] >= minupper and ecc_upper[i] + 1 <= 2 * minlower}\n        elif compute == 'periphery':\n            ruled_out = {i for i in candidates if ecc_upper[i] < maxlower and (maxlower == maxupper or ecc_lower[i] > maxupper)}\n        elif compute == 'center':\n            ruled_out = {i for i in candidates if ecc_lower[i] > minupper and (minlower == minupper or ecc_upper[i] + 1 < 2 * minlower)}\n        elif compute == 'eccentricities':\n            ruled_out = set()\n        else:\n            msg = \"compute must be one of 'diameter', 'radius', 'periphery', 'center', 'eccentricities'\"\n            raise ValueError(msg)\n        ruled_out.update((i for i in candidates if ecc_lower[i] == ecc_upper[i]))\n        candidates -= ruled_out\n        for i in candidates:\n            if minlowernode is None or (ecc_lower[i] == ecc_lower[minlowernode] and degrees[i] > degrees[minlowernode]) or ecc_lower[i] < ecc_lower[minlowernode]:\n                minlowernode = i\n            if maxuppernode is None or (ecc_upper[i] == ecc_upper[maxuppernode] and degrees[i] > degrees[maxuppernode]) or ecc_upper[i] > ecc_upper[maxuppernode]:\n                maxuppernode = i\n    if compute == 'diameter':\n        return maxlower\n    if compute == 'radius':\n        return minupper\n    if compute == 'periphery':\n        p = [v for v in G if ecc_lower[v] == maxlower]\n        return p\n    if compute == 'center':\n        c = [v for v in G if ecc_upper[v] == minupper]\n        return c\n    if compute == 'eccentricities':\n        return ecc_lower\n    return None"
 },
 {
  "docstring": "Returns the eccentricity of nodes in G.\n\nThe eccentricity of a node v is the maximum distance from v to\nall other nodes in G.\n\nParameters\n----------\nG : NetworkX graph\n   A graph\n\nv : node, optional\n   Return value of specified node\n\nsp : dict of dicts, optional\n   All pairs shortest path lengths as a dictionary of dictionaries\n\nweight : string, function, or None (default=None)\n    If this is a string, then edge weights will be accessed via the\n    edge attribute with this key (that is, the weight of the edge\n    joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\n    such edge attribute exists, the weight of the edge is assumed to\n    be one.\n\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly three\n    positional arguments: the two endpoints of an edge and the\n    dictionary of edge attributes for that edge. The function must\n    return a number.\n\n    If this is None, every edge has weight/distance/cost 1.\n\n    Weights stored as floating point values can lead to small round-off\n    errors in distances. Use integer weights to avoid this.\n\n    Weights should be positive, since they are distances.\n\nReturns\n-------\necc : dictionary\n   A dictionary of eccentricity values keyed by node.\n\nExamples\n--------\n>>> G = nx.Graph([(1, 2), (1, 3), (1, 4), (3, 4), (3, 5), (4, 5)])\n>>> dict(nx.eccentricity(G))\n{1: 2, 2: 3, 3: 2, 4: 2, 5: 3}\n\n>>> dict(nx.eccentricity(G, v=[1, 5]))  # This returns the eccentricity of node 1 & 5\n{1: 2, 5: 3}",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef eccentricity(G, v=None, sp=None, weight=None):\n    order = G.order()\n    e = {}\n    for n in G.nbunch_iter(v):\n        if sp is None:\n            length = nx.shortest_path_length(G, source=n, weight=weight)\n            L = len(length)\n        else:\n            try:\n                length = sp[n]\n                L = len(length)\n            except TypeError as err:\n                raise nx.NetworkXError('Format of \"sp\" is invalid.') from err\n        if L != order:\n            if G.is_directed():\n                msg = 'Found infinite path length because the digraph is not strongly connected'\n            else:\n                msg = 'Found infinite path length because the graph is not connected'\n            raise nx.NetworkXError(msg)\n        e[n] = max(length.values())\n    if v in G:\n        return e[v]\n    return e"
 },
 {
  "docstring": "Returns the diameter of the graph G.\n\nThe diameter is the maximum eccentricity.\n\nParameters\n----------\nG : NetworkX graph\n   A graph\n\ne : eccentricity dictionary, optional\n  A precomputed dictionary of eccentricities.\n\nweight : string, function, or None\n    If this is a string, then edge weights will be accessed via the\n    edge attribute with this key (that is, the weight of the edge\n    joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\n    such edge attribute exists, the weight of the edge is assumed to\n    be one.\n\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly three\n    positional arguments: the two endpoints of an edge and the\n    dictionary of edge attributes for that edge. The function must\n    return a number.\n\n    If this is None, every edge has weight/distance/cost 1.\n\n    Weights stored as floating point values can lead to small round-off\n    errors in distances. Use integer weights to avoid this.\n\n    Weights should be positive, since they are distances.\n\nReturns\n-------\nd : integer\n   Diameter of graph\n\nExamples\n--------\n>>> G = nx.Graph([(1, 2), (1, 3), (1, 4), (3, 4), (3, 5), (4, 5)])\n>>> nx.diameter(G)\n3\n\nSee Also\n--------\neccentricity",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef diameter(G, e=None, usebounds=False, weight=None):\n    if usebounds is True and e is None and (not G.is_directed()):\n        return _extrema_bounding(G, compute='diameter', weight=weight)\n    if e is None:\n        e = eccentricity(G, weight=weight)\n    return max(e.values())"
 },
 {
  "docstring": "Returns the periphery of the graph G.\n\nThe periphery is the set of nodes with eccentricity equal to the diameter.\n\nParameters\n----------\nG : NetworkX graph\n   A graph\n\ne : eccentricity dictionary, optional\n  A precomputed dictionary of eccentricities.\n\nweight : string, function, or None\n    If this is a string, then edge weights will be accessed via the\n    edge attribute with this key (that is, the weight of the edge\n    joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\n    such edge attribute exists, the weight of the edge is assumed to\n    be one.\n\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly three\n    positional arguments: the two endpoints of an edge and the\n    dictionary of edge attributes for that edge. The function must\n    return a number.\n\n    If this is None, every edge has weight/distance/cost 1.\n\n    Weights stored as floating point values can lead to small round-off\n    errors in distances. Use integer weights to avoid this.\n\n    Weights should be positive, since they are distances.\n\nReturns\n-------\np : list\n   List of nodes in periphery\n\nExamples\n--------\n>>> G = nx.Graph([(1, 2), (1, 3), (1, 4), (3, 4), (3, 5), (4, 5)])\n>>> nx.periphery(G)\n[2, 5]\n\nSee Also\n--------\nbarycenter\ncenter",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef periphery(G, e=None, usebounds=False, weight=None):\n    if usebounds is True and e is None and (not G.is_directed()):\n        return _extrema_bounding(G, compute='periphery', weight=weight)\n    if e is None:\n        e = eccentricity(G, weight=weight)\n    diameter = max(e.values())\n    p = [v for v in e if e[v] == diameter]\n    return p"
 },
 {
  "docstring": "Returns the radius of the graph G.\n\nThe radius is the minimum eccentricity.\n\nParameters\n----------\nG : NetworkX graph\n   A graph\n\ne : eccentricity dictionary, optional\n  A precomputed dictionary of eccentricities.\n\nweight : string, function, or None\n    If this is a string, then edge weights will be accessed via the\n    edge attribute with this key (that is, the weight of the edge\n    joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\n    such edge attribute exists, the weight of the edge is assumed to\n    be one.\n\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly three\n    positional arguments: the two endpoints of an edge and the\n    dictionary of edge attributes for that edge. The function must\n    return a number.\n\n    If this is None, every edge has weight/distance/cost 1.\n\n    Weights stored as floating point values can lead to small round-off\n    errors in distances. Use integer weights to avoid this.\n\n    Weights should be positive, since they are distances.\n\nReturns\n-------\nr : integer\n   Radius of graph\n\nExamples\n--------\n>>> G = nx.Graph([(1, 2), (1, 3), (1, 4), (3, 4), (3, 5), (4, 5)])\n>>> nx.radius(G)\n2",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef radius(G, e=None, usebounds=False, weight=None):\n    if usebounds is True and e is None and (not G.is_directed()):\n        return _extrema_bounding(G, compute='radius', weight=weight)\n    if e is None:\n        e = eccentricity(G, weight=weight)\n    return min(e.values())"
 },
 {
  "docstring": "Returns the center of the graph G.\n\nThe center is the set of nodes with eccentricity equal to radius.\n\nParameters\n----------\nG : NetworkX graph\n   A graph\n\ne : eccentricity dictionary, optional\n  A precomputed dictionary of eccentricities.\n\nweight : string, function, or None\n    If this is a string, then edge weights will be accessed via the\n    edge attribute with this key (that is, the weight of the edge\n    joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\n    such edge attribute exists, the weight of the edge is assumed to\n    be one.\n\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly three\n    positional arguments: the two endpoints of an edge and the\n    dictionary of edge attributes for that edge. The function must\n    return a number.\n\n    If this is None, every edge has weight/distance/cost 1.\n\n    Weights stored as floating point values can lead to small round-off\n    errors in distances. Use integer weights to avoid this.\n\n    Weights should be positive, since they are distances.\n\nReturns\n-------\nc : list\n   List of nodes in center\n\nExamples\n--------\n>>> G = nx.Graph([(1, 2), (1, 3), (1, 4), (3, 4), (3, 5), (4, 5)])\n>>> list(nx.center(G))\n[1, 3, 4]\n\nSee Also\n--------\nbarycenter\nperiphery",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef center(G, e=None, usebounds=False, weight=None):\n    if usebounds is True and e is None and (not G.is_directed()):\n        return _extrema_bounding(G, compute='center', weight=weight)\n    if e is None:\n        e = eccentricity(G, weight=weight)\n    radius = min(e.values())\n    p = [v for v in e if e[v] == radius]\n    return p"
 },
 {
  "docstring": "Calculate barycenter of a connected graph, optionally with edge weights.\n\nThe :dfn:`barycenter` a\n:func:`connected <networkx.algorithms.components.is_connected>` graph\n:math:`G` is the subgraph induced by the set of its nodes :math:`v`\nminimizing the objective function\n\n.. math::\n\n    \\sum_{u \\in V(G)} d_G(u, v),\n\nwhere :math:`d_G` is the (possibly weighted) :func:`path length\n<networkx.algorithms.shortest_paths.generic.shortest_path_length>`.\nThe barycenter is also called the :dfn:`median`. See [West01]_, p. 78.\n\nParameters\n----------\nG : :class:`networkx.Graph`\n    The connected graph :math:`G`.\nweight : :class:`str`, optional\n    Passed through to\n    :func:`~networkx.algorithms.shortest_paths.generic.shortest_path_length`.\nattr : :class:`str`, optional\n    If given, write the value of the objective function to each node's\n    `attr` attribute. Otherwise do not store the value.\nsp : dict of dicts, optional\n   All pairs shortest path lengths as a dictionary of dictionaries\n\nReturns\n-------\nlist\n    Nodes of `G` that induce the barycenter of `G`.\n\nRaises\n------\nNetworkXNoPath\n    If `G` is disconnected. `G` may appear disconnected to\n    :func:`barycenter` if `sp` is given but is missing shortest path\n    lengths for any pairs.\nValueError\n    If `sp` and `weight` are both given.\n\nExamples\n--------\n>>> G = nx.Graph([(1, 2), (1, 3), (1, 4), (3, 4), (3, 5), (4, 5)])\n>>> nx.barycenter(G)\n[1, 3, 4]\n\nSee Also\n--------\ncenter\nperiphery",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef barycenter(G, weight=None, attr=None, sp=None):\n    if sp is None:\n        sp = nx.shortest_path_length(G, weight=weight)\n    else:\n        sp = sp.items()\n        if weight is not None:\n            raise ValueError('Cannot use both sp, weight arguments together')\n    smallest, barycenter_vertices, n = (float('inf'), [], len(G))\n    for v, dists in sp:\n        if len(dists) < n:\n            raise nx.NetworkXNoPath(f'Input graph {G} is disconnected, so every induced subgraph has infinite barycentricity.')\n        barycentricity = sum(dists.values())\n        if attr is not None:\n            G.nodes[v][attr] = barycentricity\n        if barycentricity < smallest:\n            smallest = barycentricity\n            barycenter_vertices = [v]\n        elif barycentricity == smallest:\n            barycenter_vertices.append(v)\n    return barycenter_vertices"
 },
 {
  "docstring": "Returns the resistance distance between every pair of nodes on graph G.\n\nThe resistance distance between two nodes of a graph is akin to treating\nthe graph as a grid of resistors with a resistance equal to the provided\nweight [1]_, [2]_.\n\nIf weight is not provided, then a weight of 1 is used for all edges.\n\nIf two nodes are the same, the resistance distance is zero.\n\nParameters\n----------\nG : NetworkX graph\n   A graph\n\nnodeA : node or None, optional (default=None)\n  A node within graph G.\n  If None, compute resistance distance using all nodes as source nodes.\n\nnodeB : node or None, optional (default=None)\n  A node within graph G.\n  If None, compute resistance distance using all nodes as target nodes.\n\nweight : string or None, optional (default=None)\n   The edge data key used to compute the resistance distance.\n   If None, then each edge has weight 1.\n\ninvert_weight : boolean (default=True)\n    Proper calculation of resistance distance requires building the\n    Laplacian matrix with the reciprocal of the weight. Not required\n    if the weight is already inverted. Weight cannot be zero.\n\nReturns\n-------\nrd : dict or float\n   If `nodeA` and `nodeB` are given, resistance distance between `nodeA`\n   and `nodeB`. If `nodeA` or `nodeB` is unspecified (the default), a\n   dictionary of nodes with resistance distances as the value.\n\nRaises\n------\nNetworkXNotImplemented\n    If `G` is a directed graph.\n\nNetworkXError\n    If `G` is not connected, or contains no nodes,\n    or `nodeA` is not in `G` or `nodeB` is not in `G`.\n\nExamples\n--------\n>>> G = nx.Graph([(1, 2), (1, 3), (1, 4), (3, 4), (3, 5), (4, 5)])\n>>> round(nx.resistance_distance(G, 1, 3), 10)\n0.625\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch(edge_attrs='weight')\ndef resistance_distance(G, nodeA=None, nodeB=None, weight=None, invert_weight=True):\n    import numpy as np\n    if len(G) == 0:\n        raise nx.NetworkXError('Graph G must contain at least one node.')\n    if not nx.is_connected(G):\n        raise nx.NetworkXError('Graph G must be strongly connected.')\n    if nodeA is not None and nodeA not in G:\n        raise nx.NetworkXError('Node A is not in graph G.')\n    if nodeB is not None and nodeB not in G:\n        raise nx.NetworkXError('Node B is not in graph G.')\n    G = G.copy()\n    node_list = list(G)\n    if invert_weight and weight is not None:\n        if G.is_multigraph():\n            for u, v, k, d in G.edges(keys=True, data=True):\n                d[weight] = 1 / d[weight]\n        else:\n            for u, v, d in G.edges(data=True):\n                d[weight] = 1 / d[weight]\n    L = nx.laplacian_matrix(G, weight=weight).todense()\n    Linv = np.linalg.pinv(L, hermitian=True)\n    if nodeA is not None and nodeB is not None:\n        i = node_list.index(nodeA)\n        j = node_list.index(nodeB)\n        return Linv[i, i] + Linv[j, j] - Linv[i, j] - Linv[j, i]\n    elif nodeA is not None:\n        i = node_list.index(nodeA)\n        d = {}\n        for n in G:\n            j = node_list.index(n)\n            d[n] = Linv[i, i] + Linv[j, j] - Linv[i, j] - Linv[j, i]\n        return d\n    elif nodeB is not None:\n        j = node_list.index(nodeB)\n        d = {}\n        for n in G:\n            i = node_list.index(n)\n            d[n] = Linv[i, i] + Linv[j, j] - Linv[i, j] - Linv[j, i]\n        return d\n    else:\n        d = {}\n        for n in G:\n            i = node_list.index(n)\n            d[n] = {}\n            for n2 in G:\n                j = node_list.index(n2)\n                d[n][n2] = Linv[i, i] + Linv[j, j] - Linv[i, j] - Linv[j, i]\n        return d"
 },
 {
  "docstring": "Returns the Kemeny constant of the given graph.\n\nThe *Kemeny constant* (or Kemeny's constant) of a graph `G`\ncan be computed by regarding the graph as a Markov chain.\nThe Kemeny constant is then the expected number of time steps\nto transition from a starting state i to a random destination state\nsampled from the Markov chain's stationary distribution.\nThe Kemeny constant is independent of the chosen initial state [1]_.\n\nThe Kemeny constant measures the time needed for spreading\nacross a graph. Low values indicate a closely connected graph\nwhereas high values indicate a spread-out graph.\n\nIf weight is not provided, then a weight of 1 is used for all edges.\n\nSince `G` represents a Markov chain, the weights must be positive.\n\nParameters\n----------\nG : NetworkX graph\n\nweight : string or None, optional (default=None)\n   The edge data key used to compute the Kemeny constant.\n   If None, then each edge has weight 1.\n\nReturns\n-------\nK : float\n    The Kemeny constant of the graph `G`.\n\nRaises\n------\nNetworkXNotImplemented\n    If the graph `G` is directed.\n\nNetworkXError\n    If the graph `G` is not connected, or contains no nodes,\n    or has edges with negative weights.\n\nExamples\n--------\n>>> G = nx.complete_graph(5)\n>>> round(nx.kemeny_constant(G), 10)\n3.2\n\n",
  "code": "@nx.utils.not_implemented_for('directed')\n@nx._dispatch(edge_attrs='weight')\ndef kemeny_constant(G, *, weight=None):\n    import numpy as np\n    import scipy as sp\n    if len(G) == 0:\n        raise nx.NetworkXError('Graph G must contain at least one node.')\n    if not nx.is_connected(G):\n        raise nx.NetworkXError('Graph G must be connected.')\n    if nx.is_negatively_weighted(G, weight=weight):\n        raise nx.NetworkXError('The weights of graph G must be nonnegative.')\n    A = nx.adjacency_matrix(G, weight=weight)\n    n, m = A.shape\n    diags = A.sum(axis=1)\n    with np.errstate(divide='ignore'):\n        diags_sqrt = 1.0 / np.sqrt(diags)\n    diags_sqrt[np.isinf(diags_sqrt)] = 0\n    DH = sp.sparse.csr_array(sp.sparse.spdiags(diags_sqrt, 0, m, n, format='csr'))\n    H = DH @ (A @ DH)\n    eig = np.sort(sp.linalg.eigvalsh(H.todense()))\n    return np.sum(1 / (1 - eig[:-1]))"
 },
 {
  "docstring": "Returns True if the graph is distance regular, False otherwise.\n\nA connected graph G is distance-regular if for any nodes x,y\nand any integers i,j=0,1,...,d (where d is the graph\ndiameter), the number of vertices at distance i from x and\ndistance j from y depends only on i,j and the graph distance\nbetween x and y, independently of the choice of x and y.\n\nParameters\n----------\nG: Networkx graph (undirected)\n\nReturns\n-------\nbool\n  True if the graph is Distance Regular, False otherwise\n\nExamples\n--------\n>>> G = nx.hypercube_graph(6)\n>>> nx.is_distance_regular(G)\nTrue\n\nSee Also\n--------\nintersection_array, global_parameters\n\n",
  "code": "@nx._dispatch\ndef is_distance_regular(G):\n    try:\n        intersection_array(G)\n        return True\n    except nx.NetworkXError:\n        return False"
 },
 {
  "docstring": "Returns global parameters for a given intersection array.\n\nGiven a distance-regular graph G with integers b_i, c_i,i = 0,....,d\nsuch that for any 2 vertices x,y in G at a distance i=d(x,y), there\nare exactly c_i neighbors of y at a distance of i-1 from x and b_i\nneighbors of y at a distance of i+1 from x.\n\nThus, a distance regular graph has the global parameters,\n[[c_0,a_0,b_0],[c_1,a_1,b_1],......,[c_d,a_d,b_d]] for the\nintersection array  [b_0,b_1,.....b_{d-1};c_1,c_2,.....c_d]\nwhere a_i+b_i+c_i=k , k= degree of every vertex.\n\nParameters\n----------\nb : list\n\nc : list\n\nReturns\n-------\niterable\n   An iterable over three tuples.\n\nExamples\n--------\n>>> G = nx.dodecahedral_graph()\n>>> b, c = nx.intersection_array(G)\n>>> list(nx.global_parameters(b, c))\n[(0, 0, 3), (1, 0, 2), (1, 1, 1), (1, 1, 1), (2, 0, 1), (3, 0, 0)]\n\n",
  "code": "def global_parameters(b, c):\n    return ((y, b[0] - x - y, x) for x, y in zip(b + [0], [0] + c))"
 },
 {
  "docstring": "Returns the intersection array of a distance-regular graph.\n\nGiven a distance-regular graph G with integers b_i, c_i,i = 0,....,d\nsuch that for any 2 vertices x,y in G at a distance i=d(x,y), there\nare exactly c_i neighbors of y at a distance of i-1 from x and b_i\nneighbors of y at a distance of i+1 from x.\n\nA distance regular graph's intersection array is given by,\n[b_0,b_1,.....b_{d-1};c_1,c_2,.....c_d]\n\nParameters\n----------\nG: Networkx graph (undirected)\n\nReturns\n-------\nb,c: tuple of lists\n\nExamples\n--------\n>>> G = nx.icosahedral_graph()\n>>> nx.intersection_array(G)\n([5, 2, 1], [1, 2, 5])\n\n",
  "code": "@not_implemented_for('directed', 'multigraph')\n@nx._dispatch\ndef intersection_array(G):\n    degree = iter(G.degree())\n    _, k = next(degree)\n    for _, knext in degree:\n        if knext != k:\n            raise nx.NetworkXError('Graph is not distance regular.')\n        k = knext\n    path_length = dict(nx.all_pairs_shortest_path_length(G))\n    diameter = max((max(path_length[n].values()) for n in path_length))\n    bint = {}\n    cint = {}\n    for u in G:\n        for v in G:\n            try:\n                i = path_length[u][v]\n            except KeyError as err:\n                raise nx.NetworkXError('Graph is not distance regular.') from err\n            c = len([n for n in G[v] if path_length[n][u] == i - 1])\n            b = len([n for n in G[v] if path_length[n][u] == i + 1])\n            if cint.get(i, c) != c or bint.get(i, b) != b:\n                raise nx.NetworkXError('Graph is not distance regular')\n            bint[i] = b\n            cint[i] = c\n    return ([bint.get(j, 0) for j in range(diameter)], [cint.get(j + 1, 0) for j in range(diameter)])"
 },
 {
  "docstring": "Returns True if and only if the given graph is strongly\nregular.\n\nAn undirected graph is *strongly regular* if\n\n* it is regular,\n* each pair of adjacent vertices has the same number of neighbors in\n  common,\n* each pair of nonadjacent vertices has the same number of neighbors\n  in common.\n\nEach strongly regular graph is a distance-regular graph.\nConversely, if a distance-regular graph has diameter two, then it is\na strongly regular graph. For more information on distance-regular\ngraphs, see :func:`is_distance_regular`.\n\nParameters\n----------\nG : NetworkX graph\n    An undirected graph.\n\nReturns\n-------\nbool\n    Whether `G` is strongly regular.\n\nExamples\n--------\n\nThe cycle graph on five vertices is strongly regular. It is\ntwo-regular, each pair of adjacent vertices has no shared neighbors,\nand each pair of nonadjacent vertices has one shared neighbor::\n\n    >>> G = nx.cycle_graph(5)\n    >>> nx.is_strongly_regular(G)\n    True",
  "code": "@not_implemented_for('directed', 'multigraph')\n@nx._dispatch\ndef is_strongly_regular(G):\n    return is_distance_regular(G) and diameter(G) == 2"
 },
 {
  "docstring": "Returns the immediate dominators of all nodes of a directed graph.\n\nParameters\n----------\nG : a DiGraph or MultiDiGraph\n    The graph where dominance is to be computed.\n\nstart : node\n    The start node of dominance computation.\n\nReturns\n-------\nidom : dict keyed by nodes\n    A dict containing the immediate dominators of each node reachable from\n    `start`.\n\nRaises\n------\nNetworkXNotImplemented\n    If `G` is undirected.\n\nNetworkXError\n    If `start` is not in `G`.\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef immediate_dominators(G, start):\n    if start not in G:\n        raise nx.NetworkXError('start is not in G')\n    idom = {start: start}\n    order = list(nx.dfs_postorder_nodes(G, start))\n    dfn = {u: i for i, u in enumerate(order)}\n    order.pop()\n    order.reverse()\n\n    def intersect(u, v):\n        while u != v:\n            while dfn[u] < dfn[v]:\n                u = idom[u]\n            while dfn[u] > dfn[v]:\n                v = idom[v]\n        return u\n    changed = True\n    while changed:\n        changed = False\n        for u in order:\n            new_idom = reduce(intersect, (v for v in G.pred[u] if v in idom))\n            if u not in idom or idom[u] != new_idom:\n                idom[u] = new_idom\n                changed = True\n    return idom"
 },
 {
  "docstring": "Returns the dominance frontiers of all nodes of a directed graph.\n\nParameters\n----------\nG : a DiGraph or MultiDiGraph\n    The graph where dominance is to be computed.\n\nstart : node\n    The start node of dominance computation.\n\nReturns\n-------\ndf : dict keyed by nodes\n    A dict containing the dominance frontiers of each node reachable from\n    `start` as lists.\n\nRaises\n------\nNetworkXNotImplemented\n    If `G` is undirected.\n\nNetworkXError\n    If `start` is not in `G`.\n\nExamples\n--------\n>>> G = nx.DiGraph([(1, 2), (1, 3), (2, 5), (3, 4), (4, 5)])\n>>> sorted((u, sorted(df)) for u, df in nx.dominance_frontiers(G, 1).items())\n[(1, []), (2, [5]), (3, [5]), (4, [5]), (5, [])]\n\n",
  "code": "@nx._dispatch\ndef dominance_frontiers(G, start):\n    idom = nx.immediate_dominators(G, start)\n    df = {u: set() for u in idom}\n    for u in idom:\n        if len(G.pred[u]) >= 2:\n            for v in G.pred[u]:\n                if v in idom:\n                    while v != idom[u]:\n                        df[v].add(u)\n                        v = idom[v]\n    return df"
 },
 {
  "docstring": "Finds a dominating set for the graph G.\n\nA *dominating set* for a graph with node set *V* is a subset *D* of\n*V* such that every node not in *D* is adjacent to at least one\nmember of *D* [1]_.\n\nParameters\n----------\nG : NetworkX graph\n\nstart_with : node (default=None)\n    Node to use as a starting point for the algorithm.\n\nReturns\n-------\nD : set\n    A dominating set for G.\n\n",
  "code": "@nx._dispatch\ndef dominating_set(G, start_with=None):\n    all_nodes = set(G)\n    if start_with is None:\n        start_with = arbitrary_element(all_nodes)\n    if start_with not in G:\n        raise nx.NetworkXError(f'node {start_with} is not in G')\n    dominating_set = {start_with}\n    dominated_nodes = set(G[start_with])\n    remaining_nodes = all_nodes - dominated_nodes - dominating_set\n    while remaining_nodes:\n        v = remaining_nodes.pop()\n        undominated_neighbors = set(G[v]) - dominating_set\n        dominating_set.add(v)\n        dominated_nodes |= undominated_neighbors\n        remaining_nodes -= undominated_neighbors\n    return dominating_set"
 },
 {
  "docstring": "Checks if `nbunch` is a dominating set for `G`.\n\nA *dominating set* for a graph with node set *V* is a subset *D* of\n*V* such that every node not in *D* is adjacent to at least one\nmember of *D* [1]_.\n\nParameters\n----------\nG : NetworkX graph\n\nnbunch : iterable\n    An iterable of nodes in the graph `G`.\n\n",
  "code": "@nx._dispatch\ndef is_dominating_set(G, nbunch):\n    testset = {n for n in nbunch if n in G}\n    nbrs = set(chain.from_iterable((G[n] for n in testset)))\n    return len(set(G) - testset - nbrs) == 0"
 },
 {
  "docstring": "Return whether node sets ``x`` and ``y`` are d-separated by ``z``.\n\nParameters\n----------\nG : graph\n    A NetworkX DAG.\n\nx : set\n    First set of nodes in ``G``.\n\ny : set\n    Second set of nodes in ``G``.\n\nz : set\n    Set of conditioning nodes in ``G``. Can be empty set.\n\nReturns\n-------\nb : bool\n    A boolean that is true if ``x`` is d-separated from ``y`` given ``z`` in ``G``.\n\nRaises\n------\nNetworkXError\n    The *d-separation* test is commonly used with directed\n    graphical models which are acyclic.  Accordingly, the algorithm\n    raises a :exc:`NetworkXError` if the input graph is not a DAG.\n\nNodeNotFound\n    If any of the input nodes are not found in the graph,\n    a :exc:`NodeNotFound` exception is raised.\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef d_separated(G, x, y, z):\n    if not nx.is_directed_acyclic_graph(G):\n        raise nx.NetworkXError('graph should be directed acyclic')\n    union_xyz = x.union(y).union(z)\n    if any((n not in G.nodes for n in union_xyz)):\n        raise nx.NodeNotFound('one or more specified nodes not found in the graph')\n    G_copy = G.copy()\n    leaves = deque([n for n in G_copy.nodes if G_copy.out_degree[n] == 0])\n    while len(leaves) > 0:\n        leaf = leaves.popleft()\n        if leaf not in union_xyz:\n            for p in G_copy.predecessors(leaf):\n                if G_copy.out_degree[p] == 1:\n                    leaves.append(p)\n            G_copy.remove_node(leaf)\n    edges_to_remove = list(G_copy.out_edges(z))\n    G_copy.remove_edges_from(edges_to_remove)\n    disjoint_set = UnionFind(G_copy.nodes())\n    for component in nx.weakly_connected_components(G_copy):\n        disjoint_set.union(*component)\n    disjoint_set.union(*x)\n    disjoint_set.union(*y)\n    if x and y and (disjoint_set[next(iter(x))] == disjoint_set[next(iter(y))]):\n        return False\n    else:\n        return True"
 },
 {
  "docstring": "Compute a minimal d-separating set between 'u' and 'v'.\n\nA d-separating set in a DAG is a set of nodes that blocks all paths\nbetween the two nodes, 'u' and 'v'. This function\nconstructs a d-separating set that is \"minimal\", meaning it is the smallest\nd-separating set for 'u' and 'v'. This is not necessarily\nunique. For more details, see Notes.\n\nParameters\n----------\nG : graph\n    A networkx DAG.\nu : node\n    A node in the graph, G.\nv : node\n    A node in the graph, G.\n\nRaises\n------\nNetworkXError\n    Raises a :exc:`NetworkXError` if the input graph is not a DAG.\n\nNodeNotFound\n    If any of the input nodes are not found in the graph,\n    a :exc:`NodeNotFound` exception is raised.\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef minimal_d_separator(G, u, v):\n    if not nx.is_directed_acyclic_graph(G):\n        raise nx.NetworkXError('graph should be directed acyclic')\n    union_uv = {u, v}\n    if any((n not in G.nodes for n in union_uv)):\n        raise nx.NodeNotFound('one or more specified nodes not found in the graph')\n    x_anc = nx.ancestors(G, u)\n    y_anc = nx.ancestors(G, v)\n    D_anc_xy = x_anc.union(y_anc)\n    D_anc_xy.update((u, v))\n    moral_G = nx.moral_graph(G.subgraph(D_anc_xy))\n    Z_prime = set(G.predecessors(u)).union(set(G.predecessors(v)))\n    Z_dprime = _bfs_with_marks(moral_G, u, Z_prime)\n    Z = _bfs_with_marks(moral_G, v, Z_dprime)\n    return Z"
 },
 {
  "docstring": "Determine if a d-separating set is minimal.\n\nA d-separating set, `z`, in a DAG is a set of nodes that blocks\nall paths between the two nodes, `u` and `v`. This function\nverifies that a set is \"minimal\", meaning there is no smaller\nd-separating set between the two nodes.\n\nNote: This function checks whether `z` is a d-separator AND is minimal.\nOne can use the function `d_separated` to only check if `z` is a d-separator.\nSee examples below.\n\nParameters\n----------\nG : nx.DiGraph\n    The graph.\nu : node\n    A node in the graph.\nv : node\n    A node in the graph.\nz : Set of nodes\n    The set of nodes to check if it is a minimal d-separating set.\n    The function :func:`d_separated` is called inside this function\n    to verify that `z` is in fact a d-separator.\n\nReturns\n-------\nbool\n    Whether or not the set `z` is a d-separator and is also minimal.\n\nExamples\n--------\n>>> G = nx.path_graph([0, 1, 2, 3], create_using=nx.DiGraph)\n>>> G.add_node(4)\n>>> nx.is_minimal_d_separator(G, 0, 2, {1})\nTrue\n>>> # since {1} is the minimal d-separator, {1, 3, 4} is not minimal\n>>> nx.is_minimal_d_separator(G, 0, 2, {1, 3, 4})\nFalse\n>>> # alternatively, if we only want to check that {1, 3, 4} is a d-separator\n>>> nx.d_separated(G, {0}, {4}, {1, 3, 4})\nTrue\n\nRaises\n------\nNetworkXError\n    Raises a :exc:`NetworkXError` if the input graph is not a DAG.\n\nNodeNotFound\n    If any of the input nodes are not found in the graph,\n    a :exc:`NodeNotFound` exception is raised.\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef is_minimal_d_separator(G, u, v, z):\n    if not nx.d_separated(G, {u}, {v}, z):\n        return False\n    x_anc = nx.ancestors(G, u)\n    y_anc = nx.ancestors(G, v)\n    xy_anc = x_anc.union(y_anc)\n    if any((node not in xy_anc for node in z)):\n        return False\n    D_anc_xy = x_anc.union(y_anc)\n    D_anc_xy.update((u, v))\n    moral_G = nx.moral_graph(G.subgraph(D_anc_xy))\n    marks = _bfs_with_marks(moral_G, u, z)\n    if any((node not in marks for node in z)):\n        return False\n    marks = _bfs_with_marks(moral_G, v, z)\n    if any((node not in marks for node in z)):\n        return False\n    return True"
 },
 {
  "docstring": "Breadth-first-search with markings.\n\nPerforms BFS starting from ``start_node`` and whenever a node\ninside ``check_set`` is met, it is \"marked\". Once a node is marked,\nBFS does not continue along that path. The resulting marked nodes\nare returned.\n\nParameters\n----------\nG : nx.Graph\n    An undirected graph.\nstart_node : node\n    The start of the BFS.\ncheck_set : set\n    The set of nodes to check against.\n\nReturns\n-------\nmarked : set\n    A set of nodes that were marked.",
  "code": "@not_implemented_for('directed')\ndef _bfs_with_marks(G, start_node, check_set):\n    visited = {}\n    marked = set()\n    queue = []\n    visited[start_node] = None\n    queue.append(start_node)\n    while queue:\n        m = queue.pop(0)\n        for nbr in G.neighbors(m):\n            if nbr not in visited:\n                visited[nbr] = None\n                if nbr in check_set:\n                    marked.add(nbr)\n                else:\n                    queue.append(nbr)\n    return marked"
 },
 {
  "docstring": "Returns the efficiency of a pair of nodes in a graph.\n\nThe *efficiency* of a pair of nodes is the multiplicative inverse of the\nshortest path distance between the nodes [1]_. Returns 0 if no path\nbetween nodes.\n\nParameters\n----------\nG : :class:`networkx.Graph`\n    An undirected graph for which to compute the average local efficiency.\nu, v : node\n    Nodes in the graph ``G``.\n\nReturns\n-------\nfloat\n    Multiplicative inverse of the shortest path distance between the nodes.\n\nExamples\n--------\n>>> G = nx.Graph([(0, 1), (0, 2), (0, 3), (1, 2), (1, 3)])\n>>> nx.efficiency(G, 2, 3)  # this gives efficiency for node 2 and 3\n0.5\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch\ndef efficiency(G, u, v):\n    try:\n        eff = 1 / nx.shortest_path_length(G, u, v)\n    except NetworkXNoPath:\n        eff = 0\n    return eff"
 },
 {
  "docstring": "Returns the average global efficiency of the graph.\n\nThe *efficiency* of a pair of nodes in a graph is the multiplicative\ninverse of the shortest path distance between the nodes. The *average\nglobal efficiency* of a graph is the average efficiency of all pairs of\nnodes [1]_.\n\nParameters\n----------\nG : :class:`networkx.Graph`\n    An undirected graph for which to compute the average global efficiency.\n\nReturns\n-------\nfloat\n    The average global efficiency of the graph.\n\nExamples\n--------\n>>> G = nx.Graph([(0, 1), (0, 2), (0, 3), (1, 2), (1, 3)])\n>>> round(nx.global_efficiency(G), 12)\n0.916666666667\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch\ndef global_efficiency(G):\n    n = len(G)\n    denom = n * (n - 1)\n    if denom != 0:\n        lengths = nx.all_pairs_shortest_path_length(G)\n        g_eff = 0\n        for source, targets in lengths:\n            for target, distance in targets.items():\n                if distance > 0:\n                    g_eff += 1 / distance\n        g_eff /= denom\n    else:\n        g_eff = 0\n    return g_eff"
 },
 {
  "docstring": "Returns the average local efficiency of the graph.\n\nThe *efficiency* of a pair of nodes in a graph is the multiplicative\ninverse of the shortest path distance between the nodes. The *local\nefficiency* of a node in the graph is the average global efficiency of the\nsubgraph induced by the neighbors of the node. The *average local\nefficiency* is the average of the local efficiencies of each node [1]_.\n\nParameters\n----------\nG : :class:`networkx.Graph`\n    An undirected graph for which to compute the average local efficiency.\n\nReturns\n-------\nfloat\n    The average local efficiency of the graph.\n\nExamples\n--------\n>>> G = nx.Graph([(0, 1), (0, 2), (0, 3), (1, 2), (1, 3)])\n>>> nx.local_efficiency(G)\n0.9166666666666667\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch\ndef local_efficiency(G):\n    efficiency_list = (global_efficiency(G.subgraph(G[v])) for v in G)\n    return sum(efficiency_list) / len(G)"
 },
 {
  "docstring": "Returns True if and only if `G` is Eulerian.\n\nA graph is *Eulerian* if it has an Eulerian circuit. An *Eulerian\ncircuit* is a closed walk that includes each edge of a graph exactly\nonce.\n\nGraphs with isolated vertices (i.e. vertices with zero degree) are not\nconsidered to have Eulerian circuits. Therefore, if the graph is not\nconnected (or not strongly connected, for directed graphs), this function\nreturns False.\n\nParameters\n----------\nG : NetworkX graph\n   A graph, either directed or undirected.\n\nExamples\n--------\n>>> nx.is_eulerian(nx.DiGraph({0: [3], 1: [2], 2: [3], 3: [0, 1]}))\nTrue\n>>> nx.is_eulerian(nx.complete_graph(5))\nTrue\n>>> nx.is_eulerian(nx.petersen_graph())\nFalse\n\nIf you prefer to allow graphs with isolated vertices to have Eulerian circuits,\nyou can first remove such vertices and then call `is_eulerian` as below example shows.\n\n>>> G = nx.Graph([(0, 1), (1, 2), (0, 2)])\n>>> G.add_node(3)\n>>> nx.is_eulerian(G)\nFalse\n\n>>> G.remove_nodes_from(list(nx.isolates(G)))\n>>> nx.is_eulerian(G)\nTrue",
  "code": "@nx._dispatch\ndef is_eulerian(G):\n    if G.is_directed():\n        return all((G.in_degree(n) == G.out_degree(n) for n in G)) and nx.is_strongly_connected(G)\n    return all((d % 2 == 0 for v, d in G.degree())) and nx.is_connected(G)"
 },
 {
  "docstring": "Return True iff `G` is semi-Eulerian.\n\nG is semi-Eulerian if it has an Eulerian path but no Eulerian circuit.\n\nSee Also\n--------\nhas_eulerian_path\nis_eulerian",
  "code": "@nx._dispatch\ndef is_semieulerian(G):\n    return has_eulerian_path(G) and (not is_eulerian(G))"
 },
 {
  "docstring": "Return a suitable starting vertex for an Eulerian path.\n\nIf no path exists, return None.",
  "code": "def _find_path_start(G):\n    if not has_eulerian_path(G):\n        return None\n    if is_eulerian(G):\n        return arbitrary_element(G)\n    if G.is_directed():\n        v1, v2 = (v for v in G if G.in_degree(v) != G.out_degree(v))\n        if G.out_degree(v1) > G.in_degree(v1):\n            return v1\n        else:\n            return v2\n    else:\n        start = [v for v in G if G.degree(v) % 2 != 0][0]\n        return start"
 },
 {
  "docstring": "Returns an iterator over the edges of an Eulerian circuit in `G`.\n\nAn *Eulerian circuit* is a closed walk that includes each edge of a\ngraph exactly once.\n\nParameters\n----------\nG : NetworkX graph\n   A graph, either directed or undirected.\n\nsource : node, optional\n   Starting node for circuit.\n\nkeys : bool\n   If False, edges generated by this function will be of the form\n   ``(u, v)``. Otherwise, edges will be of the form ``(u, v, k)``.\n   This option is ignored unless `G` is a multigraph.\n\nReturns\n-------\nedges : iterator\n   An iterator over edges in the Eulerian circuit.\n\nRaises\n------\nNetworkXError\n   If the graph is not Eulerian.\n\nSee Also\n--------\nis_eulerian\n\n",
  "code": "@nx._dispatch\ndef eulerian_circuit(G, source=None, keys=False):\n    if not is_eulerian(G):\n        raise nx.NetworkXError('G is not Eulerian.')\n    if G.is_directed():\n        G = G.reverse()\n    else:\n        G = G.copy()\n    if source is None:\n        source = arbitrary_element(G)\n    if G.is_multigraph():\n        for u, v, k in _multigraph_eulerian_circuit(G, source):\n            if keys:\n                yield (u, v, k)\n            else:\n                yield (u, v)\n    else:\n        yield from _simplegraph_eulerian_circuit(G, source)"
 },
 {
  "docstring": "Return True iff `G` has an Eulerian path.\n\nAn Eulerian path is a path in a graph which uses each edge of a graph\nexactly once. If `source` is specified, then this function checks\nwhether an Eulerian path that starts at node `source` exists.\n\nA directed graph has an Eulerian path iff:\n    - at most one vertex has out_degree - in_degree = 1,\n    - at most one vertex has in_degree - out_degree = 1,\n    - every other vertex has equal in_degree and out_degree,\n    - and all of its vertices belong to a single connected\n      component of the underlying undirected graph.\n\nIf `source` is not None, an Eulerian path starting at `source` exists if no\nother node has out_degree - in_degree = 1. This is equivalent to either\nthere exists an Eulerian circuit or `source` has out_degree - in_degree = 1\nand the conditions above hold.\n\nAn undirected graph has an Eulerian path iff:\n    - exactly zero or two vertices have odd degree,\n    - and all of its vertices belong to a single connected component.\n\nIf `source` is not None, an Eulerian path starting at `source` exists if\neither there exists an Eulerian circuit or `source` has an odd degree and the\nconditions above hold.\n\nGraphs with isolated vertices (i.e. vertices with zero degree) are not considered\nto have an Eulerian path. Therefore, if the graph is not connected (or not strongly\nconnected, for directed graphs), this function returns False.\n\nParameters\n----------\nG : NetworkX Graph\n    The graph to find an euler path in.\n\nsource : node, optional\n    Starting node for path.\n\nReturns\n-------\nBool : True if G has an Eulerian path.\n\nExamples\n--------\nIf you prefer to allow graphs with isolated vertices to have Eulerian path,\nyou can first remove such vertices and then call `has_eulerian_path` as below example shows.\n\n>>> G = nx.Graph([(0, 1), (1, 2), (0, 2)])\n>>> G.add_node(3)\n>>> nx.has_eulerian_path(G)\nFalse\n\n>>> G.remove_nodes_from(list(nx.isolates(G)))\n>>> nx.has_eulerian_path(G)\nTrue\n\nSee Also\n--------\nis_eulerian\neulerian_path",
  "code": "@nx._dispatch\ndef has_eulerian_path(G, source=None):\n    if nx.is_eulerian(G):\n        return True\n    if G.is_directed():\n        ins = G.in_degree\n        outs = G.out_degree\n        if source is not None and outs[source] - ins[source] != 1:\n            return False\n        unbalanced_ins = 0\n        unbalanced_outs = 0\n        for v in G:\n            if ins[v] - outs[v] == 1:\n                unbalanced_ins += 1\n            elif outs[v] - ins[v] == 1:\n                unbalanced_outs += 1\n            elif ins[v] != outs[v]:\n                return False\n        return unbalanced_ins <= 1 and unbalanced_outs <= 1 and nx.is_weakly_connected(G)\n    else:\n        if source is not None and G.degree[source] % 2 != 1:\n            return False\n        return sum((d % 2 == 1 for v, d in G.degree())) == 2 and nx.is_connected(G)"
 },
 {
  "docstring": "Return an iterator over the edges of an Eulerian path in `G`.\n\nParameters\n----------\nG : NetworkX Graph\n    The graph in which to look for an eulerian path.\nsource : node or None (default: None)\n    The node at which to start the search. None means search over all\n    starting nodes.\nkeys : Bool (default: False)\n    Indicates whether to yield edge 3-tuples (u, v, edge_key).\n    The default yields edge 2-tuples\n\nYields\n------\nEdge tuples along the eulerian path.\n\nWarning: If `source` provided is not the start node of an Euler path\nwill raise error even if an Euler Path exists.",
  "code": "@nx._dispatch\ndef eulerian_path(G, source=None, keys=False):\n    if not has_eulerian_path(G, source):\n        raise nx.NetworkXError('Graph has no Eulerian paths.')\n    if G.is_directed():\n        G = G.reverse()\n        if source is None or nx.is_eulerian(G) is False:\n            source = _find_path_start(G)\n        if G.is_multigraph():\n            for u, v, k in _multigraph_eulerian_circuit(G, source):\n                if keys:\n                    yield (u, v, k)\n                else:\n                    yield (u, v)\n        else:\n            yield from _simplegraph_eulerian_circuit(G, source)\n    else:\n        G = G.copy()\n        if source is None:\n            source = _find_path_start(G)\n        if G.is_multigraph():\n            if keys:\n                yield from reversed([(v, u, k) for u, v, k in _multigraph_eulerian_circuit(G, source)])\n            else:\n                yield from reversed([(v, u) for u, v, k in _multigraph_eulerian_circuit(G, source)])\n        else:\n            yield from reversed([(v, u) for u, v in _simplegraph_eulerian_circuit(G, source)])"
 },
 {
  "docstring": "Transforms a graph into an Eulerian graph.\n\nIf `G` is Eulerian the result is `G` as a MultiGraph, otherwise the result is a smallest\n(in terms of the number of edges) multigraph whose underlying simple graph is `G`.\n\nParameters\n----------\nG : NetworkX graph\n   An undirected graph\n\nReturns\n-------\nG : NetworkX multigraph\n\nRaises\n------\nNetworkXError\n   If the graph is not connected.\n\nSee Also\n--------\nis_eulerian\neulerian_circuit\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch\ndef eulerize(G):\n    if G.order() == 0:\n        raise nx.NetworkXPointlessConcept('Cannot Eulerize null graph')\n    if not nx.is_connected(G):\n        raise nx.NetworkXError('G is not connected')\n    odd_degree_nodes = [n for n, d in G.degree() if d % 2 == 1]\n    G = nx.MultiGraph(G)\n    if len(odd_degree_nodes) == 0:\n        return G\n    odd_deg_pairs_paths = [(m, {n: nx.shortest_path(G, source=m, target=n)}) for m, n in combinations(odd_degree_nodes, 2)]\n    upper_bound_on_max_path_length = len(G) + 1\n    Gp = nx.Graph()\n    for n, Ps in odd_deg_pairs_paths:\n        for m, P in Ps.items():\n            if n != m:\n                Gp.add_edge(m, n, weight=upper_bound_on_max_path_length - len(P), path=P)\n    best_matching = nx.Graph(list(nx.max_weight_matching(Gp)))\n    for m, n in best_matching.edges():\n        path = Gp[m][n]['path']\n        G.add_edges_from(nx.utils.pairwise(path))\n    return G"
 },
 {
  "docstring": "Returns True if sequence is a valid degree sequence.\n\nA degree sequence is valid if some graph can realize it.\n\nParameters\n----------\nsequence : list or iterable container\n    A sequence of integer node degrees\n\nmethod : \"eg\" | \"hh\"  (default: 'eg')\n    The method used to validate the degree sequence.\n    \"eg\" corresponds to the Erd\u0151s-Gallai algorithm\n    [EG1960]_, [choudum1986]_, and\n    \"hh\" to the Havel-Hakimi algorithm\n    [havel1955]_, [hakimi1962]_, [CL1996]_.\n\nReturns\n-------\nvalid : bool\n    True if the sequence is a valid degree sequence and False if not.\n\nExamples\n--------\n>>> G = nx.path_graph(4)\n>>> sequence = (d for n, d in G.degree())\n>>> nx.is_graphical(sequence)\nTrue\n\nTo test a non-graphical sequence:\n>>> sequence_list = [d for n, d in G.degree()]\n>>> sequence_list[-1] += 1\n>>> nx.is_graphical(sequence_list)\nFalse\n\n",
  "code": "@nx._dispatch(graphs=None)\ndef is_graphical(sequence, method='eg'):\n    if method == 'eg':\n        valid = is_valid_degree_sequence_erdos_gallai(list(sequence))\n    elif method == 'hh':\n        valid = is_valid_degree_sequence_havel_hakimi(list(sequence))\n    else:\n        msg = \"`method` must be 'eg' or 'hh'\"\n        raise nx.NetworkXException(msg)\n    return valid"
 },
 {
  "docstring": "Returns True if deg_sequence can be realized by a simple graph.\n\nThe validation proceeds using the Havel-Hakimi theorem\n[havel1955]_, [hakimi1962]_, [CL1996]_.\nWorst-case run time is $O(s)$ where $s$ is the sum of the sequence.\n\nParameters\n----------\ndeg_sequence : list\n    A list of integers where each element specifies the degree of a node\n    in a graph.\n\nReturns\n-------\nvalid : bool\n    True if deg_sequence is graphical and False if not.\n\nExamples\n--------\n>>> G = nx.Graph([(1, 2), (1, 3), (2, 3), (3, 4), (4, 2), (5, 1), (5, 4)])\n>>> sequence = (d for _, d in G.degree())\n>>> nx.is_valid_degree_sequence_havel_hakimi(sequence)\nTrue\n\nTo test a non-valid sequence:\n>>> sequence_list = [d for _, d in G.degree()]\n>>> sequence_list[-1] += 1\n>>> nx.is_valid_degree_sequence_havel_hakimi(sequence_list)\nFalse\n\n",
  "code": "@nx._dispatch(graphs=None)\ndef is_valid_degree_sequence_havel_hakimi(deg_sequence):\n    try:\n        dmax, dmin, dsum, n, num_degs = _basic_graphical_tests(deg_sequence)\n    except nx.NetworkXUnfeasible:\n        return False\n    if n == 0 or 4 * dmin * n >= (dmax + dmin + 1) * (dmax + dmin + 1):\n        return True\n    modstubs = [0] * (dmax + 1)\n    while n > 0:\n        while num_degs[dmax] == 0:\n            dmax -= 1\n        if dmax > n - 1:\n            return False\n        num_degs[dmax], n = (num_degs[dmax] - 1, n - 1)\n        mslen = 0\n        k = dmax\n        for i in range(dmax):\n            while num_degs[k] == 0:\n                k -= 1\n            num_degs[k], n = (num_degs[k] - 1, n - 1)\n            if k > 1:\n                modstubs[mslen] = k - 1\n                mslen += 1\n        for i in range(mslen):\n            stub = modstubs[i]\n            num_degs[stub], n = (num_degs[stub] + 1, n + 1)\n    return True"
 },
 {
  "docstring": "Returns True if deg_sequence can be realized by a simple graph.\n\nThe validation is done using the Erd\u0151s-Gallai theorem [EG1960]_.\n\nParameters\n----------\ndeg_sequence : list\n    A list of integers\n\nReturns\n-------\nvalid : bool\n    True if deg_sequence is graphical and False if not.\n\nExamples\n--------\n>>> G = nx.Graph([(1, 2), (1, 3), (2, 3), (3, 4), (4, 2), (5, 1), (5, 4)])\n>>> sequence = (d for _, d in G.degree())\n>>> nx.is_valid_degree_sequence_erdos_gallai(sequence)\nTrue\n\nTo test a non-valid sequence:\n>>> sequence_list = [d for _, d in G.degree()]\n>>> sequence_list[-1] += 1\n>>> nx.is_valid_degree_sequence_erdos_gallai(sequence_list)\nFalse\n\n",
  "code": "@nx._dispatch(graphs=None)\ndef is_valid_degree_sequence_erdos_gallai(deg_sequence):\n    try:\n        dmax, dmin, dsum, n, num_degs = _basic_graphical_tests(deg_sequence)\n    except nx.NetworkXUnfeasible:\n        return False\n    if n == 0 or 4 * dmin * n >= (dmax + dmin + 1) * (dmax + dmin + 1):\n        return True\n    k, sum_deg, sum_nj, sum_jnj = (0, 0, 0, 0)\n    for dk in range(dmax, dmin - 1, -1):\n        if dk < k + 1:\n            return True\n        if num_degs[dk] > 0:\n            run_size = num_degs[dk]\n            if dk < k + run_size:\n                run_size = dk - k\n            sum_deg += run_size * dk\n            for v in range(run_size):\n                sum_nj += num_degs[k + v]\n                sum_jnj += (k + v) * num_degs[k + v]\n            k += run_size\n            if sum_deg > k * (n - 1) - k * sum_nj + sum_jnj:\n                return False\n    return True"
 },
 {
  "docstring": "Returns True if some multigraph can realize the sequence.\n\nParameters\n----------\nsequence : list\n    A list of integers\n\nReturns\n-------\nvalid : bool\n    True if deg_sequence is a multigraphic degree sequence and False if not.\n\nExamples\n--------\n>>> G = nx.MultiGraph([(1, 2), (1, 3), (2, 3), (3, 4), (4, 2), (5, 1), (5, 4)])\n>>> sequence = (d for _, d in G.degree())\n>>> nx.is_multigraphical(sequence)\nTrue\n\nTo test a non-multigraphical sequence:\n>>> sequence_list = [d for _, d in G.degree()]\n>>> sequence_list[-1] += 1\n>>> nx.is_multigraphical(sequence_list)\nFalse\n\n",
  "code": "@nx._dispatch(graphs=None)\ndef is_multigraphical(sequence):\n    try:\n        deg_sequence = nx.utils.make_list_of_ints(sequence)\n    except nx.NetworkXError:\n        return False\n    dsum, dmax = (0, 0)\n    for d in deg_sequence:\n        if d < 0:\n            return False\n        dsum, dmax = (dsum + d, max(dmax, d))\n    if dsum % 2 or dsum < 2 * dmax:\n        return False\n    return True"
 },
 {
  "docstring": "Returns True if some pseudograph can realize the sequence.\n\nEvery nonnegative integer sequence with an even sum is pseudographical\n(see [1]_).\n\nParameters\n----------\nsequence : list or iterable container\n    A sequence of integer node degrees\n\nReturns\n-------\nvalid : bool\n  True if the sequence is a pseudographic degree sequence and False if not.\n\nExamples\n--------\n>>> G = nx.Graph([(1, 2), (1, 3), (2, 3), (3, 4), (4, 2), (5, 1), (5, 4)])\n>>> sequence = (d for _, d in G.degree())\n>>> nx.is_pseudographical(sequence)\nTrue\n\nTo test a non-pseudographical sequence:\n>>> sequence_list = [d for _, d in G.degree()]\n>>> sequence_list[-1] += 1\n>>> nx.is_pseudographical(sequence_list)\nFalse\n\n",
  "code": "@nx._dispatch(graphs=None)\ndef is_pseudographical(sequence):\n    try:\n        deg_sequence = nx.utils.make_list_of_ints(sequence)\n    except nx.NetworkXError:\n        return False\n    return sum(deg_sequence) % 2 == 0 and min(deg_sequence) >= 0"
 },
 {
  "docstring": "Returns True if some directed graph can realize the in- and out-degree\nsequences.\n\nParameters\n----------\nin_sequence : list or iterable container\n    A sequence of integer node in-degrees\n\nout_sequence : list or iterable container\n    A sequence of integer node out-degrees\n\nReturns\n-------\nvalid : bool\n  True if in and out-sequences are digraphic False if not.\n\nExamples\n--------\n>>> G = nx.DiGraph([(1, 2), (1, 3), (2, 3), (3, 4), (4, 2), (5, 1), (5, 4)])\n>>> in_seq = (d for n, d in G.in_degree())\n>>> out_seq = (d for n, d in G.out_degree())\n>>> nx.is_digraphical(in_seq, out_seq)\nTrue\n\nTo test a non-digraphical scenario:\n>>> in_seq_list = [d for n, d in G.in_degree()]\n>>> in_seq_list[-1] += 1\n>>> nx.is_digraphical(in_seq_list, out_seq)\nFalse\n\n",
  "code": "@nx._dispatch(graphs=None)\ndef is_digraphical(in_sequence, out_sequence):\n    try:\n        in_deg_sequence = nx.utils.make_list_of_ints(in_sequence)\n        out_deg_sequence = nx.utils.make_list_of_ints(out_sequence)\n    except nx.NetworkXError:\n        return False\n    sumin, sumout, nin, nout = (0, 0, len(in_deg_sequence), len(out_deg_sequence))\n    maxn = max(nin, nout)\n    maxin = 0\n    if maxn == 0:\n        return True\n    stubheap, zeroheap = ([], [])\n    for n in range(maxn):\n        in_deg, out_deg = (0, 0)\n        if n < nout:\n            out_deg = out_deg_sequence[n]\n        if n < nin:\n            in_deg = in_deg_sequence[n]\n        if in_deg < 0 or out_deg < 0:\n            return False\n        sumin, sumout, maxin = (sumin + in_deg, sumout + out_deg, max(maxin, in_deg))\n        if in_deg > 0:\n            stubheap.append((-1 * out_deg, -1 * in_deg))\n        elif out_deg > 0:\n            zeroheap.append(-1 * out_deg)\n    if sumin != sumout:\n        return False\n    heapq.heapify(stubheap)\n    heapq.heapify(zeroheap)\n    modstubs = [(0, 0)] * (maxin + 1)\n    while stubheap:\n        freeout, freein = heapq.heappop(stubheap)\n        freein *= -1\n        if freein > len(stubheap) + len(zeroheap):\n            return False\n        mslen = 0\n        for i in range(freein):\n            if zeroheap and (not stubheap or stubheap[0][0] > zeroheap[0]):\n                stubout = heapq.heappop(zeroheap)\n                stubin = 0\n            else:\n                stubout, stubin = heapq.heappop(stubheap)\n            if stubout == 0:\n                return False\n            if stubout + 1 < 0 or stubin < 0:\n                modstubs[mslen] = (stubout + 1, stubin)\n                mslen += 1\n        for i in range(mslen):\n            stub = modstubs[i]\n            if stub[1] < 0:\n                heapq.heappush(stubheap, stub)\n            else:\n                heapq.heappush(zeroheap, stub[0])\n        if freeout < 0:\n            heapq.heappush(zeroheap, freeout)\n    return True"
 },
 {
  "docstring": "Compute new labels for given node by aggregating\nthe labels of each node's neighbors.",
  "code": "def _neighborhood_aggregate(G, node, node_labels, edge_attr=None):\n    label_list = []\n    for nbr in G.neighbors(node):\n        prefix = '' if edge_attr is None else str(G[node][nbr][edge_attr])\n        label_list.append(prefix + node_labels[nbr])\n    return node_labels[node] + ''.join(sorted(label_list))"
 },
 {
  "docstring": "Return Weisfeiler Lehman (WL) graph hash.\n\nThe function iteratively aggregates and hashes neighbourhoods of each node.\nAfter each node's neighbors are hashed to obtain updated node labels,\na hashed histogram of resulting labels is returned as the final hash.\n\nHashes are identical for isomorphic graphs and strong guarantees that\nnon-isomorphic graphs will get different hashes. See [1]_ for details.\n\nIf no node or edge attributes are provided, the degree of each node\nis used as its initial label.\nOtherwise, node and/or edge labels are used to compute the hash.\n\nParameters\n----------\nG: graph\n    The graph to be hashed.\n    Can have node and/or edge attributes. Can also have no attributes.\nedge_attr: string, default=None\n    The key in edge attribute dictionary to be used for hashing.\n    If None, edge labels are ignored.\nnode_attr: string, default=None\n    The key in node attribute dictionary to be used for hashing.\n    If None, and no edge_attr given, use the degrees of the nodes as labels.\niterations: int, default=3\n    Number of neighbor aggregations to perform.\n    Should be larger for larger graphs.\ndigest_size: int, default=16\n    Size (in bits) of blake2b hash digest to use for hashing node labels.\n\nReturns\n-------\nh : string\n    Hexadecimal string corresponding to hash of the input graph.\n\nExamples\n--------\nTwo graphs with edge attributes that are isomorphic, except for\ndifferences in the edge labels.\n\n>>> G1 = nx.Graph()\n>>> G1.add_edges_from(\n...     [\n...         (1, 2, {\"label\": \"A\"}),\n...         (2, 3, {\"label\": \"A\"}),\n...         (3, 1, {\"label\": \"A\"}),\n...         (1, 4, {\"label\": \"B\"}),\n...     ]\n... )\n>>> G2 = nx.Graph()\n>>> G2.add_edges_from(\n...     [\n...         (5, 6, {\"label\": \"B\"}),\n...         (6, 7, {\"label\": \"A\"}),\n...         (7, 5, {\"label\": \"A\"}),\n...         (7, 8, {\"label\": \"A\"}),\n...     ]\n... )\n\nOmitting the `edge_attr` option, results in identical hashes.\n\n>>> nx.weisfeiler_lehman_graph_hash(G1)\n'7bc4dde9a09d0b94c5097b219891d81a'\n>>> nx.weisfeiler_lehman_graph_hash(G2)\n'7bc4dde9a09d0b94c5097b219891d81a'\n\nWith edge labels, the graphs are no longer assigned\nthe same hash digest.\n\n>>> nx.weisfeiler_lehman_graph_hash(G1, edge_attr=\"label\")\n'c653d85538bcf041d88c011f4f905f10'\n>>> nx.weisfeiler_lehman_graph_hash(G2, edge_attr=\"label\")\n'3dcd84af1ca855d0eff3c978d88e7ec7'\n\n",
  "code": "@nx._dispatch(edge_attrs={'edge_attr': None}, node_attrs='node_attr')\ndef weisfeiler_lehman_graph_hash(G, edge_attr=None, node_attr=None, iterations=3, digest_size=16):\n\n    def weisfeiler_lehman_step(G, labels, edge_attr=None):\n        \"\"\"\n        Apply neighborhood aggregation to each node\n        in the graph.\n        Computes a dictionary with labels for each node.\n        \"\"\"\n        new_labels = {}\n        for node in G.nodes():\n            label = _neighborhood_aggregate(G, node, labels, edge_attr=edge_attr)\n            new_labels[node] = _hash_label(label, digest_size)\n        return new_labels\n    node_labels = _init_node_labels(G, edge_attr, node_attr)\n    subgraph_hash_counts = []\n    for _ in range(iterations):\n        node_labels = weisfeiler_lehman_step(G, node_labels, edge_attr=edge_attr)\n        counter = Counter(node_labels.values())\n        subgraph_hash_counts.extend(sorted(counter.items(), key=lambda x: x[0]))\n    return _hash_label(str(tuple(subgraph_hash_counts)), digest_size)"
 },
 {
  "docstring": "Return a dictionary of subgraph hashes by node.\n\nDictionary keys are nodes in `G`, and values are a list of hashes.\nEach hash corresponds to a subgraph rooted at a given node u in `G`.\nLists of subgraph hashes are sorted in increasing order of depth from\ntheir root node, with the hash at index i corresponding to a subgraph\nof nodes at most i edges distance from u. Thus, each list will contain\n``iterations + 1`` elements - a hash for a subgraph at each depth, and\nadditionally a hash of the initial node label (or equivalently a\nsubgraph of depth 0)\n\nThe function iteratively aggregates and hashes neighbourhoods of each node.\nThis is achieved for each step by replacing for each node its label from\nthe previous iteration with its hashed 1-hop neighborhood aggregate.\nThe new node label is then appended to a list of node labels for each\nnode.\n\nTo aggregate neighborhoods at each step for a node $n$, all labels of\nnodes adjacent to $n$ are concatenated. If the `edge_attr` parameter is set,\nlabels for each neighboring node are prefixed with the value of this attribute\nalong the connecting edge from this neighbor to node $n$. The resulting string\nis then hashed to compress this information into a fixed digest size.\n\nThus, at the $i$-th iteration, nodes within $i$ hops influence any given\nhashed node label. We can therefore say that at depth $i$ for node $n$\nwe have a hash for a subgraph induced by the $2i$-hop neighborhood of $n$.\n\nThe output can be used to to create general Weisfeiler-Lehman graph kernels,\nor generate features for graphs or nodes - for example to generate 'words' in\na graph as seen in the 'graph2vec' algorithm.\nSee [1]_ & [2]_ respectively for details.\n\nHashes are identical for isomorphic subgraphs and there exist strong\nguarantees that non-isomorphic graphs will get different hashes.\nSee [1]_ for details.\n\nIf no node or edge attributes are provided, the degree of each node\nis used as its initial label.\nOtherwise, node and/or edge labels are used to compute the hash.\n\nParameters\n----------\nG: graph\n    The graph to be hashed.\n    Can have node and/or edge attributes. Can also have no attributes.\nedge_attr: string, default=None\n    The key in edge attribute dictionary to be used for hashing.\n    If None, edge labels are ignored.\nnode_attr: string, default=None\n    The key in node attribute dictionary to be used for hashing.\n    If None, and no edge_attr given, use the degrees of the nodes as labels.\niterations: int, default=3\n    Number of neighbor aggregations to perform.\n    Should be larger for larger graphs.\ndigest_size: int, default=16\n    Size (in bits) of blake2b hash digest to use for hashing node labels.\n    The default size is 16 bits\n\nReturns\n-------\nnode_subgraph_hashes : dict\n    A dictionary with each key given by a node in G, and each value given\n    by the subgraph hashes in order of depth from the key node.\n\nExamples\n--------\nFinding similar nodes in different graphs:\n\n>>> G1 = nx.Graph()\n>>> G1.add_edges_from([\n...     (1, 2), (2, 3), (2, 4), (3, 5), (4, 6), (5, 7), (6, 7)\n... ])\n>>> G2 = nx.Graph()\n>>> G2.add_edges_from([\n...     (1, 3), (2, 3), (1, 6), (1, 5), (4, 6)\n... ])\n>>> g1_hashes = nx.weisfeiler_lehman_subgraph_hashes(G1, iterations=3, digest_size=8)\n>>> g2_hashes = nx.weisfeiler_lehman_subgraph_hashes(G2, iterations=3, digest_size=8)\n\nEven though G1 and G2 are not isomorphic (they have different numbers of edges),\nthe hash sequence of depth 3 for node 1 in G1 and node 5 in G2 are similar:\n\n>>> g1_hashes[1]\n['a93b64973cfc8897', 'db1b43ae35a1878f', '57872a7d2059c1c0']\n>>> g2_hashes[5]\n['a93b64973cfc8897', 'db1b43ae35a1878f', '1716d2a4012fa4bc']\n\nThe first 2 WL subgraph hashes match. From this we can conclude that it's very\nlikely the neighborhood of 4 hops around these nodes are isomorphic: each\niteration aggregates 1-hop neighbourhoods meaning hashes at depth $n$ are influenced\nby every node within $2n$ hops.\n\nHowever the neighborhood of 6 hops is no longer isomorphic since their 3rd hash does\nnot match.\n\nThese nodes may be candidates to be classified together since their local topology\nis similar.\n\n",
  "code": "@nx._dispatch(edge_attrs={'edge_attr': None}, node_attrs='node_attr')\ndef weisfeiler_lehman_subgraph_hashes(G, edge_attr=None, node_attr=None, iterations=3, digest_size=16):\n\n    def weisfeiler_lehman_step(G, labels, node_subgraph_hashes, edge_attr=None):\n        \"\"\"\n        Apply neighborhood aggregation to each node\n        in the graph.\n        Computes a dictionary with labels for each node.\n        Appends the new hashed label to the dictionary of subgraph hashes\n        originating from and indexed by each node in G\n        \"\"\"\n        new_labels = {}\n        for node in G.nodes():\n            label = _neighborhood_aggregate(G, node, labels, edge_attr=edge_attr)\n            hashed_label = _hash_label(label, digest_size)\n            new_labels[node] = hashed_label\n            node_subgraph_hashes[node].append(hashed_label)\n        return new_labels\n    node_labels = _init_node_labels(G, edge_attr, node_attr)\n    node_subgraph_hashes = defaultdict(list)\n    for _ in range(iterations):\n        node_labels = weisfeiler_lehman_step(G, node_labels, node_subgraph_hashes, edge_attr)\n    return dict(node_subgraph_hashes)"
 },
 {
  "docstring": "Apply neighborhood aggregation to each node\nin the graph.\nComputes a dictionary with labels for each node.",
  "code": "def weisfeiler_lehman_step(G, labels, edge_attr=None):\n    new_labels = {}\n    for node in G.nodes():\n        label = _neighborhood_aggregate(G, node, labels, edge_attr=edge_attr)\n        new_labels[node] = _hash_label(label, digest_size)\n    return new_labels"
 },
 {
  "docstring": "Apply neighborhood aggregation to each node\nin the graph.\nComputes a dictionary with labels for each node.\nAppends the new hashed label to the dictionary of subgraph hashes\noriginating from and indexed by each node in G",
  "code": "def weisfeiler_lehman_step(G, labels, node_subgraph_hashes, edge_attr=None):\n    new_labels = {}\n    for node in G.nodes():\n        label = _neighborhood_aggregate(G, node, labels, edge_attr=edge_attr)\n        hashed_label = _hash_label(label, digest_size)\n        new_labels[node] = hashed_label\n        node_subgraph_hashes[node].append(hashed_label)\n    return new_labels"
 },
 {
  "docstring": "Returns the flow hierarchy of a directed network.\n\nFlow hierarchy is defined as the fraction of edges not participating\nin cycles in a directed graph [1]_.\n\nParameters\n----------\nG : DiGraph or MultiDiGraph\n   A directed graph\n\nweight : string, optional (default=None)\n   Attribute to use for edge weights. If None the weight defaults to 1.\n\nReturns\n-------\nh : float\n   Flow hierarchy value\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef flow_hierarchy(G, weight=None):\n    if not G.is_directed():\n        raise nx.NetworkXError('G must be a digraph in flow_hierarchy')\n    scc = nx.strongly_connected_components(G)\n    return 1 - sum((G.subgraph(c).size(weight) for c in scc)) / G.size(weight)"
 },
 {
  "docstring": "Returns the maximum locally `(k, l)`-connected subgraph of `G`.\n\nA graph is locally `(k, l)`-connected if for each edge `(u, v)` in the\ngraph there are at least `l` edge-disjoint paths of length at most `k`\njoining `u` to `v`.\n\nParameters\n----------\nG : NetworkX graph\n    The graph in which to find a maximum locally `(k, l)`-connected\n    subgraph.\n\nk : integer\n    The maximum length of paths to consider. A higher number means a looser\n    connectivity requirement.\n\nl : integer\n    The number of edge-disjoint paths. A higher number means a stricter\n    connectivity requirement.\n\nlow_memory : bool\n    If this is True, this function uses an algorithm that uses slightly\n    more time but less memory.\n\nsame_as_graph : bool\n    If True then return a tuple of the form `(H, is_same)`,\n    where `H` is the maximum locally `(k, l)`-connected subgraph and\n    `is_same` is a Boolean representing whether `G` is locally `(k,\n    l)`-connected (and hence, whether `H` is simply a copy of the input\n    graph `G`).\n\nReturns\n-------\nNetworkX graph or two-tuple\n    If `same_as_graph` is True, then this function returns a\n    two-tuple as described above. Otherwise, it returns only the maximum\n    locally `(k, l)`-connected subgraph.\n\n",
  "code": "@nx._dispatch\ndef kl_connected_subgraph(G, k, l, low_memory=False, same_as_graph=False):\n    H = copy.deepcopy(G)\n    graphOK = True\n    deleted_some = True\n    while deleted_some:\n        deleted_some = False\n        for edge in list(H.edges()):\n            u, v = edge\n            if low_memory:\n                verts = {u, v}\n                for i in range(k):\n                    for w in verts.copy():\n                        verts.update(G[w])\n                G2 = G.subgraph(verts).copy()\n            else:\n                G2 = copy.deepcopy(G)\n            path = [u, v]\n            cnt = 0\n            accept = 0\n            while path:\n                cnt += 1\n                if cnt >= l:\n                    accept = 1\n                    break\n                prev = u\n                for w in path:\n                    if prev != w:\n                        G2.remove_edge(prev, w)\n                        prev = w\n                try:\n                    path = nx.shortest_path(G2, u, v)\n                except nx.NetworkXNoPath:\n                    path = False\n            if accept == 0:\n                H.remove_edge(u, v)\n                deleted_some = True\n                if graphOK:\n                    graphOK = False\n    if same_as_graph:\n        return (H, graphOK)\n    return H"
 },
 {
  "docstring": "Returns True if and only if `G` is locally `(k, l)`-connected.\n\nA graph is locally `(k, l)`-connected if for each edge `(u, v)` in the\ngraph there are at least `l` edge-disjoint paths of length at most `k`\njoining `u` to `v`.\n\nParameters\n----------\nG : NetworkX graph\n    The graph to test for local `(k, l)`-connectedness.\n\nk : integer\n    The maximum length of paths to consider. A higher number means a looser\n    connectivity requirement.\n\nl : integer\n    The number of edge-disjoint paths. A higher number means a stricter\n    connectivity requirement.\n\nlow_memory : bool\n    If this is True, this function uses an algorithm that uses slightly\n    more time but less memory.\n\nReturns\n-------\nbool\n    Whether the graph is locally `(k, l)`-connected subgraph.\n\n",
  "code": "@nx._dispatch\ndef is_kl_connected(G, k, l, low_memory=False):\n    graphOK = True\n    for edge in G.edges():\n        u, v = edge\n        if low_memory:\n            verts = {u, v}\n            for i in range(k):\n                [verts.update(G.neighbors(w)) for w in verts.copy()]\n            G2 = G.subgraph(verts)\n        else:\n            G2 = copy.deepcopy(G)\n        path = [u, v]\n        cnt = 0\n        accept = 0\n        while path:\n            cnt += 1\n            if cnt >= l:\n                accept = 1\n                break\n            prev = u\n            for w in path:\n                if w != prev:\n                    G2.remove_edge(prev, w)\n                    prev = w\n            try:\n                path = nx.shortest_path(G2, u, v)\n            except nx.NetworkXNoPath:\n                path = False\n        if accept == 0:\n            graphOK = False\n            break\n    return graphOK"
 },
 {
  "docstring": "Determines whether a node is an isolate.\n\nAn *isolate* is a node with no neighbors (that is, with degree\nzero). For directed graphs, this means no in-neighbors and no\nout-neighbors.\n\nParameters\n----------\nG : NetworkX graph\n\nn : node\n    A node in `G`.\n\nReturns\n-------\nis_isolate : bool\n   True if and only if `n` has no neighbors.\n\nExamples\n--------\n>>> G = nx.Graph()\n>>> G.add_edge(1, 2)\n>>> G.add_node(3)\n>>> nx.is_isolate(G, 2)\nFalse\n>>> nx.is_isolate(G, 3)\nTrue",
  "code": "@nx._dispatch\ndef is_isolate(G, n):\n    return G.degree(n) == 0"
 },
 {
  "docstring": "Iterator over isolates in the graph.\n\nAn *isolate* is a node with no neighbors (that is, with degree\nzero). For directed graphs, this means no in-neighbors and no\nout-neighbors.\n\nParameters\n----------\nG : NetworkX graph\n\nReturns\n-------\niterator\n    An iterator over the isolates of `G`.\n\nExamples\n--------\nTo get a list of all isolates of a graph, use the :class:`list`\nconstructor::\n\n    >>> G = nx.Graph()\n    >>> G.add_edge(1, 2)\n    >>> G.add_node(3)\n    >>> list(nx.isolates(G))\n    [3]\n\nTo remove all isolates in the graph, first create a list of the\nisolates, then use :meth:`Graph.remove_nodes_from`::\n\n    >>> G.remove_nodes_from(list(nx.isolates(G)))\n    >>> list(G)\n    [1, 2]\n\nFor digraphs, isolates have zero in-degree and zero out_degre::\n\n    >>> G = nx.DiGraph([(0, 1), (1, 2)])\n    >>> G.add_node(3)\n    >>> list(nx.isolates(G))\n    [3]",
  "code": "@nx._dispatch\ndef isolates(G):\n    return (n for n, d in G.degree() if d == 0)"
 },
 {
  "docstring": "Returns the number of isolates in the graph.\n\nAn *isolate* is a node with no neighbors (that is, with degree\nzero). For directed graphs, this means no in-neighbors and no\nout-neighbors.\n\nParameters\n----------\nG : NetworkX graph\n\nReturns\n-------\nint\n    The number of degree zero nodes in the graph `G`.",
  "code": "@nx._dispatch\ndef number_of_isolates(G):\n    return sum((1 for v in isolates(G)))"
 },
 {
  "docstring": "Applies the given function to each edge in the specified iterable\nof edges.\n\n`G` is an instance of :class:`networkx.Graph`.\n\n`func` is a function on two inputs, each of which is a node in the\ngraph. The function can return anything, but it should return a\nvalue representing a prediction of the likelihood of a \"link\"\njoining the two nodes.\n\n`ebunch` is an iterable of pairs of nodes. If not specified, all\nnon-edges in the graph `G` will be used.",
  "code": "def _apply_prediction(G, func, ebunch=None):\n    if ebunch is None:\n        ebunch = nx.non_edges(G)\n    return ((u, v, func(u, v)) for u, v in ebunch)"
 },
 {
  "docstring": "Compute the resource allocation index of all node pairs in ebunch.\n\nResource allocation index of `u` and `v` is defined as\n\n.. math::\n\n    \\sum_{w \\in \\Gamma(u) \\cap \\Gamma(v)} \\frac{1}{|\\Gamma(w)|}\n\nwhere $\\Gamma(u)$ denotes the set of neighbors of $u$.\n\nParameters\n----------\nG : graph\n    A NetworkX undirected graph.\n\nebunch : iterable of node pairs, optional (default = None)\n    Resource allocation index will be computed for each pair of\n    nodes given in the iterable. The pairs must be given as\n    2-tuples (u, v) where u and v are nodes in the graph. If ebunch\n    is None then all nonexistent edges in the graph will be used.\n    Default value: None.\n\nReturns\n-------\npiter : iterator\n    An iterator of 3-tuples in the form (u, v, p) where (u, v) is a\n    pair of nodes and p is their resource allocation index.\n\nExamples\n--------\n>>> G = nx.complete_graph(5)\n>>> preds = nx.resource_allocation_index(G, [(0, 1), (2, 3)])\n>>> for u, v, p in preds:\n...     print(f\"({u}, {v}) -> {p:.8f}\")\n(0, 1) -> 0.75000000\n(2, 3) -> 0.75000000\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef resource_allocation_index(G, ebunch=None):\n\n    def predict(u, v):\n        return sum((1 / G.degree(w) for w in nx.common_neighbors(G, u, v)))\n    return _apply_prediction(G, predict, ebunch)"
 },
 {
  "docstring": "Compute the Jaccard coefficient of all node pairs in ebunch.\n\nJaccard coefficient of nodes `u` and `v` is defined as\n\n.. math::\n\n    \\frac{|\\Gamma(u) \\cap \\Gamma(v)|}{|\\Gamma(u) \\cup \\Gamma(v)|}\n\nwhere $\\Gamma(u)$ denotes the set of neighbors of $u$.\n\nParameters\n----------\nG : graph\n    A NetworkX undirected graph.\n\nebunch : iterable of node pairs, optional (default = None)\n    Jaccard coefficient will be computed for each pair of nodes\n    given in the iterable. The pairs must be given as 2-tuples\n    (u, v) where u and v are nodes in the graph. If ebunch is None\n    then all nonexistent edges in the graph will be used.\n    Default value: None.\n\nReturns\n-------\npiter : iterator\n    An iterator of 3-tuples in the form (u, v, p) where (u, v) is a\n    pair of nodes and p is their Jaccard coefficient.\n\nExamples\n--------\n>>> G = nx.complete_graph(5)\n>>> preds = nx.jaccard_coefficient(G, [(0, 1), (2, 3)])\n>>> for u, v, p in preds:\n...     print(f\"({u}, {v}) -> {p:.8f}\")\n(0, 1) -> 0.60000000\n(2, 3) -> 0.60000000\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef jaccard_coefficient(G, ebunch=None):\n\n    def predict(u, v):\n        union_size = len(set(G[u]) | set(G[v]))\n        if union_size == 0:\n            return 0\n        return len(list(nx.common_neighbors(G, u, v))) / union_size\n    return _apply_prediction(G, predict, ebunch)"
 },
 {
  "docstring": "Compute the Adamic-Adar index of all node pairs in ebunch.\n\nAdamic-Adar index of `u` and `v` is defined as\n\n.. math::\n\n    \\sum_{w \\in \\Gamma(u) \\cap \\Gamma(v)} \\frac{1}{\\log |\\Gamma(w)|}\n\nwhere $\\Gamma(u)$ denotes the set of neighbors of $u$.\nThis index leads to zero-division for nodes only connected via self-loops.\nIt is intended to be used when no self-loops are present.\n\nParameters\n----------\nG : graph\n    NetworkX undirected graph.\n\nebunch : iterable of node pairs, optional (default = None)\n    Adamic-Adar index will be computed for each pair of nodes given\n    in the iterable. The pairs must be given as 2-tuples (u, v)\n    where u and v are nodes in the graph. If ebunch is None then all\n    nonexistent edges in the graph will be used.\n    Default value: None.\n\nReturns\n-------\npiter : iterator\n    An iterator of 3-tuples in the form (u, v, p) where (u, v) is a\n    pair of nodes and p is their Adamic-Adar index.\n\nExamples\n--------\n>>> G = nx.complete_graph(5)\n>>> preds = nx.adamic_adar_index(G, [(0, 1), (2, 3)])\n>>> for u, v, p in preds:\n...     print(f\"({u}, {v}) -> {p:.8f}\")\n(0, 1) -> 2.16404256\n(2, 3) -> 2.16404256\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef adamic_adar_index(G, ebunch=None):\n\n    def predict(u, v):\n        return sum((1 / log(G.degree(w)) for w in nx.common_neighbors(G, u, v)))\n    return _apply_prediction(G, predict, ebunch)"
 },
 {
  "docstring": "Return the CCPA score for each pair of nodes.\n\nCompute the Common Neighbor and Centrality based Parameterized Algorithm(CCPA)\nscore of all node pairs in ebunch.\n\nCCPA score of `u` and `v` is defined as\n\n.. math::\n\n    \\alpha \\cdot (|\\Gamma (u){\\cap }^{}\\Gamma (v)|)+(1-\\alpha )\\cdot \\frac{N}{{d}_{uv}}\n\nwhere $\\Gamma(u)$ denotes the set of neighbors of $u$, $\\Gamma(v)$ denotes the\nset of neighbors of $v$, $\\alpha$ is  parameter varies between [0,1], $N$ denotes\ntotal number of nodes in the Graph and ${d}_{uv}$ denotes shortest distance\nbetween $u$ and $v$.\n\nThis algorithm is based on two vital properties of nodes, namely the number\nof common neighbors and their centrality. Common neighbor refers to the common\nnodes between two nodes. Centrality refers to the prestige that a node enjoys\nin a network.\n\n.. seealso::\n\n    :func:`common_neighbors`\n\nParameters\n----------\nG : graph\n    NetworkX undirected graph.\n\nebunch : iterable of node pairs, optional (default = None)\n    Preferential attachment score will be computed for each pair of\n    nodes given in the iterable. The pairs must be given as\n    2-tuples (u, v) where u and v are nodes in the graph. If ebunch\n    is None then all nonexistent edges in the graph will be used.\n    Default value: None.\n\nalpha : Parameter defined for participation of Common Neighbor\n        and Centrality Algorithm share. Values for alpha should\n        normally be between 0 and 1. Default value set to 0.8\n        because author found better performance at 0.8 for all the\n        dataset.\n        Default value: 0.8\n\n\nReturns\n-------\npiter : iterator\n    An iterator of 3-tuples in the form (u, v, p) where (u, v) is a\n    pair of nodes and p is their Common Neighbor and Centrality based\n    Parameterized Algorithm(CCPA) score.\n\nExamples\n--------\n>>> G = nx.complete_graph(5)\n>>> preds = nx.common_neighbor_centrality(G, [(0, 1), (2, 3)])\n>>> for u, v, p in preds:\n...     print(f\"({u}, {v}) -> {p}\")\n(0, 1) -> 3.4000000000000004\n(2, 3) -> 3.4000000000000004\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef common_neighbor_centrality(G, ebunch=None, alpha=0.8):\n    if alpha == 1:\n\n        def predict(u, v):\n            if u == v:\n                raise nx.NetworkXAlgorithmError('Self links are not supported')\n            return sum((1 for _ in nx.common_neighbors(G, u, v)))\n    else:\n        spl = dict(nx.shortest_path_length(G))\n        inf = float('inf')\n\n        def predict(u, v):\n            if u == v:\n                raise nx.NetworkXAlgorithmError('Self links are not supported')\n            path_len = spl[u].get(v, inf)\n            return alpha * sum((1 for _ in nx.common_neighbors(G, u, v))) + (1 - alpha) * (G.number_of_nodes() / path_len)\n    return _apply_prediction(G, predict, ebunch)"
 },
 {
  "docstring": "Compute the preferential attachment score of all node pairs in ebunch.\n\nPreferential attachment score of `u` and `v` is defined as\n\n.. math::\n\n    |\\Gamma(u)| |\\Gamma(v)|\n\nwhere $\\Gamma(u)$ denotes the set of neighbors of $u$.\n\nParameters\n----------\nG : graph\n    NetworkX undirected graph.\n\nebunch : iterable of node pairs, optional (default = None)\n    Preferential attachment score will be computed for each pair of\n    nodes given in the iterable. The pairs must be given as\n    2-tuples (u, v) where u and v are nodes in the graph. If ebunch\n    is None then all nonexistent edges in the graph will be used.\n    Default value: None.\n\nReturns\n-------\npiter : iterator\n    An iterator of 3-tuples in the form (u, v, p) where (u, v) is a\n    pair of nodes and p is their preferential attachment score.\n\nExamples\n--------\n>>> G = nx.complete_graph(5)\n>>> preds = nx.preferential_attachment(G, [(0, 1), (2, 3)])\n>>> for u, v, p in preds:\n...     print(f\"({u}, {v}) -> {p}\")\n(0, 1) -> 16\n(2, 3) -> 16\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef preferential_attachment(G, ebunch=None):\n\n    def predict(u, v):\n        return G.degree(u) * G.degree(v)\n    return _apply_prediction(G, predict, ebunch)"
 },
 {
  "docstring": "Count the number of common neighbors of all node pairs in ebunch\n    using community information.\n\nFor two nodes $u$ and $v$, this function computes the number of\ncommon neighbors and bonus one for each common neighbor belonging to\nthe same community as $u$ and $v$. Mathematically,\n\n.. math::\n\n    |\\Gamma(u) \\cap \\Gamma(v)| + \\sum_{w \\in \\Gamma(u) \\cap \\Gamma(v)} f(w)\n\nwhere $f(w)$ equals 1 if $w$ belongs to the same community as $u$\nand $v$ or 0 otherwise and $\\Gamma(u)$ denotes the set of\nneighbors of $u$.\n\nParameters\n----------\nG : graph\n    A NetworkX undirected graph.\n\nebunch : iterable of node pairs, optional (default = None)\n    The score will be computed for each pair of nodes given in the\n    iterable. The pairs must be given as 2-tuples (u, v) where u\n    and v are nodes in the graph. If ebunch is None then all\n    nonexistent edges in the graph will be used.\n    Default value: None.\n\ncommunity : string, optional (default = 'community')\n    Nodes attribute name containing the community information.\n    G[u][community] identifies which community u belongs to. Each\n    node belongs to at most one community. Default value: 'community'.\n\nReturns\n-------\npiter : iterator\n    An iterator of 3-tuples in the form (u, v, p) where (u, v) is a\n    pair of nodes and p is their score.\n\nExamples\n--------\n>>> G = nx.path_graph(3)\n>>> G.nodes[0][\"community\"] = 0\n>>> G.nodes[1][\"community\"] = 0\n>>> G.nodes[2][\"community\"] = 0\n>>> preds = nx.cn_soundarajan_hopcroft(G, [(0, 2)])\n>>> for u, v, p in preds:\n...     print(f\"({u}, {v}) -> {p}\")\n(0, 2) -> 2\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch(node_attrs='community')\ndef cn_soundarajan_hopcroft(G, ebunch=None, community='community'):\n\n    def predict(u, v):\n        Cu = _community(G, u, community)\n        Cv = _community(G, v, community)\n        cnbors = list(nx.common_neighbors(G, u, v))\n        neighbors = sum((_community(G, w, community) == Cu for w in cnbors)) if Cu == Cv else 0\n        return len(cnbors) + neighbors\n    return _apply_prediction(G, predict, ebunch)"
 },
 {
  "docstring": "Compute the resource allocation index of all node pairs in\nebunch using community information.\n\nFor two nodes $u$ and $v$, this function computes the resource\nallocation index considering only common neighbors belonging to the\nsame community as $u$ and $v$. Mathematically,\n\n.. math::\n\n    \\sum_{w \\in \\Gamma(u) \\cap \\Gamma(v)} \\frac{f(w)}{|\\Gamma(w)|}\n\nwhere $f(w)$ equals 1 if $w$ belongs to the same community as $u$\nand $v$ or 0 otherwise and $\\Gamma(u)$ denotes the set of\nneighbors of $u$.\n\nParameters\n----------\nG : graph\n    A NetworkX undirected graph.\n\nebunch : iterable of node pairs, optional (default = None)\n    The score will be computed for each pair of nodes given in the\n    iterable. The pairs must be given as 2-tuples (u, v) where u\n    and v are nodes in the graph. If ebunch is None then all\n    nonexistent edges in the graph will be used.\n    Default value: None.\n\ncommunity : string, optional (default = 'community')\n    Nodes attribute name containing the community information.\n    G[u][community] identifies which community u belongs to. Each\n    node belongs to at most one community. Default value: 'community'.\n\nReturns\n-------\npiter : iterator\n    An iterator of 3-tuples in the form (u, v, p) where (u, v) is a\n    pair of nodes and p is their score.\n\nExamples\n--------\n>>> G = nx.Graph()\n>>> G.add_edges_from([(0, 1), (0, 2), (1, 3), (2, 3)])\n>>> G.nodes[0][\"community\"] = 0\n>>> G.nodes[1][\"community\"] = 0\n>>> G.nodes[2][\"community\"] = 1\n>>> G.nodes[3][\"community\"] = 0\n>>> preds = nx.ra_index_soundarajan_hopcroft(G, [(0, 3)])\n>>> for u, v, p in preds:\n...     print(f\"({u}, {v}) -> {p:.8f}\")\n(0, 3) -> 0.50000000\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch(node_attrs='community')\ndef ra_index_soundarajan_hopcroft(G, ebunch=None, community='community'):\n\n    def predict(u, v):\n        Cu = _community(G, u, community)\n        Cv = _community(G, v, community)\n        if Cu != Cv:\n            return 0\n        cnbors = nx.common_neighbors(G, u, v)\n        return sum((1 / G.degree(w) for w in cnbors if _community(G, w, community) == Cu))\n    return _apply_prediction(G, predict, ebunch)"
 },
 {
  "docstring": "Compute the ratio of within- and inter-cluster common neighbors\nof all node pairs in ebunch.\n\nFor two nodes `u` and `v`, if a common neighbor `w` belongs to the\nsame community as them, `w` is considered as within-cluster common\nneighbor of `u` and `v`. Otherwise, it is considered as\ninter-cluster common neighbor of `u` and `v`. The ratio between the\nsize of the set of within- and inter-cluster common neighbors is\ndefined as the WIC measure. [1]_\n\nParameters\n----------\nG : graph\n    A NetworkX undirected graph.\n\nebunch : iterable of node pairs, optional (default = None)\n    The WIC measure will be computed for each pair of nodes given in\n    the iterable. The pairs must be given as 2-tuples (u, v) where\n    u and v are nodes in the graph. If ebunch is None then all\n    nonexistent edges in the graph will be used.\n    Default value: None.\n\ndelta : float, optional (default = 0.001)\n    Value to prevent division by zero in case there is no\n    inter-cluster common neighbor between two nodes. See [1]_ for\n    details. Default value: 0.001.\n\ncommunity : string, optional (default = 'community')\n    Nodes attribute name containing the community information.\n    G[u][community] identifies which community u belongs to. Each\n    node belongs to at most one community. Default value: 'community'.\n\nReturns\n-------\npiter : iterator\n    An iterator of 3-tuples in the form (u, v, p) where (u, v) is a\n    pair of nodes and p is their WIC measure.\n\nExamples\n--------\n>>> G = nx.Graph()\n>>> G.add_edges_from([(0, 1), (0, 2), (0, 3), (1, 4), (2, 4), (3, 4)])\n>>> G.nodes[0][\"community\"] = 0\n>>> G.nodes[1][\"community\"] = 1\n>>> G.nodes[2][\"community\"] = 0\n>>> G.nodes[3][\"community\"] = 0\n>>> G.nodes[4][\"community\"] = 0\n>>> preds = nx.within_inter_cluster(G, [(0, 4)])\n>>> for u, v, p in preds:\n...     print(f\"({u}, {v}) -> {p:.8f}\")\n(0, 4) -> 1.99800200\n>>> preds = nx.within_inter_cluster(G, [(0, 4)], delta=0.5)\n>>> for u, v, p in preds:\n...     print(f\"({u}, {v}) -> {p:.8f}\")\n(0, 4) -> 1.33333333\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch(node_attrs='community')\ndef within_inter_cluster(G, ebunch=None, delta=0.001, community='community'):\n    if delta <= 0:\n        raise nx.NetworkXAlgorithmError('Delta must be greater than zero')\n\n    def predict(u, v):\n        Cu = _community(G, u, community)\n        Cv = _community(G, v, community)\n        if Cu != Cv:\n            return 0\n        cnbors = set(nx.common_neighbors(G, u, v))\n        within = {w for w in cnbors if _community(G, w, community) == Cu}\n        inter = cnbors - within\n        return len(within) / (len(inter) + delta)\n    return _apply_prediction(G, predict, ebunch)"
 },
 {
  "docstring": "Get the community of the given node.",
  "code": "def _community(G, u, community):\n    node_u = G.nodes[u]\n    try:\n        return node_u[community]\n    except KeyError as err:\n        raise nx.NetworkXAlgorithmError('No community information') from err"
 },
 {
  "docstring": "Find a maximal matching in the graph.\n\nA matching is a subset of edges in which no node occurs more than once.\nA maximal matching cannot add more edges and still be a matching.\n\nParameters\n----------\nG : NetworkX graph\n    Undirected graph\n\nReturns\n-------\nmatching : set\n    A maximal matching of the graph.\n\nExamples\n--------\n>>> G = nx.Graph([(1, 2), (1, 3), (2, 3), (2, 4), (3, 5), (4, 5)])\n>>> sorted(nx.maximal_matching(G))\n[(1, 2), (3, 5)]\n\n",
  "code": "@not_implemented_for('multigraph')\n@not_implemented_for('directed')\n@nx._dispatch\ndef maximal_matching(G):\n    matching = set()\n    nodes = set()\n    for edge in G.edges():\n        u, v = edge\n        if u not in nodes and v not in nodes and (u != v):\n            matching.add(edge)\n            nodes.update(edge)\n    return matching"
 },
 {
  "docstring": "Converts matching dict format to matching set format\n\nConverts a dictionary representing a matching (as returned by\n:func:`max_weight_matching`) to a set representing a matching (as\nreturned by :func:`maximal_matching`).\n\nIn the definition of maximal matching adopted by NetworkX,\nself-loops are not allowed, so the provided dictionary is expected\nto never have any mapping from a key to itself. However, the\ndictionary is expected to have mirrored key/value pairs, for\nexample, key ``u`` with value ``v`` and key ``v`` with value ``u``.",
  "code": "def matching_dict_to_set(matching):\n    edges = set()\n    for edge in matching.items():\n        u, v = edge\n        if (v, u) in edges or edge in edges:\n            continue\n        if u == v:\n            raise nx.NetworkXError(f'Selfloops cannot appear in matchings {edge}')\n        edges.add(edge)\n    return edges"
 },
 {
  "docstring": "Return True if ``matching`` is a valid matching of ``G``\n\nA *matching* in a graph is a set of edges in which no two distinct\nedges share a common endpoint. Each node is incident to at most one\nedge in the matching. The edges are said to be independent.\n\nParameters\n----------\nG : NetworkX graph\n\nmatching : dict or set\n    A dictionary or set representing a matching. If a dictionary, it\n    must have ``matching[u] == v`` and ``matching[v] == u`` for each\n    edge ``(u, v)`` in the matching. If a set, it must have elements\n    of the form ``(u, v)``, where ``(u, v)`` is an edge in the\n    matching.\n\nReturns\n-------\nbool\n    Whether the given set or dictionary represents a valid matching\n    in the graph.\n\nRaises\n------\nNetworkXError\n    If the proposed matching has an edge to a node not in G.\n    Or if the matching is not a collection of 2-tuple edges.\n\nExamples\n--------\n>>> G = nx.Graph([(1, 2), (1, 3), (2, 3), (2, 4), (3, 5), (4, 5)])\n>>> nx.is_maximal_matching(G, {1: 3, 2: 4})  # using dict to represent matching\nTrue\n\n>>> nx.is_matching(G, {(1, 3), (2, 4)})  # using set to represent matching\nTrue",
  "code": "@nx._dispatch\ndef is_matching(G, matching):\n    if isinstance(matching, dict):\n        matching = matching_dict_to_set(matching)\n    nodes = set()\n    for edge in matching:\n        if len(edge) != 2:\n            raise nx.NetworkXError(f'matching has non-2-tuple edge {edge}')\n        u, v = edge\n        if u not in G or v not in G:\n            raise nx.NetworkXError(f'matching contains edge {edge} with node not in G')\n        if u == v:\n            return False\n        if not G.has_edge(u, v):\n            return False\n        if u in nodes or v in nodes:\n            return False\n        nodes.update(edge)\n    return True"
 },
 {
  "docstring": "Return True if ``matching`` is a maximal matching of ``G``\n\nA *maximal matching* in a graph is a matching in which adding any\nedge would cause the set to no longer be a valid matching.\n\nParameters\n----------\nG : NetworkX graph\n\nmatching : dict or set\n    A dictionary or set representing a matching. If a dictionary, it\n    must have ``matching[u] == v`` and ``matching[v] == u`` for each\n    edge ``(u, v)`` in the matching. If a set, it must have elements\n    of the form ``(u, v)``, where ``(u, v)`` is an edge in the\n    matching.\n\nReturns\n-------\nbool\n    Whether the given set or dictionary represents a valid maximal\n    matching in the graph.\n\nExamples\n--------\n>>> G = nx.Graph([(1, 2), (1, 3), (2, 3), (3, 4), (3, 5)])\n>>> nx.is_maximal_matching(G, {(1, 2), (3, 4)})\nTrue",
  "code": "@nx._dispatch\ndef is_maximal_matching(G, matching):\n    if isinstance(matching, dict):\n        matching = matching_dict_to_set(matching)\n    edges = set()\n    nodes = set()\n    for edge in matching:\n        if len(edge) != 2:\n            raise nx.NetworkXError(f'matching has non-2-tuple edge {edge}')\n        u, v = edge\n        if u not in G or v not in G:\n            raise nx.NetworkXError(f'matching contains edge {edge} with node not in G')\n        if u == v:\n            return False\n        if not G.has_edge(u, v):\n            return False\n        if u in nodes or v in nodes:\n            return False\n        nodes.update(edge)\n        edges.add(edge)\n        edges.add((v, u))\n    for u, v in G.edges:\n        if (u, v) not in edges:\n            if u not in nodes and v not in nodes and (u != v):\n                return False\n    return True"
 },
 {
  "docstring": "Return True if ``matching`` is a perfect matching for ``G``\n\nA *perfect matching* in a graph is a matching in which exactly one edge\nis incident upon each vertex.\n\nParameters\n----------\nG : NetworkX graph\n\nmatching : dict or set\n    A dictionary or set representing a matching. If a dictionary, it\n    must have ``matching[u] == v`` and ``matching[v] == u`` for each\n    edge ``(u, v)`` in the matching. If a set, it must have elements\n    of the form ``(u, v)``, where ``(u, v)`` is an edge in the\n    matching.\n\nReturns\n-------\nbool\n    Whether the given set or dictionary represents a valid perfect\n    matching in the graph.\n\nExamples\n--------\n>>> G = nx.Graph([(1, 2), (1, 3), (2, 3), (2, 4), (3, 5), (4, 5), (4, 6)])\n>>> my_match = {1: 2, 3: 5, 4: 6}\n>>> nx.is_perfect_matching(G, my_match)\nTrue",
  "code": "@nx._dispatch\ndef is_perfect_matching(G, matching):\n    if isinstance(matching, dict):\n        matching = matching_dict_to_set(matching)\n    nodes = set()\n    for edge in matching:\n        if len(edge) != 2:\n            raise nx.NetworkXError(f'matching has non-2-tuple edge {edge}')\n        u, v = edge\n        if u not in G or v not in G:\n            raise nx.NetworkXError(f'matching contains edge {edge} with node not in G')\n        if u == v:\n            return False\n        if not G.has_edge(u, v):\n            return False\n        if u in nodes or v in nodes:\n            return False\n        nodes.update(edge)\n    return len(nodes) == len(G)"
 },
 {
  "docstring": "Computing a minimum-weight maximal matching of G.\n\nUse the maximum-weight algorithm with edge weights subtracted\nfrom the maximum weight of all edges.\n\nA matching is a subset of edges in which no node occurs more than once.\nThe weight of a matching is the sum of the weights of its edges.\nA maximal matching cannot add more edges and still be a matching.\nThe cardinality of a matching is the number of matched edges.\n\nThis method replaces the edge weights with 1 plus the maximum edge weight\nminus the original edge weight.\n\nnew_weight = (max_weight + 1) - edge_weight\n\nthen runs :func:`max_weight_matching` with the new weights.\nThe max weight matching with these new weights corresponds\nto the min weight matching using the original weights.\nAdding 1 to the max edge weight keeps all edge weights positive\nand as integers if they started as integers.\n\nYou might worry that adding 1 to each weight would make the algorithm\nfavor matchings with more edges. But we use the parameter\n`maxcardinality=True` in `max_weight_matching` to ensure that the\nnumber of edges in the competing matchings are the same and thus\nthe optimum does not change due to changes in the number of edges.\n\nRead the documentation of `max_weight_matching` for more information.\n\nParameters\n----------\nG : NetworkX graph\n  Undirected graph\n\nweight: string, optional (default='weight')\n   Edge data key corresponding to the edge weight.\n   If key not found, uses 1 as weight.\n\nReturns\n-------\nmatching : set\n    A minimal weight matching of the graph.\n\nSee Also\n--------\nmax_weight_matching",
  "code": "@not_implemented_for('multigraph')\n@not_implemented_for('directed')\n@nx._dispatch(edge_attrs='weight')\ndef min_weight_matching(G, weight='weight'):\n    if len(G.edges) == 0:\n        return max_weight_matching(G, maxcardinality=True, weight=weight)\n    G_edges = G.edges(data=weight, default=1)\n    max_weight = 1 + max((w for _, _, w in G_edges))\n    InvG = nx.Graph()\n    edges = ((u, v, max_weight - w) for u, v, w in G_edges)\n    InvG.add_weighted_edges_from(edges, weight=weight)\n    return max_weight_matching(InvG, maxcardinality=True, weight=weight)"
 },
 {
  "docstring": "Compute a maximum-weighted matching of G.\n\nA matching is a subset of edges in which no node occurs more than once.\nThe weight of a matching is the sum of the weights of its edges.\nA maximal matching cannot add more edges and still be a matching.\nThe cardinality of a matching is the number of matched edges.\n\nParameters\n----------\nG : NetworkX graph\n  Undirected graph\n\nmaxcardinality: bool, optional (default=False)\n   If maxcardinality is True, compute the maximum-cardinality matching\n   with maximum weight among all maximum-cardinality matchings.\n\nweight: string, optional (default='weight')\n   Edge data key corresponding to the edge weight.\n   If key not found, uses 1 as weight.\n\n\nReturns\n-------\nmatching : set\n    A maximal matching of the graph.\n\n Examples\n--------\n>>> G = nx.Graph()\n>>> edges = [(1, 2, 6), (1, 3, 2), (2, 3, 1), (2, 4, 7), (3, 5, 9), (4, 5, 3)]\n>>> G.add_weighted_edges_from(edges)\n>>> sorted(nx.max_weight_matching(G))\n[(2, 4), (5, 3)]\n\n",
  "code": "@not_implemented_for('multigraph')\n@not_implemented_for('directed')\n@nx._dispatch(edge_attrs='weight')\ndef max_weight_matching(G, maxcardinality=False, weight='weight'):\n\n    class NoNode:\n        \"\"\"Dummy value which is different from any node.\"\"\"\n\n    class Blossom:\n        \"\"\"Representation of a non-trivial blossom or sub-blossom.\"\"\"\n        __slots__ = ['childs', 'edges', 'mybestedges']\n\n        def leaves(self):\n            stack = [*self.childs]\n            while stack:\n                t = stack.pop()\n                if isinstance(t, Blossom):\n                    stack.extend(t.childs)\n                else:\n                    yield t\n    gnodes = list(G)\n    if not gnodes:\n        return set()\n    maxweight = 0\n    allinteger = True\n    for i, j, d in G.edges(data=True):\n        wt = d.get(weight, 1)\n        if i != j and wt > maxweight:\n            maxweight = wt\n        allinteger = allinteger and str(type(wt)).split(\"'\")[1] in ('int', 'long')\n    mate = {}\n    label = {}\n    labeledge = {}\n    inblossom = dict(zip(gnodes, gnodes))\n    blossomparent = dict(zip(gnodes, repeat(None)))\n    blossombase = dict(zip(gnodes, gnodes))\n    bestedge = {}\n    dualvar = dict(zip(gnodes, repeat(maxweight)))\n    blossomdual = {}\n    allowedge = {}\n    queue = []\n\n    def slack(v, w):\n        return dualvar[v] + dualvar[w] - 2 * G[v][w].get(weight, 1)\n\n    def assignLabel(w, t, v):\n        b = inblossom[w]\n        assert label.get(w) is None and label.get(b) is None\n        label[w] = label[b] = t\n        if v is not None:\n            labeledge[w] = labeledge[b] = (v, w)\n        else:\n            labeledge[w] = labeledge[b] = None\n        bestedge[w] = bestedge[b] = None\n        if t == 1:\n            if isinstance(b, Blossom):\n                queue.extend(b.leaves())\n            else:\n                queue.append(b)\n        elif t == 2:\n            base = blossombase[b]\n            assignLabel(mate[base], 1, base)\n\n    def scanBlossom(v, w):\n        path = []\n        base = NoNode\n        while v is not NoNode:\n            b = inblossom[v]\n            if label[b] & 4:\n                base = blossombase[b]\n                break\n            assert label[b] == 1\n            path.append(b)\n            label[b] = 5\n            if labeledge[b] is None:\n                assert blossombase[b] not in mate\n                v = NoNode\n            else:\n                assert labeledge[b][0] == mate[blossombase[b]]\n                v = labeledge[b][0]\n                b = inblossom[v]\n                assert label[b] == 2\n                v = labeledge[b][0]\n            if w is not NoNode:\n                v, w = (w, v)\n        for b in path:\n            label[b] = 1\n        return base\n\n    def addBlossom(base, v, w):\n        bb = inblossom[base]\n        bv = inblossom[v]\n        bw = inblossom[w]\n        b = Blossom()\n        blossombase[b] = base\n        blossomparent[b] = None\n        blossomparent[bb] = b\n        b.childs = path = []\n        b.edges = edgs = [(v, w)]\n        while bv != bb:\n            blossomparent[bv] = b\n            path.append(bv)\n            edgs.append(labeledge[bv])\n            assert label[bv] == 2 or (label[bv] == 1 and labeledge[bv][0] == mate[blossombase[bv]])\n            v = labeledge[bv][0]\n            bv = inblossom[v]\n        path.append(bb)\n        path.reverse()\n        edgs.reverse()\n        while bw != bb:\n            blossomparent[bw] = b\n            path.append(bw)\n            edgs.append((labeledge[bw][1], labeledge[bw][0]))\n            assert label[bw] == 2 or (label[bw] == 1 and labeledge[bw][0] == mate[blossombase[bw]])\n            w = labeledge[bw][0]\n            bw = inblossom[w]\n        assert label[bb] == 1\n        label[b] = 1\n        labeledge[b] = labeledge[bb]\n        blossomdual[b] = 0\n        for v in b.leaves():\n            if label[inblossom[v]] == 2:\n                queue.append(v)\n            inblossom[v] = b\n        bestedgeto = {}\n        for bv in path:\n            if isinstance(bv, Blossom):\n                if bv.mybestedges is not None:\n                    nblist = bv.mybestedges\n                    bv.mybestedges = None\n                else:\n                    nblist = [(v, w) for v in bv.leaves() for w in G.neighbors(v) if v != w]\n            else:\n                nblist = [(bv, w) for w in G.neighbors(bv) if bv != w]\n            for k in nblist:\n                i, j = k\n                if inblossom[j] == b:\n                    i, j = (j, i)\n                bj = inblossom[j]\n                if bj != b and label.get(bj) == 1 and (bj not in bestedgeto or slack(i, j) < slack(*bestedgeto[bj])):\n                    bestedgeto[bj] = k\n            bestedge[bv] = None\n        b.mybestedges = list(bestedgeto.values())\n        mybestedge = None\n        bestedge[b] = None\n        for k in b.mybestedges:\n            kslack = slack(*k)\n            if mybestedge is None or kslack < mybestslack:\n                mybestedge = k\n                mybestslack = kslack\n        bestedge[b] = mybestedge\n\n    def expandBlossom(b, endstage):\n\n        def _recurse(b, endstage):\n            for s in b.childs:\n                blossomparent[s] = None\n                if isinstance(s, Blossom):\n                    if endstage and blossomdual[s] == 0:\n                        yield s\n                    else:\n                        for v in s.leaves():\n                            inblossom[v] = s\n                else:\n                    inblossom[s] = s\n            if not endstage and label.get(b) == 2:\n                entrychild = inblossom[labeledge[b][1]]\n                j = b.childs.index(entrychild)\n                if j & 1:\n                    j -= len(b.childs)\n                    jstep = 1\n                else:\n                    jstep = -1\n                v, w = labeledge[b]\n                while j != 0:\n                    if jstep == 1:\n                        p, q = b.edges[j]\n                    else:\n                        q, p = b.edges[j - 1]\n                    label[w] = None\n                    label[q] = None\n                    assignLabel(w, 2, v)\n                    allowedge[p, q] = allowedge[q, p] = True\n                    j += jstep\n                    if jstep == 1:\n                        v, w = b.edges[j]\n                    else:\n                        w, v = b.edges[j - 1]\n                    allowedge[v, w] = allowedge[w, v] = True\n                    j += jstep\n                bw = b.childs[j]\n                label[w] = label[bw] = 2\n                labeledge[w] = labeledge[bw] = (v, w)\n                bestedge[bw] = None\n                j += jstep\n                while b.childs[j] != entrychild:\n                    bv = b.childs[j]\n                    if label.get(bv) == 1:\n                        j += jstep\n                        continue\n                    if isinstance(bv, Blossom):\n                        for v in bv.leaves():\n                            if label.get(v):\n                                break\n                    else:\n                        v = bv\n                    if label.get(v):\n                        assert label[v] == 2\n                        assert inblossom[v] == bv\n                        label[v] = None\n                        label[mate[blossombase[bv]]] = None\n                        assignLabel(v, 2, labeledge[v][0])\n                    j += jstep\n            label.pop(b, None)\n            labeledge.pop(b, None)\n            bestedge.pop(b, None)\n            del blossomparent[b]\n            del blossombase[b]\n            del blossomdual[b]\n        stack = [_recurse(b, endstage)]\n        while stack:\n            top = stack[-1]\n            for s in top:\n                stack.append(_recurse(s, endstage))\n                break\n            else:\n                stack.pop()\n\n    def augmentBlossom(b, v):\n\n        def _recurse(b, v):\n            t = v\n            while blossomparent[t] != b:\n                t = blossomparent[t]\n            if isinstance(t, Blossom):\n                yield (t, v)\n            i = j = b.childs.index(t)\n            if i & 1:\n                j -= len(b.childs)\n                jstep = 1\n            else:\n                jstep = -1\n            while j != 0:\n                j += jstep\n                t = b.childs[j]\n                if jstep == 1:\n                    w, x = b.edges[j]\n                else:\n                    x, w = b.edges[j - 1]\n                if isinstance(t, Blossom):\n                    yield (t, w)\n                j += jstep\n                t = b.childs[j]\n                if isinstance(t, Blossom):\n                    yield (t, x)\n                mate[w] = x\n                mate[x] = w\n            b.childs = b.childs[i:] + b.childs[:i]\n            b.edges = b.edges[i:] + b.edges[:i]\n            blossombase[b] = blossombase[b.childs[0]]\n            assert blossombase[b] == v\n        stack = [_recurse(b, v)]\n        while stack:\n            top = stack[-1]\n            for args in top:\n                stack.append(_recurse(*args))\n                break\n            else:\n                stack.pop()\n\n    def augmentMatching(v, w):\n        for s, j in ((v, w), (w, v)):\n            while 1:\n                bs = inblossom[s]\n                assert label[bs] == 1\n                assert labeledge[bs] is None and blossombase[bs] not in mate or labeledge[bs][0] == mate[blossombase[bs]]\n                if isinstance(bs, Blossom):\n                    augmentBlossom(bs, s)\n                mate[s] = j\n                if labeledge[bs] is None:\n                    break\n                t = labeledge[bs][0]\n                bt = inblossom[t]\n                assert label[bt] == 2\n                s, j = labeledge[bt]\n                assert blossombase[bt] == t\n                if isinstance(bt, Blossom):\n                    augmentBlossom(bt, j)\n                mate[j] = s\n\n    def verifyOptimum():\n        if maxcardinality:\n            vdualoffset = max(0, -min(dualvar.values()))\n        else:\n            vdualoffset = 0\n        assert min(dualvar.values()) + vdualoffset >= 0\n        assert len(blossomdual) == 0 or min(blossomdual.values()) >= 0\n        for i, j, d in G.edges(data=True):\n            wt = d.get(weight, 1)\n            if i == j:\n                continue\n            s = dualvar[i] + dualvar[j] - 2 * wt\n            iblossoms = [i]\n            jblossoms = [j]\n            while blossomparent[iblossoms[-1]] is not None:\n                iblossoms.append(blossomparent[iblossoms[-1]])\n            while blossomparent[jblossoms[-1]] is not None:\n                jblossoms.append(blossomparent[jblossoms[-1]])\n            iblossoms.reverse()\n            jblossoms.reverse()\n            for bi, bj in zip(iblossoms, jblossoms):\n                if bi != bj:\n                    break\n                s += 2 * blossomdual[bi]\n            assert s >= 0\n            if mate.get(i) == j or mate.get(j) == i:\n                assert mate[i] == j and mate[j] == i\n                assert s == 0\n        for v in gnodes:\n            assert v in mate or dualvar[v] + vdualoffset == 0\n        for b in blossomdual:\n            if blossomdual[b] > 0:\n                assert len(b.edges) % 2 == 1\n                for i, j in b.edges[1::2]:\n                    assert mate[i] == j and mate[j] == i\n    while 1:\n        label.clear()\n        labeledge.clear()\n        bestedge.clear()\n        for b in blossomdual:\n            b.mybestedges = None\n        allowedge.clear()\n        queue[:] = []\n        for v in gnodes:\n            if v not in mate and label.get(inblossom[v]) is None:\n                assignLabel(v, 1, None)\n        augmented = 0\n        while 1:\n            while queue and (not augmented):\n                v = queue.pop()\n                assert label[inblossom[v]] == 1\n                for w in G.neighbors(v):\n                    if w == v:\n                        continue\n                    bv = inblossom[v]\n                    bw = inblossom[w]\n                    if bv == bw:\n                        continue\n                    if (v, w) not in allowedge:\n                        kslack = slack(v, w)\n                        if kslack <= 0:\n                            allowedge[v, w] = allowedge[w, v] = True\n                    if (v, w) in allowedge:\n                        if label.get(bw) is None:\n                            assignLabel(w, 2, v)\n                        elif label.get(bw) == 1:\n                            base = scanBlossom(v, w)\n                            if base is not NoNode:\n                                addBlossom(base, v, w)\n                            else:\n                                augmentMatching(v, w)\n                                augmented = 1\n                                break\n                        elif label.get(w) is None:\n                            assert label[bw] == 2\n                            label[w] = 2\n                            labeledge[w] = (v, w)\n                    elif label.get(bw) == 1:\n                        if bestedge.get(bv) is None or kslack < slack(*bestedge[bv]):\n                            bestedge[bv] = (v, w)\n                    elif label.get(w) is None:\n                        if bestedge.get(w) is None or kslack < slack(*bestedge[w]):\n                            bestedge[w] = (v, w)\n            if augmented:\n                break\n            deltatype = -1\n            delta = deltaedge = deltablossom = None\n            if not maxcardinality:\n                deltatype = 1\n                delta = min(dualvar.values())\n            for v in G.nodes():\n                if label.get(inblossom[v]) is None and bestedge.get(v) is not None:\n                    d = slack(*bestedge[v])\n                    if deltatype == -1 or d < delta:\n                        delta = d\n                        deltatype = 2\n                        deltaedge = bestedge[v]\n            for b in blossomparent:\n                if blossomparent[b] is None and label.get(b) == 1 and (bestedge.get(b) is not None):\n                    kslack = slack(*bestedge[b])\n                    if allinteger:\n                        assert kslack % 2 == 0\n                        d = kslack // 2\n                    else:\n                        d = kslack / 2.0\n                    if deltatype == -1 or d < delta:\n                        delta = d\n                        deltatype = 3\n                        deltaedge = bestedge[b]\n            for b in blossomdual:\n                if blossomparent[b] is None and label.get(b) == 2 and (deltatype == -1 or blossomdual[b] < delta):\n                    delta = blossomdual[b]\n                    deltatype = 4\n                    deltablossom = b\n            if deltatype == -1:\n                assert maxcardinality\n                deltatype = 1\n                delta = max(0, min(dualvar.values()))\n            for v in gnodes:\n                if label.get(inblossom[v]) == 1:\n                    dualvar[v] -= delta\n                elif label.get(inblossom[v]) == 2:\n                    dualvar[v] += delta\n            for b in blossomdual:\n                if blossomparent[b] is None:\n                    if label.get(b) == 1:\n                        blossomdual[b] += delta\n                    elif label.get(b) == 2:\n                        blossomdual[b] -= delta\n            if deltatype == 1:\n                break\n            elif deltatype == 2:\n                v, w = deltaedge\n                assert label[inblossom[v]] == 1\n                allowedge[v, w] = allowedge[w, v] = True\n                queue.append(v)\n            elif deltatype == 3:\n                v, w = deltaedge\n                allowedge[v, w] = allowedge[w, v] = True\n                assert label[inblossom[v]] == 1\n                queue.append(v)\n            elif deltatype == 4:\n                expandBlossom(deltablossom, False)\n        for v in mate:\n            assert mate[mate[v]] == v\n        if not augmented:\n            break\n        for b in list(blossomdual.keys()):\n            if b not in blossomdual:\n                continue\n            if blossomparent[b] is None and label.get(b) == 1 and (blossomdual[b] == 0):\n                expandBlossom(b, True)\n    if allinteger:\n        verifyOptimum()\n    return matching_dict_to_set(mate)"
 },
 {
  "docstring": "Returns a random maximal independent set guaranteed to contain\na given set of nodes.\n\nAn independent set is a set of nodes such that the subgraph\nof G induced by these nodes contains no edges. A maximal\nindependent set is an independent set such that it is not possible\nto add a new node and still get an independent set.\n\nParameters\n----------\nG : NetworkX graph\n\nnodes : list or iterable\n   Nodes that must be part of the independent set. This set of nodes\n   must be independent.\n\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\nReturns\n-------\nindep_nodes : list\n   List of nodes that are part of a maximal independent set.\n\nRaises\n------\nNetworkXUnfeasible\n   If the nodes in the provided list are not part of the graph or\n   do not form an independent set, an exception is raised.\n\nNetworkXNotImplemented\n    If `G` is directed.\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> nx.maximal_independent_set(G)  # doctest: +SKIP\n[4, 0, 2]\n>>> nx.maximal_independent_set(G, [1])  # doctest: +SKIP\n[1, 3]\n\n",
  "code": "@not_implemented_for('directed')\n@py_random_state(2)\n@nx._dispatch\ndef maximal_independent_set(G, nodes=None, seed=None):\n    if not nodes:\n        nodes = {seed.choice(list(G))}\n    else:\n        nodes = set(nodes)\n    if not nodes.issubset(G):\n        raise nx.NetworkXUnfeasible(f'{nodes} is not a subset of the nodes of G')\n    neighbors = set.union(*[set(G.adj[v]) for v in nodes])\n    if set.intersection(neighbors, nodes):\n        raise nx.NetworkXUnfeasible(f'{nodes} is not an independent set of G')\n    indep_nodes = list(nodes)\n    available_nodes = set(G.nodes()).difference(neighbors.union(nodes))\n    while available_nodes:\n        node = seed.choice(list(available_nodes))\n        indep_nodes.append(node)\n        available_nodes.difference_update(list(G.adj[node]) + [node])\n    return indep_nodes"
 },
 {
  "docstring": "Return the Moral Graph\n\nReturns the moralized graph of a given directed graph.\n\nParameters\n----------\nG : NetworkX graph\n    Directed graph\n\nReturns\n-------\nH : NetworkX graph\n    The undirected moralized graph of G\n\nRaises\n------\nNetworkXNotImplemented\n    If `G` is undirected.\n\nExamples\n--------\n>>> G = nx.DiGraph([(1, 2), (2, 3), (2, 5), (3, 4), (4, 3)])\n>>> G_moral = nx.moral_graph(G)\n>>> G_moral.edges()\nEdgeView([(1, 2), (2, 3), (2, 5), (2, 4), (3, 4)])\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef moral_graph(G):\n    H = G.to_undirected()\n    for preds in G.pred.values():\n        predecessors_combinations = itertools.combinations(preds, r=2)\n        H.add_edges_from(predecessors_combinations)\n    return H"
 },
 {
  "docstring": "Node classification by Harmonic function\n\nFunction for computing Harmonic function algorithm by Zhu et al.\n\nParameters\n----------\nG : NetworkX Graph\nmax_iter : int\n    maximum number of iterations allowed\nlabel_name : string\n    name of target labels to predict\n\nReturns\n-------\npredicted : list\n    List of length ``len(G)`` with the predicted labels for each node.\n\nRaises\n------\nNetworkXError\n    If no nodes in `G` have attribute `label_name`.\n\nExamples\n--------\n>>> from networkx.algorithms import node_classification\n>>> G = nx.path_graph(4)\n>>> G.nodes[0][\"label\"] = \"A\"\n>>> G.nodes[3][\"label\"] = \"B\"\n>>> G.nodes(data=True)\nNodeDataView({0: {'label': 'A'}, 1: {}, 2: {}, 3: {'label': 'B'}})\n>>> G.edges()\nEdgeView([(0, 1), (1, 2), (2, 3)])\n>>> predicted = node_classification.harmonic_function(G)\n>>> predicted\n['A', 'A', 'B', 'B']\n\n",
  "code": "@nx.utils.not_implemented_for('directed')\n@nx._dispatch(node_attrs='label_name')\ndef harmonic_function(G, max_iter=30, label_name='label'):\n    import numpy as np\n    import scipy as sp\n    X = nx.to_scipy_sparse_array(G)\n    labels, label_dict = _get_label_info(G, label_name)\n    if labels.shape[0] == 0:\n        raise nx.NetworkXError(f\"No node on the input graph is labeled by '{label_name}'.\")\n    n_samples = X.shape[0]\n    n_classes = label_dict.shape[0]\n    F = np.zeros((n_samples, n_classes))\n    degrees = X.sum(axis=0)\n    degrees[degrees == 0] = 1\n    D = sp.sparse.csr_array(sp.sparse.diags(1.0 / degrees, offsets=0))\n    P = (D @ X).tolil()\n    P[labels[:, 0]] = 0\n    B = np.zeros((n_samples, n_classes))\n    B[labels[:, 0], labels[:, 1]] = 1\n    for _ in range(max_iter):\n        F = P @ F + B\n    return label_dict[np.argmax(F, axis=1)].tolist()"
 },
 {
  "docstring": "Node classification by Local and Global Consistency\n\nFunction for computing Local and global consistency algorithm by Zhou et al.\n\nParameters\n----------\nG : NetworkX Graph\nalpha : float\n    Clamping factor\nmax_iter : int\n    Maximum number of iterations allowed\nlabel_name : string\n    Name of target labels to predict\n\nReturns\n-------\npredicted : list\n    List of length ``len(G)`` with the predicted labels for each node.\n\nRaises\n------\nNetworkXError\n    If no nodes in `G` have attribute `label_name`.\n\nExamples\n--------\n>>> from networkx.algorithms import node_classification\n>>> G = nx.path_graph(4)\n>>> G.nodes[0][\"label\"] = \"A\"\n>>> G.nodes[3][\"label\"] = \"B\"\n>>> G.nodes(data=True)\nNodeDataView({0: {'label': 'A'}, 1: {}, 2: {}, 3: {'label': 'B'}})\n>>> G.edges()\nEdgeView([(0, 1), (1, 2), (2, 3)])\n>>> predicted = node_classification.local_and_global_consistency(G)\n>>> predicted\n['A', 'A', 'B', 'B']\n\n",
  "code": "@nx.utils.not_implemented_for('directed')\n@nx._dispatch(node_attrs='label_name')\ndef local_and_global_consistency(G, alpha=0.99, max_iter=30, label_name='label'):\n    import numpy as np\n    import scipy as sp\n    X = nx.to_scipy_sparse_array(G)\n    labels, label_dict = _get_label_info(G, label_name)\n    if labels.shape[0] == 0:\n        raise nx.NetworkXError(f\"No node on the input graph is labeled by '{label_name}'.\")\n    n_samples = X.shape[0]\n    n_classes = label_dict.shape[0]\n    F = np.zeros((n_samples, n_classes))\n    degrees = X.sum(axis=0)\n    degrees[degrees == 0] = 1\n    D2 = np.sqrt(sp.sparse.csr_array(sp.sparse.diags(1.0 / degrees, offsets=0)))\n    P = alpha * (D2 @ X @ D2)\n    B = np.zeros((n_samples, n_classes))\n    B[labels[:, 0], labels[:, 1]] = 1 - alpha\n    for _ in range(max_iter):\n        F = P @ F + B\n    return label_dict[np.argmax(F, axis=1)].tolist()"
 },
 {
  "docstring": "Get and return information of labels from the input graph\n\nParameters\n----------\nG : Network X graph\nlabel_name : string\n    Name of the target label\n\nReturns\n-------\nlabels : numpy array, shape = [n_labeled_samples, 2]\n    Array of pairs of labeled node ID and label ID\nlabel_dict : numpy array, shape = [n_classes]\n    Array of labels\n    i-th element contains the label corresponding label ID `i`",
  "code": "def _get_label_info(G, label_name):\n    import numpy as np\n    labels = []\n    label_to_id = {}\n    lid = 0\n    for i, n in enumerate(G.nodes(data=True)):\n        if label_name in n[1]:\n            label = n[1][label_name]\n            if label not in label_to_id:\n                label_to_id[label] = lid\n                lid += 1\n            labels.append([i, label_to_id[label]])\n    labels = np.array(labels)\n    label_dict = np.array([label for label, _ in sorted(label_to_id.items(), key=lambda x: x[1])])\n    return (labels, label_dict)"
 },
 {
  "docstring": "Compute the non-randomness of graph G.\n\nThe first returned value nr is the sum of non-randomness values of all\nedges within the graph (where the non-randomness of an edge tends to be\nsmall when the two nodes linked by that edge are from two different\ncommunities).\n\nThe second computed value nr_rd is a relative measure that indicates\nto what extent graph G is different from random graphs in terms\nof probability. When it is close to 0, the graph tends to be more\nlikely generated by an Erdos Renyi model.\n\nParameters\n----------\nG : NetworkX graph\n    Graph must be symmetric, connected, and without self-loops.\n\nk : int\n    The number of communities in G.\n    If k is not set, the function will use a default community\n    detection algorithm to set it.\n\nweight : string or None, optional (default=None)\n    The name of an edge attribute that holds the numerical value used\n    as a weight. If None, then each edge has weight 1, i.e., the graph is\n    binary.\n\nReturns\n-------\nnon-randomness : (float, float) tuple\n    Non-randomness, Relative non-randomness w.r.t.\n    Erdos Renyi random graphs.\n\nRaises\n------\nNetworkXException\n    if the input graph is not connected.\nNetworkXError\n    if the input graph contains self-loops.\n\nExamples\n--------\n>>> G = nx.karate_club_graph()\n>>> nr, nr_rd = nx.non_randomness(G, 2)\n>>> nr, nr_rd = nx.non_randomness(G, 2, 'weight')\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch(edge_attrs='weight')\ndef non_randomness(G, k=None, weight='weight'):\n    import numpy as np\n    if not nx.is_connected(G):\n        raise nx.NetworkXException('Non connected graph.')\n    if len(list(nx.selfloop_edges(G))) > 0:\n        raise nx.NetworkXError('Graph must not contain self-loops')\n    if k is None:\n        k = len(tuple(nx.community.label_propagation_communities(G)))\n    eigenvalues = np.linalg.eigvals(nx.to_numpy_array(G, weight=weight))\n    nr = np.real(np.sum(eigenvalues[:k]))\n    n = G.number_of_nodes()\n    m = G.number_of_edges()\n    p = 2 * k * m / (n * (n - k))\n    nr_rd = (nr - ((n - 2 * k) * p + k)) / math.sqrt(2 * k * p * (1 - p))\n    return (nr, nr_rd)"
 },
 {
  "docstring": "Returns True if and only if `G` is planar.\n\nA graph is *planar* iff it can be drawn in a plane without\nany edge intersections.\n\nParameters\n----------\nG : NetworkX graph\n\nReturns\n-------\nbool\n   Whether the graph is planar.\n\nExamples\n--------\n>>> G = nx.Graph([(0, 1), (0, 2)])\n>>> nx.is_planar(G)\nTrue\n>>> nx.is_planar(nx.complete_graph(5))\nFalse\n\nSee Also\n--------\ncheck_planarity :\n    Check if graph is planar *and* return a `PlanarEmbedding` instance if True.",
  "code": "@nx._dispatch\ndef is_planar(G):\n    return check_planarity(G, counterexample=False)[0]"
 },
 {
  "docstring": "Check if a graph is planar and return a counterexample or an embedding.\n\nA graph is planar iff it can be drawn in a plane without\nany edge intersections.\n\nParameters\n----------\nG : NetworkX graph\ncounterexample : bool\n    A Kuratowski subgraph (to proof non planarity) is only returned if set\n    to true.\n\nReturns\n-------\n(is_planar, certificate) : (bool, NetworkX graph) tuple\n    is_planar is true if the graph is planar.\n    If the graph is planar `certificate` is a PlanarEmbedding\n    otherwise it is a Kuratowski subgraph.\n\nExamples\n--------\n>>> G = nx.Graph([(0, 1), (0, 2)])\n>>> is_planar, P = nx.check_planarity(G)\n>>> print(is_planar)\nTrue\n\nWhen `G` is planar, a `PlanarEmbedding` instance is returned:\n\n>>> P.get_data()\n{0: [1, 2], 1: [0], 2: [0]}\n\n",
  "code": "@nx._dispatch\ndef check_planarity(G, counterexample=False):\n    planarity_state = LRPlanarity(G)\n    embedding = planarity_state.lr_planarity()\n    if embedding is None:\n        if counterexample:\n            return (False, get_counterexample(G))\n        else:\n            return (False, None)\n    else:\n        return (True, embedding)"
 },
 {
  "docstring": "Recursive version of :meth:`check_planarity`.",
  "code": "@nx._dispatch\ndef check_planarity_recursive(G, counterexample=False):\n    planarity_state = LRPlanarity(G)\n    embedding = planarity_state.lr_planarity_recursive()\n    if embedding is None:\n        if counterexample:\n            return (False, get_counterexample_recursive(G))\n        else:\n            return (False, None)\n    else:\n        return (True, embedding)"
 },
 {
  "docstring": "Obtains a Kuratowski subgraph.\n\nRaises nx.NetworkXException if G is planar.\n\nThe function removes edges such that the graph is still not planar.\nAt some point the removal of any edge would make the graph planar.\nThis subgraph must be a Kuratowski subgraph.\n\nParameters\n----------\nG : NetworkX graph\n\nReturns\n-------\nsubgraph : NetworkX graph\n    A Kuratowski subgraph that proves that G is not planar.",
  "code": "@nx._dispatch\ndef get_counterexample(G):\n    G = nx.Graph(G)\n    if check_planarity(G)[0]:\n        raise nx.NetworkXException('G is planar - no counter example.')\n    subgraph = nx.Graph()\n    for u in G:\n        nbrs = list(G[u])\n        for v in nbrs:\n            G.remove_edge(u, v)\n            if check_planarity(G)[0]:\n                G.add_edge(u, v)\n                subgraph.add_edge(u, v)\n    return subgraph"
 },
 {
  "docstring": "Recursive version of :meth:`get_counterexample`.",
  "code": "@nx._dispatch\ndef get_counterexample_recursive(G):\n    G = nx.Graph(G)\n    if check_planarity_recursive(G)[0]:\n        raise nx.NetworkXException('G is planar - no counter example.')\n    subgraph = nx.Graph()\n    for u in G:\n        nbrs = list(G[u])\n        for v in nbrs:\n            G.remove_edge(u, v)\n            if check_planarity_recursive(G)[0]:\n                G.add_edge(u, v)\n                subgraph.add_edge(u, v)\n    return subgraph"
 },
 {
  "docstring": "Returns the element on top of the stack.",
  "code": "def top_of_stack(l):\n    if not l:\n        return None\n    return l[-1]"
 },
 {
  "docstring": "Check if the interval is empty",
  "code": "def empty(self):\n    return self.low is None and self.high is None"
 },
 {
  "docstring": "Returns a copy of this interval",
  "code": "def copy(self):\n    return Interval(self.low, self.high)"
 },
 {
  "docstring": "Returns True if interval I conflicts with edge b",
  "code": "def conflicting(self, b, planarity_state):\n    return not self.empty() and planarity_state.lowpt[self.high] > planarity_state.lowpt[b]"
 },
 {
  "docstring": "Swap left and right intervals",
  "code": "def swap(self):\n    temp = self.left\n    self.left = self.right\n    self.right = temp"
 },
 {
  "docstring": "Returns the lowest lowpoint of a conflict pair",
  "code": "def lowest(self, planarity_state):\n    if self.left.empty():\n        return planarity_state.lowpt[self.right.low]\n    if self.right.empty():\n        return planarity_state.lowpt[self.left.low]\n    return min(planarity_state.lowpt[self.left.low], planarity_state.lowpt[self.right.low])"
 },
 {
  "docstring": "Execute the LR planarity test.\n\nReturns\n-------\nembedding : dict\n    If the graph is planar an embedding is returned. Otherwise None.",
  "code": "def lr_planarity(self):\n    if self.G.order() > 2 and self.G.size() > 3 * self.G.order() - 6:\n        return None\n    for v in self.G:\n        self.adjs[v] = list(self.G[v])\n    for v in self.G:\n        if self.height[v] is None:\n            self.height[v] = 0\n            self.roots.append(v)\n            self.dfs_orientation(v)\n    self.G = None\n    self.lowpt2 = None\n    self.adjs = None\n    for v in self.DG:\n        self.ordered_adjs[v] = sorted(self.DG[v], key=lambda x: self.nesting_depth[v, x])\n    for v in self.roots:\n        if not self.dfs_testing(v):\n            return None\n    self.height = None\n    self.lowpt = None\n    self.S = None\n    self.stack_bottom = None\n    self.lowpt_edge = None\n    for e in self.DG.edges:\n        self.nesting_depth[e] = self.sign(e) * self.nesting_depth[e]\n    self.embedding.add_nodes_from(self.DG.nodes)\n    for v in self.DG:\n        self.ordered_adjs[v] = sorted(self.DG[v], key=lambda x: self.nesting_depth[v, x])\n        previous_node = None\n        for w in self.ordered_adjs[v]:\n            self.embedding.add_half_edge_cw(v, w, previous_node)\n            previous_node = w\n    self.DG = None\n    self.nesting_depth = None\n    self.ref = None\n    for v in self.roots:\n        self.dfs_embedding(v)\n    self.roots = None\n    self.parent_edge = None\n    self.ordered_adjs = None\n    self.left_ref = None\n    self.right_ref = None\n    self.side = None\n    return self.embedding"
 },
 {
  "docstring": "Recursive version of :meth:`lr_planarity`.",
  "code": "def lr_planarity_recursive(self):\n    if self.G.order() > 2 and self.G.size() > 3 * self.G.order() - 6:\n        return None\n    for v in self.G:\n        if self.height[v] is None:\n            self.height[v] = 0\n            self.roots.append(v)\n            self.dfs_orientation_recursive(v)\n    self.G = None\n    for v in self.DG:\n        self.ordered_adjs[v] = sorted(self.DG[v], key=lambda x: self.nesting_depth[v, x])\n    for v in self.roots:\n        if not self.dfs_testing_recursive(v):\n            return None\n    for e in self.DG.edges:\n        self.nesting_depth[e] = self.sign_recursive(e) * self.nesting_depth[e]\n    self.embedding.add_nodes_from(self.DG.nodes)\n    for v in self.DG:\n        self.ordered_adjs[v] = sorted(self.DG[v], key=lambda x: self.nesting_depth[v, x])\n        previous_node = None\n        for w in self.ordered_adjs[v]:\n            self.embedding.add_half_edge_cw(v, w, previous_node)\n            previous_node = w\n    for v in self.roots:\n        self.dfs_embedding_recursive(v)\n    return self.embedding"
 },
 {
  "docstring": "Orient the graph by DFS, compute lowpoints and nesting order.",
  "code": "def dfs_orientation(self, v):\n    dfs_stack = [v]\n    ind = defaultdict(lambda: 0)\n    skip_init = defaultdict(lambda: False)\n    while dfs_stack:\n        v = dfs_stack.pop()\n        e = self.parent_edge[v]\n        for w in self.adjs[v][ind[v]:]:\n            vw = (v, w)\n            if not skip_init[vw]:\n                if (v, w) in self.DG.edges or (w, v) in self.DG.edges:\n                    ind[v] += 1\n                    continue\n                self.DG.add_edge(v, w)\n                self.lowpt[vw] = self.height[v]\n                self.lowpt2[vw] = self.height[v]\n                if self.height[w] is None:\n                    self.parent_edge[w] = vw\n                    self.height[w] = self.height[v] + 1\n                    dfs_stack.append(v)\n                    dfs_stack.append(w)\n                    skip_init[vw] = True\n                    break\n                else:\n                    self.lowpt[vw] = self.height[w]\n            self.nesting_depth[vw] = 2 * self.lowpt[vw]\n            if self.lowpt2[vw] < self.height[v]:\n                self.nesting_depth[vw] += 1\n            if e is not None:\n                if self.lowpt[vw] < self.lowpt[e]:\n                    self.lowpt2[e] = min(self.lowpt[e], self.lowpt2[vw])\n                    self.lowpt[e] = self.lowpt[vw]\n                elif self.lowpt[vw] > self.lowpt[e]:\n                    self.lowpt2[e] = min(self.lowpt2[e], self.lowpt[vw])\n                else:\n                    self.lowpt2[e] = min(self.lowpt2[e], self.lowpt2[vw])\n            ind[v] += 1"
 },
 {
  "docstring": "Recursive version of :meth:`dfs_orientation`.",
  "code": "def dfs_orientation_recursive(self, v):\n    e = self.parent_edge[v]\n    for w in self.G[v]:\n        if (v, w) in self.DG.edges or (w, v) in self.DG.edges:\n            continue\n        vw = (v, w)\n        self.DG.add_edge(v, w)\n        self.lowpt[vw] = self.height[v]\n        self.lowpt2[vw] = self.height[v]\n        if self.height[w] is None:\n            self.parent_edge[w] = vw\n            self.height[w] = self.height[v] + 1\n            self.dfs_orientation_recursive(w)\n        else:\n            self.lowpt[vw] = self.height[w]\n        self.nesting_depth[vw] = 2 * self.lowpt[vw]\n        if self.lowpt2[vw] < self.height[v]:\n            self.nesting_depth[vw] += 1\n        if e is not None:\n            if self.lowpt[vw] < self.lowpt[e]:\n                self.lowpt2[e] = min(self.lowpt[e], self.lowpt2[vw])\n                self.lowpt[e] = self.lowpt[vw]\n            elif self.lowpt[vw] > self.lowpt[e]:\n                self.lowpt2[e] = min(self.lowpt2[e], self.lowpt[vw])\n            else:\n                self.lowpt2[e] = min(self.lowpt2[e], self.lowpt2[vw])"
 },
 {
  "docstring": "Test for LR partition.",
  "code": "def dfs_testing(self, v):\n    dfs_stack = [v]\n    ind = defaultdict(lambda: 0)\n    skip_init = defaultdict(lambda: False)\n    while dfs_stack:\n        v = dfs_stack.pop()\n        e = self.parent_edge[v]\n        skip_final = False\n        for w in self.ordered_adjs[v][ind[v]:]:\n            ei = (v, w)\n            if not skip_init[ei]:\n                self.stack_bottom[ei] = top_of_stack(self.S)\n                if ei == self.parent_edge[w]:\n                    dfs_stack.append(v)\n                    dfs_stack.append(w)\n                    skip_init[ei] = True\n                    skip_final = True\n                    break\n                else:\n                    self.lowpt_edge[ei] = ei\n                    self.S.append(ConflictPair(right=Interval(ei, ei)))\n            if self.lowpt[ei] < self.height[v]:\n                if w == self.ordered_adjs[v][0]:\n                    self.lowpt_edge[e] = self.lowpt_edge[ei]\n                elif not self.add_constraints(ei, e):\n                    return False\n            ind[v] += 1\n        if not skip_final:\n            if e is not None:\n                self.remove_back_edges(e)\n    return True"
 },
 {
  "docstring": "Recursive version of :meth:`dfs_testing`.",
  "code": "def dfs_testing_recursive(self, v):\n    e = self.parent_edge[v]\n    for w in self.ordered_adjs[v]:\n        ei = (v, w)\n        self.stack_bottom[ei] = top_of_stack(self.S)\n        if ei == self.parent_edge[w]:\n            if not self.dfs_testing_recursive(w):\n                return False\n        else:\n            self.lowpt_edge[ei] = ei\n            self.S.append(ConflictPair(right=Interval(ei, ei)))\n        if self.lowpt[ei] < self.height[v]:\n            if w == self.ordered_adjs[v][0]:\n                self.lowpt_edge[e] = self.lowpt_edge[ei]\n            elif not self.add_constraints(ei, e):\n                return False\n    if e is not None:\n        self.remove_back_edges(e)\n    return True"
 },
 {
  "docstring": "Completes the embedding.",
  "code": "def dfs_embedding(self, v):\n    dfs_stack = [v]\n    ind = defaultdict(lambda: 0)\n    while dfs_stack:\n        v = dfs_stack.pop()\n        for w in self.ordered_adjs[v][ind[v]:]:\n            ind[v] += 1\n            ei = (v, w)\n            if ei == self.parent_edge[w]:\n                self.embedding.add_half_edge_first(w, v)\n                self.left_ref[v] = w\n                self.right_ref[v] = w\n                dfs_stack.append(v)\n                dfs_stack.append(w)\n                break\n            elif self.side[ei] == 1:\n                self.embedding.add_half_edge_cw(w, v, self.right_ref[w])\n            else:\n                self.embedding.add_half_edge_ccw(w, v, self.left_ref[w])\n                self.left_ref[w] = v"
 },
 {
  "docstring": "Recursive version of :meth:`dfs_embedding`.",
  "code": "def dfs_embedding_recursive(self, v):\n    for w in self.ordered_adjs[v]:\n        ei = (v, w)\n        if ei == self.parent_edge[w]:\n            self.embedding.add_half_edge_first(w, v)\n            self.left_ref[v] = w\n            self.right_ref[v] = w\n            self.dfs_embedding_recursive(w)\n        elif self.side[ei] == 1:\n            self.embedding.add_half_edge_cw(w, v, self.right_ref[w])\n        else:\n            self.embedding.add_half_edge_ccw(w, v, self.left_ref[w])\n            self.left_ref[w] = v"
 },
 {
  "docstring": "Resolve the relative side of an edge to the absolute side.",
  "code": "def sign(self, e):\n    dfs_stack = [e]\n    old_ref = defaultdict(lambda: None)\n    while dfs_stack:\n        e = dfs_stack.pop()\n        if self.ref[e] is not None:\n            dfs_stack.append(e)\n            dfs_stack.append(self.ref[e])\n            old_ref[e] = self.ref[e]\n            self.ref[e] = None\n        else:\n            self.side[e] *= self.side[old_ref[e]]\n    return self.side[e]"
 },
 {
  "docstring": "Recursive version of :meth:`sign`.",
  "code": "def sign_recursive(self, e):\n    if self.ref[e] is not None:\n        self.side[e] = self.side[e] * self.sign_recursive(self.ref[e])\n        self.ref[e] = None\n    return self.side[e]"
 },
 {
  "docstring": "Converts the adjacency structure into a better readable structure.\n\nReturns\n-------\nembedding : dict\n    A dict mapping all nodes to a list of neighbors sorted in\n    clockwise order.\n\nSee Also\n--------\nset_data",
  "code": "def get_data(self):\n    embedding = {}\n    for v in self:\n        embedding[v] = list(self.neighbors_cw_order(v))\n    return embedding"
 },
 {
  "docstring": "Inserts edges according to given sorted neighbor list.\n\nThe input format is the same as the output format of get_data().\n\nParameters\n----------\ndata : dict\n    A dict mapping all nodes to a list of neighbors sorted in\n    clockwise order.\n\nSee Also\n--------\nget_data",
  "code": "def set_data(self, data):\n    for v in data:\n        for w in reversed(data[v]):\n            self.add_half_edge_first(v, w)"
 },
 {
  "docstring": "Generator for the neighbors of v in clockwise order.\n\nParameters\n----------\nv : node\n\nYields\n------\nnode",
  "code": "def neighbors_cw_order(self, v):\n    if len(self[v]) == 0:\n        return\n    start_node = self.nodes[v]['first_nbr']\n    yield start_node\n    current_node = self[v][start_node]['cw']\n    while start_node != current_node:\n        yield current_node\n        current_node = self[v][current_node]['cw']"
 },
 {
  "docstring": "Runs without exceptions if this object is valid.\n\nChecks that the following properties are fulfilled:\n\n* Edges go in both directions (because the edge attributes differ).\n* Every edge has a 'cw' and 'ccw' attribute which corresponds to a\n  correct planar embedding.\n* A node with a degree larger than 0 has a node attribute 'first_nbr'.\n\nRunning this method verifies that the underlying Graph must be planar.\n\nRaises\n------\nNetworkXException\n    This exception is raised with a short explanation if the\n    PlanarEmbedding is invalid.",
  "code": "def check_structure(self):\n    for v in self:\n        try:\n            sorted_nbrs = set(self.neighbors_cw_order(v))\n        except KeyError as err:\n            msg = f'Bad embedding. Missing orientation for a neighbor of {v}'\n            raise nx.NetworkXException(msg) from err\n        unsorted_nbrs = set(self[v])\n        if sorted_nbrs != unsorted_nbrs:\n            msg = 'Bad embedding. Edge orientations not set correctly.'\n            raise nx.NetworkXException(msg)\n        for w in self[v]:\n            if not self.has_edge(w, v):\n                msg = 'Bad embedding. Opposite half-edge is missing.'\n                raise nx.NetworkXException(msg)\n    counted_half_edges = set()\n    for component in nx.connected_components(self):\n        if len(component) == 1:\n            continue\n        num_nodes = len(component)\n        num_half_edges = 0\n        num_faces = 0\n        for v in component:\n            for w in self.neighbors_cw_order(v):\n                num_half_edges += 1\n                if (v, w) not in counted_half_edges:\n                    num_faces += 1\n                    self.traverse_face(v, w, counted_half_edges)\n        num_edges = num_half_edges // 2\n        if num_nodes - num_edges + num_faces != 2:\n            msg = \"Bad embedding. The graph does not match Euler's formula\"\n            raise nx.NetworkXException(msg)"
 },
 {
  "docstring": "Adds a half-edge from start_node to end_node.\n\nThe half-edge is added counter clockwise next to the existing half-edge\n(start_node, reference_neighbor).\n\nParameters\n----------\nstart_node : node\n    Start node of inserted edge.\nend_node : node\n    End node of inserted edge.\nreference_neighbor: node\n    End node of reference edge.\n\nRaises\n------\nNetworkXException\n    If the reference_neighbor does not exist.\n\nSee Also\n--------\nadd_half_edge_cw\nconnect_components\nadd_half_edge_first",
  "code": "def add_half_edge_ccw(self, start_node, end_node, reference_neighbor):\n    if reference_neighbor is None:\n        self.add_edge(start_node, end_node)\n        self[start_node][end_node]['cw'] = end_node\n        self[start_node][end_node]['ccw'] = end_node\n        self.nodes[start_node]['first_nbr'] = end_node\n    else:\n        ccw_reference = self[start_node][reference_neighbor]['ccw']\n        self.add_half_edge_cw(start_node, end_node, ccw_reference)\n        if reference_neighbor == self.nodes[start_node].get('first_nbr', None):\n            self.nodes[start_node]['first_nbr'] = end_node"
 },
 {
  "docstring": "Adds a half-edge from start_node to end_node.\n\nThe half-edge is added clockwise next to the existing half-edge\n(start_node, reference_neighbor).\n\nParameters\n----------\nstart_node : node\n    Start node of inserted edge.\nend_node : node\n    End node of inserted edge.\nreference_neighbor: node\n    End node of reference edge.\n\nRaises\n------\nNetworkXException\n    If the reference_neighbor does not exist.\n\nSee Also\n--------\nadd_half_edge_ccw\nconnect_components\nadd_half_edge_first",
  "code": "def add_half_edge_cw(self, start_node, end_node, reference_neighbor):\n    self.add_edge(start_node, end_node)\n    if reference_neighbor is None:\n        self[start_node][end_node]['cw'] = end_node\n        self[start_node][end_node]['ccw'] = end_node\n        self.nodes[start_node]['first_nbr'] = end_node\n        return\n    if reference_neighbor not in self[start_node]:\n        raise nx.NetworkXException('Cannot add edge. Reference neighbor does not exist')\n    cw_reference = self[start_node][reference_neighbor]['cw']\n    self[start_node][reference_neighbor]['cw'] = end_node\n    self[start_node][end_node]['cw'] = cw_reference\n    self[start_node][cw_reference]['ccw'] = end_node\n    self[start_node][end_node]['ccw'] = reference_neighbor"
 },
 {
  "docstring": "Adds half-edges for (v, w) and (w, v) at some position.\n\nThis method should only be called if v and w are in different\ncomponents, or it might break the embedding.\nThis especially means that if `connect_components(v, w)`\nis called it is not allowed to call `connect_components(w, v)`\nafterwards. The neighbor orientations in both directions are\nall set correctly after the first call.\n\nParameters\n----------\nv : node\nw : node\n\nSee Also\n--------\nadd_half_edge_ccw\nadd_half_edge_cw\nadd_half_edge_first",
  "code": "def connect_components(self, v, w):\n    self.add_half_edge_first(v, w)\n    self.add_half_edge_first(w, v)"
 },
 {
  "docstring": "The added half-edge is inserted at the first position in the order.\n\nParameters\n----------\nstart_node : node\nend_node : node\n\nSee Also\n--------\nadd_half_edge_ccw\nadd_half_edge_cw\nconnect_components",
  "code": "def add_half_edge_first(self, start_node, end_node):\n    if start_node in self and 'first_nbr' in self.nodes[start_node]:\n        reference = self.nodes[start_node]['first_nbr']\n    else:\n        reference = None\n    self.add_half_edge_ccw(start_node, end_node, reference)"
 },
 {
  "docstring": "Returns the following half-edge left of a face.\n\nParameters\n----------\nv : node\nw : node\n\nReturns\n-------\nhalf-edge : tuple",
  "code": "def next_face_half_edge(self, v, w):\n    new_node = self[w][v]['ccw']\n    return (w, new_node)"
 },
 {
  "docstring": "Returns nodes on the face that belong to the half-edge (v, w).\n\nThe face that is traversed lies to the right of the half-edge (in an\norientation where v is below w).\n\nOptionally it is possible to pass a set to which all encountered half\nedges are added. Before calling this method, this set must not include\nany half-edges that belong to the face.\n\nParameters\n----------\nv : node\n    Start node of half-edge.\nw : node\n    End node of half-edge.\nmark_half_edges: set, optional\n    Set to which all encountered half-edges are added.\n\nReturns\n-------\nface : list\n    A list of nodes that lie on this face.",
  "code": "def traverse_face(self, v, w, mark_half_edges=None):\n    if mark_half_edges is None:\n        mark_half_edges = set()\n    face_nodes = [v]\n    mark_half_edges.add((v, w))\n    prev_node = v\n    cur_node = w\n    incoming_node = self[v][w]['cw']\n    while cur_node != v or prev_node != incoming_node:\n        face_nodes.append(cur_node)\n        prev_node, cur_node = self.next_face_half_edge(prev_node, cur_node)\n        if (prev_node, cur_node) in mark_half_edges:\n            raise nx.NetworkXException('Bad planar embedding. Impossible face.')\n        mark_half_edges.add((prev_node, cur_node))\n    return face_nodes"
 },
 {
  "docstring": "A valid PlanarEmbedding is undirected.\n\nAll reverse edges are contained, i.e. for every existing\nhalf-edge (v, w) the half-edge in the opposite direction (w, v) is also\ncontained.",
  "code": "def is_directed(self):\n    return False"
 },
 {
  "docstring": "Assigns every node a (x, y) position based on the given embedding\n\nThe algorithm iteratively inserts nodes of the input graph in a certain\norder and rearranges previously inserted nodes so that the planar drawing\nstays valid. This is done efficiently by only maintaining relative\npositions during the node placements and calculating the absolute positions\nat the end. For more information see [1]_.\n\nParameters\n----------\nembedding : nx.PlanarEmbedding\n    This defines the order of the edges\n\nfully_triangulate : bool\n    If set to True the algorithm adds edges to a copy of the input\n    embedding and makes it chordal.\n\nReturns\n-------\npos : dict\n    Maps each node to a tuple that defines the (x, y) position\n\n",
  "code": "def combinatorial_embedding_to_pos(embedding, fully_triangulate=False):\n    if len(embedding.nodes()) < 4:\n        default_positions = [(0, 0), (2, 0), (1, 1)]\n        pos = {}\n        for i, v in enumerate(embedding.nodes()):\n            pos[v] = default_positions[i]\n        return pos\n    embedding, outer_face = triangulate_embedding(embedding, fully_triangulate)\n    left_t_child = {}\n    right_t_child = {}\n    delta_x = {}\n    y_coordinate = {}\n    node_list = get_canonical_ordering(embedding, outer_face)\n    v1, v2, v3 = (node_list[0][0], node_list[1][0], node_list[2][0])\n    delta_x[v1] = 0\n    y_coordinate[v1] = 0\n    right_t_child[v1] = v3\n    left_t_child[v1] = None\n    delta_x[v2] = 1\n    y_coordinate[v2] = 0\n    right_t_child[v2] = None\n    left_t_child[v2] = None\n    delta_x[v3] = 1\n    y_coordinate[v3] = 1\n    right_t_child[v3] = v2\n    left_t_child[v3] = None\n    for k in range(3, len(node_list)):\n        vk, contour_neighbors = node_list[k]\n        wp = contour_neighbors[0]\n        wp1 = contour_neighbors[1]\n        wq = contour_neighbors[-1]\n        wq1 = contour_neighbors[-2]\n        adds_mult_tri = len(contour_neighbors) > 2\n        delta_x[wp1] += 1\n        delta_x[wq] += 1\n        delta_x_wp_wq = sum((delta_x[x] for x in contour_neighbors[1:]))\n        delta_x[vk] = (-y_coordinate[wp] + delta_x_wp_wq + y_coordinate[wq]) // 2\n        y_coordinate[vk] = (y_coordinate[wp] + delta_x_wp_wq + y_coordinate[wq]) // 2\n        delta_x[wq] = delta_x_wp_wq - delta_x[vk]\n        if adds_mult_tri:\n            delta_x[wp1] -= delta_x[vk]\n        right_t_child[wp] = vk\n        right_t_child[vk] = wq\n        if adds_mult_tri:\n            left_t_child[vk] = wp1\n            right_t_child[wq1] = None\n        else:\n            left_t_child[vk] = None\n    pos = {}\n    pos[v1] = (0, y_coordinate[v1])\n    remaining_nodes = [v1]\n    while remaining_nodes:\n        parent_node = remaining_nodes.pop()\n        set_position(parent_node, left_t_child, remaining_nodes, delta_x, y_coordinate, pos)\n        set_position(parent_node, right_t_child, remaining_nodes, delta_x, y_coordinate, pos)\n    return pos"
 },
 {
  "docstring": "Helper method to calculate the absolute position of nodes.",
  "code": "def set_position(parent, tree, remaining_nodes, delta_x, y_coordinate, pos):\n    child = tree[parent]\n    parent_node_x = pos[parent][0]\n    if child is not None:\n        child_x = parent_node_x + delta_x[child]\n        pos[child] = (child_x, y_coordinate[child])\n        remaining_nodes.append(child)"
 },
 {
  "docstring": "Returns a canonical ordering of the nodes\n\nThe canonical ordering of nodes (v1, ..., vn) must fulfill the following\nconditions:\n(See Lemma 1 in [2]_)\n\n- For the subgraph G_k of the input graph induced by v1, ..., vk it holds:\n    - 2-connected\n    - internally triangulated\n    - the edge (v1, v2) is part of the outer face\n- For a node v(k+1) the following holds:\n    - The node v(k+1) is part of the outer face of G_k\n    - It has at least two neighbors in G_k\n    - All neighbors of v(k+1) in G_k lie consecutively on the outer face of\n      G_k (excluding the edge (v1, v2)).\n\nThe algorithm used here starts with G_n (containing all nodes). It first\nselects the nodes v1 and v2. And then tries to find the order of the other\nnodes by checking which node can be removed in order to fulfill the\nconditions mentioned above. This is done by calculating the number of\nchords of nodes on the outer face. For more information see [1]_.\n\nParameters\n----------\nembedding : nx.PlanarEmbedding\n    The embedding must be triangulated\nouter_face : list\n    The nodes on the outer face of the graph\n\nReturns\n-------\nordering : list\n    A list of tuples `(vk, wp_wq)`. Here `vk` is the node at this position\n    in the canonical ordering. The element `wp_wq` is a list of nodes that\n    make up the outer face of G_k.\n\n",
  "code": "def get_canonical_ordering(embedding, outer_face):\n    v1 = outer_face[0]\n    v2 = outer_face[1]\n    chords = defaultdict(int)\n    marked_nodes = set()\n    ready_to_pick = set(outer_face)\n    outer_face_ccw_nbr = {}\n    prev_nbr = v2\n    for idx in range(2, len(outer_face)):\n        outer_face_ccw_nbr[prev_nbr] = outer_face[idx]\n        prev_nbr = outer_face[idx]\n    outer_face_ccw_nbr[prev_nbr] = v1\n    outer_face_cw_nbr = {}\n    prev_nbr = v1\n    for idx in range(len(outer_face) - 1, 0, -1):\n        outer_face_cw_nbr[prev_nbr] = outer_face[idx]\n        prev_nbr = outer_face[idx]\n\n    def is_outer_face_nbr(x, y):\n        if x not in outer_face_ccw_nbr:\n            return outer_face_cw_nbr[x] == y\n        if x not in outer_face_cw_nbr:\n            return outer_face_ccw_nbr[x] == y\n        return outer_face_ccw_nbr[x] == y or outer_face_cw_nbr[x] == y\n\n    def is_on_outer_face(x):\n        return x not in marked_nodes and (x in outer_face_ccw_nbr or x == v1)\n    for v in outer_face:\n        for nbr in embedding.neighbors_cw_order(v):\n            if is_on_outer_face(nbr) and (not is_outer_face_nbr(v, nbr)):\n                chords[v] += 1\n                ready_to_pick.discard(v)\n    canonical_ordering = [None] * len(embedding.nodes())\n    canonical_ordering[0] = (v1, [])\n    canonical_ordering[1] = (v2, [])\n    ready_to_pick.discard(v1)\n    ready_to_pick.discard(v2)\n    for k in range(len(embedding.nodes()) - 1, 1, -1):\n        v = ready_to_pick.pop()\n        marked_nodes.add(v)\n        wp = None\n        wq = None\n        nbr_iterator = iter(embedding.neighbors_cw_order(v))\n        while True:\n            nbr = next(nbr_iterator)\n            if nbr in marked_nodes:\n                continue\n            if is_on_outer_face(nbr):\n                if nbr == v1:\n                    wp = v1\n                elif nbr == v2:\n                    wq = v2\n                elif outer_face_cw_nbr[nbr] == v:\n                    wp = nbr\n                else:\n                    wq = nbr\n            if wp is not None and wq is not None:\n                break\n        wp_wq = [wp]\n        nbr = wp\n        while nbr != wq:\n            next_nbr = embedding[v][nbr]['ccw']\n            wp_wq.append(next_nbr)\n            outer_face_cw_nbr[nbr] = next_nbr\n            outer_face_ccw_nbr[next_nbr] = nbr\n            nbr = next_nbr\n        if len(wp_wq) == 2:\n            chords[wp] -= 1\n            if chords[wp] == 0:\n                ready_to_pick.add(wp)\n            chords[wq] -= 1\n            if chords[wq] == 0:\n                ready_to_pick.add(wq)\n        else:\n            new_face_nodes = set(wp_wq[1:-1])\n            for w in new_face_nodes:\n                ready_to_pick.add(w)\n                for nbr in embedding.neighbors_cw_order(w):\n                    if is_on_outer_face(nbr) and (not is_outer_face_nbr(w, nbr)):\n                        chords[w] += 1\n                        ready_to_pick.discard(w)\n                        if nbr not in new_face_nodes:\n                            chords[nbr] += 1\n                            ready_to_pick.discard(nbr)\n        canonical_ordering[k] = (v, wp_wq)\n    return canonical_ordering"
 },
 {
  "docstring": "Triangulates the face given by half edge (v, w)\n\nParameters\n----------\nembedding : nx.PlanarEmbedding\nv1 : node\n    The half-edge (v1, v2) belongs to the face that gets triangulated\nv2 : node",
  "code": "def triangulate_face(embedding, v1, v2):\n    _, v3 = embedding.next_face_half_edge(v1, v2)\n    _, v4 = embedding.next_face_half_edge(v2, v3)\n    if v1 in (v2, v3):\n        return\n    while v1 != v4:\n        if embedding.has_edge(v1, v3):\n            v1, v2, v3 = (v2, v3, v4)\n        else:\n            embedding.add_half_edge_cw(v1, v3, v2)\n            embedding.add_half_edge_ccw(v3, v1, v2)\n            v1, v2, v3 = (v1, v3, v4)\n        _, v4 = embedding.next_face_half_edge(v2, v3)"
 },
 {
  "docstring": "Triangulates the embedding.\n\nTraverses faces of the embedding and adds edges to a copy of the\nembedding to triangulate it.\nThe method also ensures that the resulting graph is 2-connected by adding\nedges if the same vertex is contained twice on a path around a face.\n\nParameters\n----------\nembedding : nx.PlanarEmbedding\n    The input graph must contain at least 3 nodes.\n\nfully_triangulate : bool\n    If set to False the face with the most nodes is chooses as outer face.\n    This outer face does not get triangulated.\n\nReturns\n-------\n(embedding, outer_face) : (nx.PlanarEmbedding, list) tuple\n    The element `embedding` is a new embedding containing all edges from\n    the input embedding and the additional edges to triangulate the graph.\n    The element `outer_face` is a list of nodes that lie on the outer face.\n    If the graph is fully triangulated these are three arbitrary connected\n    nodes.",
  "code": "def triangulate_embedding(embedding, fully_triangulate=True):\n    if len(embedding.nodes) <= 1:\n        return (embedding, list(embedding.nodes))\n    embedding = nx.PlanarEmbedding(embedding)\n    component_nodes = [next(iter(x)) for x in nx.connected_components(embedding)]\n    for i in range(len(component_nodes) - 1):\n        v1 = component_nodes[i]\n        v2 = component_nodes[i + 1]\n        embedding.connect_components(v1, v2)\n    outer_face = []\n    face_list = []\n    edges_visited = set()\n    for v in embedding.nodes():\n        for w in embedding.neighbors_cw_order(v):\n            new_face = make_bi_connected(embedding, v, w, edges_visited)\n            if new_face:\n                face_list.append(new_face)\n                if len(new_face) > len(outer_face):\n                    outer_face = new_face\n    for face in face_list:\n        if face is not outer_face or fully_triangulate:\n            triangulate_face(embedding, face[0], face[1])\n    if fully_triangulate:\n        v1 = outer_face[0]\n        v2 = outer_face[1]\n        v3 = embedding[v2][v1]['ccw']\n        outer_face = [v1, v2, v3]\n    return (embedding, outer_face)"
 },
 {
  "docstring": "Triangulate a face and make it 2-connected\n\nThis method also adds all edges on the face to `edges_counted`.\n\nParameters\n----------\nembedding: nx.PlanarEmbedding\n    The embedding that defines the faces\nstarting_node : node\n    A node on the face\noutgoing_node : node\n    A node such that the half edge (starting_node, outgoing_node) belongs\n    to the face\nedges_counted: set\n    Set of all half-edges that belong to a face that have been visited\n\nReturns\n-------\nface_nodes: list\n    A list of all nodes at the border of this face",
  "code": "def make_bi_connected(embedding, starting_node, outgoing_node, edges_counted):\n    if (starting_node, outgoing_node) in edges_counted:\n        return []\n    edges_counted.add((starting_node, outgoing_node))\n    v1 = starting_node\n    v2 = outgoing_node\n    face_list = [starting_node]\n    face_set = set(face_list)\n    _, v3 = embedding.next_face_half_edge(v1, v2)\n    while v2 != starting_node or v3 != outgoing_node:\n        if v1 == v2:\n            raise nx.NetworkXException('Invalid half-edge')\n        if v2 in face_set:\n            embedding.add_half_edge_cw(v1, v3, v2)\n            embedding.add_half_edge_ccw(v3, v1, v2)\n            edges_counted.add((v2, v3))\n            edges_counted.add((v3, v1))\n            v2 = v1\n        else:\n            face_set.add(v2)\n            face_list.append(v2)\n        v1 = v2\n        v2, v3 = embedding.next_face_half_edge(v2, v3)\n        edges_counted.add((v1, v2))\n    return face_list"
 },
 {
  "docstring": "Returns the Tutte polynomial of `G`\n\nThis function computes the Tutte polynomial via an iterative version of\nthe deletion-contraction algorithm.\n\nThe Tutte polynomial `T_G(x, y)` is a fundamental graph polynomial invariant in\ntwo variables. It encodes a wide array of information related to the\nedge-connectivity of a graph; \"Many problems about graphs can be reduced to\nproblems of finding and evaluating the Tutte polynomial at certain values\" [1]_.\nIn fact, every deletion-contraction-expressible feature of a graph is a\nspecialization of the Tutte polynomial [2]_ (see Notes for examples).\n\nThere are several equivalent definitions; here are three:\n\nDef 1 (rank-nullity expansion): For `G` an undirected graph, `n(G)` the\nnumber of vertices of `G`, `E` the edge set of `G`, `V` the vertex set of\n`G`, and `c(A)` the number of connected components of the graph with vertex\nset `V` and edge set `A` [3]_:\n\n.. math::\n\n    T_G(x, y) = \\sum_{A \\in E} (x-1)^{c(A) - c(E)} (y-1)^{c(A) + |A| - n(G)}\n\nDef 2 (spanning tree expansion): Let `G` be an undirected graph, `T` a spanning\ntree of `G`, and `E` the edge set of `G`. Let `E` have an arbitrary strict\nlinear order `L`. Let `B_e` be the unique minimal nonempty edge cut of\n$E \\setminus T \\cup {e}$. An edge `e` is internally active with respect to\n`T` and `L` if `e` is the least edge in `B_e` according to the linear order\n`L`. The internal activity of `T` (denoted `i(T)`) is the number of edges\nin $E \\setminus T$ that are internally active with respect to `T` and `L`.\nLet `P_e` be the unique path in $T \\cup {e}$ whose source and target vertex\nare the same. An edge `e` is externally active with respect to `T` and `L`\nif `e` is the least edge in `P_e` according to the linear order `L`. The\nexternal activity of `T` (denoted `e(T)`) is the number of edges in\n$E \\setminus T$ that are externally active with respect to `T` and `L`.\nThen [4]_ [5]_:\n\n.. math::\n\n    T_G(x, y) = \\sum_{T \\text{ a spanning tree of } G} x^{i(T)} y^{e(T)}\n\nDef 3 (deletion-contraction recurrence): For `G` an undirected graph, `G-e`\nthe graph obtained from `G` by deleting edge `e`, `G/e` the graph obtained\nfrom `G` by contracting edge `e`, `k(G)` the number of cut-edges of `G`,\nand `l(G)` the number of self-loops of `G`:\n\n.. math::\n    T_G(x, y) = \\begin{cases}\n       x^{k(G)} y^{l(G)}, & \\text{if all edges are cut-edges or self-loops} \\\\\n       T_{G-e}(x, y) + T_{G/e}(x, y), & \\text{otherwise, for an arbitrary edge $e$ not a cut-edge or loop}\n    \\end{cases}\n\nParameters\n----------\nG : NetworkX graph\n\nReturns\n-------\ninstance of `sympy.core.add.Add`\n    A Sympy expression representing the Tutte polynomial for `G`.\n\nExamples\n--------\n>>> C = nx.cycle_graph(5)\n>>> nx.tutte_polynomial(C)\nx**4 + x**3 + x**2 + x + y\n\n>>> D = nx.diamond_graph()\n>>> nx.tutte_polynomial(D)\nx**3 + 2*x**2 + 2*x*y + x + y**2 + y\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch\ndef tutte_polynomial(G):\n    import sympy\n    x = sympy.Symbol('x')\n    y = sympy.Symbol('y')\n    stack = deque()\n    stack.append(nx.MultiGraph(G))\n    polynomial = 0\n    while stack:\n        G = stack.pop()\n        bridges = set(nx.bridges(G))\n        e = None\n        for i in G.edges:\n            if (i[0], i[1]) not in bridges and i[0] != i[1]:\n                e = i\n                break\n        if not e:\n            loops = list(nx.selfloop_edges(G, keys=True))\n            polynomial += x ** len(bridges) * y ** len(loops)\n        else:\n            C = nx.contracted_edge(G, e, self_loops=True)\n            C.remove_edge(e[0], e[0])\n            G.remove_edge(*e)\n            stack.append(G)\n            stack.append(C)\n    return sympy.simplify(polynomial)"
 },
 {
  "docstring": "Returns the chromatic polynomial of `G`\n\nThis function computes the chromatic polynomial via an iterative version of\nthe deletion-contraction algorithm.\n\nThe chromatic polynomial `X_G(x)` is a fundamental graph polynomial\ninvariant in one variable. Evaluating `X_G(k)` for an natural number `k`\nenumerates the proper k-colorings of `G`.\n\nThere are several equivalent definitions; here are three:\n\nDef 1 (explicit formula):\nFor `G` an undirected graph, `c(G)` the number of connected components of\n`G`, `E` the edge set of `G`, and `G(S)` the spanning subgraph of `G` with\nedge set `S` [1]_:\n\n.. math::\n\n    X_G(x) = \\sum_{S \\subseteq E} (-1)^{|S|} x^{c(G(S))}\n\n\nDef 2 (interpolating polynomial):\nFor `G` an undirected graph, `n(G)` the number of vertices of `G`, `k_0 = 0`,\nand `k_i` the number of distinct ways to color the vertices of `G` with `i`\nunique colors (for `i` a natural number at most `n(G)`), `X_G(x)` is the\nunique Lagrange interpolating polynomial of degree `n(G)` through the points\n`(0, k_0), (1, k_1), \\dots, (n(G), k_{n(G)})` [2]_.\n\n\nDef 3 (chromatic recurrence):\nFor `G` an undirected graph, `G-e` the graph obtained from `G` by deleting\nedge `e`, `G/e` the graph obtained from `G` by contracting edge `e`, `n(G)`\nthe number of vertices of `G`, and `e(G)` the number of edges of `G` [3]_:\n\n.. math::\n    X_G(x) = \\begin{cases}\n       x^{n(G)}, & \\text{if $e(G)=0$} \\\\\n       X_{G-e}(x) - X_{G/e}(x), & \\text{otherwise, for an arbitrary edge $e$}\n    \\end{cases}\n\nThis formulation is also known as the Fundamental Reduction Theorem [4]_.\n\n\nParameters\n----------\nG : NetworkX graph\n\nReturns\n-------\ninstance of `sympy.core.add.Add`\n    A Sympy expression representing the chromatic polynomial for `G`.\n\nExamples\n--------\n>>> C = nx.cycle_graph(5)\n>>> nx.chromatic_polynomial(C)\nx**5 - 5*x**4 + 10*x**3 - 10*x**2 + 4*x\n\n>>> G = nx.complete_graph(4)\n>>> nx.chromatic_polynomial(G)\nx**4 - 6*x**3 + 11*x**2 - 6*x\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch\ndef chromatic_polynomial(G):\n    import sympy\n    x = sympy.Symbol('x')\n    stack = deque()\n    stack.append(nx.MultiGraph(G, contraction_idx=0))\n    polynomial = 0\n    while stack:\n        G = stack.pop()\n        edges = list(G.edges)\n        if not edges:\n            polynomial += (-1) ** G.graph['contraction_idx'] * x ** len(G)\n        else:\n            e = edges[0]\n            C = nx.contracted_edge(G, e, self_loops=True)\n            C.graph['contraction_idx'] = G.graph['contraction_idx'] + 1\n            C.remove_edge(e[0], e[0])\n            G.remove_edge(*e)\n            stack.append(G)\n            stack.append(C)\n    return polynomial"
 },
 {
  "docstring": "Compute the reciprocity in a directed graph.\n\nThe reciprocity of a directed graph is defined as the ratio\nof the number of edges pointing in both directions to the total\nnumber of edges in the graph.\nFormally, $r = |{(u,v) \\in G|(v,u) \\in G}| / |{(u,v) \\in G}|$.\n\nThe reciprocity of a single node u is defined similarly,\nit is the ratio of the number of edges in both directions to\nthe total number of edges attached to node u.\n\nParameters\n----------\nG : graph\n   A networkx directed graph\nnodes : container of nodes, optional (default=whole graph)\n   Compute reciprocity for nodes in this container.\n\nReturns\n-------\nout : dictionary\n   Reciprocity keyed by node label.\n\n",
  "code": "@not_implemented_for('undirected', 'multigraph')\n@nx._dispatch\ndef reciprocity(G, nodes=None):\n    if nodes is None:\n        return overall_reciprocity(G)\n    if nodes in G:\n        reciprocity = next(_reciprocity_iter(G, nodes))[1]\n        if reciprocity is None:\n            raise NetworkXError('Not defined for isolated nodes.')\n        else:\n            return reciprocity\n    return dict(_reciprocity_iter(G, nodes))"
 },
 {
  "docstring": "Return an iterator of (node, reciprocity).",
  "code": "def _reciprocity_iter(G, nodes):\n    n = G.nbunch_iter(nodes)\n    for node in n:\n        pred = set(G.predecessors(node))\n        succ = set(G.successors(node))\n        overlap = pred & succ\n        n_total = len(pred) + len(succ)\n        if n_total == 0:\n            yield (node, None)\n        else:\n            reciprocity = 2 * len(overlap) / n_total\n            yield (node, reciprocity)"
 },
 {
  "docstring": "Compute the reciprocity for the whole graph.\n\nSee the doc of reciprocity for the definition.\n\nParameters\n----------\nG : graph\n   A networkx graph",
  "code": "@not_implemented_for('undirected', 'multigraph')\n@nx._dispatch\ndef overall_reciprocity(G):\n    n_all_edge = G.number_of_edges()\n    n_overlap_edge = (n_all_edge - G.to_undirected().number_of_edges()) * 2\n    if n_all_edge == 0:\n        raise NetworkXError('Not defined for empty graphs')\n    return n_overlap_edge / n_all_edge"
 },
 {
  "docstring": "Determines whether the graph ``G`` is a regular graph.\n\nA regular graph is a graph where each vertex has the same degree. A\nregular digraph is a graph where the indegree and outdegree of each\nvertex are equal.\n\nParameters\n----------\nG : NetworkX graph\n\nReturns\n-------\nbool\n    Whether the given graph or digraph is regular.\n\nExamples\n--------\n>>> G = nx.DiGraph([(1, 2), (2, 3), (3, 4), (4, 1)])\n>>> nx.is_regular(G)\nTrue",
  "code": "@nx._dispatch\ndef is_regular(G):\n    n1 = nx.utils.arbitrary_element(G)\n    if not G.is_directed():\n        d1 = G.degree(n1)\n        return all((d1 == d for _, d in G.degree))\n    else:\n        d_in = G.in_degree(n1)\n        in_regular = all((d_in == d for _, d in G.in_degree))\n        d_out = G.out_degree(n1)\n        out_regular = all((d_out == d for _, d in G.out_degree))\n        return in_regular and out_regular"
 },
 {
  "docstring": "Determines whether the graph ``G`` is a k-regular graph.\n\nA k-regular graph is a graph where each vertex has degree k.\n\nParameters\n----------\nG : NetworkX graph\n\nReturns\n-------\nbool\n    Whether the given graph is k-regular.\n\nExamples\n--------\n>>> G = nx.Graph([(1, 2), (2, 3), (3, 4), (4, 1)])\n>>> nx.is_k_regular(G, k=3)\nFalse",
  "code": "@not_implemented_for('directed')\n@nx._dispatch\ndef is_k_regular(G, k):\n    return all((d == k for n, d in G.degree))"
 },
 {
  "docstring": "Compute a k-factor of G\n\nA k-factor of a graph is a spanning k-regular subgraph.\nA spanning k-regular subgraph of G is a subgraph that contains\neach vertex of G and a subset of the edges of G such that each\nvertex has degree k.\n\nParameters\n----------\nG : NetworkX graph\n  Undirected graph\n\nmatching_weight: string, optional (default='weight')\n   Edge data key corresponding to the edge weight.\n   Used for finding the max-weighted perfect matching.\n   If key not found, uses 1 as weight.\n\nReturns\n-------\nG2 : NetworkX graph\n    A k-factor of G\n\nExamples\n--------\n>>> G = nx.Graph([(1, 2), (2, 3), (3, 4), (4, 1)])\n>>> G2 = nx.k_factor(G, k=1)\n>>> G2.edges()\nEdgeView([(1, 2), (3, 4)])\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch(preserve_edge_attrs=True)\ndef k_factor(G, k, matching_weight='weight'):\n    from networkx.algorithms.matching import is_perfect_matching, max_weight_matching\n\n    class LargeKGadget:\n\n        def __init__(self, k, degree, node, g):\n            self.original = node\n            self.g = g\n            self.k = k\n            self.degree = degree\n            self.outer_vertices = [(node, x) for x in range(degree)]\n            self.core_vertices = [(node, x + degree) for x in range(degree - k)]\n\n        def replace_node(self):\n            adj_view = self.g[self.original]\n            neighbors = list(adj_view.keys())\n            edge_attrs = list(adj_view.values())\n            for outer, neighbor, edge_attrs in zip(self.outer_vertices, neighbors, edge_attrs):\n                self.g.add_edge(outer, neighbor, **edge_attrs)\n            for core in self.core_vertices:\n                for outer in self.outer_vertices:\n                    self.g.add_edge(core, outer)\n            self.g.remove_node(self.original)\n\n        def restore_node(self):\n            self.g.add_node(self.original)\n            for outer in self.outer_vertices:\n                adj_view = self.g[outer]\n                for neighbor, edge_attrs in list(adj_view.items()):\n                    if neighbor not in self.core_vertices:\n                        self.g.add_edge(self.original, neighbor, **edge_attrs)\n                        break\n            g.remove_nodes_from(self.outer_vertices)\n            g.remove_nodes_from(self.core_vertices)\n\n    class SmallKGadget:\n\n        def __init__(self, k, degree, node, g):\n            self.original = node\n            self.k = k\n            self.degree = degree\n            self.g = g\n            self.outer_vertices = [(node, x) for x in range(degree)]\n            self.inner_vertices = [(node, x + degree) for x in range(degree)]\n            self.core_vertices = [(node, x + 2 * degree) for x in range(k)]\n\n        def replace_node(self):\n            adj_view = self.g[self.original]\n            for outer, inner, (neighbor, edge_attrs) in zip(self.outer_vertices, self.inner_vertices, list(adj_view.items())):\n                self.g.add_edge(outer, inner)\n                self.g.add_edge(outer, neighbor, **edge_attrs)\n            for core in self.core_vertices:\n                for inner in self.inner_vertices:\n                    self.g.add_edge(core, inner)\n            self.g.remove_node(self.original)\n\n        def restore_node(self):\n            self.g.add_node(self.original)\n            for outer in self.outer_vertices:\n                adj_view = self.g[outer]\n                for neighbor, edge_attrs in adj_view.items():\n                    if neighbor not in self.core_vertices:\n                        self.g.add_edge(self.original, neighbor, **edge_attrs)\n                        break\n            self.g.remove_nodes_from(self.outer_vertices)\n            self.g.remove_nodes_from(self.inner_vertices)\n            self.g.remove_nodes_from(self.core_vertices)\n    if any((d < k for _, d in G.degree)):\n        raise nx.NetworkXUnfeasible('Graph contains a vertex with degree less than k')\n    g = G.copy()\n    gadgets = []\n    for node, degree in list(g.degree):\n        if k < degree / 2.0:\n            gadget = SmallKGadget(k, degree, node, g)\n        else:\n            gadget = LargeKGadget(k, degree, node, g)\n        gadget.replace_node()\n        gadgets.append(gadget)\n    matching = max_weight_matching(g, maxcardinality=True, weight=matching_weight)\n    if not is_perfect_matching(g, matching):\n        raise nx.NetworkXUnfeasible('Cannot find k-factor because no perfect matching exists')\n    for edge in g.edges():\n        if edge not in matching and (edge[1], edge[0]) not in matching:\n            g.remove_edge(edge[0], edge[1])\n    for gadget in gadgets:\n        gadget.restore_node()\n    return g"
 },
 {
  "docstring": "Returns the rich-club coefficient of the graph `G`.\n\nFor each degree *k*, the *rich-club coefficient* is the ratio of the\nnumber of actual to the number of potential edges for nodes with\ndegree greater than *k*:\n\n.. math::\n\n    \\phi(k) = \\frac{2 E_k}{N_k (N_k - 1)}\n\nwhere `N_k` is the number of nodes with degree larger than *k*, and\n`E_k` is the number of edges among those nodes.\n\nParameters\n----------\nG : NetworkX graph\n    Undirected graph with neither parallel edges nor self-loops.\nnormalized : bool (optional)\n    Normalize using randomized network as in [1]_\nQ : float (optional, default=100)\n    If `normalized` is True, perform `Q * m` double-edge\n    swaps, where `m` is the number of edges in `G`, to use as a\n    null-model for normalization.\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\nReturns\n-------\nrc : dictionary\n   A dictionary, keyed by degree, with rich-club coefficient values.\n\nExamples\n--------\n>>> G = nx.Graph([(0, 1), (0, 2), (1, 2), (1, 3), (1, 4), (4, 5)])\n>>> rc = nx.rich_club_coefficient(G, normalized=False, seed=42)\n>>> rc[0]\n0.4\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef rich_club_coefficient(G, normalized=True, Q=100, seed=None):\n    if nx.number_of_selfloops(G) > 0:\n        raise Exception('rich_club_coefficient is not implemented for graphs with self loops.')\n    rc = _compute_rc(G)\n    if normalized:\n        R = G.copy()\n        E = R.number_of_edges()\n        nx.double_edge_swap(R, Q * E, max_tries=Q * E * 10, seed=seed)\n        rcran = _compute_rc(R)\n        rc = {k: v / rcran[k] for k, v in rc.items()}\n    return rc"
 },
 {
  "docstring": "Returns the rich-club coefficient for each degree in the graph\n`G`.\n\n`G` is an undirected graph without multiedges.\n\nReturns a dictionary mapping degree to rich-club coefficient for\nthat degree.",
  "code": "def _compute_rc(G):\n    deghist = nx.degree_histogram(G)\n    total = sum(deghist)\n    nks = (total - cs for cs in accumulate(deghist) if total - cs > 1)\n    edge_degrees = sorted((sorted(map(G.degree, e)) for e in G.edges()), reverse=True)\n    ek = G.number_of_edges()\n    k1, k2 = edge_degrees.pop()\n    rc = {}\n    for d, nk in enumerate(nks):\n        while k1 <= d:\n            if len(edge_degrees) == 0:\n                ek = 0\n                break\n            k1, k2 = edge_degrees.pop()\n            ek -= 1\n        rc[d] = 2 * ek / (nk * (nk - 1))\n    return rc"
 },
 {
  "docstring": "Returns GED (graph edit distance) between graphs G1 and G2.\n\nGraph edit distance is a graph similarity measure analogous to\nLevenshtein distance for strings.  It is defined as minimum cost\nof edit path (sequence of node and edge edit operations)\ntransforming graph G1 to graph isomorphic to G2.\n\nParameters\n----------\nG1, G2: graphs\n    The two graphs G1 and G2 must be of the same type.\n\nnode_match : callable\n    A function that returns True if node n1 in G1 and n2 in G2\n    should be considered equal during matching.\n\n    The function will be called like\n\n       node_match(G1.nodes[n1], G2.nodes[n2]).\n\n    That is, the function will receive the node attribute\n    dictionaries for n1 and n2 as inputs.\n\n    Ignored if node_subst_cost is specified.  If neither\n    node_match nor node_subst_cost are specified then node\n    attributes are not considered.\n\nedge_match : callable\n    A function that returns True if the edge attribute dictionaries\n    for the pair of nodes (u1, v1) in G1 and (u2, v2) in G2 should\n    be considered equal during matching.\n\n    The function will be called like\n\n       edge_match(G1[u1][v1], G2[u2][v2]).\n\n    That is, the function will receive the edge attribute\n    dictionaries of the edges under consideration.\n\n    Ignored if edge_subst_cost is specified.  If neither\n    edge_match nor edge_subst_cost are specified then edge\n    attributes are not considered.\n\nnode_subst_cost, node_del_cost, node_ins_cost : callable\n    Functions that return the costs of node substitution, node\n    deletion, and node insertion, respectively.\n\n    The functions will be called like\n\n       node_subst_cost(G1.nodes[n1], G2.nodes[n2]),\n       node_del_cost(G1.nodes[n1]),\n       node_ins_cost(G2.nodes[n2]).\n\n    That is, the functions will receive the node attribute\n    dictionaries as inputs.  The functions are expected to return\n    positive numeric values.\n\n    Function node_subst_cost overrides node_match if specified.\n    If neither node_match nor node_subst_cost are specified then\n    default node substitution cost of 0 is used (node attributes\n    are not considered during matching).\n\n    If node_del_cost is not specified then default node deletion\n    cost of 1 is used.  If node_ins_cost is not specified then\n    default node insertion cost of 1 is used.\n\nedge_subst_cost, edge_del_cost, edge_ins_cost : callable\n    Functions that return the costs of edge substitution, edge\n    deletion, and edge insertion, respectively.\n\n    The functions will be called like\n\n       edge_subst_cost(G1[u1][v1], G2[u2][v2]),\n       edge_del_cost(G1[u1][v1]),\n       edge_ins_cost(G2[u2][v2]).\n\n    That is, the functions will receive the edge attribute\n    dictionaries as inputs.  The functions are expected to return\n    positive numeric values.\n\n    Function edge_subst_cost overrides edge_match if specified.\n    If neither edge_match nor edge_subst_cost are specified then\n    default edge substitution cost of 0 is used (edge attributes\n    are not considered during matching).\n\n    If edge_del_cost is not specified then default edge deletion\n    cost of 1 is used.  If edge_ins_cost is not specified then\n    default edge insertion cost of 1 is used.\n\nroots : 2-tuple\n    Tuple where first element is a node in G1 and the second\n    is a node in G2.\n    These nodes are forced to be matched in the comparison to\n    allow comparison between rooted graphs.\n\nupper_bound : numeric\n    Maximum edit distance to consider.  Return None if no edit\n    distance under or equal to upper_bound exists.\n\ntimeout : numeric\n    Maximum number of seconds to execute.\n    After timeout is met, the current best GED is returned.\n\nExamples\n--------\n>>> G1 = nx.cycle_graph(6)\n>>> G2 = nx.wheel_graph(7)\n>>> nx.graph_edit_distance(G1, G2)\n7.0\n\n>>> G1 = nx.star_graph(5)\n>>> G2 = nx.star_graph(5)\n>>> nx.graph_edit_distance(G1, G2, roots=(0, 0))\n0.0\n>>> nx.graph_edit_distance(G1, G2, roots=(1, 0))\n8.0\n\nSee Also\n--------\noptimal_edit_paths, optimize_graph_edit_distance,\n\nis_isomorphic: test for graph edit distance of 0\n\n",
  "code": "@nx._dispatch(graphs={'G1': 0, 'G2': 1}, preserve_edge_attrs=True, preserve_node_attrs=True)\ndef graph_edit_distance(G1, G2, node_match=None, edge_match=None, node_subst_cost=None, node_del_cost=None, node_ins_cost=None, edge_subst_cost=None, edge_del_cost=None, edge_ins_cost=None, roots=None, upper_bound=None, timeout=None):\n    bestcost = None\n    for _, _, cost in optimize_edit_paths(G1, G2, node_match, edge_match, node_subst_cost, node_del_cost, node_ins_cost, edge_subst_cost, edge_del_cost, edge_ins_cost, upper_bound, True, roots, timeout):\n        bestcost = cost\n    return bestcost"
 },
 {
  "docstring": "Returns all minimum-cost edit paths transforming G1 to G2.\n\nGraph edit path is a sequence of node and edge edit operations\ntransforming graph G1 to graph isomorphic to G2.  Edit operations\ninclude substitutions, deletions, and insertions.\n\nParameters\n----------\nG1, G2: graphs\n    The two graphs G1 and G2 must be of the same type.\n\nnode_match : callable\n    A function that returns True if node n1 in G1 and n2 in G2\n    should be considered equal during matching.\n\n    The function will be called like\n\n       node_match(G1.nodes[n1], G2.nodes[n2]).\n\n    That is, the function will receive the node attribute\n    dictionaries for n1 and n2 as inputs.\n\n    Ignored if node_subst_cost is specified.  If neither\n    node_match nor node_subst_cost are specified then node\n    attributes are not considered.\n\nedge_match : callable\n    A function that returns True if the edge attribute dictionaries\n    for the pair of nodes (u1, v1) in G1 and (u2, v2) in G2 should\n    be considered equal during matching.\n\n    The function will be called like\n\n       edge_match(G1[u1][v1], G2[u2][v2]).\n\n    That is, the function will receive the edge attribute\n    dictionaries of the edges under consideration.\n\n    Ignored if edge_subst_cost is specified.  If neither\n    edge_match nor edge_subst_cost are specified then edge\n    attributes are not considered.\n\nnode_subst_cost, node_del_cost, node_ins_cost : callable\n    Functions that return the costs of node substitution, node\n    deletion, and node insertion, respectively.\n\n    The functions will be called like\n\n       node_subst_cost(G1.nodes[n1], G2.nodes[n2]),\n       node_del_cost(G1.nodes[n1]),\n       node_ins_cost(G2.nodes[n2]).\n\n    That is, the functions will receive the node attribute\n    dictionaries as inputs.  The functions are expected to return\n    positive numeric values.\n\n    Function node_subst_cost overrides node_match if specified.\n    If neither node_match nor node_subst_cost are specified then\n    default node substitution cost of 0 is used (node attributes\n    are not considered during matching).\n\n    If node_del_cost is not specified then default node deletion\n    cost of 1 is used.  If node_ins_cost is not specified then\n    default node insertion cost of 1 is used.\n\nedge_subst_cost, edge_del_cost, edge_ins_cost : callable\n    Functions that return the costs of edge substitution, edge\n    deletion, and edge insertion, respectively.\n\n    The functions will be called like\n\n       edge_subst_cost(G1[u1][v1], G2[u2][v2]),\n       edge_del_cost(G1[u1][v1]),\n       edge_ins_cost(G2[u2][v2]).\n\n    That is, the functions will receive the edge attribute\n    dictionaries as inputs.  The functions are expected to return\n    positive numeric values.\n\n    Function edge_subst_cost overrides edge_match if specified.\n    If neither edge_match nor edge_subst_cost are specified then\n    default edge substitution cost of 0 is used (edge attributes\n    are not considered during matching).\n\n    If edge_del_cost is not specified then default edge deletion\n    cost of 1 is used.  If edge_ins_cost is not specified then\n    default edge insertion cost of 1 is used.\n\nupper_bound : numeric\n    Maximum edit distance to consider.\n\nReturns\n-------\nedit_paths : list of tuples (node_edit_path, edge_edit_path)\n    node_edit_path : list of tuples (u, v)\n    edge_edit_path : list of tuples ((u1, v1), (u2, v2))\n\ncost : numeric\n    Optimal edit path cost (graph edit distance).\n\nExamples\n--------\n>>> G1 = nx.cycle_graph(4)\n>>> G2 = nx.wheel_graph(5)\n>>> paths, cost = nx.optimal_edit_paths(G1, G2)\n>>> len(paths)\n40\n>>> cost\n5.0\n\nSee Also\n--------\ngraph_edit_distance, optimize_edit_paths\n\n",
  "code": "@nx._dispatch(graphs={'G1': 0, 'G2': 1})\ndef optimal_edit_paths(G1, G2, node_match=None, edge_match=None, node_subst_cost=None, node_del_cost=None, node_ins_cost=None, edge_subst_cost=None, edge_del_cost=None, edge_ins_cost=None, upper_bound=None):\n    paths = []\n    bestcost = None\n    for vertex_path, edge_path, cost in optimize_edit_paths(G1, G2, node_match, edge_match, node_subst_cost, node_del_cost, node_ins_cost, edge_subst_cost, edge_del_cost, edge_ins_cost, upper_bound, False):\n        if bestcost is not None and cost < bestcost:\n            paths = []\n        paths.append((vertex_path, edge_path))\n        bestcost = cost\n    return (paths, bestcost)"
 },
 {
  "docstring": "Returns consecutive approximations of GED (graph edit distance)\nbetween graphs G1 and G2.\n\nGraph edit distance is a graph similarity measure analogous to\nLevenshtein distance for strings.  It is defined as minimum cost\nof edit path (sequence of node and edge edit operations)\ntransforming graph G1 to graph isomorphic to G2.\n\nParameters\n----------\nG1, G2: graphs\n    The two graphs G1 and G2 must be of the same type.\n\nnode_match : callable\n    A function that returns True if node n1 in G1 and n2 in G2\n    should be considered equal during matching.\n\n    The function will be called like\n\n       node_match(G1.nodes[n1], G2.nodes[n2]).\n\n    That is, the function will receive the node attribute\n    dictionaries for n1 and n2 as inputs.\n\n    Ignored if node_subst_cost is specified.  If neither\n    node_match nor node_subst_cost are specified then node\n    attributes are not considered.\n\nedge_match : callable\n    A function that returns True if the edge attribute dictionaries\n    for the pair of nodes (u1, v1) in G1 and (u2, v2) in G2 should\n    be considered equal during matching.\n\n    The function will be called like\n\n       edge_match(G1[u1][v1], G2[u2][v2]).\n\n    That is, the function will receive the edge attribute\n    dictionaries of the edges under consideration.\n\n    Ignored if edge_subst_cost is specified.  If neither\n    edge_match nor edge_subst_cost are specified then edge\n    attributes are not considered.\n\nnode_subst_cost, node_del_cost, node_ins_cost : callable\n    Functions that return the costs of node substitution, node\n    deletion, and node insertion, respectively.\n\n    The functions will be called like\n\n       node_subst_cost(G1.nodes[n1], G2.nodes[n2]),\n       node_del_cost(G1.nodes[n1]),\n       node_ins_cost(G2.nodes[n2]).\n\n    That is, the functions will receive the node attribute\n    dictionaries as inputs.  The functions are expected to return\n    positive numeric values.\n\n    Function node_subst_cost overrides node_match if specified.\n    If neither node_match nor node_subst_cost are specified then\n    default node substitution cost of 0 is used (node attributes\n    are not considered during matching).\n\n    If node_del_cost is not specified then default node deletion\n    cost of 1 is used.  If node_ins_cost is not specified then\n    default node insertion cost of 1 is used.\n\nedge_subst_cost, edge_del_cost, edge_ins_cost : callable\n    Functions that return the costs of edge substitution, edge\n    deletion, and edge insertion, respectively.\n\n    The functions will be called like\n\n       edge_subst_cost(G1[u1][v1], G2[u2][v2]),\n       edge_del_cost(G1[u1][v1]),\n       edge_ins_cost(G2[u2][v2]).\n\n    That is, the functions will receive the edge attribute\n    dictionaries as inputs.  The functions are expected to return\n    positive numeric values.\n\n    Function edge_subst_cost overrides edge_match if specified.\n    If neither edge_match nor edge_subst_cost are specified then\n    default edge substitution cost of 0 is used (edge attributes\n    are not considered during matching).\n\n    If edge_del_cost is not specified then default edge deletion\n    cost of 1 is used.  If edge_ins_cost is not specified then\n    default edge insertion cost of 1 is used.\n\nupper_bound : numeric\n    Maximum edit distance to consider.\n\nReturns\n-------\nGenerator of consecutive approximations of graph edit distance.\n\nExamples\n--------\n>>> G1 = nx.cycle_graph(6)\n>>> G2 = nx.wheel_graph(7)\n>>> for v in nx.optimize_graph_edit_distance(G1, G2):\n...     minv = v\n>>> minv\n7.0\n\nSee Also\n--------\ngraph_edit_distance, optimize_edit_paths\n\n",
  "code": "@nx._dispatch(graphs={'G1': 0, 'G2': 1})\ndef optimize_graph_edit_distance(G1, G2, node_match=None, edge_match=None, node_subst_cost=None, node_del_cost=None, node_ins_cost=None, edge_subst_cost=None, edge_del_cost=None, edge_ins_cost=None, upper_bound=None):\n    for _, _, cost in optimize_edit_paths(G1, G2, node_match, edge_match, node_subst_cost, node_del_cost, node_ins_cost, edge_subst_cost, edge_del_cost, edge_ins_cost, upper_bound, True):\n        yield cost"
 },
 {
  "docstring": "GED (graph edit distance) calculation: advanced interface.\n\nGraph edit path is a sequence of node and edge edit operations\ntransforming graph G1 to graph isomorphic to G2.  Edit operations\ninclude substitutions, deletions, and insertions.\n\nGraph edit distance is defined as minimum cost of edit path.\n\nParameters\n----------\nG1, G2: graphs\n    The two graphs G1 and G2 must be of the same type.\n\nnode_match : callable\n    A function that returns True if node n1 in G1 and n2 in G2\n    should be considered equal during matching.\n\n    The function will be called like\n\n       node_match(G1.nodes[n1], G2.nodes[n2]).\n\n    That is, the function will receive the node attribute\n    dictionaries for n1 and n2 as inputs.\n\n    Ignored if node_subst_cost is specified.  If neither\n    node_match nor node_subst_cost are specified then node\n    attributes are not considered.\n\nedge_match : callable\n    A function that returns True if the edge attribute dictionaries\n    for the pair of nodes (u1, v1) in G1 and (u2, v2) in G2 should\n    be considered equal during matching.\n\n    The function will be called like\n\n       edge_match(G1[u1][v1], G2[u2][v2]).\n\n    That is, the function will receive the edge attribute\n    dictionaries of the edges under consideration.\n\n    Ignored if edge_subst_cost is specified.  If neither\n    edge_match nor edge_subst_cost are specified then edge\n    attributes are not considered.\n\nnode_subst_cost, node_del_cost, node_ins_cost : callable\n    Functions that return the costs of node substitution, node\n    deletion, and node insertion, respectively.\n\n    The functions will be called like\n\n       node_subst_cost(G1.nodes[n1], G2.nodes[n2]),\n       node_del_cost(G1.nodes[n1]),\n       node_ins_cost(G2.nodes[n2]).\n\n    That is, the functions will receive the node attribute\n    dictionaries as inputs.  The functions are expected to return\n    positive numeric values.\n\n    Function node_subst_cost overrides node_match if specified.\n    If neither node_match nor node_subst_cost are specified then\n    default node substitution cost of 0 is used (node attributes\n    are not considered during matching).\n\n    If node_del_cost is not specified then default node deletion\n    cost of 1 is used.  If node_ins_cost is not specified then\n    default node insertion cost of 1 is used.\n\nedge_subst_cost, edge_del_cost, edge_ins_cost : callable\n    Functions that return the costs of edge substitution, edge\n    deletion, and edge insertion, respectively.\n\n    The functions will be called like\n\n       edge_subst_cost(G1[u1][v1], G2[u2][v2]),\n       edge_del_cost(G1[u1][v1]),\n       edge_ins_cost(G2[u2][v2]).\n\n    That is, the functions will receive the edge attribute\n    dictionaries as inputs.  The functions are expected to return\n    positive numeric values.\n\n    Function edge_subst_cost overrides edge_match if specified.\n    If neither edge_match nor edge_subst_cost are specified then\n    default edge substitution cost of 0 is used (edge attributes\n    are not considered during matching).\n\n    If edge_del_cost is not specified then default edge deletion\n    cost of 1 is used.  If edge_ins_cost is not specified then\n    default edge insertion cost of 1 is used.\n\nupper_bound : numeric\n    Maximum edit distance to consider.\n\nstrictly_decreasing : bool\n    If True, return consecutive approximations of strictly\n    decreasing cost.  Otherwise, return all edit paths of cost\n    less than or equal to the previous minimum cost.\n\nroots : 2-tuple\n    Tuple where first element is a node in G1 and the second\n    is a node in G2.\n    These nodes are forced to be matched in the comparison to\n    allow comparison between rooted graphs.\n\ntimeout : numeric\n    Maximum number of seconds to execute.\n    After timeout is met, the current best GED is returned.\n\nReturns\n-------\nGenerator of tuples (node_edit_path, edge_edit_path, cost)\n    node_edit_path : list of tuples (u, v)\n    edge_edit_path : list of tuples ((u1, v1), (u2, v2))\n    cost : numeric\n\nSee Also\n--------\ngraph_edit_distance, optimize_graph_edit_distance, optimal_edit_paths\n\n",
  "code": "@nx._dispatch(graphs={'G1': 0, 'G2': 1}, preserve_edge_attrs=True, preserve_node_attrs=True)\ndef optimize_edit_paths(G1, G2, node_match=None, edge_match=None, node_subst_cost=None, node_del_cost=None, node_ins_cost=None, edge_subst_cost=None, edge_del_cost=None, edge_ins_cost=None, upper_bound=None, strictly_decreasing=True, roots=None, timeout=None):\n    import numpy as np\n    import scipy as sp\n\n    @dataclass\n    class CostMatrix:\n        C: ...\n        lsa_row_ind: ...\n        lsa_col_ind: ...\n        ls: ...\n\n    def make_CostMatrix(C, m, n):\n        lsa_row_ind, lsa_col_ind = sp.optimize.linear_sum_assignment(C)\n        indexes = zip(range(len(lsa_row_ind)), lsa_row_ind, lsa_col_ind)\n        subst_ind = [k for k, i, j in indexes if i < m and j < n]\n        indexes = zip(range(len(lsa_row_ind)), lsa_row_ind, lsa_col_ind)\n        dummy_ind = [k for k, i, j in indexes if i >= m and j >= n]\n        lsa_row_ind[dummy_ind] = lsa_col_ind[subst_ind] + m\n        lsa_col_ind[dummy_ind] = lsa_row_ind[subst_ind] + n\n        return CostMatrix(C, lsa_row_ind, lsa_col_ind, C[lsa_row_ind, lsa_col_ind].sum())\n\n    def extract_C(C, i, j, m, n):\n        row_ind = [k in i or k - m in j for k in range(m + n)]\n        col_ind = [k in j or k - n in i for k in range(m + n)]\n        return C[row_ind, :][:, col_ind]\n\n    def reduce_C(C, i, j, m, n):\n        row_ind = [k not in i and k - m not in j for k in range(m + n)]\n        col_ind = [k not in j and k - n not in i for k in range(m + n)]\n        return C[row_ind, :][:, col_ind]\n\n    def reduce_ind(ind, i):\n        rind = ind[[k not in i for k in ind]]\n        for k in set(i):\n            rind[rind >= k] -= 1\n        return rind\n\n    def match_edges(u, v, pending_g, pending_h, Ce, matched_uv=None):\n        \"\"\"\n        Parameters:\n            u, v: matched vertices, u=None or v=None for\n               deletion/insertion\n            pending_g, pending_h: lists of edges not yet mapped\n            Ce: CostMatrix of pending edge mappings\n            matched_uv: partial vertex edit path\n                list of tuples (u, v) of previously matched vertex\n                    mappings u<->v, u=None or v=None for\n                    deletion/insertion\n\n        Returns:\n            list of (i, j): indices of edge mappings g<->h\n            localCe: local CostMatrix of edge mappings\n                (basically submatrix of Ce at cross of rows i, cols j)\n        \"\"\"\n        M = len(pending_g)\n        N = len(pending_h)\n        if matched_uv is None or len(matched_uv) == 0:\n            g_ind = []\n            h_ind = []\n        else:\n            g_ind = [i for i in range(M) if pending_g[i][:2] == (u, u) or any((pending_g[i][:2] in ((p, u), (u, p), (p, p)) for p, q in matched_uv))]\n            h_ind = [j for j in range(N) if pending_h[j][:2] == (v, v) or any((pending_h[j][:2] in ((q, v), (v, q), (q, q)) for p, q in matched_uv))]\n        m = len(g_ind)\n        n = len(h_ind)\n        if m or n:\n            C = extract_C(Ce.C, g_ind, h_ind, M, N)\n            for k, i in enumerate(g_ind):\n                g = pending_g[i][:2]\n                for l, j in enumerate(h_ind):\n                    h = pending_h[j][:2]\n                    if nx.is_directed(G1) or nx.is_directed(G2):\n                        if any((g == (p, u) and h == (q, v) or (g == (u, p) and h == (v, q)) for p, q in matched_uv)):\n                            continue\n                    elif any((g in ((p, u), (u, p)) and h in ((q, v), (v, q)) for p, q in matched_uv)):\n                        continue\n                    if g == (u, u) or any((g == (p, p) for p, q in matched_uv)):\n                        continue\n                    if h == (v, v) or any((h == (q, q) for p, q in matched_uv)):\n                        continue\n                    C[k, l] = inf\n            localCe = make_CostMatrix(C, m, n)\n            ij = [(g_ind[k] if k < m else M + h_ind[l], h_ind[l] if l < n else N + g_ind[k]) for k, l in zip(localCe.lsa_row_ind, localCe.lsa_col_ind) if k < m or l < n]\n        else:\n            ij = []\n            localCe = CostMatrix(np.empty((0, 0)), [], [], 0)\n        return (ij, localCe)\n\n    def reduce_Ce(Ce, ij, m, n):\n        if len(ij):\n            i, j = zip(*ij)\n            m_i = m - sum((1 for t in i if t < m))\n            n_j = n - sum((1 for t in j if t < n))\n            return make_CostMatrix(reduce_C(Ce.C, i, j, m, n), m_i, n_j)\n        return Ce\n\n    def get_edit_ops(matched_uv, pending_u, pending_v, Cv, pending_g, pending_h, Ce, matched_cost):\n        \"\"\"\n        Parameters:\n            matched_uv: partial vertex edit path\n                list of tuples (u, v) of vertex mappings u<->v,\n                u=None or v=None for deletion/insertion\n            pending_u, pending_v: lists of vertices not yet mapped\n            Cv: CostMatrix of pending vertex mappings\n            pending_g, pending_h: lists of edges not yet mapped\n            Ce: CostMatrix of pending edge mappings\n            matched_cost: cost of partial edit path\n\n        Returns:\n            sequence of\n                (i, j): indices of vertex mapping u<->v\n                Cv_ij: reduced CostMatrix of pending vertex mappings\n                    (basically Cv with row i, col j removed)\n                list of (x, y): indices of edge mappings g<->h\n                Ce_xy: reduced CostMatrix of pending edge mappings\n                    (basically Ce with rows x, cols y removed)\n                cost: total cost of edit operation\n            NOTE: most promising ops first\n        \"\"\"\n        m = len(pending_u)\n        n = len(pending_v)\n        i, j = min(((k, l) for k, l in zip(Cv.lsa_row_ind, Cv.lsa_col_ind) if k < m or l < n))\n        xy, localCe = match_edges(pending_u[i] if i < m else None, pending_v[j] if j < n else None, pending_g, pending_h, Ce, matched_uv)\n        Ce_xy = reduce_Ce(Ce, xy, len(pending_g), len(pending_h))\n        if prune(matched_cost + Cv.ls + localCe.ls + Ce_xy.ls):\n            pass\n        else:\n            Cv_ij = CostMatrix(reduce_C(Cv.C, (i,), (j,), m, n), reduce_ind(Cv.lsa_row_ind, (i, m + j)), reduce_ind(Cv.lsa_col_ind, (j, n + i)), Cv.ls - Cv.C[i, j])\n            yield ((i, j), Cv_ij, xy, Ce_xy, Cv.C[i, j] + localCe.ls)\n        other = []\n        fixed_i, fixed_j = (i, j)\n        if m <= n:\n            candidates = ((t, fixed_j) for t in range(m + n) if t != fixed_i and (t < m or t == m + fixed_j))\n        else:\n            candidates = ((fixed_i, t) for t in range(m + n) if t != fixed_j and (t < n or t == n + fixed_i))\n        for i, j in candidates:\n            if prune(matched_cost + Cv.C[i, j] + Ce.ls):\n                continue\n            Cv_ij = make_CostMatrix(reduce_C(Cv.C, (i,), (j,), m, n), m - 1 if i < m else m, n - 1 if j < n else n)\n            if prune(matched_cost + Cv.C[i, j] + Cv_ij.ls + Ce.ls):\n                continue\n            xy, localCe = match_edges(pending_u[i] if i < m else None, pending_v[j] if j < n else None, pending_g, pending_h, Ce, matched_uv)\n            if prune(matched_cost + Cv.C[i, j] + Cv_ij.ls + localCe.ls):\n                continue\n            Ce_xy = reduce_Ce(Ce, xy, len(pending_g), len(pending_h))\n            if prune(matched_cost + Cv.C[i, j] + Cv_ij.ls + localCe.ls + Ce_xy.ls):\n                continue\n            other.append(((i, j), Cv_ij, xy, Ce_xy, Cv.C[i, j] + localCe.ls))\n        yield from sorted(other, key=lambda t: t[4] + t[1].ls + t[3].ls)\n\n    def get_edit_paths(matched_uv, pending_u, pending_v, Cv, matched_gh, pending_g, pending_h, Ce, matched_cost):\n        \"\"\"\n        Parameters:\n            matched_uv: partial vertex edit path\n                list of tuples (u, v) of vertex mappings u<->v,\n                u=None or v=None for deletion/insertion\n            pending_u, pending_v: lists of vertices not yet mapped\n            Cv: CostMatrix of pending vertex mappings\n            matched_gh: partial edge edit path\n                list of tuples (g, h) of edge mappings g<->h,\n                g=None or h=None for deletion/insertion\n            pending_g, pending_h: lists of edges not yet mapped\n            Ce: CostMatrix of pending edge mappings\n            matched_cost: cost of partial edit path\n\n        Returns:\n            sequence of (vertex_path, edge_path, cost)\n                vertex_path: complete vertex edit path\n                    list of tuples (u, v) of vertex mappings u<->v,\n                    u=None or v=None for deletion/insertion\n                edge_path: complete edge edit path\n                    list of tuples (g, h) of edge mappings g<->h,\n                    g=None or h=None for deletion/insertion\n                cost: total cost of edit path\n            NOTE: path costs are non-increasing\n        \"\"\"\n        if prune(matched_cost + Cv.ls + Ce.ls):\n            return\n        if not max(len(pending_u), len(pending_v)):\n            nonlocal maxcost_value\n            maxcost_value = min(maxcost_value, matched_cost)\n            yield (matched_uv, matched_gh, matched_cost)\n        else:\n            edit_ops = get_edit_ops(matched_uv, pending_u, pending_v, Cv, pending_g, pending_h, Ce, matched_cost)\n            for ij, Cv_ij, xy, Ce_xy, edit_cost in edit_ops:\n                i, j = ij\n                if prune(matched_cost + edit_cost + Cv_ij.ls + Ce_xy.ls):\n                    continue\n                u = pending_u.pop(i) if i < len(pending_u) else None\n                v = pending_v.pop(j) if j < len(pending_v) else None\n                matched_uv.append((u, v))\n                for x, y in xy:\n                    len_g = len(pending_g)\n                    len_h = len(pending_h)\n                    matched_gh.append((pending_g[x] if x < len_g else None, pending_h[y] if y < len_h else None))\n                sortedx = sorted((x for x, y in xy))\n                sortedy = sorted((y for x, y in xy))\n                G = [pending_g.pop(x) if x < len(pending_g) else None for x in reversed(sortedx)]\n                H = [pending_h.pop(y) if y < len(pending_h) else None for y in reversed(sortedy)]\n                yield from get_edit_paths(matched_uv, pending_u, pending_v, Cv_ij, matched_gh, pending_g, pending_h, Ce_xy, matched_cost + edit_cost)\n                if u is not None:\n                    pending_u.insert(i, u)\n                if v is not None:\n                    pending_v.insert(j, v)\n                matched_uv.pop()\n                for x, g in zip(sortedx, reversed(G)):\n                    if g is not None:\n                        pending_g.insert(x, g)\n                for y, h in zip(sortedy, reversed(H)):\n                    if h is not None:\n                        pending_h.insert(y, h)\n                for _ in xy:\n                    matched_gh.pop()\n    pending_u = list(G1.nodes)\n    pending_v = list(G2.nodes)\n    initial_cost = 0\n    if roots:\n        root_u, root_v = roots\n        if root_u not in pending_u or root_v not in pending_v:\n            raise nx.NodeNotFound('Root node not in graph.')\n        pending_u.remove(root_u)\n        pending_v.remove(root_v)\n    m = len(pending_u)\n    n = len(pending_v)\n    C = np.zeros((m + n, m + n))\n    if node_subst_cost:\n        C[0:m, 0:n] = np.array([node_subst_cost(G1.nodes[u], G2.nodes[v]) for u in pending_u for v in pending_v]).reshape(m, n)\n        if roots:\n            initial_cost = node_subst_cost(G1.nodes[root_u], G2.nodes[root_v])\n    elif node_match:\n        C[0:m, 0:n] = np.array([1 - int(node_match(G1.nodes[u], G2.nodes[v])) for u in pending_u for v in pending_v]).reshape(m, n)\n        if roots:\n            initial_cost = 1 - node_match(G1.nodes[root_u], G2.nodes[root_v])\n    else:\n        pass\n    if node_del_cost:\n        del_costs = [node_del_cost(G1.nodes[u]) for u in pending_u]\n    else:\n        del_costs = [1] * len(pending_u)\n    if node_ins_cost:\n        ins_costs = [node_ins_cost(G2.nodes[v]) for v in pending_v]\n    else:\n        ins_costs = [1] * len(pending_v)\n    inf = C[0:m, 0:n].sum() + sum(del_costs) + sum(ins_costs) + 1\n    C[0:m, n:n + m] = np.array([del_costs[i] if i == j else inf for i in range(m) for j in range(m)]).reshape(m, m)\n    C[m:m + n, 0:n] = np.array([ins_costs[i] if i == j else inf for i in range(n) for j in range(n)]).reshape(n, n)\n    Cv = make_CostMatrix(C, m, n)\n    pending_g = list(G1.edges)\n    pending_h = list(G2.edges)\n    m = len(pending_g)\n    n = len(pending_h)\n    C = np.zeros((m + n, m + n))\n    if edge_subst_cost:\n        C[0:m, 0:n] = np.array([edge_subst_cost(G1.edges[g], G2.edges[h]) for g in pending_g for h in pending_h]).reshape(m, n)\n    elif edge_match:\n        C[0:m, 0:n] = np.array([1 - int(edge_match(G1.edges[g], G2.edges[h])) for g in pending_g for h in pending_h]).reshape(m, n)\n    else:\n        pass\n    if edge_del_cost:\n        del_costs = [edge_del_cost(G1.edges[g]) for g in pending_g]\n    else:\n        del_costs = [1] * len(pending_g)\n    if edge_ins_cost:\n        ins_costs = [edge_ins_cost(G2.edges[h]) for h in pending_h]\n    else:\n        ins_costs = [1] * len(pending_h)\n    inf = C[0:m, 0:n].sum() + sum(del_costs) + sum(ins_costs) + 1\n    C[0:m, n:n + m] = np.array([del_costs[i] if i == j else inf for i in range(m) for j in range(m)]).reshape(m, m)\n    C[m:m + n, 0:n] = np.array([ins_costs[i] if i == j else inf for i in range(n) for j in range(n)]).reshape(n, n)\n    Ce = make_CostMatrix(C, m, n)\n    maxcost_value = Cv.C.sum() + Ce.C.sum() + 1\n    if timeout is not None:\n        if timeout <= 0:\n            raise nx.NetworkXError('Timeout value must be greater than 0')\n        start = time.perf_counter()\n\n    def prune(cost):\n        if timeout is not None:\n            if time.perf_counter() - start > timeout:\n                return True\n        if upper_bound is not None:\n            if cost > upper_bound:\n                return True\n        if cost > maxcost_value:\n            return True\n        if strictly_decreasing and cost >= maxcost_value:\n            return True\n        return False\n    done_uv = [] if roots is None else [roots]\n    for vertex_path, edge_path, cost in get_edit_paths(done_uv, pending_u, pending_v, Cv, [], pending_g, pending_h, Ce, initial_cost):\n        yield (list(vertex_path), list(edge_path), cost)"
 },
 {
  "docstring": "Returns the SimRank similarity of nodes in the graph ``G``.\n\nSimRank is a similarity metric that says \"two objects are considered\nto be similar if they are referenced by similar objects.\" [1]_.\n\nThe pseudo-code definition from the paper is::\n\n    def simrank(G, u, v):\n        in_neighbors_u = G.predecessors(u)\n        in_neighbors_v = G.predecessors(v)\n        scale = C / (len(in_neighbors_u) * len(in_neighbors_v))\n        return scale * sum(simrank(G, w, x)\n                           for w, x in product(in_neighbors_u,\n                                               in_neighbors_v))\n\nwhere ``G`` is the graph, ``u`` is the source, ``v`` is the target,\nand ``C`` is a float decay or importance factor between 0 and 1.\n\nThe SimRank algorithm for determining node similarity is defined in\n[2]_.\n\nParameters\n----------\nG : NetworkX graph\n    A NetworkX graph\n\nsource : node\n    If this is specified, the returned dictionary maps each node\n    ``v`` in the graph to the similarity between ``source`` and\n    ``v``.\n\ntarget : node\n    If both ``source`` and ``target`` are specified, the similarity\n    value between ``source`` and ``target`` is returned. If\n    ``target`` is specified but ``source`` is not, this argument is\n    ignored.\n\nimportance_factor : float\n    The relative importance of indirect neighbors with respect to\n    direct neighbors.\n\nmax_iterations : integer\n    Maximum number of iterations.\n\ntolerance : float\n    Error tolerance used to check convergence. When an iteration of\n    the algorithm finds that no similarity value changes more than\n    this amount, the algorithm halts.\n\nReturns\n-------\nsimilarity : dictionary or float\n    If ``source`` and ``target`` are both ``None``, this returns a\n    dictionary of dictionaries, where keys are node pairs and value\n    are similarity of the pair of nodes.\n\n    If ``source`` is not ``None`` but ``target`` is, this returns a\n    dictionary mapping node to the similarity of ``source`` and that\n    node.\n\n    If neither ``source`` nor ``target`` is ``None``, this returns\n    the similarity value for the given pair of nodes.\n\nExamples\n--------\n>>> G = nx.cycle_graph(2)\n>>> nx.simrank_similarity(G)\n{0: {0: 1.0, 1: 0.0}, 1: {0: 0.0, 1: 1.0}}\n>>> nx.simrank_similarity(G, source=0)\n{0: 1.0, 1: 0.0}\n>>> nx.simrank_similarity(G, source=0, target=0)\n1.0\n\nThe result of this function can be converted to a numpy array\nrepresenting the SimRank matrix by using the node order of the\ngraph to determine which row and column represent each node.\nOther ordering of nodes is also possible.\n\n>>> import numpy as np\n>>> sim = nx.simrank_similarity(G)\n>>> np.array([[sim[u][v] for v in G] for u in G])\narray([[1., 0.],\n       [0., 1.]])\n>>> sim_1d = nx.simrank_similarity(G, source=0)\n>>> np.array([sim[0][v] for v in G])\narray([1., 0.])\n\n",
  "code": "@nx._dispatch\ndef simrank_similarity(G, source=None, target=None, importance_factor=0.9, max_iterations=1000, tolerance=0.0001):\n    import numpy as np\n    nodelist = list(G)\n    s_indx = None if source is None else nodelist.index(source)\n    t_indx = None if target is None else nodelist.index(target)\n    x = _simrank_similarity_numpy(G, s_indx, t_indx, importance_factor, max_iterations, tolerance)\n    if isinstance(x, np.ndarray):\n        if x.ndim == 1:\n            return dict(zip(G, x))\n        return {u: dict(zip(G, row)) for u, row in zip(G, x)}\n    return x"
 },
 {
  "docstring": "Returns the SimRank similarity of nodes in the graph ``G``.\n\nThis pure Python version is provided for pedagogical purposes.\n\nExamples\n--------\n>>> G = nx.cycle_graph(2)\n>>> nx.similarity._simrank_similarity_python(G)\n{0: {0: 1, 1: 0.0}, 1: {0: 0.0, 1: 1}}\n>>> nx.similarity._simrank_similarity_python(G, source=0)\n{0: 1, 1: 0.0}\n>>> nx.similarity._simrank_similarity_python(G, source=0, target=0)\n1",
  "code": "def _simrank_similarity_python(G, source=None, target=None, importance_factor=0.9, max_iterations=1000, tolerance=0.0001):\n    newsim = {u: {v: 1 if u == v else 0 for v in G} for u in G}\n\n    def avg_sim(s):\n        return sum((newsim[w][x] for w, x in s)) / len(s) if s else 0.0\n    Gadj = G.pred if G.is_directed() else G.adj\n\n    def sim(u, v):\n        return importance_factor * avg_sim(list(product(Gadj[u], Gadj[v])))\n    for its in range(max_iterations):\n        oldsim = newsim\n        newsim = {u: {v: sim(u, v) if u != v else 1 for v in G} for u in G}\n        is_close = all((all((abs(newsim[u][v] - old) <= tolerance * (1 + abs(old)) for v, old in nbrs.items())) for u, nbrs in oldsim.items()))\n        if is_close:\n            break\n    if its + 1 == max_iterations:\n        raise nx.ExceededMaxIterations(f'simrank did not converge after {max_iterations} iterations.')\n    if source is not None and target is not None:\n        return newsim[source][target]\n    if source is not None:\n        return newsim[source]\n    return newsim"
 },
 {
  "docstring": "Calculate SimRank of nodes in ``G`` using matrices with ``numpy``.\n\nThe SimRank algorithm for determining node similarity is defined in\n[1]_.\n\nParameters\n----------\nG : NetworkX graph\n    A NetworkX graph\n\nsource : node\n    If this is specified, the returned dictionary maps each node\n    ``v`` in the graph to the similarity between ``source`` and\n    ``v``.\n\ntarget : node\n    If both ``source`` and ``target`` are specified, the similarity\n    value between ``source`` and ``target`` is returned. If\n    ``target`` is specified but ``source`` is not, this argument is\n    ignored.\n\nimportance_factor : float\n    The relative importance of indirect neighbors with respect to\n    direct neighbors.\n\nmax_iterations : integer\n    Maximum number of iterations.\n\ntolerance : float\n    Error tolerance used to check convergence. When an iteration of\n    the algorithm finds that no similarity value changes more than\n    this amount, the algorithm halts.\n\nReturns\n-------\nsimilarity : numpy array or float\n    If ``source`` and ``target`` are both ``None``, this returns a\n    2D array containing SimRank scores of the nodes.\n\n    If ``source`` is not ``None`` but ``target`` is, this returns an\n    1D array containing SimRank scores of ``source`` and that\n    node.\n\n    If neither ``source`` nor ``target`` is ``None``, this returns\n    the similarity value for the given pair of nodes.\n\nExamples\n--------\n>>> G = nx.cycle_graph(2)\n>>> nx.similarity._simrank_similarity_numpy(G)\narray([[1., 0.],\n       [0., 1.]])\n>>> nx.similarity._simrank_similarity_numpy(G, source=0)\narray([1., 0.])\n>>> nx.similarity._simrank_similarity_numpy(G, source=0, target=0)\n1.0\n\n",
  "code": "def _simrank_similarity_numpy(G, source=None, target=None, importance_factor=0.9, max_iterations=1000, tolerance=0.0001):\n    import numpy as np\n    adjacency_matrix = nx.to_numpy_array(G)\n    s = np.array(adjacency_matrix.sum(axis=0))\n    s[s == 0] = 1\n    adjacency_matrix /= s\n    newsim = np.eye(len(G), dtype=np.float64)\n    for its in range(max_iterations):\n        prevsim = newsim.copy()\n        newsim = importance_factor * (adjacency_matrix.T @ prevsim @ adjacency_matrix)\n        np.fill_diagonal(newsim, 1.0)\n        if np.allclose(prevsim, newsim, atol=tolerance):\n            break\n    if its + 1 == max_iterations:\n        raise nx.ExceededMaxIterations(f'simrank did not converge after {max_iterations} iterations.')\n    if source is not None and target is not None:\n        return newsim[source, target]\n    if source is not None:\n        return newsim[source]\n    return newsim"
 },
 {
  "docstring": "Returns the Panther similarity of nodes in the graph `G` to node ``v``.\n\nPanther is a similarity metric that says \"two objects are considered\nto be similar if they frequently appear on the same paths.\" [1]_.\n\nParameters\n----------\nG : NetworkX graph\n    A NetworkX graph\nsource : node\n    Source node for which to find the top `k` similar other nodes\nk : int (default = 5)\n    The number of most similar nodes to return\npath_length : int (default = 5)\n    How long the randomly generated paths should be (``T`` in [1]_)\nc : float (default = 0.5)\n    A universal positive constant used to scale the number\n    of sample random paths to generate.\ndelta : float (default = 0.1)\n    The probability that the similarity $S$ is not an epsilon-approximation to (R, phi),\n    where $R$ is the number of random paths and $\\phi$ is the probability\n    that an element sampled from a set $A \\subseteq D$, where $D$ is the domain.\neps : float or None (default = None)\n    The error bound. Per [1]_, a good value is ``sqrt(1/|E|)``. Therefore,\n    if no value is provided, the recommended computed value will be used.\nweight : string or None, optional (default=\"weight\")\n    The name of an edge attribute that holds the numerical value\n    used as a weight. If None then each edge has weight 1.\n\nReturns\n-------\nsimilarity : dictionary\n    Dictionary of nodes to similarity scores (as floats). Note:\n    the self-similarity (i.e., ``v``) will not be included in\n    the returned dictionary.\n\nExamples\n--------\n>>> G = nx.star_graph(10)\n>>> sim = nx.panther_similarity(G, 0)\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef panther_similarity(G, source, k=5, path_length=5, c=0.5, delta=0.1, eps=None, weight='weight'):\n    import numpy as np\n    num_nodes = G.number_of_nodes()\n    if num_nodes < k:\n        warnings.warn(f'Number of nodes is {num_nodes}, but requested k is {k}. Setting k to number of nodes.')\n        k = num_nodes\n    if eps is None:\n        eps = np.sqrt(1.0 / G.number_of_edges())\n    inv_node_map = {name: index for index, name in enumerate(G.nodes)}\n    node_map = np.array(G)\n    t_choose_2 = math.comb(path_length, 2)\n    sample_size = int(c / eps ** 2 * (np.log2(t_choose_2) + 1 + np.log(1 / delta)))\n    index_map = {}\n    _ = list(generate_random_paths(G, sample_size, path_length=path_length, index_map=index_map, weight=weight))\n    S = np.zeros(num_nodes)\n    inv_sample_size = 1 / sample_size\n    source_paths = set(index_map[source])\n    for node, paths in index_map.items():\n        common_paths = source_paths.intersection(paths)\n        S[inv_node_map[node]] = len(common_paths) * inv_sample_size\n    top_k_unsorted = np.argpartition(S, -k)[-k:]\n    top_k_sorted = top_k_unsorted[np.argsort(S[top_k_unsorted])][::-1]\n    top_k_sorted_names = (node_map[n] for n in top_k_sorted)\n    top_k_with_val = dict(zip(top_k_sorted_names, S[top_k_sorted]))\n    top_k_with_val.pop(source, None)\n    return top_k_with_val"
 },
 {
  "docstring": "Randomly generate `sample_size` paths of length `path_length`.\n\nParameters\n----------\nG : NetworkX graph\n    A NetworkX graph\nsample_size : integer\n    The number of paths to generate. This is ``R`` in [1]_.\npath_length : integer (default = 5)\n    The maximum size of the path to randomly generate.\n    This is ``T`` in [1]_. According to the paper, ``T >= 5`` is\n    recommended.\nindex_map : dictionary, optional\n    If provided, this will be populated with the inverted\n    index of nodes mapped to the set of generated random path\n    indices within ``paths``.\nweight : string or None, optional (default=\"weight\")\n    The name of an edge attribute that holds the numerical value\n    used as a weight. If None then each edge has weight 1.\n\nReturns\n-------\npaths : generator of lists\n    Generator of `sample_size` paths each with length `path_length`.\n\nExamples\n--------\nNote that the return value is the list of paths:\n\n>>> G = nx.star_graph(3)\n>>> random_path = nx.generate_random_paths(G, 2)\n\nBy passing a dictionary into `index_map`, it will build an\ninverted index mapping of nodes to the paths in which that node is present:\n\n>>> G = nx.star_graph(3)\n>>> index_map = {}\n>>> random_path = nx.generate_random_paths(G, 3, index_map=index_map)\n>>> paths_containing_node_0 = [random_path[path_idx] for path_idx in index_map.get(0, [])]\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef generate_random_paths(G, sample_size, path_length=5, index_map=None, weight='weight'):\n    import numpy as np\n    adj_mat = nx.to_numpy_array(G, weight=weight)\n    inv_row_sums = np.reciprocal(adj_mat.sum(axis=1)).reshape(-1, 1)\n    transition_probabilities = adj_mat * inv_row_sums\n    node_map = np.array(G)\n    num_nodes = G.number_of_nodes()\n    for path_index in range(sample_size):\n        node_index = np.random.randint(0, high=num_nodes)\n        node = node_map[node_index]\n        path = [node]\n        if index_map is not None:\n            if node in index_map:\n                index_map[node].add(path_index)\n            else:\n                index_map[node] = {path_index}\n        starting_index = node_index\n        for _ in range(path_length):\n            neighbor_index = np.random.choice(num_nodes, p=transition_probabilities[starting_index])\n            starting_index = neighbor_index\n            neighbor_node = node_map[neighbor_index]\n            path.append(neighbor_node)\n            if index_map is not None:\n                if neighbor_node in index_map:\n                    index_map[neighbor_node].add(path_index)\n                else:\n                    index_map[neighbor_node] = {path_index}\n        yield path"
 },
 {
  "docstring": "Parameters:\n    u, v: matched vertices, u=None or v=None for\n       deletion/insertion\n    pending_g, pending_h: lists of edges not yet mapped\n    Ce: CostMatrix of pending edge mappings\n    matched_uv: partial vertex edit path\n        list of tuples (u, v) of previously matched vertex\n            mappings u<->v, u=None or v=None for\n            deletion/insertion\n\nReturns:\n    list of (i, j): indices of edge mappings g<->h\n    localCe: local CostMatrix of edge mappings\n        (basically submatrix of Ce at cross of rows i, cols j)",
  "code": "def match_edges(u, v, pending_g, pending_h, Ce, matched_uv=None):\n    M = len(pending_g)\n    N = len(pending_h)\n    if matched_uv is None or len(matched_uv) == 0:\n        g_ind = []\n        h_ind = []\n    else:\n        g_ind = [i for i in range(M) if pending_g[i][:2] == (u, u) or any((pending_g[i][:2] in ((p, u), (u, p), (p, p)) for p, q in matched_uv))]\n        h_ind = [j for j in range(N) if pending_h[j][:2] == (v, v) or any((pending_h[j][:2] in ((q, v), (v, q), (q, q)) for p, q in matched_uv))]\n    m = len(g_ind)\n    n = len(h_ind)\n    if m or n:\n        C = extract_C(Ce.C, g_ind, h_ind, M, N)\n        for k, i in enumerate(g_ind):\n            g = pending_g[i][:2]\n            for l, j in enumerate(h_ind):\n                h = pending_h[j][:2]\n                if nx.is_directed(G1) or nx.is_directed(G2):\n                    if any((g == (p, u) and h == (q, v) or (g == (u, p) and h == (v, q)) for p, q in matched_uv)):\n                        continue\n                elif any((g in ((p, u), (u, p)) and h in ((q, v), (v, q)) for p, q in matched_uv)):\n                    continue\n                if g == (u, u) or any((g == (p, p) for p, q in matched_uv)):\n                    continue\n                if h == (v, v) or any((h == (q, q) for p, q in matched_uv)):\n                    continue\n                C[k, l] = inf\n        localCe = make_CostMatrix(C, m, n)\n        ij = [(g_ind[k] if k < m else M + h_ind[l], h_ind[l] if l < n else N + g_ind[k]) for k, l in zip(localCe.lsa_row_ind, localCe.lsa_col_ind) if k < m or l < n]\n    else:\n        ij = []\n        localCe = CostMatrix(np.empty((0, 0)), [], [], 0)\n    return (ij, localCe)"
 },
 {
  "docstring": "Parameters:\n    matched_uv: partial vertex edit path\n        list of tuples (u, v) of vertex mappings u<->v,\n        u=None or v=None for deletion/insertion\n    pending_u, pending_v: lists of vertices not yet mapped\n    Cv: CostMatrix of pending vertex mappings\n    pending_g, pending_h: lists of edges not yet mapped\n    Ce: CostMatrix of pending edge mappings\n    matched_cost: cost of partial edit path\n\nReturns:\n    sequence of\n        (i, j): indices of vertex mapping u<->v\n        Cv_ij: reduced CostMatrix of pending vertex mappings\n            (basically Cv with row i, col j removed)\n        list of (x, y): indices of edge mappings g<->h\n        Ce_xy: reduced CostMatrix of pending edge mappings\n            (basically Ce with rows x, cols y removed)\n        cost: total cost of edit operation\n    NOTE: most promising ops first",
  "code": "def get_edit_ops(matched_uv, pending_u, pending_v, Cv, pending_g, pending_h, Ce, matched_cost):\n    m = len(pending_u)\n    n = len(pending_v)\n    i, j = min(((k, l) for k, l in zip(Cv.lsa_row_ind, Cv.lsa_col_ind) if k < m or l < n))\n    xy, localCe = match_edges(pending_u[i] if i < m else None, pending_v[j] if j < n else None, pending_g, pending_h, Ce, matched_uv)\n    Ce_xy = reduce_Ce(Ce, xy, len(pending_g), len(pending_h))\n    if prune(matched_cost + Cv.ls + localCe.ls + Ce_xy.ls):\n        pass\n    else:\n        Cv_ij = CostMatrix(reduce_C(Cv.C, (i,), (j,), m, n), reduce_ind(Cv.lsa_row_ind, (i, m + j)), reduce_ind(Cv.lsa_col_ind, (j, n + i)), Cv.ls - Cv.C[i, j])\n        yield ((i, j), Cv_ij, xy, Ce_xy, Cv.C[i, j] + localCe.ls)\n    other = []\n    fixed_i, fixed_j = (i, j)\n    if m <= n:\n        candidates = ((t, fixed_j) for t in range(m + n) if t != fixed_i and (t < m or t == m + fixed_j))\n    else:\n        candidates = ((fixed_i, t) for t in range(m + n) if t != fixed_j and (t < n or t == n + fixed_i))\n    for i, j in candidates:\n        if prune(matched_cost + Cv.C[i, j] + Ce.ls):\n            continue\n        Cv_ij = make_CostMatrix(reduce_C(Cv.C, (i,), (j,), m, n), m - 1 if i < m else m, n - 1 if j < n else n)\n        if prune(matched_cost + Cv.C[i, j] + Cv_ij.ls + Ce.ls):\n            continue\n        xy, localCe = match_edges(pending_u[i] if i < m else None, pending_v[j] if j < n else None, pending_g, pending_h, Ce, matched_uv)\n        if prune(matched_cost + Cv.C[i, j] + Cv_ij.ls + localCe.ls):\n            continue\n        Ce_xy = reduce_Ce(Ce, xy, len(pending_g), len(pending_h))\n        if prune(matched_cost + Cv.C[i, j] + Cv_ij.ls + localCe.ls + Ce_xy.ls):\n            continue\n        other.append(((i, j), Cv_ij, xy, Ce_xy, Cv.C[i, j] + localCe.ls))\n    yield from sorted(other, key=lambda t: t[4] + t[1].ls + t[3].ls)"
 },
 {
  "docstring": "Parameters:\n    matched_uv: partial vertex edit path\n        list of tuples (u, v) of vertex mappings u<->v,\n        u=None or v=None for deletion/insertion\n    pending_u, pending_v: lists of vertices not yet mapped\n    Cv: CostMatrix of pending vertex mappings\n    matched_gh: partial edge edit path\n        list of tuples (g, h) of edge mappings g<->h,\n        g=None or h=None for deletion/insertion\n    pending_g, pending_h: lists of edges not yet mapped\n    Ce: CostMatrix of pending edge mappings\n    matched_cost: cost of partial edit path\n\nReturns:\n    sequence of (vertex_path, edge_path, cost)\n        vertex_path: complete vertex edit path\n            list of tuples (u, v) of vertex mappings u<->v,\n            u=None or v=None for deletion/insertion\n        edge_path: complete edge edit path\n            list of tuples (g, h) of edge mappings g<->h,\n            g=None or h=None for deletion/insertion\n        cost: total cost of edit path\n    NOTE: path costs are non-increasing",
  "code": "def get_edit_paths(matched_uv, pending_u, pending_v, Cv, matched_gh, pending_g, pending_h, Ce, matched_cost):\n    if prune(matched_cost + Cv.ls + Ce.ls):\n        return\n    if not max(len(pending_u), len(pending_v)):\n        nonlocal maxcost_value\n        maxcost_value = min(maxcost_value, matched_cost)\n        yield (matched_uv, matched_gh, matched_cost)\n    else:\n        edit_ops = get_edit_ops(matched_uv, pending_u, pending_v, Cv, pending_g, pending_h, Ce, matched_cost)\n        for ij, Cv_ij, xy, Ce_xy, edit_cost in edit_ops:\n            i, j = ij\n            if prune(matched_cost + edit_cost + Cv_ij.ls + Ce_xy.ls):\n                continue\n            u = pending_u.pop(i) if i < len(pending_u) else None\n            v = pending_v.pop(j) if j < len(pending_v) else None\n            matched_uv.append((u, v))\n            for x, y in xy:\n                len_g = len(pending_g)\n                len_h = len(pending_h)\n                matched_gh.append((pending_g[x] if x < len_g else None, pending_h[y] if y < len_h else None))\n            sortedx = sorted((x for x, y in xy))\n            sortedy = sorted((y for x, y in xy))\n            G = [pending_g.pop(x) if x < len(pending_g) else None for x in reversed(sortedx)]\n            H = [pending_h.pop(y) if y < len(pending_h) else None for y in reversed(sortedy)]\n            yield from get_edit_paths(matched_uv, pending_u, pending_v, Cv_ij, matched_gh, pending_g, pending_h, Ce_xy, matched_cost + edit_cost)\n            if u is not None:\n                pending_u.insert(i, u)\n            if v is not None:\n                pending_v.insert(j, v)\n            matched_uv.pop()\n            for x, g in zip(sortedx, reversed(G)):\n                if g is not None:\n                    pending_g.insert(x, g)\n            for y, h in zip(sortedy, reversed(H)):\n                if h is not None:\n                    pending_h.insert(y, h)\n            for _ in xy:\n                matched_gh.pop()"
 },
 {
  "docstring": "Returns True if and only if `nodes` form a simple path in `G`.\n\nA *simple path* in a graph is a nonempty sequence of nodes in which\nno node appears more than once in the sequence, and each adjacent\npair of nodes in the sequence is adjacent in the graph.\n\nParameters\n----------\nG : graph\n    A NetworkX graph.\nnodes : list\n    A list of one or more nodes in the graph `G`.\n\nReturns\n-------\nbool\n    Whether the given list of nodes represents a simple path in `G`.\n\n",
  "code": "@nx._dispatch\ndef is_simple_path(G, nodes):\n    if len(nodes) == 0:\n        return False\n    if len(nodes) == 1:\n        return nodes[0] in G\n    if not all((n in G for n in nodes)):\n        return False\n    if len(set(nodes)) != len(nodes):\n        return False\n    return all((v in G[u] for u, v in pairwise(nodes)))"
 },
 {
  "docstring": "Generate all simple paths in the graph G from source to target.\n\nA simple path is a path with no repeated nodes.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node\n   Starting node for path\n\ntarget : nodes\n   Single node or iterable of nodes at which to end path\n\ncutoff : integer, optional\n    Depth to stop the search. Only paths of length <= cutoff are returned.\n\nReturns\n-------\npath_generator: generator\n   A generator that produces lists of simple paths.  If there are no paths\n   between the source and target within the given cutoff the generator\n   produces no output. If it is possible to traverse the same sequence of\n   nodes in multiple ways, namely through parallel edges, then it will be\n   returned multiple times (once for each viable edge combination).\n\nExamples\n--------\nThis iterator generates lists of nodes::\n\n    >>> G = nx.complete_graph(4)\n    >>> for path in nx.all_simple_paths(G, source=0, target=3):\n    ...     print(path)\n    ...\n    [0, 1, 2, 3]\n    [0, 1, 3]\n    [0, 2, 1, 3]\n    [0, 2, 3]\n    [0, 3]\n\nYou can generate only those paths that are shorter than a certain\nlength by using the `cutoff` keyword argument::\n\n    >>> paths = nx.all_simple_paths(G, source=0, target=3, cutoff=2)\n    >>> print(list(paths))\n    [[0, 1, 3], [0, 2, 3], [0, 3]]\n\nTo get each path as the corresponding list of edges, you can use the\n:func:`networkx.utils.pairwise` helper function::\n\n    >>> paths = nx.all_simple_paths(G, source=0, target=3)\n    >>> for path in map(nx.utils.pairwise, paths):\n    ...     print(list(path))\n    [(0, 1), (1, 2), (2, 3)]\n    [(0, 1), (1, 3)]\n    [(0, 2), (2, 1), (1, 3)]\n    [(0, 2), (2, 3)]\n    [(0, 3)]\n\nPass an iterable of nodes as target to generate all paths ending in any of several nodes::\n\n    >>> G = nx.complete_graph(4)\n    >>> for path in nx.all_simple_paths(G, source=0, target=[3, 2]):\n    ...     print(path)\n    ...\n    [0, 1, 2]\n    [0, 1, 2, 3]\n    [0, 1, 3]\n    [0, 1, 3, 2]\n    [0, 2]\n    [0, 2, 1, 3]\n    [0, 2, 3]\n    [0, 3]\n    [0, 3, 1, 2]\n    [0, 3, 2]\n\nThe singleton path from ``source`` to itself is considered a simple path and is\nincluded in the results:\n\n    >>> G = nx.empty_graph(5)\n    >>> list(nx.all_simple_paths(G, source=0, target=0))\n    [[0]]\n\n    >>> G = nx.path_graph(3)\n    >>> list(nx.all_simple_paths(G, source=0, target={0, 1, 2}))\n    [[0], [0, 1], [0, 1, 2]]\n\nIterate over each path from the root nodes to the leaf nodes in a\ndirected acyclic graph using a functional programming approach::\n\n    >>> from itertools import chain\n    >>> from itertools import product\n    >>> from itertools import starmap\n    >>> from functools import partial\n    >>>\n    >>> chaini = chain.from_iterable\n    >>>\n    >>> G = nx.DiGraph([(0, 1), (1, 2), (0, 3), (3, 2)])\n    >>> roots = (v for v, d in G.in_degree() if d == 0)\n    >>> leaves = (v for v, d in G.out_degree() if d == 0)\n    >>> all_paths = partial(nx.all_simple_paths, G)\n    >>> list(chaini(starmap(all_paths, product(roots, leaves))))\n    [[0, 1, 2], [0, 3, 2]]\n\nThe same list computed using an iterative approach::\n\n    >>> G = nx.DiGraph([(0, 1), (1, 2), (0, 3), (3, 2)])\n    >>> roots = (v for v, d in G.in_degree() if d == 0)\n    >>> leaves = (v for v, d in G.out_degree() if d == 0)\n    >>> all_paths = []\n    >>> for root in roots:\n    ...     for leaf in leaves:\n    ...         paths = nx.all_simple_paths(G, root, leaf)\n    ...         all_paths.extend(paths)\n    >>> all_paths\n    [[0, 1, 2], [0, 3, 2]]\n\nIterate over each path from the root nodes to the leaf nodes in a\ndirected acyclic graph passing all leaves together to avoid unnecessary\ncompute::\n\n    >>> G = nx.DiGraph([(0, 1), (2, 1), (1, 3), (1, 4)])\n    >>> roots = (v for v, d in G.in_degree() if d == 0)\n    >>> leaves = [v for v, d in G.out_degree() if d == 0]\n    >>> all_paths = []\n    >>> for root in roots:\n    ...     paths = nx.all_simple_paths(G, root, leaves)\n    ...     all_paths.extend(paths)\n    >>> all_paths\n    [[0, 1, 3], [0, 1, 4], [2, 1, 3], [2, 1, 4]]\n\nIf parallel edges offer multiple ways to traverse a given sequence of\nnodes, this sequence of nodes will be returned multiple times:\n\n    >>> G = nx.MultiDiGraph([(0, 1), (0, 1), (1, 2)])\n    >>> list(nx.all_simple_paths(G, 0, 2))\n    [[0, 1, 2], [0, 1, 2]]\n\n",
  "code": "@nx._dispatch\ndef all_simple_paths(G, source, target, cutoff=None):\n    for edge_path in all_simple_edge_paths(G, source, target, cutoff):\n        yield ([source] + [edge[1] for edge in edge_path])"
 },
 {
  "docstring": "Generate lists of edges for all simple paths in G from source to target.\n\nA simple path is a path with no repeated nodes.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node\n   Starting node for path\n\ntarget : nodes\n   Single node or iterable of nodes at which to end path\n\ncutoff : integer, optional\n    Depth to stop the search. Only paths of length <= cutoff are returned.\n\nReturns\n-------\npath_generator: generator\n   A generator that produces lists of simple paths.  If there are no paths\n   between the source and target within the given cutoff the generator\n   produces no output.\n   For multigraphs, the list of edges have elements of the form `(u,v,k)`.\n   Where `k` corresponds to the edge key.\n\nExamples\n--------\n\nPrint the simple path edges of a Graph::\n\n    >>> g = nx.Graph([(1, 2), (2, 4), (1, 3), (3, 4)])\n    >>> for path in sorted(nx.all_simple_edge_paths(g, 1, 4)):\n    ...     print(path)\n    [(1, 2), (2, 4)]\n    [(1, 3), (3, 4)]\n\nPrint the simple path edges of a MultiGraph. Returned edges come with\ntheir associated keys::\n\n    >>> mg = nx.MultiGraph()\n    >>> mg.add_edge(1, 2, key=\"k0\")\n    'k0'\n    >>> mg.add_edge(1, 2, key=\"k1\")\n    'k1'\n    >>> mg.add_edge(2, 3, key=\"k0\")\n    'k0'\n    >>> for path in sorted(nx.all_simple_edge_paths(mg, 1, 3)):\n    ...     print(path)\n    [(1, 2, 'k0'), (2, 3, 'k0')]\n    [(1, 2, 'k1'), (2, 3, 'k0')]\n\nWhen ``source`` is one of the targets, the empty path starting and ending at\n``source`` without traversing any edge is considered a valid simple edge path\nand is included in the results:\n\n    >>> G = nx.Graph()\n    >>> G.add_node(0)\n    >>> paths = list(nx.all_simple_edge_paths(G, 0, 0))\n    >>> for path in paths:\n    ...     print (path)\n    []\n    >>> len(paths)\n    1\n\n\n",
  "code": "@nx._dispatch\ndef all_simple_edge_paths(G, source, target, cutoff=None):\n    if source not in G:\n        raise nx.NodeNotFound(f'source node {source} not in graph')\n    if target in G:\n        targets = {target}\n    else:\n        try:\n            targets = set(target)\n        except TypeError as err:\n            raise nx.NodeNotFound(f'target node {target} not in graph') from err\n    cutoff = cutoff if cutoff is not None else len(G) - 1\n    if cutoff >= 0 and targets:\n        yield from _all_simple_edge_paths(G, source, targets, cutoff)"
 },
 {
  "docstring": "Generate all simple paths in the graph G from source to target,\n   starting from shortest ones.\n\nA simple path is a path with no repeated nodes.\n\nIf a weighted shortest path search is to be used, no negative weights\nare allowed.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node\n   Starting node for path\n\ntarget : node\n   Ending node for path\n\nweight : string or function\n    If it is a string, it is the name of the edge attribute to be\n    used as a weight.\n\n    If it is a function, the weight of an edge is the value returned\n    by the function. The function must accept exactly three positional\n    arguments: the two endpoints of an edge and the dictionary of edge\n    attributes for that edge. The function must return a number.\n\n    If None all edges are considered to have unit weight. Default\n    value None.\n\nReturns\n-------\npath_generator: generator\n   A generator that produces lists of simple paths, in order from\n   shortest to longest.\n\nRaises\n------\nNetworkXNoPath\n   If no path exists between source and target.\n\nNetworkXError\n   If source or target nodes are not in the input graph.\n\nNetworkXNotImplemented\n   If the input graph is a Multi[Di]Graph.\n\nExamples\n--------\n\n>>> G = nx.cycle_graph(7)\n>>> paths = list(nx.shortest_simple_paths(G, 0, 3))\n>>> print(paths)\n[[0, 1, 2, 3], [0, 6, 5, 4, 3]]\n\nYou can use this function to efficiently compute the k shortest/best\npaths between two nodes.\n\n>>> from itertools import islice\n>>> def k_shortest_paths(G, source, target, k, weight=None):\n...     return list(\n...         islice(nx.shortest_simple_paths(G, source, target, weight=weight), k)\n...     )\n>>> for path in k_shortest_paths(G, 0, 3, 2):\n...     print(path)\n[0, 1, 2, 3]\n[0, 6, 5, 4, 3]\n\n",
  "code": "@not_implemented_for('multigraph')\n@nx._dispatch(edge_attrs='weight')\ndef shortest_simple_paths(G, source, target, weight=None):\n    if source not in G:\n        raise nx.NodeNotFound(f'source node {source} not in graph')\n    if target not in G:\n        raise nx.NodeNotFound(f'target node {target} not in graph')\n    if weight is None:\n        length_func = len\n        shortest_path_func = _bidirectional_shortest_path\n    else:\n        wt = _weight_function(G, weight)\n\n        def length_func(path):\n            return sum((wt(u, v, G.get_edge_data(u, v)) for u, v in zip(path, path[1:])))\n        shortest_path_func = _bidirectional_dijkstra\n    listA = []\n    listB = PathBuffer()\n    prev_path = None\n    while True:\n        if not prev_path:\n            length, path = shortest_path_func(G, source, target, weight=weight)\n            listB.push(length, path)\n        else:\n            ignore_nodes = set()\n            ignore_edges = set()\n            for i in range(1, len(prev_path)):\n                root = prev_path[:i]\n                root_length = length_func(root)\n                for path in listA:\n                    if path[:i] == root:\n                        ignore_edges.add((path[i - 1], path[i]))\n                try:\n                    length, spur = shortest_path_func(G, root[-1], target, ignore_nodes=ignore_nodes, ignore_edges=ignore_edges, weight=weight)\n                    path = root[:-1] + spur\n                    listB.push(root_length + length, path)\n                except nx.NetworkXNoPath:\n                    pass\n                ignore_nodes.add(root[-1])\n        if listB:\n            path = listB.pop()\n            yield path\n            listA.append(path)\n            prev_path = path\n        else:\n            break"
 },
 {
  "docstring": "Returns the shortest path between source and target ignoring\n   nodes and edges in the containers ignore_nodes and ignore_edges.\n\nThis is a custom modification of the standard bidirectional shortest\npath implementation at networkx.algorithms.unweighted\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node\n   starting node for path\n\ntarget : node\n   ending node for path\n\nignore_nodes : container of nodes\n   nodes to ignore, optional\n\nignore_edges : container of edges\n   edges to ignore, optional\n\nweight : None\n   This function accepts a weight argument for convenience of\n   shortest_simple_paths function. It will be ignored.\n\nReturns\n-------\npath: list\n   List of nodes in a path from source to target.\n\nRaises\n------\nNetworkXNoPath\n   If no path exists between source and target.\n\nSee Also\n--------\nshortest_path",
  "code": "def _bidirectional_shortest_path(G, source, target, ignore_nodes=None, ignore_edges=None, weight=None):\n    results = _bidirectional_pred_succ(G, source, target, ignore_nodes, ignore_edges)\n    pred, succ, w = results\n    path = []\n    while w is not None:\n        path.append(w)\n        w = succ[w]\n    w = pred[path[0]]\n    while w is not None:\n        path.insert(0, w)\n        w = pred[w]\n    return (len(path), path)"
 },
 {
  "docstring": "Bidirectional shortest path helper.\nReturns (pred,succ,w) where\npred is a dictionary of predecessors from w to the source, and\nsucc is a dictionary of successors from w to the target.",
  "code": "def _bidirectional_pred_succ(G, source, target, ignore_nodes=None, ignore_edges=None):\n    if ignore_nodes and (source in ignore_nodes or target in ignore_nodes):\n        raise nx.NetworkXNoPath(f'No path between {source} and {target}.')\n    if target == source:\n        return ({target: None}, {source: None}, source)\n    if G.is_directed():\n        Gpred = G.predecessors\n        Gsucc = G.successors\n    else:\n        Gpred = G.neighbors\n        Gsucc = G.neighbors\n    if ignore_nodes:\n\n        def filter_iter(nodes):\n\n            def iterate(v):\n                for w in nodes(v):\n                    if w not in ignore_nodes:\n                        yield w\n            return iterate\n        Gpred = filter_iter(Gpred)\n        Gsucc = filter_iter(Gsucc)\n    if ignore_edges:\n        if G.is_directed():\n\n            def filter_pred_iter(pred_iter):\n\n                def iterate(v):\n                    for w in pred_iter(v):\n                        if (w, v) not in ignore_edges:\n                            yield w\n                return iterate\n\n            def filter_succ_iter(succ_iter):\n\n                def iterate(v):\n                    for w in succ_iter(v):\n                        if (v, w) not in ignore_edges:\n                            yield w\n                return iterate\n            Gpred = filter_pred_iter(Gpred)\n            Gsucc = filter_succ_iter(Gsucc)\n        else:\n\n            def filter_iter(nodes):\n\n                def iterate(v):\n                    for w in nodes(v):\n                        if (v, w) not in ignore_edges and (w, v) not in ignore_edges:\n                            yield w\n                return iterate\n            Gpred = filter_iter(Gpred)\n            Gsucc = filter_iter(Gsucc)\n    pred = {source: None}\n    succ = {target: None}\n    forward_fringe = [source]\n    reverse_fringe = [target]\n    while forward_fringe and reverse_fringe:\n        if len(forward_fringe) <= len(reverse_fringe):\n            this_level = forward_fringe\n            forward_fringe = []\n            for v in this_level:\n                for w in Gsucc(v):\n                    if w not in pred:\n                        forward_fringe.append(w)\n                        pred[w] = v\n                    if w in succ:\n                        return (pred, succ, w)\n        else:\n            this_level = reverse_fringe\n            reverse_fringe = []\n            for v in this_level:\n                for w in Gpred(v):\n                    if w not in succ:\n                        succ[w] = v\n                        reverse_fringe.append(w)\n                    if w in pred:\n                        return (pred, succ, w)\n    raise nx.NetworkXNoPath(f'No path between {source} and {target}.')"
 },
 {
  "docstring": "Dijkstra's algorithm for shortest paths using bidirectional search.\n\nThis function returns the shortest path between source and target\nignoring nodes and edges in the containers ignore_nodes and\nignore_edges.\n\nThis is a custom modification of the standard Dijkstra bidirectional\nshortest path implementation at networkx.algorithms.weighted\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node\n   Starting node.\n\ntarget : node\n   Ending node.\n\nweight: string, function, optional (default='weight')\n   Edge data key or weight function corresponding to the edge weight\n\nignore_nodes : container of nodes\n   nodes to ignore, optional\n\nignore_edges : container of edges\n   edges to ignore, optional\n\nReturns\n-------\nlength : number\n    Shortest path length.\n\nReturns a tuple of two dictionaries keyed by node.\nThe first dictionary stores distance from the source.\nThe second stores the path from the source to that node.\n\nRaises\n------\nNetworkXNoPath\n    If no path exists between source and target.\n\n",
  "code": "def _bidirectional_dijkstra(G, source, target, weight='weight', ignore_nodes=None, ignore_edges=None):\n    if ignore_nodes and (source in ignore_nodes or target in ignore_nodes):\n        raise nx.NetworkXNoPath(f'No path between {source} and {target}.')\n    if source == target:\n        if source not in G:\n            raise nx.NodeNotFound(f'Node {source} not in graph')\n        return (0, [source])\n    if G.is_directed():\n        Gpred = G.predecessors\n        Gsucc = G.successors\n    else:\n        Gpred = G.neighbors\n        Gsucc = G.neighbors\n    if ignore_nodes:\n\n        def filter_iter(nodes):\n\n            def iterate(v):\n                for w in nodes(v):\n                    if w not in ignore_nodes:\n                        yield w\n            return iterate\n        Gpred = filter_iter(Gpred)\n        Gsucc = filter_iter(Gsucc)\n    if ignore_edges:\n        if G.is_directed():\n\n            def filter_pred_iter(pred_iter):\n\n                def iterate(v):\n                    for w in pred_iter(v):\n                        if (w, v) not in ignore_edges:\n                            yield w\n                return iterate\n\n            def filter_succ_iter(succ_iter):\n\n                def iterate(v):\n                    for w in succ_iter(v):\n                        if (v, w) not in ignore_edges:\n                            yield w\n                return iterate\n            Gpred = filter_pred_iter(Gpred)\n            Gsucc = filter_succ_iter(Gsucc)\n        else:\n\n            def filter_iter(nodes):\n\n                def iterate(v):\n                    for w in nodes(v):\n                        if (v, w) not in ignore_edges and (w, v) not in ignore_edges:\n                            yield w\n                return iterate\n            Gpred = filter_iter(Gpred)\n            Gsucc = filter_iter(Gsucc)\n    push = heappush\n    pop = heappop\n    dists = [{}, {}]\n    paths = [{source: [source]}, {target: [target]}]\n    fringe = [[], []]\n    seen = [{source: 0}, {target: 0}]\n    c = count()\n    push(fringe[0], (0, next(c), source))\n    push(fringe[1], (0, next(c), target))\n    neighs = [Gsucc, Gpred]\n    finalpath = []\n    dir = 1\n    while fringe[0] and fringe[1]:\n        dir = 1 - dir\n        dist, _, v = pop(fringe[dir])\n        if v in dists[dir]:\n            continue\n        dists[dir][v] = dist\n        if v in dists[1 - dir]:\n            return (finaldist, finalpath)\n        wt = _weight_function(G, weight)\n        for w in neighs[dir](v):\n            if dir == 0:\n                minweight = wt(v, w, G.get_edge_data(v, w))\n                vwLength = dists[dir][v] + minweight\n            else:\n                minweight = wt(w, v, G.get_edge_data(w, v))\n                vwLength = dists[dir][v] + minweight\n            if w in dists[dir]:\n                if vwLength < dists[dir][w]:\n                    raise ValueError('Contradictory paths found: negative weights?')\n            elif w not in seen[dir] or vwLength < seen[dir][w]:\n                seen[dir][w] = vwLength\n                push(fringe[dir], (vwLength, next(c), w))\n                paths[dir][w] = paths[dir][v] + [w]\n                if w in seen[0] and w in seen[1]:\n                    totaldist = seen[0][w] + seen[1][w]\n                    if finalpath == [] or finaldist > totaldist:\n                        finaldist = totaldist\n                        revpath = paths[1][w][:]\n                        revpath.reverse()\n                        finalpath = paths[0][w] + revpath[1:]\n    raise nx.NetworkXNoPath(f'No path between {source} and {target}.')"
 },
 {
  "docstring": "Compute a random graph by swapping edges of a given graph.\n\nParameters\n----------\nG : graph\n    An undirected graph with 4 or more nodes.\n\nniter : integer (optional, default=1)\n    An edge is rewired approximately `niter` times.\n\nconnectivity : boolean (optional, default=True)\n    When True, ensure connectivity for the randomized graph.\n\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\nReturns\n-------\nG : graph\n    The randomized graph.\n\nRaises\n------\nNetworkXError\n    If there are fewer than 4 nodes or 2 edges in `G`\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@py_random_state(3)\n@nx._dispatch\ndef random_reference(G, niter=1, connectivity=True, seed=None):\n    if len(G) < 4:\n        raise nx.NetworkXError('Graph has fewer than four nodes.')\n    if len(G.edges) < 2:\n        raise nx.NetworkXError('Graph has fewer that 2 edges')\n    from networkx.utils import cumulative_distribution, discrete_sequence\n    local_conn = nx.connectivity.local_edge_connectivity\n    G = G.copy()\n    keys, degrees = zip(*G.degree())\n    cdf = cumulative_distribution(degrees)\n    nnodes = len(G)\n    nedges = nx.number_of_edges(G)\n    niter = niter * nedges\n    ntries = int(nnodes * nedges / (nnodes * (nnodes - 1) / 2))\n    swapcount = 0\n    for i in range(niter):\n        n = 0\n        while n < ntries:\n            ai, ci = discrete_sequence(2, cdistribution=cdf, seed=seed)\n            if ai == ci:\n                continue\n            a = keys[ai]\n            c = keys[ci]\n            b = seed.choice(list(G.neighbors(a)))\n            d = seed.choice(list(G.neighbors(c)))\n            if b in [a, c, d] or d in [a, b, c]:\n                continue\n            if d not in G[a] and b not in G[c]:\n                G.add_edge(a, d)\n                G.add_edge(c, b)\n                G.remove_edge(a, b)\n                G.remove_edge(c, d)\n                if connectivity and local_conn(G, a, b) == 0:\n                    G.remove_edge(a, d)\n                    G.remove_edge(c, b)\n                    G.add_edge(a, b)\n                    G.add_edge(c, d)\n                else:\n                    swapcount += 1\n                    break\n            n += 1\n    return G"
 },
 {
  "docstring": "Latticize the given graph by swapping edges.\n\nParameters\n----------\nG : graph\n    An undirected graph.\n\nniter : integer (optional, default=1)\n    An edge is rewired approximately niter times.\n\nD : numpy.array (optional, default=None)\n    Distance to the diagonal matrix.\n\nconnectivity : boolean (optional, default=True)\n    Ensure connectivity for the latticized graph when set to True.\n\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\nReturns\n-------\nG : graph\n    The latticized graph.\n\nRaises\n------\nNetworkXError\n    If there are fewer than 4 nodes or 2 edges in `G`\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@py_random_state(4)\n@nx._dispatch\ndef lattice_reference(G, niter=5, D=None, connectivity=True, seed=None):\n    import numpy as np\n    from networkx.utils import cumulative_distribution, discrete_sequence\n    local_conn = nx.connectivity.local_edge_connectivity\n    if len(G) < 4:\n        raise nx.NetworkXError('Graph has fewer than four nodes.')\n    if len(G.edges) < 2:\n        raise nx.NetworkXError('Graph has fewer that 2 edges')\n    G = G.copy()\n    keys, degrees = zip(*G.degree())\n    cdf = cumulative_distribution(degrees)\n    nnodes = len(G)\n    nedges = nx.number_of_edges(G)\n    if D is None:\n        D = np.zeros((nnodes, nnodes))\n        un = np.arange(1, nnodes)\n        um = np.arange(nnodes - 1, 0, -1)\n        u = np.append((0,), np.where(un < um, un, um))\n        for v in range(int(np.ceil(nnodes / 2))):\n            D[nnodes - v - 1, :] = np.append(u[v + 1:], u[:v + 1])\n            D[v, :] = D[nnodes - v - 1, :][::-1]\n    niter = niter * nedges\n    max_attempts = int(nnodes * nedges / (nnodes * (nnodes - 1) / 2))\n    for _ in range(niter):\n        n = 0\n        while n < max_attempts:\n            ai, ci = discrete_sequence(2, cdistribution=cdf, seed=seed)\n            if ai == ci:\n                continue\n            a = keys[ai]\n            c = keys[ci]\n            b = seed.choice(list(G.neighbors(a)))\n            d = seed.choice(list(G.neighbors(c)))\n            bi = keys.index(b)\n            di = keys.index(d)\n            if b in [a, c, d] or d in [a, b, c]:\n                continue\n            if d not in G[a] and b not in G[c]:\n                if D[ai, bi] + D[ci, di] >= D[ai, ci] + D[bi, di]:\n                    G.add_edge(a, d)\n                    G.add_edge(c, b)\n                    G.remove_edge(a, b)\n                    G.remove_edge(c, d)\n                    if connectivity and local_conn(G, a, b) == 0:\n                        G.remove_edge(a, d)\n                        G.remove_edge(c, b)\n                        G.add_edge(a, b)\n                        G.add_edge(c, d)\n                    else:\n                        break\n            n += 1\n    return G"
 },
 {
  "docstring": "Returns the small-world coefficient (sigma) of the given graph.\n\nThe small-world coefficient is defined as:\nsigma = C/Cr / L/Lr\nwhere C and L are respectively the average clustering coefficient and\naverage shortest path length of G. Cr and Lr are respectively the average\nclustering coefficient and average shortest path length of an equivalent\nrandom graph.\n\nA graph is commonly classified as small-world if sigma>1.\n\nParameters\n----------\nG : NetworkX graph\n    An undirected graph.\nniter : integer (optional, default=100)\n    Approximate number of rewiring per edge to compute the equivalent\n    random graph.\nnrand : integer (optional, default=10)\n    Number of random graphs generated to compute the average clustering\n    coefficient (Cr) and average shortest path length (Lr).\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\nReturns\n-------\nsigma : float\n    The small-world coefficient of G.\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@py_random_state(3)\n@nx._dispatch\ndef sigma(G, niter=100, nrand=10, seed=None):\n    import numpy as np\n    randMetrics = {'C': [], 'L': []}\n    for i in range(nrand):\n        Gr = random_reference(G, niter=niter, seed=seed)\n        randMetrics['C'].append(nx.transitivity(Gr))\n        randMetrics['L'].append(nx.average_shortest_path_length(Gr))\n    C = nx.transitivity(G)\n    L = nx.average_shortest_path_length(G)\n    Cr = np.mean(randMetrics['C'])\n    Lr = np.mean(randMetrics['L'])\n    sigma = C / Cr / (L / Lr)\n    return sigma"
 },
 {
  "docstring": "Returns the small-world coefficient (omega) of a graph\n\nThe small-world coefficient of a graph G is:\n\nomega = Lr/L - C/Cl\n\nwhere C and L are respectively the average clustering coefficient and\naverage shortest path length of G. Lr is the average shortest path length\nof an equivalent random graph and Cl is the average clustering coefficient\nof an equivalent lattice graph.\n\nThe small-world coefficient (omega) measures how much G is like a lattice\nor a random graph. Negative values mean G is similar to a lattice whereas\npositive values mean G is a random graph.\nValues close to 0 mean that G has small-world characteristics.\n\nParameters\n----------\nG : NetworkX graph\n    An undirected graph.\n\nniter: integer (optional, default=5)\n    Approximate number of rewiring per edge to compute the equivalent\n    random graph.\n\nnrand: integer (optional, default=10)\n    Number of random graphs generated to compute the maximal clustering\n    coefficient (Cr) and average shortest path length (Lr).\n\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\n\nReturns\n-------\nomega : float\n    The small-world coefficient (omega)\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@py_random_state(3)\n@nx._dispatch\ndef omega(G, niter=5, nrand=10, seed=None):\n    import numpy as np\n    randMetrics = {'C': [], 'L': []}\n    Cl = nx.average_clustering(G)\n    niter_lattice_reference = niter\n    niter_random_reference = niter * 2\n    for _ in range(nrand):\n        Gr = random_reference(G, niter=niter_random_reference, seed=seed)\n        randMetrics['L'].append(nx.average_shortest_path_length(Gr))\n        Gl = lattice_reference(G, niter=niter_lattice_reference, seed=seed)\n        Cl_temp = nx.average_clustering(Gl)\n        if Cl_temp > Cl:\n            Cl = Cl_temp\n    C = nx.average_clustering(G)\n    L = nx.average_shortest_path_length(G)\n    Lr = np.mean(randMetrics['L'])\n    omega = Lr / L - C / Cl\n    return omega"
 },
 {
  "docstring": "Returns the s-metric [1]_ of graph.\n\nThe s-metric is defined as the sum of the products ``deg(u) * deg(v)``\nfor every edge ``(u, v)`` in `G`.\n\nParameters\n----------\nG : graph\n    The graph used to compute the s-metric.\nnormalized : bool (optional)\n    Normalize the value.\n\n    .. deprecated:: 3.2\n\n       The `normalized` keyword argument is deprecated and will be removed\n       in the future\n\nReturns\n-------\ns : float\n    The s-metric of the graph.\n\n",
  "code": "@nx._dispatch\ndef s_metric(G, **kwargs):\n    if kwargs:\n        if 'normalized' in kwargs:\n            import warnings\n            warnings.warn('\\n\\nThe `normalized` keyword is deprecated and will be removed\\nin the future. To silence this warning, remove `normalized`\\nwhen calling `s_metric`.\\n\\nThe value of `normalized` is ignored.', DeprecationWarning, stacklevel=3)\n        else:\n            raise TypeError(f\"s_metric got an unexpected keyword argument '{list(kwargs.keys())[0]}'\")\n    return float(sum((G.degree(u) * G.degree(v) for u, v in G.edges())))"
 },
 {
  "docstring": "Returns a spanner of the given graph with the given stretch.\n\nA spanner of a graph G = (V, E) with stretch t is a subgraph\nH = (V, E_S) such that E_S is a subset of E and the distance between\nany pair of nodes in H is at most t times the distance between the\nnodes in G.\n\nParameters\n----------\nG : NetworkX graph\n    An undirected simple graph.\n\nstretch : float\n    The stretch of the spanner.\n\nweight : object\n    The edge attribute to use as distance.\n\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\nReturns\n-------\nNetworkX graph\n    A spanner of the given graph with the given stretch.\n\nRaises\n------\nValueError\n    If a stretch less than 1 is given.\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@py_random_state(3)\n@nx._dispatch(edge_attrs='weight')\ndef spanner(G, stretch, weight=None, seed=None):\n    if stretch < 1:\n        raise ValueError('stretch must be at least 1')\n    k = (stretch + 1) // 2\n    H = nx.empty_graph()\n    H.add_nodes_from(G.nodes)\n    residual_graph = _setup_residual_graph(G, weight)\n    clustering = {v: v for v in G.nodes}\n    sample_prob = math.pow(G.number_of_nodes(), -1 / k)\n    size_limit = 2 * math.pow(G.number_of_nodes(), 1 + 1 / k)\n    i = 0\n    while i < k - 1:\n        sampled_centers = set()\n        for center in set(clustering.values()):\n            if seed.random() < sample_prob:\n                sampled_centers.add(center)\n        edges_to_add = set()\n        edges_to_remove = set()\n        new_clustering = {}\n        for v in residual_graph.nodes:\n            if clustering[v] in sampled_centers:\n                continue\n            lightest_edge_neighbor, lightest_edge_weight = _lightest_edge_dicts(residual_graph, clustering, v)\n            neighboring_sampled_centers = set(lightest_edge_weight.keys()) & sampled_centers\n            if not neighboring_sampled_centers:\n                for neighbor in lightest_edge_neighbor.values():\n                    edges_to_add.add((v, neighbor))\n                for neighbor in residual_graph.adj[v]:\n                    edges_to_remove.add((v, neighbor))\n            else:\n                closest_center = min(neighboring_sampled_centers, key=lightest_edge_weight.get)\n                closest_center_weight = lightest_edge_weight[closest_center]\n                closest_center_neighbor = lightest_edge_neighbor[closest_center]\n                edges_to_add.add((v, closest_center_neighbor))\n                new_clustering[v] = closest_center\n                for center, edge_weight in lightest_edge_weight.items():\n                    if edge_weight < closest_center_weight:\n                        neighbor = lightest_edge_neighbor[center]\n                        edges_to_add.add((v, neighbor))\n                for neighbor in residual_graph.adj[v]:\n                    neighbor_cluster = clustering[neighbor]\n                    neighbor_weight = lightest_edge_weight[neighbor_cluster]\n                    if neighbor_cluster == closest_center or neighbor_weight < closest_center_weight:\n                        edges_to_remove.add((v, neighbor))\n        if len(edges_to_add) > size_limit:\n            continue\n        i = i + 1\n        for u, v in edges_to_add:\n            _add_edge_to_spanner(H, residual_graph, u, v, weight)\n        residual_graph.remove_edges_from(edges_to_remove)\n        for node, center in clustering.items():\n            if center in sampled_centers:\n                new_clustering[node] = center\n        clustering = new_clustering\n        for u in residual_graph.nodes:\n            for v in list(residual_graph.adj[u]):\n                if clustering[u] == clustering[v]:\n                    residual_graph.remove_edge(u, v)\n        for v in list(residual_graph.nodes):\n            if v not in clustering:\n                residual_graph.remove_node(v)\n    for v in residual_graph.nodes:\n        lightest_edge_neighbor, _ = _lightest_edge_dicts(residual_graph, clustering, v)\n        for neighbor in lightest_edge_neighbor.values():\n            _add_edge_to_spanner(H, residual_graph, v, neighbor, weight)\n    return H"
 },
 {
  "docstring": "Setup residual graph as a copy of G with unique edges weights.\n\nThe node set of the residual graph corresponds to the set V' from\nthe Baswana-Sen paper and the edge set corresponds to the set E'\nfrom the paper.\n\nThis function associates distinct weights to the edges of the\nresidual graph (even for unweighted input graphs), as required by\nthe algorithm.\n\nParameters\n----------\nG : NetworkX graph\n    An undirected simple graph.\n\nweight : object\n    The edge attribute to use as distance.\n\nReturns\n-------\nNetworkX graph\n    The residual graph used for the Baswana-Sen algorithm.",
  "code": "def _setup_residual_graph(G, weight):\n    residual_graph = G.copy()\n    for u, v in G.edges():\n        if not weight:\n            residual_graph[u][v]['weight'] = (id(u), id(v))\n        else:\n            residual_graph[u][v]['weight'] = (G[u][v][weight], id(u), id(v))\n    return residual_graph"
 },
 {
  "docstring": "Find the lightest edge to each cluster.\n\nSearches for the minimum-weight edge to each cluster adjacent to\nthe given node.\n\nParameters\n----------\nresidual_graph : NetworkX graph\n    The residual graph used by the Baswana-Sen algorithm.\n\nclustering : dictionary\n    The current clustering of the nodes.\n\nnode : node\n    The node from which the search originates.\n\nReturns\n-------\nlightest_edge_neighbor, lightest_edge_weight : dictionary, dictionary\n    lightest_edge_neighbor is a dictionary that maps a center C to\n    a node v in the corresponding cluster such that the edge from\n    the given node to v is the lightest edge from the given node to\n    any node in cluster. lightest_edge_weight maps a center C to the\n    weight of the aforementioned edge.\n\n",
  "code": "def _lightest_edge_dicts(residual_graph, clustering, node):\n    lightest_edge_neighbor = {}\n    lightest_edge_weight = {}\n    for neighbor in residual_graph.adj[node]:\n        neighbor_center = clustering[neighbor]\n        weight = residual_graph[node][neighbor]['weight']\n        if neighbor_center not in lightest_edge_weight or weight < lightest_edge_weight[neighbor_center]:\n            lightest_edge_neighbor[neighbor_center] = neighbor\n            lightest_edge_weight[neighbor_center] = weight\n    return (lightest_edge_neighbor, lightest_edge_weight)"
 },
 {
  "docstring": "Add the edge {u, v} to the spanner H and take weight from\nthe residual graph.\n\nParameters\n----------\nH : NetworkX graph\n    The spanner under construction.\n\nresidual_graph : NetworkX graph\n    The residual graph used by the Baswana-Sen algorithm. The weight\n    for the edge is taken from this graph.\n\nu : node\n    One endpoint of the edge.\n\nv : node\n    The other endpoint of the edge.\n\nweight : object\n    The edge attribute to use as distance.",
  "code": "def _add_edge_to_spanner(H, residual_graph, u, v, weight):\n    H.add_edge(u, v)\n    if weight:\n        H[u][v][weight] = residual_graph[u][v]['weight'][0]"
 },
 {
  "docstring": "Returns the sum of the weights of the edge from `u` to `v` and\nthe edge from `v` to `u` in `G`.\n\n`weight` is the edge data key that represents the edge weight. If\nthe specified key is `None` or is not in the edge data for an edge,\nthat edge is assumed to have weight 1.\n\nPre-conditions: `u` and `v` must both be in `G`.",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef mutual_weight(G, u, v, weight=None):\n    try:\n        a_uv = G[u][v].get(weight, 1)\n    except KeyError:\n        a_uv = 0\n    try:\n        a_vu = G[v][u].get(weight, 1)\n    except KeyError:\n        a_vu = 0\n    return a_uv + a_vu"
 },
 {
  "docstring": "Returns normalized mutual weight of the edges from `u` to `v`\nwith respect to the mutual weights of the neighbors of `u` in `G`.\n\n`norm` specifies how the normalization factor is computed. It must\nbe a function that takes a single argument and returns a number.\nThe argument will be an iterable of mutual weights\nof pairs ``(u, w)``, where ``w`` ranges over each (in- and\nout-)neighbor of ``u``. Commons values for `normalization` are\n``sum`` and ``max``.\n\n`weight` can be ``None`` or a string, if None, all edge weights\nare considered equal. Otherwise holds the name of the edge\nattribute used as weight.",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef normalized_mutual_weight(G, u, v, norm=sum, weight=None):\n    scale = norm((mutual_weight(G, u, w, weight) for w in set(nx.all_neighbors(G, u))))\n    return 0 if scale == 0 else mutual_weight(G, u, v, weight) / scale"
 },
 {
  "docstring": "Returns the effective size of all nodes in the graph ``G``.\n\nThe *effective size* of a node's ego network is based on the concept\nof redundancy. A person's ego network has redundancy to the extent\nthat her contacts are connected to each other as well. The\nnonredundant part of a person's relationships is the effective\nsize of her ego network [1]_.  Formally, the effective size of a\nnode $u$, denoted $e(u)$, is defined by\n\n.. math::\n\n   e(u) = \\sum_{v \\in N(u) \\setminus \\{u\\}}\n   \\left(1 - \\sum_{w \\in N(v)} p_{uw} m_{vw}\\right)\n\nwhere $N(u)$ is the set of neighbors of $u$ and $p_{uw}$ is the\nnormalized mutual weight of the (directed or undirected) edges\njoining $u$ and $v$, for each vertex $u$ and $v$ [1]_. And $m_{vw}$\nis the mutual weight of $v$ and $w$ divided by $v$ highest mutual\nweight with any of its neighbors. The *mutual weight* of $u$ and $v$\nis the sum of the weights of edges joining them (edge weights are\nassumed to be one if the graph is unweighted).\n\nFor the case of unweighted and undirected graphs, Borgatti proposed\na simplified formula to compute effective size [2]_\n\n.. math::\n\n   e(u) = n - \\frac{2t}{n}\n\nwhere `t` is the number of ties in the ego network (not including\nties to ego) and `n` is the number of nodes (excluding ego).\n\nParameters\n----------\nG : NetworkX graph\n    The graph containing ``v``. Directed graphs are treated like\n    undirected graphs when computing neighbors of ``v``.\n\nnodes : container, optional\n    Container of nodes in the graph ``G`` to compute the effective size.\n    If None, the effective size of every node is computed.\n\nweight : None or string, optional\n  If None, all edge weights are considered equal.\n  Otherwise holds the name of the edge attribute used as weight.\n\nReturns\n-------\ndict\n    Dictionary with nodes as keys and the effective size of the node as values.\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef effective_size(G, nodes=None, weight=None):\n\n    def redundancy(G, u, v, weight=None):\n        nmw = normalized_mutual_weight\n        r = sum((nmw(G, u, w, weight=weight) * nmw(G, v, w, norm=max, weight=weight) for w in set(nx.all_neighbors(G, u))))\n        return 1 - r\n    effective_size = {}\n    if nodes is None:\n        nodes = G\n    if not G.is_directed() and weight is None:\n        for v in nodes:\n            if len(G[v]) == 0:\n                effective_size[v] = float('nan')\n                continue\n            E = nx.ego_graph(G, v, center=False, undirected=True)\n            effective_size[v] = len(E) - 2 * E.size() / len(E)\n    else:\n        for v in nodes:\n            if len(G[v]) == 0:\n                effective_size[v] = float('nan')\n                continue\n            effective_size[v] = sum((redundancy(G, v, u, weight) for u in set(nx.all_neighbors(G, v))))\n    return effective_size"
 },
 {
  "docstring": "Returns the constraint on all nodes in the graph ``G``.\n\nThe *constraint* is a measure of the extent to which a node *v* is\ninvested in those nodes that are themselves invested in the\nneighbors of *v*. Formally, the *constraint on v*, denoted `c(v)`,\nis defined by\n\n.. math::\n\n   c(v) = \\sum_{w \\in N(v) \\setminus \\{v\\}} \\ell(v, w)\n\nwhere $N(v)$ is the subset of the neighbors of `v` that are either\npredecessors or successors of `v` and $\\ell(v, w)$ is the local\nconstraint on `v` with respect to `w` [1]_. For the definition of local\nconstraint, see :func:`local_constraint`.\n\nParameters\n----------\nG : NetworkX graph\n    The graph containing ``v``. This can be either directed or undirected.\n\nnodes : container, optional\n    Container of nodes in the graph ``G`` to compute the constraint. If\n    None, the constraint of every node is computed.\n\nweight : None or string, optional\n  If None, all edge weights are considered equal.\n  Otherwise holds the name of the edge attribute used as weight.\n\nReturns\n-------\ndict\n    Dictionary with nodes as keys and the constraint on the node as values.\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef constraint(G, nodes=None, weight=None):\n    if nodes is None:\n        nodes = G\n    constraint = {}\n    for v in nodes:\n        if len(G[v]) == 0:\n            constraint[v] = float('nan')\n            continue\n        constraint[v] = sum((local_constraint(G, v, n, weight) for n in set(nx.all_neighbors(G, v))))\n    return constraint"
 },
 {
  "docstring": "Returns the local constraint on the node ``u`` with respect to\nthe node ``v`` in the graph ``G``.\n\nFormally, the *local constraint on u with respect to v*, denoted\n$\\ell(v)$, is defined by\n\n.. math::\n\n   \\ell(u, v) = \\left(p_{uv} + \\sum_{w \\in N(v)} p_{uw} p_{wv}\\right)^2,\n\nwhere $N(v)$ is the set of neighbors of $v$ and $p_{uv}$ is the\nnormalized mutual weight of the (directed or undirected) edges\njoining $u$ and $v$, for each vertex $u$ and $v$ [1]_. The *mutual\nweight* of $u$ and $v$ is the sum of the weights of edges joining\nthem (edge weights are assumed to be one if the graph is\nunweighted).\n\nParameters\n----------\nG : NetworkX graph\n    The graph containing ``u`` and ``v``. This can be either\n    directed or undirected.\n\nu : node\n    A node in the graph ``G``.\n\nv : node\n    A node in the graph ``G``.\n\nweight : None or string, optional\n  If None, all edge weights are considered equal.\n  Otherwise holds the name of the edge attribute used as weight.\n\nReturns\n-------\nfloat\n    The constraint of the node ``v`` in the graph ``G``.\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef local_constraint(G, u, v, weight=None):\n    nmw = normalized_mutual_weight\n    direct = nmw(G, u, v, weight=weight)\n    indirect = sum((nmw(G, u, w, weight=weight) * nmw(G, w, v, weight=weight) for w in set(nx.all_neighbors(G, u))))\n    return (direct + indirect) ** 2"
 },
 {
  "docstring": "Compresses neighborhoods around high-degree nodes\n\nReduces the number of edges to high-degree nodes by adding compressor nodes\nthat summarize multiple edges of the same type to high-degree nodes (nodes\nwith a degree greater than a given threshold).  Dedensification also has\nthe added benefit of reducing the number of edges around high-degree nodes.\nThe implementation currently supports graphs with a single edge type.\n\nParameters\n----------\nG: graph\n   A networkx graph\nthreshold: int\n   Minimum degree threshold of a node to be considered a high degree node.\n   The threshold must be greater than or equal to 2.\nprefix: str or None, optional (default: None)\n   An optional prefix for denoting compressor nodes\ncopy: bool, optional (default: True)\n   Indicates if dedensification should be done inplace\n\nReturns\n-------\ndedensified networkx graph : (graph, set)\n    2-tuple of the dedensified graph and set of compressor nodes\n\n",
  "code": "@nx._dispatch\ndef dedensify(G, threshold, prefix=None, copy=True):\n    if threshold < 2:\n        raise nx.NetworkXError('The degree threshold must be >= 2')\n    degrees = G.in_degree if G.is_directed() else G.degree\n    high_degree_nodes = {n for n, d in degrees if d > threshold}\n    low_degree_nodes = G.nodes() - high_degree_nodes\n    auxiliary = {}\n    for node in G:\n        high_degree_neighbors = frozenset(high_degree_nodes & set(G[node]))\n        if high_degree_neighbors:\n            if high_degree_neighbors in auxiliary:\n                auxiliary[high_degree_neighbors].add(node)\n            else:\n                auxiliary[high_degree_neighbors] = {node}\n    if copy:\n        G = G.copy()\n    compressor_nodes = set()\n    for index, (high_degree_nodes, low_degree_nodes) in enumerate(auxiliary.items()):\n        low_degree_node_count = len(low_degree_nodes)\n        high_degree_node_count = len(high_degree_nodes)\n        old_edges = high_degree_node_count * low_degree_node_count\n        new_edges = high_degree_node_count + low_degree_node_count\n        if old_edges <= new_edges:\n            continue\n        compression_node = ''.join((str(node) for node in high_degree_nodes))\n        if prefix:\n            compression_node = str(prefix) + compression_node\n        for node in low_degree_nodes:\n            for high_node in high_degree_nodes:\n                if G.has_edge(node, high_node):\n                    G.remove_edge(node, high_node)\n            G.add_edge(node, compression_node)\n        for node in high_degree_nodes:\n            G.add_edge(compression_node, node)\n        compressor_nodes.add(compression_node)\n    return (G, compressor_nodes)"
 },
 {
  "docstring": "Build the summary graph from the data structures produced in the SNAP aggregation algorithm\n\nUsed in the SNAP aggregation algorithm to build the output summary graph and supernode\nlookup dictionary.  This process uses the original graph and the data structures to\ncreate the supernodes with the correct node attributes, and the superedges with the correct\nedge attributes\n\nParameters\n----------\nG: networkx.Graph\n    the original graph to be summarized\ngroups: dict\n    A dictionary of unique group IDs and their corresponding node groups\nnode_attributes: iterable\n    An iterable of the node attributes considered in the summarization process\nedge_attributes: iterable\n    An iterable of the edge attributes considered in the summarization process\nneighbor_info: dict\n    A data structure indicating the number of edges a node has with the\n    groups in the current summarization of each edge type\nedge_types: dict\n    dictionary of edges in the graph and their corresponding attributes recognized\n    in the summarization\nprefix: string\n    The prefix to be added to all supernodes\nsupernode_attribute: str\n    The node attribute for recording the supernode groupings of nodes\nsuperedge_attribute: str\n    The edge attribute for recording the edge types represented by superedges\n\nReturns\n-------\nsummary graph: Networkx graph",
  "code": "def _snap_build_graph(G, groups, node_attributes, edge_attributes, neighbor_info, edge_types, prefix, supernode_attribute, superedge_attribute):\n    output = G.__class__()\n    node_label_lookup = {}\n    for index, group_id in enumerate(groups):\n        group_set = groups[group_id]\n        supernode = f'{prefix}{index}'\n        node_label_lookup[group_id] = supernode\n        supernode_attributes = {attr: G.nodes[next(iter(group_set))][attr] for attr in node_attributes}\n        supernode_attributes[supernode_attribute] = group_set\n        output.add_node(supernode, **supernode_attributes)\n    for group_id in groups:\n        group_set = groups[group_id]\n        source_supernode = node_label_lookup[group_id]\n        for other_group, group_edge_types in neighbor_info[next(iter(group_set))].items():\n            if group_edge_types:\n                target_supernode = node_label_lookup[other_group]\n                summary_graph_edge = (source_supernode, target_supernode)\n                edge_types = [dict(zip(edge_attributes, edge_type)) for edge_type in group_edge_types]\n                has_edge = output.has_edge(*summary_graph_edge)\n                if output.is_multigraph():\n                    if not has_edge:\n                        for edge_type in edge_types:\n                            output.add_edge(*summary_graph_edge, **edge_type)\n                    elif not output.is_directed():\n                        existing_edge_data = output.get_edge_data(*summary_graph_edge)\n                        for edge_type in edge_types:\n                            if edge_type not in existing_edge_data.values():\n                                output.add_edge(*summary_graph_edge, **edge_type)\n                else:\n                    superedge_attributes = {superedge_attribute: edge_types}\n                    output.add_edge(*summary_graph_edge, **superedge_attributes)\n    return output"
 },
 {
  "docstring": "Determines if a group is eligible to be split.\n\nA group is eligible to be split if all nodes in the group have edges of the same type(s)\nwith the same other groups.\n\nParameters\n----------\nG: graph\n    graph to be summarized\ngroups: dict\n    A dictionary of unique group IDs and their corresponding node groups\ngroup_lookup: dict\n    dictionary of nodes and their current corresponding group ID\nedge_types: dict\n    dictionary of edges in the graph and their corresponding attributes recognized\n    in the summarization\n\nReturns\n-------\ntuple: group ID to split, and neighbor-groups participation_counts data structure",
  "code": "def _snap_eligible_group(G, groups, group_lookup, edge_types):\n    neighbor_info = {node: {gid: Counter() for gid in groups} for node in group_lookup}\n    for group_id in groups:\n        current_group = groups[group_id]\n        for node in current_group:\n            neighbor_info[node] = {group_id: Counter() for group_id in groups}\n            edges = G.edges(node, keys=True) if G.is_multigraph() else G.edges(node)\n            for edge in edges:\n                neighbor = edge[1]\n                edge_type = edge_types[edge]\n                neighbor_group_id = group_lookup[neighbor]\n                neighbor_info[node][neighbor_group_id][edge_type] += 1\n        group_size = len(current_group)\n        for other_group_id in groups:\n            edge_counts = Counter()\n            for node in current_group:\n                edge_counts.update(neighbor_info[node][other_group_id].keys())\n            if not all((count == group_size for count in edge_counts.values())):\n                return (group_id, neighbor_info)\n    return (None, neighbor_info)"
 },
 {
  "docstring": "Splits a group based on edge types and updates the groups accordingly\n\nSplits the group with the given group_id based on the edge types\nof the nodes so that each new grouping will all have the same\nedges with other nodes.\n\nParameters\n----------\ngroups: dict\n    A dictionary of unique group IDs and their corresponding node groups\nneighbor_info: dict\n    A data structure indicating the number of edges a node has with the\n    groups in the current summarization of each edge type\nedge_types: dict\n    dictionary of edges in the graph and their corresponding attributes recognized\n    in the summarization\ngroup_lookup: dict\n    dictionary of nodes and their current corresponding group ID\ngroup_id: object\n    ID of group to be split\n\nReturns\n-------\ndict\n    The updated groups based on the split",
  "code": "def _snap_split(groups, neighbor_info, group_lookup, group_id):\n    new_group_mappings = defaultdict(set)\n    for node in groups[group_id]:\n        signature = tuple((frozenset(edge_types) for edge_types in neighbor_info[node].values()))\n        new_group_mappings[signature].add(node)\n    new_groups = sorted(new_group_mappings.values(), key=len)\n    for new_group in new_groups[:-1]:\n        new_group_id = len(groups)\n        groups[new_group_id] = new_group\n        groups[group_id] -= new_group\n        for node in new_group:\n            group_lookup[node] = new_group_id\n    return groups"
 },
 {
  "docstring": "Creates a summary graph based on attributes and connectivity.\n\nThis function uses the Summarization by Grouping Nodes on Attributes\nand Pairwise edges (SNAP) algorithm for summarizing a given\ngraph by grouping nodes by node attributes and their edge attributes\ninto supernodes in a summary graph.  This name SNAP should not be\nconfused with the Stanford Network Analysis Project (SNAP).\n\nHere is a high-level view of how this algorithm works:\n\n1) Group nodes by node attribute values.\n\n2) Iteratively split groups until all nodes in each group have edges\nto nodes in the same groups. That is, until all the groups are homogeneous\nin their member nodes' edges to other groups.  For example,\nif all the nodes in group A only have edge to nodes in group B, then the\ngroup is homogeneous and does not need to be split. If all nodes in group B\nhave edges with nodes in groups {A, C}, but some also have edges with other\nnodes in B, then group B is not homogeneous and needs to be split into\ngroups have edges with {A, C} and a group of nodes having\nedges with {A, B, C}.  This way, viewers of the summary graph can\nassume that all nodes in the group have the exact same node attributes and\nthe exact same edges.\n\n3) Build the output summary graph, where the groups are represented by\nsuper-nodes. Edges represent the edges shared between all the nodes in each\nrespective groups.\n\nA SNAP summary graph can be used to visualize graphs that are too large to display\nor visually analyze, or to efficiently identify sets of similar nodes with similar connectivity\npatterns to other sets of similar nodes based on specified node and/or edge attributes in a graph.\n\nParameters\n----------\nG: graph\n    Networkx Graph to be summarized\nnode_attributes: iterable, required\n    An iterable of the node attributes used to group nodes in the summarization process. Nodes\n    with the same values for these attributes will be grouped together in the summary graph.\nedge_attributes: iterable, optional\n    An iterable of the edge attributes considered in the summarization process.  If provided, unique\n    combinations of the attribute values found in the graph are used to\n    determine the edge types in the graph.  If not provided, all edges\n    are considered to be of the same type.\nprefix: str\n    The prefix used to denote supernodes in the summary graph. Defaults to 'Supernode-'.\nsupernode_attribute: str\n    The node attribute for recording the supernode groupings of nodes. Defaults to 'group'.\nsuperedge_attribute: str\n    The edge attribute for recording the edge types of multiple edges. Defaults to 'types'.\n\nReturns\n-------\nnetworkx.Graph: summary graph\n\nExamples\n--------\nSNAP aggregation takes a graph and summarizes it in the context of user-provided\nnode and edge attributes such that a viewer can more easily extract and\nanalyze the information represented by the graph\n\n>>> nodes = {\n...     \"A\": dict(color=\"Red\"),\n...     \"B\": dict(color=\"Red\"),\n...     \"C\": dict(color=\"Red\"),\n...     \"D\": dict(color=\"Red\"),\n...     \"E\": dict(color=\"Blue\"),\n...     \"F\": dict(color=\"Blue\"),\n... }\n>>> edges = [\n...     (\"A\", \"E\", \"Strong\"),\n...     (\"B\", \"F\", \"Strong\"),\n...     (\"C\", \"E\", \"Weak\"),\n...     (\"D\", \"F\", \"Weak\"),\n... ]\n>>> G = nx.Graph()\n>>> for node in nodes:\n...     attributes = nodes[node]\n...     G.add_node(node, **attributes)\n...\n>>> for source, target, type in edges:\n...     G.add_edge(source, target, type=type)\n...\n>>> node_attributes = ('color', )\n>>> edge_attributes = ('type', )\n>>> summary_graph = nx.snap_aggregation(G, node_attributes=node_attributes, edge_attributes=edge_attributes)\n\n",
  "code": "@nx._dispatch(node_attrs='[node_attributes]', edge_attrs='[edge_attributes]')\ndef snap_aggregation(G, node_attributes, edge_attributes=(), prefix='Supernode-', supernode_attribute='group', superedge_attribute='types'):\n    edge_types = {edge: tuple((attrs.get(attr) for attr in edge_attributes)) for edge, attrs in G.edges.items()}\n    if not G.is_directed():\n        if G.is_multigraph():\n            edges = [((v, u, k), etype) for (u, v, k), etype in edge_types.items()]\n        else:\n            edges = [((v, u), etype) for (u, v), etype in edge_types.items()]\n        edge_types.update(edges)\n    group_lookup = {node: tuple((attrs[attr] for attr in node_attributes)) for node, attrs in G.nodes.items()}\n    groups = defaultdict(set)\n    for node, node_type in group_lookup.items():\n        groups[node_type].add(node)\n    eligible_group_id, neighbor_info = _snap_eligible_group(G, groups, group_lookup, edge_types)\n    while eligible_group_id:\n        groups = _snap_split(groups, neighbor_info, group_lookup, eligible_group_id)\n        eligible_group_id, neighbor_info = _snap_eligible_group(G, groups, group_lookup, edge_types)\n    return _snap_build_graph(G, groups, node_attributes, edge_attributes, neighbor_info, edge_types, prefix, supernode_attribute, superedge_attribute)"
 },
 {
  "docstring": "Swap three edges in a directed graph while keeping the node degrees fixed.\n\nA directed edge swap swaps three edges such that a -> b -> c -> d becomes\na -> c -> b -> d. This pattern of swapping allows all possible states with the\nsame in- and out-degree distribution in a directed graph to be reached.\n\nIf the swap would create parallel edges (e.g. if a -> c already existed in the\nprevious example), another attempt is made to find a suitable trio of edges.\n\nParameters\n----------\nG : DiGraph\n   A directed graph\n\nnswap : integer (optional, default=1)\n   Number of three-edge (directed) swaps to perform\n\nmax_tries : integer (optional, default=100)\n   Maximum number of attempts to swap edges\n\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\nReturns\n-------\nG : DiGraph\n   The graph after the edges are swapped.\n\nRaises\n------\nNetworkXError\n    If `G` is not directed, or\n    If nswap > max_tries, or\n    If there are fewer than 4 nodes or 3 edges in `G`.\nNetworkXAlgorithmError\n    If the number of swap attempts exceeds `max_tries` before `nswap` swaps are made\n\n",
  "code": "@nx.utils.not_implemented_for('undirected')\n@py_random_state(3)\n@nx._dispatch\ndef directed_edge_swap(G, *, nswap=1, max_tries=100, seed=None):\n    if nswap > max_tries:\n        raise nx.NetworkXError('Number of swaps > number of tries allowed.')\n    if len(G) < 4:\n        raise nx.NetworkXError('DiGraph has fewer than four nodes.')\n    if len(G.edges) < 3:\n        raise nx.NetworkXError('DiGraph has fewer than 3 edges')\n    tries = 0\n    swapcount = 0\n    keys, degrees = zip(*G.degree())\n    cdf = nx.utils.cumulative_distribution(degrees)\n    discrete_sequence = nx.utils.discrete_sequence\n    while swapcount < nswap:\n        start_index = discrete_sequence(1, cdistribution=cdf, seed=seed)[0]\n        start = keys[start_index]\n        tries += 1\n        if tries > max_tries:\n            msg = f'Maximum number of swap attempts ({tries}) exceeded before desired swaps achieved ({nswap}).'\n            raise nx.NetworkXAlgorithmError(msg)\n        if G.out_degree(start) == 0:\n            continue\n        second = seed.choice(list(G.succ[start]))\n        if start == second:\n            continue\n        if G.out_degree(second) == 0:\n            continue\n        third = seed.choice(list(G.succ[second]))\n        if second == third:\n            continue\n        if G.out_degree(third) == 0:\n            continue\n        fourth = seed.choice(list(G.succ[third]))\n        if third == fourth:\n            continue\n        if third not in G.succ[start] and fourth not in G.succ[second] and (second not in G.succ[third]):\n            G.add_edge(start, third)\n            G.add_edge(third, second)\n            G.add_edge(second, fourth)\n            G.remove_edge(start, second)\n            G.remove_edge(second, third)\n            G.remove_edge(third, fourth)\n            swapcount += 1\n    return G"
 },
 {
  "docstring": "Swap two edges in the graph while keeping the node degrees fixed.\n\nA double-edge swap removes two randomly chosen edges u-v and x-y\nand creates the new edges u-x and v-y::\n\n u--v            u  v\n        becomes  |  |\n x--y            x  y\n\nIf either the edge u-x or v-y already exist no swap is performed\nand another attempt is made to find a suitable edge pair.\n\nParameters\n----------\nG : graph\n   An undirected graph\n\nnswap : integer (optional, default=1)\n   Number of double-edge swaps to perform\n\nmax_tries : integer (optional)\n   Maximum number of attempts to swap edges\n\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\nReturns\n-------\nG : graph\n   The graph after double edge swaps.\n\nRaises\n------\nNetworkXError\n    If `G` is directed, or\n    If `nswap` > `max_tries`, or\n    If there are fewer than 4 nodes or 2 edges in `G`.\nNetworkXAlgorithmError\n    If the number of swap attempts exceeds `max_tries` before `nswap` swaps are made\n\n",
  "code": "@py_random_state(3)\n@nx._dispatch\ndef double_edge_swap(G, nswap=1, max_tries=100, seed=None):\n    if G.is_directed():\n        raise nx.NetworkXError('double_edge_swap() not defined for directed graphs. Use directed_edge_swap instead.')\n    if nswap > max_tries:\n        raise nx.NetworkXError('Number of swaps > number of tries allowed.')\n    if len(G) < 4:\n        raise nx.NetworkXError('Graph has fewer than four nodes.')\n    if len(G.edges) < 2:\n        raise nx.NetworkXError('Graph has fewer than 2 edges')\n    n = 0\n    swapcount = 0\n    keys, degrees = zip(*G.degree())\n    cdf = nx.utils.cumulative_distribution(degrees)\n    discrete_sequence = nx.utils.discrete_sequence\n    while swapcount < nswap:\n        ui, xi = discrete_sequence(2, cdistribution=cdf, seed=seed)\n        if ui == xi:\n            continue\n        u = keys[ui]\n        x = keys[xi]\n        v = seed.choice(list(G[u]))\n        y = seed.choice(list(G[x]))\n        if v == y:\n            continue\n        if x not in G[u] and y not in G[v]:\n            G.add_edge(u, x)\n            G.add_edge(v, y)\n            G.remove_edge(u, v)\n            G.remove_edge(x, y)\n            swapcount += 1\n        if n >= max_tries:\n            e = f'Maximum number of swap attempts ({n}) exceeded before desired swaps achieved ({nswap}).'\n            raise nx.NetworkXAlgorithmError(e)\n        n += 1\n    return G"
 },
 {
  "docstring": "Attempts the specified number of double-edge swaps in the graph `G`.\n\nA double-edge swap removes two randomly chosen edges `(u, v)` and `(x,\ny)` and creates the new edges `(u, x)` and `(v, y)`::\n\n u--v            u  v\n        becomes  |  |\n x--y            x  y\n\nIf either `(u, x)` or `(v, y)` already exist, then no swap is performed\nso the actual number of swapped edges is always *at most* `nswap`.\n\nParameters\n----------\nG : graph\n   An undirected graph\n\nnswap : integer (optional, default=1)\n   Number of double-edge swaps to perform\n\n_window_threshold : integer\n\n   The window size below which connectedness of the graph will be checked\n   after each swap.\n\n   The \"window\" in this function is a dynamically updated integer that\n   represents the number of swap attempts to make before checking if the\n   graph remains connected. It is an optimization used to decrease the\n   running time of the algorithm in exchange for increased complexity of\n   implementation.\n\n   If the window size is below this threshold, then the algorithm checks\n   after each swap if the graph remains connected by checking if there is a\n   path joining the two nodes whose edge was just removed. If the window\n   size is above this threshold, then the algorithm performs do all the\n   swaps in the window and only then check if the graph is still connected.\n\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\nReturns\n-------\nint\n   The number of successful swaps\n\nRaises\n------\n\nNetworkXError\n\n   If the input graph is not connected, or if the graph has fewer than four\n   nodes.\n\n",
  "code": "@py_random_state(3)\n@nx._dispatch\ndef connected_double_edge_swap(G, nswap=1, _window_threshold=3, seed=None):\n    if not nx.is_connected(G):\n        raise nx.NetworkXError('Graph not connected')\n    if len(G) < 4:\n        raise nx.NetworkXError('Graph has fewer than four nodes.')\n    n = 0\n    swapcount = 0\n    deg = G.degree()\n    dk = [n for n, d in G.degree()]\n    cdf = nx.utils.cumulative_distribution([d for n, d in G.degree()])\n    discrete_sequence = nx.utils.discrete_sequence\n    window = 1\n    while n < nswap:\n        wcount = 0\n        swapped = []\n        if window < _window_threshold:\n            fail = False\n            while wcount < window and n < nswap:\n                ui, xi = discrete_sequence(2, cdistribution=cdf, seed=seed)\n                if ui == xi:\n                    continue\n                u = dk[ui]\n                x = dk[xi]\n                v = seed.choice(list(G.neighbors(u)))\n                y = seed.choice(list(G.neighbors(x)))\n                if v == y:\n                    continue\n                if x not in G[u] and y not in G[v]:\n                    G.remove_edge(u, v)\n                    G.remove_edge(x, y)\n                    G.add_edge(u, x)\n                    G.add_edge(v, y)\n                    swapped.append((u, v, x, y))\n                    swapcount += 1\n                n += 1\n                if nx.has_path(G, u, v):\n                    wcount += 1\n                else:\n                    G.add_edge(u, v)\n                    G.add_edge(x, y)\n                    G.remove_edge(u, x)\n                    G.remove_edge(v, y)\n                    swapcount -= 1\n                    fail = True\n            if fail:\n                window = math.ceil(window / 2)\n            else:\n                window += 1\n        else:\n            while wcount < window and n < nswap:\n                ui, xi = discrete_sequence(2, cdistribution=cdf, seed=seed)\n                if ui == xi:\n                    continue\n                u = dk[ui]\n                x = dk[xi]\n                v = seed.choice(list(G.neighbors(u)))\n                y = seed.choice(list(G.neighbors(x)))\n                if v == y:\n                    continue\n                if x not in G[u] and y not in G[v]:\n                    G.remove_edge(u, v)\n                    G.remove_edge(x, y)\n                    G.add_edge(u, x)\n                    G.add_edge(v, y)\n                    swapped.append((u, v, x, y))\n                    swapcount += 1\n                n += 1\n                wcount += 1\n            if nx.is_connected(G):\n                window += 1\n            else:\n                while swapped:\n                    u, v, x, y = swapped.pop()\n                    G.add_edge(u, v)\n                    G.add_edge(x, y)\n                    G.remove_edge(u, x)\n                    G.remove_edge(v, y)\n                    swapcount -= 1\n                window = math.ceil(window / 2)\n    return swapcount"
 },
 {
  "docstring": "Returns `True` if `G` is a threshold graph.\n\nParameters\n----------\nG : NetworkX graph instance\n    An instance of `Graph`, `DiGraph`, `MultiGraph` or `MultiDiGraph`\n\nReturns\n-------\nbool\n    `True` if `G` is a threshold graph, `False` otherwise.\n\nExamples\n--------\n>>> from networkx.algorithms.threshold import is_threshold_graph\n>>> G = nx.path_graph(3)\n>>> is_threshold_graph(G)\nTrue\n>>> G = nx.barbell_graph(3, 3)\n>>> is_threshold_graph(G)\nFalse\n\n",
  "code": "@nx._dispatch\ndef is_threshold_graph(G):\n    return is_threshold_sequence([d for n, d in G.degree()])"
 },
 {
  "docstring": "Returns True if the sequence is a threshold degree sequence.\n\nUses the property that a threshold graph must be constructed by\nadding either dominating or isolated nodes. Thus, it can be\ndeconstructed iteratively by removing a node of degree zero or a\nnode that connects to the remaining nodes.  If this deconstruction\nfails then the sequence is not a threshold sequence.",
  "code": "def is_threshold_sequence(degree_sequence):\n    ds = degree_sequence[:]\n    ds.sort()\n    while ds:\n        if ds[0] == 0:\n            ds.pop(0)\n            continue\n        if ds[-1] != len(ds) - 1:\n            return False\n        ds.pop()\n        ds = [d - 1 for d in ds]\n    return True"
 },
 {
  "docstring": "Determines the creation sequence for the given threshold degree sequence.\n\nThe creation sequence is a list of single characters 'd'\nor 'i': 'd' for dominating or 'i' for isolated vertices.\nDominating vertices are connected to all vertices present when it\nis added.  The first node added is by convention 'd'.\nThis list can be converted to a string if desired using \"\".join(cs)\n\nIf with_labels==True:\nReturns a list of 2-tuples containing the vertex number\nand a character 'd' or 'i' which describes the type of vertex.\n\nIf compact==True:\nReturns the creation sequence in a compact form that is the number\nof 'i's and 'd's alternating.\nExamples:\n[1,2,2,3] represents d,i,i,d,d,i,i,i\n[3,1,2] represents d,d,d,i,d,d\n\nNotice that the first number is the first vertex to be used for\nconstruction and so is always 'd'.\n\nwith_labels and compact cannot both be True.\n\nReturns None if the sequence is not a threshold sequence",
  "code": "def creation_sequence(degree_sequence, with_labels=False, compact=False):\n    if with_labels and compact:\n        raise ValueError('compact sequences cannot be labeled')\n    if isinstance(degree_sequence, dict):\n        ds = [[degree, label] for label, degree in degree_sequence.items()]\n    else:\n        ds = [[d, i] for i, d in enumerate(degree_sequence)]\n    ds.sort()\n    cs = []\n    while ds:\n        if ds[0][0] == 0:\n            d, v = ds.pop(0)\n            if len(ds) > 0:\n                cs.insert(0, (v, 'i'))\n            else:\n                cs.insert(0, (v, 'd'))\n            continue\n        if ds[-1][0] != len(ds) - 1:\n            return None\n        d, v = ds.pop()\n        cs.insert(0, (v, 'd'))\n        ds = [[d[0] - 1, d[1]] for d in ds]\n    if with_labels:\n        return cs\n    if compact:\n        return make_compact(cs)\n    return [v[1] for v in cs]"
 },
 {
  "docstring": "Returns the creation sequence in a compact form\nthat is the number of 'i's and 'd's alternating.\n\nExamples\n--------\n>>> from networkx.algorithms.threshold import make_compact\n>>> make_compact([\"d\", \"i\", \"i\", \"d\", \"d\", \"i\", \"i\", \"i\"])\n[1, 2, 2, 3]\n>>> make_compact([\"d\", \"d\", \"d\", \"i\", \"d\", \"d\"])\n[3, 1, 2]\n\nNotice that the first number is the first vertex\nto be used for construction and so is always 'd'.\n\nLabeled creation sequences lose their labels in the\ncompact representation.\n\n>>> make_compact([3, 1, 2])\n[3, 1, 2]",
  "code": "def make_compact(creation_sequence):\n    first = creation_sequence[0]\n    if isinstance(first, str):\n        cs = creation_sequence[:]\n    elif isinstance(first, tuple):\n        cs = [s[1] for s in creation_sequence]\n    elif isinstance(first, int):\n        return creation_sequence\n    else:\n        raise TypeError('Not a valid creation sequence type')\n    ccs = []\n    count = 1\n    for i in range(1, len(cs)):\n        if cs[i] == cs[i - 1]:\n            count += 1\n        else:\n            ccs.append(count)\n            count = 1\n    ccs.append(count)\n    return ccs"
 },
 {
  "docstring": "Converts a compact creation sequence for a threshold\ngraph to a standard creation sequence (unlabeled).\nIf the creation_sequence is already standard, return it.\nSee creation_sequence.",
  "code": "def uncompact(creation_sequence):\n    first = creation_sequence[0]\n    if isinstance(first, str):\n        return creation_sequence\n    elif isinstance(first, tuple):\n        return creation_sequence\n    elif isinstance(first, int):\n        ccscopy = creation_sequence[:]\n    else:\n        raise TypeError('Not a valid creation sequence type')\n    cs = []\n    while ccscopy:\n        cs.extend(ccscopy.pop(0) * ['d'])\n        if ccscopy:\n            cs.extend(ccscopy.pop(0) * ['i'])\n    return cs"
 },
 {
  "docstring": "Returns a list of node weights which create the threshold\ngraph designated by the creation sequence.  The weights\nare scaled so that the threshold is 1.0.  The order of the\nnodes is the same as that in the creation sequence.",
  "code": "def creation_sequence_to_weights(creation_sequence):\n    first = creation_sequence[0]\n    if isinstance(first, str):\n        if isinstance(creation_sequence, list):\n            wseq = creation_sequence[:]\n        else:\n            wseq = list(creation_sequence)\n    elif isinstance(first, tuple):\n        wseq = [v[1] for v in creation_sequence]\n    elif isinstance(first, int):\n        wseq = uncompact(creation_sequence)\n    else:\n        raise TypeError('Not a valid creation sequence type')\n    wseq.reverse()\n    w = 0\n    prev = 'i'\n    for j, s in enumerate(wseq):\n        if s == 'i':\n            wseq[j] = w\n            prev = s\n        elif prev == 'i':\n            prev = s\n            w += 1\n    wseq.reverse()\n    for j, s in enumerate(wseq):\n        if s == 'd':\n            wseq[j] = w\n            prev = s\n        elif prev == 'd':\n            prev = s\n            w += 1\n    if prev == 'd':\n        w += 1\n    wscale = 1 / w\n    return [ww * wscale for ww in wseq]"
 },
 {
  "docstring": "Returns a creation sequence for a threshold graph\ndetermined by the weights and threshold given as input.\nIf the sum of two node weights is greater than the\nthreshold value, an edge is created between these nodes.\n\nThe creation sequence is a list of single characters 'd'\nor 'i': 'd' for dominating or 'i' for isolated vertices.\nDominating vertices are connected to all vertices present\nwhen it is added.  The first node added is by convention 'd'.\n\nIf with_labels==True:\nReturns a list of 2-tuples containing the vertex number\nand a character 'd' or 'i' which describes the type of vertex.\n\nIf compact==True:\nReturns the creation sequence in a compact form that is the number\nof 'i's and 'd's alternating.\nExamples:\n[1,2,2,3] represents d,i,i,d,d,i,i,i\n[3,1,2] represents d,d,d,i,d,d\n\nNotice that the first number is the first vertex to be used for\nconstruction and so is always 'd'.\n\nwith_labels and compact cannot both be True.",
  "code": "def weights_to_creation_sequence(weights, threshold=1, with_labels=False, compact=False):\n    if with_labels and compact:\n        raise ValueError('compact sequences cannot be labeled')\n    if isinstance(weights, dict):\n        wseq = [[w, label] for label, w in weights.items()]\n    else:\n        wseq = [[w, i] for i, w in enumerate(weights)]\n    wseq.sort()\n    cs = []\n    cutoff = threshold - wseq[-1][0]\n    while wseq:\n        if wseq[0][0] < cutoff:\n            w, label = wseq.pop(0)\n            cs.append((label, 'i'))\n        else:\n            w, label = wseq.pop()\n            cs.append((label, 'd'))\n            cutoff = threshold - wseq[-1][0]\n        if len(wseq) == 1:\n            w, label = wseq.pop()\n            cs.append((label, 'd'))\n    cs.reverse()\n    if with_labels:\n        return cs\n    if compact:\n        return make_compact(cs)\n    return [v[1] for v in cs]"
 },
 {
  "docstring": "Create a threshold graph from the creation sequence or compact\ncreation_sequence.\n\nThe input sequence can be a\n\ncreation sequence (e.g. ['d','i','d','d','d','i'])\nlabeled creation sequence (e.g. [(0,'d'),(2,'d'),(1,'i')])\ncompact creation sequence (e.g. [2,1,1,2,0])\n\nUse cs=creation_sequence(degree_sequence,labeled=True)\nto convert a degree sequence to a creation sequence.\n\nReturns None if the sequence is not valid",
  "code": "@nx._dispatch(graphs=None)\ndef threshold_graph(creation_sequence, create_using=None):\n    first = creation_sequence[0]\n    if isinstance(first, str):\n        ci = list(enumerate(creation_sequence))\n    elif isinstance(first, tuple):\n        ci = creation_sequence[:]\n    elif isinstance(first, int):\n        cs = uncompact(creation_sequence)\n        ci = list(enumerate(cs))\n    else:\n        print('not a valid creation sequence type')\n        return None\n    G = nx.empty_graph(0, create_using)\n    if G.is_directed():\n        raise nx.NetworkXError('Directed Graph not supported')\n    G.name = 'Threshold Graph'\n    while ci:\n        v, node_type = ci.pop(0)\n        if node_type == 'd':\n            for u in list(G):\n                G.add_edge(v, u)\n        G.add_node(v)\n    return G"
 },
 {
  "docstring": "Returns False if there aren't any alternating 4 cycles.\nOtherwise returns the cycle as [a,b,c,d] where (a,b)\nand (c,d) are edges and (a,c) and (b,d) are not.",
  "code": "@nx._dispatch\ndef find_alternating_4_cycle(G):\n    for u, v in G.edges():\n        for w in G.nodes():\n            if not G.has_edge(u, w) and u != w:\n                for x in G.neighbors(w):\n                    if not G.has_edge(v, x) and v != x:\n                        return [u, v, w, x]\n    return False"
 },
 {
  "docstring": "Returns a threshold subgraph that is close to largest in `G`.\n\nThe threshold graph will contain the largest degree node in G.\n\nParameters\n----------\nG : NetworkX graph instance\n    An instance of `Graph`, or `MultiDiGraph`\ncreate_using : NetworkX graph class or `None` (default), optional\n    Type of graph to use when constructing the threshold graph.\n    If `None`, infer the appropriate graph type from the input.\n\nReturns\n-------\ngraph :\n    A graph instance representing the threshold graph\n\nExamples\n--------\n>>> from networkx.algorithms.threshold import find_threshold_graph\n>>> G = nx.barbell_graph(3, 3)\n>>> T = find_threshold_graph(G)\n>>> T.nodes # may vary\nNodeView((7, 8, 5, 6))\n\n",
  "code": "@nx._dispatch\ndef find_threshold_graph(G, create_using=None):\n    return threshold_graph(find_creation_sequence(G), create_using)"
 },
 {
  "docstring": "Find a threshold subgraph that is close to largest in G.\nReturns the labeled creation sequence of that threshold graph.",
  "code": "@nx._dispatch\ndef find_creation_sequence(G):\n    cs = []\n    H = G\n    while H.order() > 0:\n        dsdict = dict(H.degree())\n        ds = [(d, v) for v, d in dsdict.items()]\n        ds.sort()\n        if ds[-1][0] == 0:\n            cs.extend(zip(dsdict, ['i'] * (len(ds) - 1) + ['d']))\n            break\n        while ds[0][0] == 0:\n            d, iso = ds.pop(0)\n            cs.append((iso, 'i'))\n        d, bigv = ds.pop()\n        cs.append((bigv, 'd'))\n        H = H.subgraph(H.neighbors(bigv))\n    cs.reverse()\n    return cs"
 },
 {
  "docstring": "Compute number of triangles in the threshold graph with the\ngiven creation sequence.",
  "code": "def triangles(creation_sequence):\n    cs = creation_sequence\n    dr = cs.count('d')\n    ntri = dr * (dr - 1) * (dr - 2) / 6\n    for i, typ in enumerate(cs):\n        if typ == 'i':\n            ntri += dr * (dr - 1) / 2\n        else:\n            dr -= 1\n    return ntri"
 },
 {
  "docstring": "Return triangle sequence for the given threshold graph creation sequence.",
  "code": "def triangle_sequence(creation_sequence):\n    cs = creation_sequence\n    seq = []\n    dr = cs.count('d')\n    dcur = (dr - 1) * (dr - 2) // 2\n    irun = 0\n    drun = 0\n    for i, sym in enumerate(cs):\n        if sym == 'd':\n            drun += 1\n            tri = dcur + (dr - 1) * irun\n        else:\n            if prevsym == 'd':\n                dcur += (dr - 1) * irun\n                irun = 0\n                dr -= drun\n                drun = 0\n            irun += 1\n            tri = dr * (dr - 1) // 2\n        seq.append(tri)\n        prevsym = sym\n    return seq"
 },
 {
  "docstring": "Return cluster sequence for the given threshold graph creation sequence.",
  "code": "def cluster_sequence(creation_sequence):\n    triseq = triangle_sequence(creation_sequence)\n    degseq = degree_sequence(creation_sequence)\n    cseq = []\n    for i, deg in enumerate(degseq):\n        tri = triseq[i]\n        if deg <= 1:\n            cseq.append(0)\n            continue\n        max_size = deg * (deg - 1) // 2\n        cseq.append(tri / max_size)\n    return cseq"
 },
 {
  "docstring": "Return degree sequence for the threshold graph with the given\ncreation sequence",
  "code": "def degree_sequence(creation_sequence):\n    cs = creation_sequence\n    seq = []\n    rd = cs.count('d')\n    for i, sym in enumerate(cs):\n        if sym == 'd':\n            rd -= 1\n            seq.append(rd + i)\n        else:\n            seq.append(rd)\n    return seq"
 },
 {
  "docstring": "Return the density of the graph with this creation_sequence.\nThe density is the fraction of possible edges present.",
  "code": "def density(creation_sequence):\n    N = len(creation_sequence)\n    two_size = sum(degree_sequence(creation_sequence))\n    two_possible = N * (N - 1)\n    den = two_size / two_possible\n    return den"
 },
 {
  "docstring": "Return the degree-degree correlation over all edges.",
  "code": "def degree_correlation(creation_sequence):\n    cs = creation_sequence\n    s1 = 0\n    s2 = 0\n    s3 = 0\n    m = 0\n    rd = cs.count('d')\n    rdi = [i for i, sym in enumerate(cs) if sym == 'd']\n    ds = degree_sequence(cs)\n    for i, sym in enumerate(cs):\n        if sym == 'd':\n            if i != rdi[0]:\n                print('Logic error in degree_correlation', i, rdi)\n                raise ValueError\n            rdi.pop(0)\n        degi = ds[i]\n        for dj in rdi:\n            degj = ds[dj]\n            s1 += degj * degi\n            s2 += degi ** 2 + degj ** 2\n            s3 += degi + degj\n            m += 1\n    denom = 2 * m * s2 - s3 * s3\n    numer = 4 * m * s1 - s3 * s3\n    if denom == 0:\n        if numer == 0:\n            return 1\n        raise ValueError(f'Zero Denominator but Numerator is {numer}')\n    return numer / denom"
 },
 {
  "docstring": "Find the shortest path between u and v in a\nthreshold graph G with the given creation_sequence.\n\nFor an unlabeled creation_sequence, the vertices\nu and v must be integers in (0,len(sequence)) referring\nto the position of the desired vertices in the sequence.\n\nFor a labeled creation_sequence, u and v are labels of vertices.\n\nUse cs=creation_sequence(degree_sequence,with_labels=True)\nto convert a degree sequence to a creation sequence.\n\nReturns a list of vertices from u to v.\nExample: if they are neighbors, it returns [u,v]",
  "code": "def shortest_path(creation_sequence, u, v):\n    first = creation_sequence[0]\n    if isinstance(first, str):\n        cs = [(i, creation_sequence[i]) for i in range(len(creation_sequence))]\n    elif isinstance(first, tuple):\n        cs = creation_sequence[:]\n    elif isinstance(first, int):\n        ci = uncompact(creation_sequence)\n        cs = [(i, ci[i]) for i in range(len(ci))]\n    else:\n        raise TypeError('Not a valid creation sequence type')\n    verts = [s[0] for s in cs]\n    if v not in verts:\n        raise ValueError(f'Vertex {v} not in graph from creation_sequence')\n    if u not in verts:\n        raise ValueError(f'Vertex {u} not in graph from creation_sequence')\n    if u == v:\n        return [u]\n    uindex = verts.index(u)\n    vindex = verts.index(v)\n    bigind = max(uindex, vindex)\n    if cs[bigind][1] == 'd':\n        return [u, v]\n    cs = cs[bigind:]\n    while cs:\n        vert = cs.pop()\n        if vert[1] == 'd':\n            return [u, vert[0], v]\n    return -1"
 },
 {
  "docstring": "Return the shortest path length from indicated node to\nevery other node for the threshold graph with the given\ncreation sequence.\nNode is indicated by index i in creation_sequence unless\ncreation_sequence is labeled in which case, i is taken to\nbe the label of the node.\n\nPaths lengths in threshold graphs are at most 2.\nLength to unreachable nodes is set to -1.",
  "code": "def shortest_path_length(creation_sequence, i):\n    first = creation_sequence[0]\n    if isinstance(first, str):\n        if isinstance(creation_sequence, list):\n            cs = creation_sequence[:]\n        else:\n            cs = list(creation_sequence)\n    elif isinstance(first, tuple):\n        cs = [v[1] for v in creation_sequence]\n        i = [v[0] for v in creation_sequence].index(i)\n    elif isinstance(first, int):\n        cs = uncompact(creation_sequence)\n    else:\n        raise TypeError('Not a valid creation sequence type')\n    N = len(cs)\n    spl = [2] * N\n    spl[i] = 0\n    for j in range(i + 1, N):\n        if cs[j] == 'd':\n            spl[j] = 1\n    if cs[i] == 'd':\n        for j in range(i):\n            spl[j] = 1\n    for j in range(N - 1, 0, -1):\n        if cs[j] == 'd':\n            break\n        spl[j] = -1\n    return spl"
 },
 {
  "docstring": "Return betweenness for the threshold graph with the given creation\nsequence.  The result is unscaled.  To scale the values\nto the interval [0,1] divide by (n-1)*(n-2).",
  "code": "def betweenness_sequence(creation_sequence, normalized=True):\n    cs = creation_sequence\n    seq = []\n    lastchar = 'd'\n    dr = float(cs.count('d'))\n    irun = 0\n    drun = 0\n    dlast = 0.0\n    for i, c in enumerate(cs):\n        if c == 'd':\n            b = dlast + (irun - 1) * irun / dr + 2 * irun * (i - drun - irun) / dr\n            drun += 1\n        else:\n            if lastchar == 'd':\n                dlast = b\n                dr -= drun\n                drun = 0\n                irun = 0\n            b = 0\n            irun += 1\n        seq.append(float(b))\n        lastchar = c\n    if normalized:\n        order = len(cs)\n        scale = 1.0 / ((order - 1) * (order - 2))\n        seq = [s * scale for s in seq]\n    return seq"
 },
 {
  "docstring": "Return a 2-tuple of Laplacian eigenvalues and eigenvectors\nfor the threshold network with creation_sequence.\nThe first value is a list of eigenvalues.\nThe second value is a list of eigenvectors.\nThe lists are in the same order so corresponding eigenvectors\nand eigenvalues are in the same position in the two lists.\n\nNotice that the order of the eigenvalues returned by eigenvalues(cs)\nmay not correspond to the order of these eigenvectors.",
  "code": "def eigenvectors(creation_sequence):\n    ccs = make_compact(creation_sequence)\n    N = sum(ccs)\n    vec = [0] * N\n    val = vec[:]\n    dr = sum(ccs[::2])\n    nn = ccs[0]\n    vec[0] = [1.0 / sqrt(N)] * N\n    val[0] = 0\n    e = dr\n    dr -= nn\n    type_d = True\n    i = 1\n    dd = 1\n    while dd < nn:\n        scale = 1.0 / sqrt(dd * dd + i)\n        vec[i] = i * [-scale] + [dd * scale] + [0] * (N - i - 1)\n        val[i] = e\n        i += 1\n        dd += 1\n    if len(ccs) == 1:\n        return (val, vec)\n    for nn in ccs[1:]:\n        scale = 1.0 / sqrt(nn * i * (i + nn))\n        vec[i] = i * [-nn * scale] + nn * [i * scale] + [0] * (N - i - nn)\n        type_d = not type_d\n        if type_d:\n            e = i + dr\n            dr -= nn\n        else:\n            e = dr\n        val[i] = e\n        st = i\n        i += 1\n        dd = 1\n        while dd < nn:\n            scale = 1.0 / sqrt(i - st + dd * dd)\n            vec[i] = [0] * st + (i - st) * [-scale] + [dd * scale] + [0] * (N - i - 1)\n            val[i] = e\n            i += 1\n            dd += 1\n    return (val, vec)"
 },
 {
  "docstring": "Returns the coefficients of each eigenvector\nin a projection of the vector u onto the normalized\neigenvectors which are contained in eigenpairs.\n\neigenpairs should be a list of two objects.  The\nfirst is a list of eigenvalues and the second a list\nof eigenvectors.  The eigenvectors should be lists.\n\nThere's not a lot of error checking on lengths of\narrays, etc. so be careful.",
  "code": "def spectral_projection(u, eigenpairs):\n    coeff = []\n    evect = eigenpairs[1]\n    for ev in evect:\n        c = sum((evv * uv for evv, uv in zip(ev, u)))\n        coeff.append(c)\n    return coeff"
 },
 {
  "docstring": "Return sequence of eigenvalues of the Laplacian of the threshold\ngraph for the given creation_sequence.\n\nBased on the Ferrer's diagram method.  The spectrum is integral\nand is the conjugate of the degree sequence.\n\nSee::\n\n  @Article{degree-merris-1994,\n   author = {Russel Merris},\n   title = {Degree maximal graphs are Laplacian integral},\n   journal = {Linear Algebra Appl.},\n   year = {1994},\n   volume = {199},\n   pages = {381--389},\n  }",
  "code": "def eigenvalues(creation_sequence):\n    degseq = degree_sequence(creation_sequence)\n    degseq.sort()\n    eiglist = []\n    eig = 0\n    row = len(degseq)\n    bigdeg = degseq.pop()\n    while row:\n        if bigdeg < row:\n            eiglist.append(eig)\n            row -= 1\n        else:\n            eig += 1\n            if degseq:\n                bigdeg = degseq.pop()\n            else:\n                bigdeg = 0\n    return eiglist"
 },
 {
  "docstring": "Create a random threshold sequence of size n.\nA creation sequence is built by randomly choosing d's with\nprobability p and i's with probability 1-p.\n\ns=nx.random_threshold_sequence(10,0.5)\n\nreturns a threshold sequence of length 10 with equal\nprobably of an i or a d at each position.\n\nA \"random\" threshold graph can be built with\n\nG=nx.threshold_graph(s)\n\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.",
  "code": "@py_random_state(2)\ndef random_threshold_sequence(n, p, seed=None):\n    if not 0 <= p <= 1:\n        raise ValueError('p must be in [0,1]')\n    cs = ['d']\n    for i in range(1, n):\n        if seed.random() < p:\n            cs.append('d')\n        else:\n            cs.append('i')\n    return cs"
 },
 {
  "docstring": "Create a skewed threshold graph with a given number\nof vertices (n) and a given number of edges (m).\n\nThe routine returns an unlabeled creation sequence\nfor the threshold graph.\n\nFIXME: describe algorithm",
  "code": "def right_d_threshold_sequence(n, m):\n    cs = ['d'] + ['i'] * (n - 1)\n    if m < n:\n        cs[m] = 'd'\n        return cs\n    if m > n * (n - 1) / 2:\n        raise ValueError('Too many edges for this many nodes.')\n    ind = n - 1\n    sum = n - 1\n    while sum < m:\n        cs[ind] = 'd'\n        ind -= 1\n        sum += ind\n    ind = m - (sum - ind)\n    cs[ind] = 'd'\n    return cs"
 },
 {
  "docstring": "Create a skewed threshold graph with a given number\nof vertices (n) and a given number of edges (m).\n\nThe routine returns an unlabeled creation sequence\nfor the threshold graph.\n\nFIXME: describe algorithm",
  "code": "def left_d_threshold_sequence(n, m):\n    cs = ['d'] + ['i'] * (n - 1)\n    if m < n:\n        cs[m] = 'd'\n        return cs\n    if m > n * (n - 1) / 2:\n        raise ValueError('Too many edges for this many nodes.')\n    cs[n - 1] = 'd'\n    sum = n - 1\n    ind = 1\n    while sum < m:\n        cs[ind] = 'd'\n        sum += ind\n        ind += 1\n    if sum > m:\n        cs[sum - m] = 'i'\n    return cs"
 },
 {
  "docstring": "Perform a \"swap\" operation on a threshold sequence.\n\nThe swap preserves the number of nodes and edges\nin the graph for the given sequence.\nThe resulting sequence is still a threshold sequence.\n\nPerform one split and one combine operation on the\n'd's of a creation sequence for a threshold graph.\nThis operation maintains the number of nodes and edges\nin the graph, but shifts the edges from node to node\nmaintaining the threshold quality of the graph.\n\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.",
  "code": "@py_random_state(3)\ndef swap_d(cs, p_split=1.0, p_combine=1.0, seed=None):\n    dlist = [i for i, node_type in enumerate(cs[1:-1]) if node_type == 'd']\n    if seed.random() < p_split:\n        choice = seed.choice(dlist)\n        split_to = seed.choice(range(choice))\n        flip_side = choice - split_to\n        if split_to != flip_side and cs[split_to] == 'i' and (cs[flip_side] == 'i'):\n            cs[choice] = 'i'\n            cs[split_to] = 'd'\n            cs[flip_side] = 'd'\n            dlist.remove(choice)\n    if seed.random() < p_combine and dlist:\n        first_choice = seed.choice(dlist)\n        second_choice = seed.choice(dlist)\n        target = first_choice + second_choice\n        if target >= len(cs) or cs[target] == 'd' or first_choice == second_choice:\n            return cs\n        cs[first_choice] = 'i'\n        cs[second_choice] = 'i'\n        cs[target] = 'd'\n    return cs"
 },
 {
  "docstring": "Compute the CD index for `node` within the graph `G`.\n\nCalculates the CD index for the given node of the graph,\nconsidering only its predecessors who have the `time` attribute\nsmaller than or equal to the `time` attribute of the `node`\nplus `time_delta`.\n\nParameters\n----------\nG : graph\n   A directed networkx graph whose nodes have `time` attributes and optionally\n   `weight` attributes (if a weight is not given, it is considered 1).\nnode : node\n   The node for which the CD index is calculated.\ntime_delta : numeric or timedelta\n   Amount of time after the `time` attribute of the `node`. The value of\n   `time_delta` must support comparison with the `time` node attribute. For\n   example, if the `time` attribute of the nodes are `datetime.datetime`\n   objects, then `time_delta` should be a `datetime.timedelta` object.\ntime : string (Optional, default is \"time\")\n    The name of the node attribute that will be used for the calculations.\nweight : string (Optional, default is None)\n    The name of the node attribute used as weight.\n\nReturns\n-------\nfloat\n   The CD index calculated for the node `node` within the graph `G`.\n\nRaises\n------\nNetworkXError\n   If not all nodes have a `time` attribute or\n   `time_delta` and `time` attribute types are not compatible or\n   `n` equals 0.\n\nNetworkXNotImplemented\n    If `G` is a non-directed graph or a multigraph.\n\nExamples\n--------\n>>> from datetime import datetime, timedelta\n>>> G = nx.DiGraph()\n>>> nodes = {\n...     1: {\"time\": datetime(2015, 1, 1)},\n...     2: {\"time\": datetime(2012, 1, 1), 'weight': 4},\n...     3: {\"time\": datetime(2010, 1, 1)},\n...     4: {\"time\": datetime(2008, 1, 1)},\n...     5: {\"time\": datetime(2014, 1, 1)}\n... }\n>>> G.add_nodes_from([(n, nodes[n]) for n in nodes])\n>>> edges = [(1, 3), (1, 4), (2, 3), (3, 4), (3, 5)]\n>>> G.add_edges_from(edges)\n>>> delta = timedelta(days=5 * 365)\n>>> nx.cd_index(G, 3, time_delta=delta, time=\"time\")\n0.5\n>>> nx.cd_index(G, 3, time_delta=delta, time=\"time\", weight=\"weight\")\n0.12\n\nIntegers can also be used for the time values:\n>>> node_times = {1: 2015, 2: 2012, 3: 2010, 4: 2008, 5: 2014}\n>>> nx.set_node_attributes(G, node_times, \"new_time\")\n>>> nx.cd_index(G, 3, time_delta=4, time=\"new_time\")\n0.5\n>>> nx.cd_index(G, 3, time_delta=4, time=\"new_time\", weight=\"weight\")\n0.12\n\n",
  "code": "@not_implemented_for('undirected')\n@not_implemented_for('multigraph')\n@nx._dispatch(node_attrs={'time': None, 'weight': 1})\ndef cd_index(G, node, time_delta, *, time='time', weight=None):\n    if not all((time in G.nodes[n] for n in G)):\n        raise nx.NetworkXError(\"Not all nodes have a 'time' attribute.\")\n    try:\n        target_date = G.nodes[node][time] + time_delta\n        pred = {i for i in G.pred[node] if G.nodes[i][time] <= target_date}\n    except:\n        raise nx.NetworkXError(\"Addition and comparison are not supported between 'time_delta' and 'time' types.\")\n    b = [-1 if any((j in G[i] for j in G[node])) else 1 for i in pred]\n    n = len(pred.union(*(G.pred[s].keys() - {node} for s in G[node])))\n    if n == 0:\n        raise nx.NetworkXError('The cd index cannot be defined.')\n    if weight is None:\n        return round(sum((bi for bi in b)) / n, 2)\n    else:\n        weights = [G.nodes[i].get(weight, 1) for i in pred]\n        return round(sum((bi / wt for bi, wt in zip(b, weights))) / n, 2)"
 },
 {
  "docstring": "Returns the index of the first element in `iterable` that\nsatisfies the given condition.\n\nIf no such element is found (that is, when the iterable is\nexhausted), this returns the length of the iterable (that is, one\ngreater than the last index of the iterable).\n\n`iterable` must not be empty. If `iterable` is empty, this\nfunction raises :exc:`ValueError`.",
  "code": "def index_satisfying(iterable, condition):\n    for i, x in enumerate(iterable):\n        if condition(x):\n            return i\n    try:\n        return i + 1\n    except NameError as err:\n        raise ValueError('iterable must be non-empty') from err"
 },
 {
  "docstring": "Returns True if and only if `G` is a tournament.\n\nA tournament is a directed graph, with neither self-loops nor\nmulti-edges, in which there is exactly one directed edge joining\neach pair of distinct nodes.\n\nParameters\n----------\nG : NetworkX graph\n    A directed graph representing a tournament.\n\nReturns\n-------\nbool\n    Whether the given graph is a tournament graph.\n\nExamples\n--------\n>>> G = nx.DiGraph([(0, 1), (1, 2), (2, 0)])\n>>> nx.is_tournament(G)\nTrue\n\n",
  "code": "@not_implemented_for('undirected')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef is_tournament(G):\n    return all(((v in G[u]) ^ (u in G[v]) for u, v in combinations(G, 2))) and nx.number_of_selfloops(G) == 0"
 },
 {
  "docstring": "Returns a Hamiltonian path in the given tournament graph.\n\nEach tournament has a Hamiltonian path. If furthermore, the\ntournament is strongly connected, then the returned Hamiltonian path\nis a Hamiltonian cycle (by joining the endpoints of the path).\n\nParameters\n----------\nG : NetworkX graph\n    A directed graph representing a tournament.\n\nReturns\n-------\npath : list\n    A list of nodes which form a Hamiltonian path in `G`.\n\nExamples\n--------\n>>> G = nx.DiGraph([(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)])\n>>> nx.is_tournament(G)\nTrue\n>>> nx.tournament.hamiltonian_path(G)\n[0, 1, 2, 3]\n\n",
  "code": "@not_implemented_for('undirected')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef hamiltonian_path(G):\n    if len(G) == 0:\n        return []\n    if len(G) == 1:\n        return [arbitrary_element(G)]\n    v = arbitrary_element(G)\n    hampath = hamiltonian_path(G.subgraph(set(G) - {v}))\n    index = index_satisfying(hampath, lambda u: v not in G[u])\n    hampath.insert(index, v)\n    return hampath"
 },
 {
  "docstring": "Returns a random tournament graph on `n` nodes.\n\nParameters\n----------\nn : int\n    The number of nodes in the returned graph.\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\nReturns\n-------\nG : DiGraph\n    A tournament on `n` nodes, with exactly one directed edge joining\n    each pair of distinct nodes.\n\n",
  "code": "@py_random_state(1)\n@nx._dispatch(graphs=None)\ndef random_tournament(n, seed=None):\n    coins = (seed.random() for i in range(n * (n - 1) // 2))\n    pairs = combinations(range(n), 2)\n    edges = ((u, v) if r < 0.5 else (v, u) for (u, v), r in zip(pairs, coins))\n    return nx.DiGraph(edges)"
 },
 {
  "docstring": "Returns the score sequence for the given tournament graph.\n\nThe score sequence is the sorted list of the out-degrees of the\nnodes of the graph.\n\nParameters\n----------\nG : NetworkX graph\n    A directed graph representing a tournament.\n\nReturns\n-------\nlist\n    A sorted list of the out-degrees of the nodes of `G`.\n\nExamples\n--------\n>>> G = nx.DiGraph([(1, 0), (1, 3), (0, 2), (0, 3), (2, 1), (3, 2)])\n>>> nx.is_tournament(G)\nTrue\n>>> nx.tournament.score_sequence(G)\n[1, 1, 2, 2]",
  "code": "@not_implemented_for('undirected')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef score_sequence(G):\n    return sorted((d for v, d in G.out_degree()))"
 },
 {
  "docstring": "Returns the tournament matrix for the given tournament graph.\n\nThis function requires SciPy.\n\nThe *tournament matrix* of a tournament graph with edge set *E* is\nthe matrix *T* defined by\n\n.. math::\n\n   T_{i j} =\n   \\begin{cases}\n   +1 & \\text{if } (i, j) \\in E \\\\\n   -1 & \\text{if } (j, i) \\in E \\\\\n   0 & \\text{if } i == j.\n   \\end{cases}\n\nAn equivalent definition is `T = A - A^T`, where *A* is the\nadjacency matrix of the graph `G`.\n\nParameters\n----------\nG : NetworkX graph\n    A directed graph representing a tournament.\n\nReturns\n-------\nSciPy sparse array\n    The tournament matrix of the tournament graph `G`.\n\nRaises\n------\nImportError\n    If SciPy is not available.",
  "code": "@not_implemented_for('undirected')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef tournament_matrix(G):\n    A = nx.adjacency_matrix(G)\n    return A - A.T"
 },
 {
  "docstring": "Decides whether there is a path from `s` to `t` in the\ntournament.\n\nThis function is more theoretically efficient than the reachability\nchecks than the shortest path algorithms in\n:mod:`networkx.algorithms.shortest_paths`.\n\nThe given graph **must** be a tournament, otherwise this function's\nbehavior is undefined.\n\nParameters\n----------\nG : NetworkX graph\n    A directed graph representing a tournament.\n\ns : node\n    A node in the graph.\n\nt : node\n    A node in the graph.\n\nReturns\n-------\nbool\n    Whether there is a path from `s` to `t` in `G`.\n\nExamples\n--------\n>>> G = nx.DiGraph([(1, 0), (1, 3), (1, 2), (2, 3), (2, 0), (3, 0)])\n>>> nx.is_tournament(G)\nTrue\n>>> nx.tournament.is_reachable(G, 1, 3)\nTrue\n>>> nx.tournament.is_reachable(G, 3, 2)\nFalse\n\n",
  "code": "@not_implemented_for('undirected')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef is_reachable(G, s, t):\n\n    def two_neighborhood(G, v):\n        \"\"\"Returns the set of nodes at distance at most two from `v`.\n\n        `G` must be a graph and `v` a node in that graph.\n\n        The returned set includes the nodes at distance zero (that is,\n        the node `v` itself), the nodes at distance one (that is, the\n        out-neighbors of `v`), and the nodes at distance two.\n\n        \"\"\"\n        return {x for x in G if x == v or x in G[v] or any((is_path(G, [v, z, x]) for z in G))}\n\n    def is_closed(G, nodes):\n        \"\"\"Decides whether the given set of nodes is closed.\n\n        A set *S* of nodes is *closed* if for each node *u* in the graph\n        not in *S* and for each node *v* in *S*, there is an edge from\n        *u* to *v*.\n\n        \"\"\"\n        return all((v in G[u] for u in set(G) - nodes for v in nodes))\n    neighborhoods = [two_neighborhood(G, v) for v in G]\n    return all((not (is_closed(G, S) and s in S and (t not in S)) for S in neighborhoods))"
 },
 {
  "docstring": "Decides whether the given tournament is strongly connected.\n\nThis function is more theoretically efficient than the\n:func:`~networkx.algorithms.components.is_strongly_connected`\nfunction.\n\nThe given graph **must** be a tournament, otherwise this function's\nbehavior is undefined.\n\nParameters\n----------\nG : NetworkX graph\n    A directed graph representing a tournament.\n\nReturns\n-------\nbool\n    Whether the tournament is strongly connected.\n\nExamples\n--------\n>>> G = nx.DiGraph([(0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (3, 0)])\n>>> nx.is_tournament(G)\nTrue\n>>> nx.tournament.is_strongly_connected(G)\nTrue\n>>> G.remove_edge(3, 0)\n>>> G.add_edge(0, 3)\n>>> nx.is_tournament(G)\nTrue\n>>> nx.tournament.is_strongly_connected(G)\nFalse\n\n",
  "code": "@not_implemented_for('undirected')\n@not_implemented_for('multigraph')\n@nx._dispatch(name='tournament_is_strongly_connected')\ndef is_strongly_connected(G):\n    return all((is_reachable(G, u, v) for u in G for v in G))"
 },
 {
  "docstring": "Returns the set of nodes at distance at most two from `v`.\n\n`G` must be a graph and `v` a node in that graph.\n\nThe returned set includes the nodes at distance zero (that is,\nthe node `v` itself), the nodes at distance one (that is, the\nout-neighbors of `v`), and the nodes at distance two.",
  "code": "def two_neighborhood(G, v):\n    return {x for x in G if x == v or x in G[v] or any((is_path(G, [v, z, x]) for z in G))}"
 },
 {
  "docstring": "Decides whether the given set of nodes is closed.\n\nA set *S* of nodes is *closed* if for each node *u* in the graph\nnot in *S* and for each node *v* in *S*, there is an edge from\n*u* to *v*.",
  "code": "def is_closed(G, nodes):\n    return all((v in G[u] for u in set(G) - nodes for v in nodes))"
 },
 {
  "docstring": "Returns the integer code of the given triad.\n\nThis is some fancy magic that comes from Batagelj and Mrvar's paper. It\ntreats each edge joining a pair of `v`, `u`, and `w` as a bit in\nthe binary representation of an integer.",
  "code": "def _tricode(G, v, u, w):\n    combos = ((v, u, 1), (u, v, 2), (v, w, 4), (w, v, 8), (u, w, 16), (w, u, 32))\n    return sum((x for u, v, x in combos if v in G[u]))"
 },
 {
  "docstring": "Determines the triadic census of a directed graph.\n\nThe triadic census is a count of how many of the 16 possible types of\ntriads are present in a directed graph. If a list of nodes is passed, then\nonly those triads are taken into account which have elements of nodelist in them.\n\nParameters\n----------\nG : digraph\n   A NetworkX DiGraph\nnodelist : list\n    List of nodes for which you want to calculate triadic census\n\nReturns\n-------\ncensus : dict\n   Dictionary with triad type as keys and number of occurrences as values.\n\nExamples\n--------\n>>> G = nx.DiGraph([(1, 2), (2, 3), (3, 1), (3, 4), (4, 1), (4, 2)])\n>>> triadic_census = nx.triadic_census(G)\n>>> for key, value in triadic_census.items():\n...     print(f\"{key}: {value}\")\n...\n003: 0\n012: 0\n102: 0\n021D: 0\n021U: 0\n021C: 0\n111D: 0\n111U: 0\n030T: 2\n030C: 2\n201: 0\n120D: 0\n120U: 0\n120C: 0\n210: 0\n300: 0\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef triadic_census(G, nodelist=None):\n    nodeset = set(G.nbunch_iter(nodelist))\n    if nodelist is not None and len(nodelist) != len(nodeset):\n        raise ValueError('nodelist includes duplicate nodes or nodes not in G')\n    N = len(G)\n    Nnot = N - len(nodeset)\n    m = {n: i for i, n in enumerate(nodeset)}\n    if Nnot:\n        not_nodeset = G.nodes - nodeset\n        m.update(((n, i + N) for i, n in enumerate(not_nodeset)))\n    nbrs = {n: G.pred[n].keys() | G.succ[n].keys() for n in G}\n    dbl_nbrs = {n: G.pred[n].keys() & G.succ[n].keys() for n in G}\n    if Nnot:\n        sgl_nbrs = {n: G.pred[n].keys() ^ G.succ[n].keys() for n in not_nodeset}\n        sgl = sum((1 for n in not_nodeset for nbr in sgl_nbrs[n] if nbr not in nodeset))\n        sgl_edges_outside = sgl // 2\n        dbl = sum((1 for n in not_nodeset for nbr in dbl_nbrs[n] if nbr not in nodeset))\n        dbl_edges_outside = dbl // 2\n    census = {name: 0 for name in TRIAD_NAMES}\n    for v in nodeset:\n        vnbrs = nbrs[v]\n        dbl_vnbrs = dbl_nbrs[v]\n        if Nnot:\n            sgl_unbrs_bdy = sgl_unbrs_out = dbl_unbrs_bdy = dbl_unbrs_out = 0\n        for u in vnbrs:\n            if m[u] <= m[v]:\n                continue\n            unbrs = nbrs[u]\n            neighbors = (vnbrs | unbrs) - {u, v}\n            for w in neighbors:\n                if m[u] < m[w] or (m[v] < m[w] < m[u] and v not in nbrs[w]):\n                    code = _tricode(G, v, u, w)\n                    census[TRICODE_TO_NAME[code]] += 1\n            if u in dbl_vnbrs:\n                census['102'] += N - len(neighbors) - 2\n            else:\n                census['012'] += N - len(neighbors) - 2\n            if Nnot and u not in nodeset:\n                sgl_unbrs = sgl_nbrs[u]\n                sgl_unbrs_bdy += len(sgl_unbrs & vnbrs - nodeset)\n                sgl_unbrs_out += len(sgl_unbrs - vnbrs - nodeset)\n                dbl_unbrs = dbl_nbrs[u]\n                dbl_unbrs_bdy += len(dbl_unbrs & vnbrs - nodeset)\n                dbl_unbrs_out += len(dbl_unbrs - vnbrs - nodeset)\n        if Nnot:\n            census['012'] += sgl_edges_outside - (sgl_unbrs_out + sgl_unbrs_bdy // 2)\n            census['102'] += dbl_edges_outside - (dbl_unbrs_out + dbl_unbrs_bdy // 2)\n    total_triangles = N * (N - 1) * (N - 2) // 6\n    triangles_without_nodeset = Nnot * (Nnot - 1) * (Nnot - 2) // 6\n    total_census = total_triangles - triangles_without_nodeset\n    census['003'] = total_census - sum(census.values())\n    return census"
 },
 {
  "docstring": "Returns True if the graph G is a triad, else False.\n\nParameters\n----------\nG : graph\n   A NetworkX Graph\n\nReturns\n-------\nistriad : boolean\n   Whether G is a valid triad\n\nExamples\n--------\n>>> G = nx.DiGraph([(1, 2), (2, 3), (3, 1)])\n>>> nx.is_triad(G)\nTrue\n>>> G.add_edge(0, 1)\n>>> nx.is_triad(G)\nFalse",
  "code": "@nx._dispatch\ndef is_triad(G):\n    if isinstance(G, nx.Graph):\n        if G.order() == 3 and nx.is_directed(G):\n            if not any(((n, n) in G.edges() for n in G.nodes())):\n                return True\n    return False"
 },
 {
  "docstring": "Returns a generator of all possible sets of 3 nodes in a DiGraph.\n\n.. deprecated:: 3.3\n\n   all_triplets is deprecated and will be removed in NetworkX version 3.5.\n   Use `itertools.combinations` instead::\n\n      all_triplets = itertools.combinations(G, 3)\n\nParameters\n----------\nG : digraph\n   A NetworkX DiGraph\n\nReturns\n-------\ntriplets : generator of 3-tuples\n   Generator of tuples of 3 nodes\n\nExamples\n--------\n>>> G = nx.DiGraph([(1, 2), (2, 3), (3, 4)])\n>>> list(nx.all_triplets(G))\n[(1, 2, 3), (1, 2, 4), (1, 3, 4), (2, 3, 4)]",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef all_triplets(G):\n    import warnings\n    warnings.warn('\\n\\nall_triplets is deprecated and will be rmoved in v3.5.\\nUse `itertools.combinations(G, 3)` instead.', category=DeprecationWarning, stacklevel=4)\n    triplets = combinations(G.nodes(), 3)\n    return triplets"
 },
 {
  "docstring": "A generator of all possible triads in G.\n\nParameters\n----------\nG : digraph\n   A NetworkX DiGraph\n\nReturns\n-------\nall_triads : generator of DiGraphs\n   Generator of triads (order-3 DiGraphs)\n\nExamples\n--------\n>>> G = nx.DiGraph([(1, 2), (2, 3), (3, 1), (3, 4), (4, 1), (4, 2)])\n>>> for triad in nx.all_triads(G):\n...     print(triad.edges)\n[(1, 2), (2, 3), (3, 1)]\n[(1, 2), (4, 1), (4, 2)]\n[(3, 1), (3, 4), (4, 1)]\n[(2, 3), (3, 4), (4, 2)]",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef all_triads(G):\n    triplets = combinations(G.nodes(), 3)\n    for triplet in triplets:\n        yield G.subgraph(triplet).copy()"
 },
 {
  "docstring": "Returns a list of all triads for each triad type in a directed graph.\nThere are exactly 16 different types of triads possible. Suppose 1, 2, 3 are three\nnodes, they will be classified as a particular triad type if their connections\nare as follows:\n\n- 003: 1, 2, 3\n- 012: 1 -> 2, 3\n- 102: 1 <-> 2, 3\n- 021D: 1 <- 2 -> 3\n- 021U: 1 -> 2 <- 3\n- 021C: 1 -> 2 -> 3\n- 111D: 1 <-> 2 <- 3\n- 111U: 1 <-> 2 -> 3\n- 030T: 1 -> 2 -> 3, 1 -> 3\n- 030C: 1 <- 2 <- 3, 1 -> 3\n- 201: 1 <-> 2 <-> 3\n- 120D: 1 <- 2 -> 3, 1 <-> 3\n- 120U: 1 -> 2 <- 3, 1 <-> 3\n- 120C: 1 -> 2 -> 3, 1 <-> 3\n- 210: 1 -> 2 <-> 3, 1 <-> 3\n- 300: 1 <-> 2 <-> 3, 1 <-> 3\n\nRefer to the :doc:`example gallery </auto_examples/graph/plot_triad_types>`\nfor visual examples of the triad types.\n\nParameters\n----------\nG : digraph\n   A NetworkX DiGraph\n\nReturns\n-------\ntri_by_type : dict\n   Dictionary with triad types as keys and lists of triads as values.\n\nExamples\n--------\n>>> G = nx.DiGraph([(1, 2), (1, 3), (2, 3), (3, 1), (5, 6), (5, 4), (6, 7)])\n>>> dict = nx.triads_by_type(G)\n>>> dict['120C'][0].edges()\nOutEdgeView([(1, 2), (1, 3), (2, 3), (3, 1)])\n>>> dict['012'][0].edges()\nOutEdgeView([(1, 2)])\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef triads_by_type(G):\n    all_tri = all_triads(G)\n    tri_by_type = defaultdict(list)\n    for triad in all_tri:\n        name = triad_type(triad)\n        tri_by_type[name].append(triad)\n    return tri_by_type"
 },
 {
  "docstring": "Returns the sociological triad type for a triad.\n\nParameters\n----------\nG : digraph\n   A NetworkX DiGraph with 3 nodes\n\nReturns\n-------\ntriad_type : str\n   A string identifying the triad type\n\nExamples\n--------\n>>> G = nx.DiGraph([(1, 2), (2, 3), (3, 1)])\n>>> nx.triad_type(G)\n'030C'\n>>> G.add_edge(1, 3)\n>>> nx.triad_type(G)\n'120C'\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef triad_type(G):\n    if not is_triad(G):\n        raise nx.NetworkXAlgorithmError('G is not a triad (order-3 DiGraph)')\n    num_edges = len(G.edges())\n    if num_edges == 0:\n        return '003'\n    elif num_edges == 1:\n        return '012'\n    elif num_edges == 2:\n        e1, e2 = G.edges()\n        if set(e1) == set(e2):\n            return '102'\n        elif e1[0] == e2[0]:\n            return '021D'\n        elif e1[1] == e2[1]:\n            return '021U'\n        elif e1[1] == e2[0] or e2[1] == e1[0]:\n            return '021C'\n    elif num_edges == 3:\n        for e1, e2, e3 in permutations(G.edges(), 3):\n            if set(e1) == set(e2):\n                if e3[0] in e1:\n                    return '111U'\n                return '111D'\n            elif set(e1).symmetric_difference(set(e2)) == set(e3):\n                if {e1[0], e2[0], e3[0]} == {e1[0], e2[0], e3[0]} == set(G.nodes()):\n                    return '030C'\n                return '030T'\n    elif num_edges == 4:\n        for e1, e2, e3, e4 in permutations(G.edges(), 4):\n            if set(e1) == set(e2):\n                if set(e3) == set(e4):\n                    return '201'\n                if {e3[0]} == {e4[0]} == set(e3).intersection(set(e4)):\n                    return '120D'\n                if {e3[1]} == {e4[1]} == set(e3).intersection(set(e4)):\n                    return '120U'\n                if e3[1] == e4[0]:\n                    return '120C'\n    elif num_edges == 5:\n        return '210'\n    elif num_edges == 6:\n        return '300'"
 },
 {
  "docstring": "Returns a random triad from a directed graph.\n\n.. deprecated:: 3.3\n\n   random_triad is deprecated and will be removed in version 3.5.\n   Use random sampling directly instead::\n\n      G.subgraph(random.sample(list(G), 3))\n\nParameters\n----------\nG : digraph\n   A NetworkX DiGraph\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\nReturns\n-------\nG2 : subgraph\n   A randomly selected triad (order-3 NetworkX DiGraph)\n\nRaises\n------\nNetworkXError\n    If the input Graph has less than 3 nodes.\n\nExamples\n--------\n>>> G = nx.DiGraph([(1, 2), (1, 3), (2, 3), (3, 1), (5, 6), (5, 4), (6, 7)])\n>>> triad = nx.random_triad(G, seed=1)\n>>> triad.edges\nOutEdgeView([(1, 2)])",
  "code": "@not_implemented_for('undirected')\n@py_random_state(1)\n@nx._dispatch(preserve_all_attrs=True)\ndef random_triad(G, seed=None):\n    import warnings\n    warnings.warn('\\n\\nrandom_triad is deprecated and will be removed in NetworkX v3.5.\\nUse random.sample instead, e.g.::\\n\\n\\tG.subgraph(random.sample(list(G), 3))\\n', category=DeprecationWarning, stacklevel=5)\n    if len(G) < 3:\n        raise nx.NetworkXError(f'G needs at least 3 nodes to form a triad; (it has {len(G)} nodes)')\n    nodes = seed.sample(list(G.nodes()), 3)\n    G2 = G.subgraph(nodes)\n    return G2"
 },
 {
  "docstring": "Returns the closeness vitality for nodes in the graph.\n\nThe *closeness vitality* of a node, defined in Section 3.6.2 of [1],\nis the change in the sum of distances between all node pairs when\nexcluding that node.\n\nParameters\n----------\nG : NetworkX graph\n    A strongly-connected graph.\n\nweight : string\n    The name of the edge attribute used as weight. This is passed\n    directly to the :func:`~networkx.wiener_index` function.\n\nnode : object\n    If specified, only the closeness vitality for this node will be\n    returned. Otherwise, a dictionary mapping each node to its\n    closeness vitality will be returned.\n\nOther parameters\n----------------\nwiener_index : number\n    If you have already computed the Wiener index of the graph\n    `G`, you can provide that value here. Otherwise, it will be\n    computed for you.\n\nReturns\n-------\ndictionary or float\n    If `node` is None, this function returns a dictionary\n    with nodes as keys and closeness vitality as the\n    value. Otherwise, it returns only the closeness vitality for the\n    specified `node`.\n\n    The closeness vitality of a node may be negative infinity if\n    removing that node would disconnect the graph.\n\nExamples\n--------\n>>> G = nx.cycle_graph(3)\n>>> nx.closeness_vitality(G)\n{0: 2.0, 1: 2.0, 2: 2.0}\n\nSee Also\n--------\ncloseness_centrality\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef closeness_vitality(G, node=None, weight=None, wiener_index=None):\n    if wiener_index is None:\n        wiener_index = nx.wiener_index(G, weight=weight)\n    if node is not None:\n        after = nx.wiener_index(G.subgraph(set(G) - {node}), weight=weight)\n        return wiener_index - after\n    vitality = partial(closeness_vitality, G, weight=weight, wiener_index=wiener_index)\n    return {v: vitality(node=v) for v in G}"
 },
 {
  "docstring": "Returns the Voronoi cells centered at `center_nodes` with respect\nto the shortest-path distance metric.\n\nIf $C$ is a set of nodes in the graph and $c$ is an element of $C$,\nthe *Voronoi cell* centered at a node $c$ is the set of all nodes\n$v$ that are closer to $c$ than to any other center node in $C$ with\nrespect to the shortest-path distance metric. [1]_\n\nFor directed graphs, this will compute the \"outward\" Voronoi cells,\nas defined in [1]_, in which distance is measured from the center\nnodes to the target node. For the \"inward\" Voronoi cells, use the\n:meth:`DiGraph.reverse` method to reverse the orientation of the\nedges before invoking this function on the directed graph.\n\nParameters\n----------\nG : NetworkX graph\n\ncenter_nodes : set\n    A nonempty set of nodes in the graph `G` that represent the\n    center of the Voronoi cells.\n\nweight : string or function\n    The edge attribute (or an arbitrary function) representing the\n    weight of an edge. This keyword argument is as described in the\n    documentation for :func:`~networkx.multi_source_dijkstra_path`,\n    for example.\n\nReturns\n-------\ndictionary\n    A mapping from center node to set of all nodes in the graph\n    closer to that center node than to any other center node. The\n    keys of the dictionary are the element of `center_nodes`, and\n    the values of the dictionary form a partition of the nodes of\n    `G`.\n\nExamples\n--------\nTo get only the partition of the graph induced by the Voronoi cells,\ntake the collection of all values in the returned dictionary::\n\n    >>> G = nx.path_graph(6)\n    >>> center_nodes = {0, 3}\n    >>> cells = nx.voronoi_cells(G, center_nodes)\n    >>> partition = set(map(frozenset, cells.values()))\n    >>> sorted(map(sorted, partition))\n    [[0, 1], [2, 3, 4, 5]]\n\nRaises\n------\nValueError\n    If `center_nodes` is empty.\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef voronoi_cells(G, center_nodes, weight='weight'):\n    paths = nx.multi_source_dijkstra_path(G, center_nodes, weight=weight)\n    nearest = {v: p[0] for v, p in paths.items()}\n    cells = groups(nearest)\n    unreachable = set(G) - set(nearest)\n    if unreachable:\n        cells['unreachable'] = unreachable\n    return cells"
 },
 {
  "docstring": "Returns the number of walks connecting each pair of nodes in `G`\n\nA *walk* is a sequence of nodes in which each adjacent pair of nodes\nin the sequence is adjacent in the graph. A walk can repeat the same\nedge and go in the opposite direction just as people can walk on a\nset of paths, but standing still is not counted as part of the walk.\n\nThis function only counts the walks with `walk_length` edges. Note that\nthe number of nodes in the walk sequence is one more than `walk_length`.\nThe number of walks can grow very quickly on a larger graph\nand with a larger walk length.\n\nParameters\n----------\nG : NetworkX graph\n\nwalk_length : int\n    A nonnegative integer representing the length of a walk.\n\nReturns\n-------\ndict\n    A dictionary of dictionaries in which outer keys are source\n    nodes, inner keys are target nodes, and inner values are the\n    number of walks of length `walk_length` connecting those nodes.\n\nRaises\n------\nValueError\n    If `walk_length` is negative\n\nExamples\n--------\n\n>>> G = nx.Graph([(0, 1), (1, 2)])\n>>> walks = nx.number_of_walks(G, 2)\n>>> walks\n{0: {0: 1, 1: 0, 2: 1}, 1: {0: 0, 1: 2, 2: 0}, 2: {0: 1, 1: 0, 2: 1}}\n>>> total_walks = sum(sum(tgts.values()) for _, tgts in walks.items())\n\nYou can also get the number of walks from a specific source node using the\nreturned dictionary. For example, number of walks of length 1 from node 0\ncan be found as follows:\n\n>>> walks = nx.number_of_walks(G, 1)\n>>> walks[0]\n{0: 0, 1: 1, 2: 0}\n>>> sum(walks[0].values())  # walks from 0 of length 1\n1\n\nSimilarly, a target node can also be specified:\n\n>>> walks[0][1]\n1",
  "code": "@nx._dispatch\ndef number_of_walks(G, walk_length):\n    import numpy as np\n    if walk_length < 0:\n        raise ValueError(f'`walk_length` cannot be negative: {walk_length}')\n    A = nx.adjacency_matrix(G, weight=None)\n    power = np.linalg.matrix_power(A.toarray(), walk_length)\n    result = {u: {v: power[u_idx, v_idx] for v_idx, v in enumerate(G)} for u_idx, u in enumerate(G)}\n    return result"
 },
 {
  "docstring": "Returns the Wiener index of the given graph.\n\nThe *Wiener index* of a graph is the sum of the shortest-path\ndistances between each pair of reachable nodes. For pairs of nodes\nin undirected graphs, only one orientation of the pair is counted.\n\nParameters\n----------\nG : NetworkX graph\n\nweight : object\n    The edge attribute to use as distance when computing\n    shortest-path distances. This is passed directly to the\n    :func:`networkx.shortest_path_length` function.\n\nReturns\n-------\nfloat\n    The Wiener index of the graph `G`.\n\nRaises\n------\nNetworkXError\n    If the graph `G` is not connected.\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef wiener_index(G, weight=None):\n    is_directed = G.is_directed()\n    if is_directed and (not is_strongly_connected(G)) or (not is_directed and (not is_connected(G))):\n        return float('inf')\n    total = sum(chaini((p.values() for v, p in spl(G, weight=weight))))\n    return total if is_directed else total / 2"
 },
 {
  "docstring": "Returns an approximate maximum independent set.\n\nIndependent set or stable set is a set of vertices in a graph, no two of\nwhich are adjacent. That is, it is a set I of vertices such that for every\ntwo vertices in I, there is no edge connecting the two. Equivalently, each\nedge in the graph has at most one endpoint in I. The size of an independent\nset is the number of vertices it contains [1]_.\n\nA maximum independent set is a largest independent set for a given graph G\nand its size is denoted $\\alpha(G)$. The problem of finding such a set is called\nthe maximum independent set problem and is an NP-hard optimization problem.\nAs such, it is unlikely that there exists an efficient algorithm for finding\na maximum independent set of a graph.\n\nThe Independent Set algorithm is based on [2]_.\n\nParameters\n----------\nG : NetworkX graph\n    Undirected graph\n\nReturns\n-------\niset : Set\n    The apx-maximum independent set\n\nExamples\n--------\n>>> G = nx.path_graph(10)\n>>> nx.approximation.maximum_independent_set(G)\n{0, 2, 4, 6, 9}\n\nRaises\n------\nNetworkXNotImplemented\n    If the graph is directed or is a multigraph.\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef maximum_independent_set(G):\n    iset, _ = clique_removal(G)\n    return iset"
 },
 {
  "docstring": "Find the Maximum Clique\n\nFinds the $O(|V|/(log|V|)^2)$ apx of maximum clique/independent set\nin the worst case.\n\nParameters\n----------\nG : NetworkX graph\n    Undirected graph\n\nReturns\n-------\nclique : set\n    The apx-maximum clique of the graph\n\nExamples\n--------\n>>> G = nx.path_graph(10)\n>>> nx.approximation.max_clique(G)\n{8, 9}\n\nRaises\n------\nNetworkXNotImplemented\n    If the graph is directed or is a multigraph.\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef max_clique(G):\n    cgraph = nx.complement(G)\n    iset, _ = clique_removal(cgraph)\n    return iset"
 },
 {
  "docstring": "Repeatedly remove cliques from the graph.\n\nResults in a $O(|V|/(\\log |V|)^2)$ approximation of maximum clique\nand independent set. Returns the largest independent set found, along\nwith found maximal cliques.\n\nParameters\n----------\nG : NetworkX graph\n    Undirected graph\n\nReturns\n-------\nmax_ind_cliques : (set, list) tuple\n    2-tuple of Maximal Independent Set and list of maximal cliques (sets).\n\nExamples\n--------\n>>> G = nx.path_graph(10)\n>>> nx.approximation.clique_removal(G)\n({0, 2, 4, 6, 9}, [{0, 1}, {2, 3}, {4, 5}, {6, 7}, {8, 9}])\n\nRaises\n------\nNetworkXNotImplemented\n    If the graph is directed or is a multigraph.\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef clique_removal(G):\n    graph = G.copy()\n    c_i, i_i = ramsey.ramsey_R2(graph)\n    cliques = [c_i]\n    isets = [i_i]\n    while graph:\n        graph.remove_nodes_from(c_i)\n        c_i, i_i = ramsey.ramsey_R2(graph)\n        if c_i:\n            cliques.append(c_i)\n        if i_i:\n            isets.append(i_i)\n    maxiset = max(isets, key=len)\n    return (maxiset, cliques)"
 },
 {
  "docstring": "Find the size of a large clique in a graph.\n\nA *clique* is a subset of nodes in which each pair of nodes is\nadjacent. This function is a heuristic for finding the size of a\nlarge clique in the graph.\n\nParameters\n----------\nG : NetworkX graph\n\nReturns\n-------\nk: integer\n   The size of a large clique in the graph.\n\nExamples\n--------\n>>> G = nx.path_graph(10)\n>>> nx.approximation.large_clique_size(G)\n2\n\nRaises\n------\nNetworkXNotImplemented\n    If the graph is directed or is a multigraph.\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef large_clique_size(G):\n    degrees = G.degree\n\n    def _clique_heuristic(G, U, size, best_size):\n        if not U:\n            return max(best_size, size)\n        u = max(U, key=degrees)\n        U.remove(u)\n        N_prime = {v for v in G[u] if degrees[v] >= best_size}\n        return _clique_heuristic(G, U & N_prime, size + 1, best_size)\n    best_size = 0\n    nodes = (u for u in G if degrees[u] >= best_size)\n    for u in nodes:\n        neighbors = {v for v in G[u] if degrees[v] >= best_size}\n        best_size = _clique_heuristic(G, neighbors, 1, best_size)\n    return best_size"
 },
 {
  "docstring": "Estimates the average clustering coefficient of G.\n\nThe local clustering of each node in `G` is the fraction of triangles\nthat actually exist over all possible triangles in its neighborhood.\nThe average clustering coefficient of a graph `G` is the mean of\nlocal clusterings.\n\nThis function finds an approximate average clustering coefficient\nfor G by repeating `n` times (defined in `trials`) the following\nexperiment: choose a node at random, choose two of its neighbors\nat random, and check if they are connected. The approximate\ncoefficient is the fraction of triangles found over the number\nof trials [1]_.\n\nParameters\n----------\nG : NetworkX graph\n\ntrials : integer\n    Number of trials to perform (default 1000).\n\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\nReturns\n-------\nc : float\n    Approximated average clustering coefficient.\n\nExamples\n--------\n>>> from networkx.algorithms import approximation\n>>> G = nx.erdos_renyi_graph(10, 0.2, seed=10)\n>>> approximation.average_clustering(G, trials=1000, seed=10)\n0.214\n\n",
  "code": "@not_implemented_for('directed')\n@py_random_state(2)\n@nx._dispatch(name='approximate_average_clustering')\ndef average_clustering(G, trials=1000, seed=None):\n    n = len(G)\n    triangles = 0\n    nodes = list(G)\n    for i in [int(seed.random() * n) for i in range(trials)]:\n        nbrs = list(G[nodes[i]])\n        if len(nbrs) < 2:\n            continue\n        u, v = seed.sample(nbrs, 2)\n        if u in G[v]:\n            triangles += 1\n    return triangles / trials"
 },
 {
  "docstring": "Compute node connectivity between source and target.\n\nPairwise or local node connectivity between two distinct and nonadjacent\nnodes is the minimum number of nodes that must be removed (minimum\nseparating cutset) to disconnect them. By Menger's theorem, this is equal\nto the number of node independent paths (paths that share no nodes other\nthan source and target). Which is what we compute in this function.\n\nThis algorithm is a fast approximation that gives an strict lower\nbound on the actual number of node independent paths between two nodes [1]_.\nIt works for both directed and undirected graphs.\n\nParameters\n----------\n\nG : NetworkX graph\n\nsource : node\n    Starting node for node connectivity\n\ntarget : node\n    Ending node for node connectivity\n\ncutoff : integer\n    Maximum node connectivity to consider. If None, the minimum degree\n    of source or target is used as a cutoff. Default value None.\n\nReturns\n-------\nk: integer\n   pairwise node connectivity\n\nExamples\n--------\n>>> # Platonic octahedral graph has node connectivity 4\n>>> # for each non adjacent node pair\n>>> from networkx.algorithms import approximation as approx\n>>> G = nx.octahedral_graph()\n>>> approx.local_node_connectivity(G, 0, 5)\n4\n\n",
  "code": "@nx._dispatch(name='approximate_local_node_connectivity')\ndef local_node_connectivity(G, source, target, cutoff=None):\n    if target == source:\n        raise nx.NetworkXError('source and target have to be different nodes.')\n    if G.is_directed():\n        possible = min(G.out_degree(source), G.in_degree(target))\n    else:\n        possible = min(G.degree(source), G.degree(target))\n    K = 0\n    if not possible:\n        return K\n    if cutoff is None:\n        cutoff = float('inf')\n    exclude = set()\n    for i in range(min(possible, cutoff)):\n        try:\n            path = _bidirectional_shortest_path(G, source, target, exclude)\n            exclude.update(set(path))\n            K += 1\n        except nx.NetworkXNoPath:\n            break\n    return K"
 },
 {
  "docstring": "Returns an approximation for node connectivity for a graph or digraph G.\n\nNode connectivity is equal to the minimum number of nodes that\nmust be removed to disconnect G or render it trivial. By Menger's theorem,\nthis is equal to the number of node independent paths (paths that\nshare no nodes other than source and target).\n\nIf source and target nodes are provided, this function returns the\nlocal node connectivity: the minimum number of nodes that must be\nremoved to break all paths from source to target in G.\n\nThis algorithm is based on a fast approximation that gives an strict lower\nbound on the actual number of node independent paths between two nodes [1]_.\nIt works for both directed and undirected graphs.\n\nParameters\n----------\nG : NetworkX graph\n    Undirected graph\n\ns : node\n    Source node. Optional. Default value: None.\n\nt : node\n    Target node. Optional. Default value: None.\n\nReturns\n-------\nK : integer\n    Node connectivity of G, or local node connectivity if source\n    and target are provided.\n\nExamples\n--------\n>>> # Platonic octahedral graph is 4-node-connected\n>>> from networkx.algorithms import approximation as approx\n>>> G = nx.octahedral_graph()\n>>> approx.node_connectivity(G)\n4\n\n",
  "code": "@nx._dispatch(name='approximate_node_connectivity')\ndef node_connectivity(G, s=None, t=None):\n    if s is not None and t is None or (s is None and t is not None):\n        raise nx.NetworkXError('Both source and target must be specified.')\n    if s is not None and t is not None:\n        if s not in G:\n            raise nx.NetworkXError(f'node {s} not in graph')\n        if t not in G:\n            raise nx.NetworkXError(f'node {t} not in graph')\n        return local_node_connectivity(G, s, t)\n    if G.is_directed():\n        connected_func = nx.is_weakly_connected\n        iter_func = itertools.permutations\n\n        def neighbors(v):\n            return itertools.chain(G.predecessors(v), G.successors(v))\n    else:\n        connected_func = nx.is_connected\n        iter_func = itertools.combinations\n        neighbors = G.neighbors\n    if not connected_func(G):\n        return 0\n    v, minimum_degree = min(G.degree(), key=itemgetter(1))\n    K = minimum_degree\n    for w in set(G) - set(neighbors(v)) - {v}:\n        K = min(K, local_node_connectivity(G, v, w, cutoff=K))\n    for x, y in iter_func(neighbors(v), 2):\n        if y not in G[x] and x != y:\n            K = min(K, local_node_connectivity(G, x, y, cutoff=K))\n    return K"
 },
 {
  "docstring": "Compute node connectivity between all pairs of nodes.\n\nPairwise or local node connectivity between two distinct and nonadjacent\nnodes is the minimum number of nodes that must be removed (minimum\nseparating cutset) to disconnect them. By Menger's theorem, this is equal\nto the number of node independent paths (paths that share no nodes other\nthan source and target). Which is what we compute in this function.\n\nThis algorithm is a fast approximation that gives an strict lower\nbound on the actual number of node independent paths between two nodes [1]_.\nIt works for both directed and undirected graphs.\n\n\nParameters\n----------\nG : NetworkX graph\n\nnbunch: container\n    Container of nodes. If provided node connectivity will be computed\n    only over pairs of nodes in nbunch.\n\ncutoff : integer\n    Maximum node connectivity to consider. If None, the minimum degree\n    of source or target is used as a cutoff in each pair of nodes.\n    Default value None.\n\nReturns\n-------\nK : dictionary\n    Dictionary, keyed by source and target, of pairwise node connectivity\n\nExamples\n--------\nA 3 node cycle with one extra node attached has connectivity 2 between all\nnodes in the cycle and connectivity 1 between the extra node and the rest:\n\n>>> G = nx.cycle_graph(3)\n>>> G.add_edge(2, 3)\n>>> import pprint  # for nice dictionary formatting\n>>> pprint.pprint(nx.all_pairs_node_connectivity(G))\n{0: {1: 2, 2: 2, 3: 1},\n 1: {0: 2, 2: 2, 3: 1},\n 2: {0: 2, 1: 2, 3: 1},\n 3: {0: 1, 1: 1, 2: 1}}\n\nSee Also\n--------\nlocal_node_connectivity\nnode_connectivity\n\n",
  "code": "@nx._dispatch(name='approximate_all_pairs_node_connectivity')\ndef all_pairs_node_connectivity(G, nbunch=None, cutoff=None):\n    if nbunch is None:\n        nbunch = G\n    else:\n        nbunch = set(nbunch)\n    directed = G.is_directed()\n    if directed:\n        iter_func = itertools.permutations\n    else:\n        iter_func = itertools.combinations\n    all_pairs = {n: {} for n in nbunch}\n    for u, v in iter_func(nbunch, 2):\n        k = local_node_connectivity(G, u, v, cutoff=cutoff)\n        all_pairs[u][v] = k\n        if not directed:\n            all_pairs[v][u] = k\n    return all_pairs"
 },
 {
  "docstring": "Returns shortest path between source and target ignoring nodes in the\ncontainer 'exclude'.\n\nParameters\n----------\n\nG : NetworkX graph\n\nsource : node\n    Starting node for path\n\ntarget : node\n    Ending node for path\n\nexclude: container\n    Container for nodes to exclude from the search for shortest paths\n\nReturns\n-------\npath: list\n    Shortest path between source and target ignoring nodes in 'exclude'\n\nRaises\n------\nNetworkXNoPath\n    If there is no path or if nodes are adjacent and have only one path\n    between them\n\n",
  "code": "def _bidirectional_shortest_path(G, source, target, exclude):\n    results = _bidirectional_pred_succ(G, source, target, exclude)\n    pred, succ, w = results\n    path = []\n    while w is not None:\n        path.append(w)\n        w = pred[w]\n    path.reverse()\n    w = succ[path[-1]]\n    while w is not None:\n        path.append(w)\n        w = succ[w]\n    return path"
 },
 {
  "docstring": "Returns a lower bound on the diameter of the graph G.\n\nThe function computes a lower bound on the diameter (i.e., the maximum eccentricity)\nof a directed or undirected graph G. The procedure used varies depending on the graph\nbeing directed or not.\n\nIf G is an `undirected` graph, then the function uses the `2-sweep` algorithm [1]_.\nThe main idea is to pick the farthest node from a random node and return its eccentricity.\n\nOtherwise, if G is a `directed` graph, the function uses the `2-dSweep` algorithm [2]_,\nThe procedure starts by selecting a random source node $s$ from which it performs a\nforward and a backward BFS. Let $a_1$ and $a_2$ be the farthest nodes in the forward and\nbackward cases, respectively. Then, it computes the backward eccentricity of $a_1$ using\na backward BFS and the forward eccentricity of $a_2$ using a forward BFS.\nFinally, it returns the best lower bound between the two.\n\nIn both cases, the time complexity is linear with respect to the size of G.\n\nParameters\n----------\nG : NetworkX graph\n\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\nReturns\n-------\nd : integer\n   Lower Bound on the Diameter of G\n\nRaises\n------\nNetworkXError\n    If the graph is empty or\n    If the graph is undirected and not connected or\n    If the graph is directed and not strongly connected.\n\nSee Also\n--------\nnetworkx.algorithms.distance_measures.diameter\n\n",
  "code": "@py_random_state(1)\n@nx._dispatch(name='approximate_diameter')\ndef diameter(G, seed=None):\n    if not G:\n        raise nx.NetworkXError('Expected non-empty NetworkX graph!')\n    if G.number_of_nodes() == 1:\n        return 0\n    if G.is_directed():\n        return _two_sweep_directed(G, seed)\n    return _two_sweep_undirected(G, seed)"
 },
 {
  "docstring": "Helper function for finding a lower bound on the diameter\n    for undirected Graphs.\n\n    The idea is to pick the farthest node from a random node\n    and return its eccentricity.\n\n    ``G`` is a NetworkX undirected graph.\n\n.. note::\n\n    ``seed`` is a random.Random or numpy.random.RandomState instance",
  "code": "def _two_sweep_undirected(G, seed):\n    source = seed.choice(list(G))\n    distances = nx.shortest_path_length(G, source)\n    if len(distances) != len(G):\n        raise nx.NetworkXError('Graph not connected.')\n    *_, node = distances\n    return nx.eccentricity(G, node)"
 },
 {
  "docstring": "Helper function for finding a lower bound on the diameter\n    for directed Graphs.\n\n    It implements 2-dSweep, the directed version of the 2-sweep algorithm.\n    The algorithm follows the following steps.\n    1. Select a source node $s$ at random.\n    2. Perform a forward BFS from $s$ to select a node $a_1$ at the maximum\n    distance from the source, and compute $LB_1$, the backward eccentricity of $a_1$.\n    3. Perform a backward BFS from $s$ to select a node $a_2$ at the maximum\n    distance from the source, and compute $LB_2$, the forward eccentricity of $a_2$.\n    4. Return the maximum between $LB_1$ and $LB_2$.\n\n    ``G`` is a NetworkX directed graph.\n\n.. note::\n\n    ``seed`` is a random.Random or numpy.random.RandomState instance",
  "code": "def _two_sweep_directed(G, seed):\n    G_reversed = G.reverse()\n    source = seed.choice(list(G))\n    forward_distances = nx.shortest_path_length(G, source)\n    backward_distances = nx.shortest_path_length(G_reversed, source)\n    n = len(G)\n    if len(forward_distances) != n or len(backward_distances) != n:\n        raise nx.NetworkXError('DiGraph not strongly connected.')\n    *_, a_1 = forward_distances\n    *_, a_2 = backward_distances\n    return max(nx.eccentricity(G_reversed, a_1), nx.eccentricity(G, a_2))"
 },
 {
  "docstring": "Returns a dominating set that approximates the minimum weight node\ndominating set.\n\nParameters\n----------\nG : NetworkX graph\n    Undirected graph.\n\nweight : string\n    The node attribute storing the weight of an node. If provided,\n    the node attribute with this key must be a number for each\n    node. If not provided, each node is assumed to have weight one.\n\nReturns\n-------\nmin_weight_dominating_set : set\n    A set of nodes, the sum of whose weights is no more than `(\\log\n    w(V)) w(V^*)`, where `w(V)` denotes the sum of the weights of\n    each node in the graph and `w(V^*)` denotes the sum of the\n    weights of each node in the minimum weight dominating set.\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch(node_attrs='weight')\ndef min_weighted_dominating_set(G, weight=None):\n    if len(G) == 0:\n        return set()\n    dom_set = set()\n\n    def _cost(node_and_neighborhood):\n        \"\"\"Returns the cost-effectiveness of greedily choosing the given\n        node.\n\n        `node_and_neighborhood` is a two-tuple comprising a node and its\n        closed neighborhood.\n\n        \"\"\"\n        v, neighborhood = node_and_neighborhood\n        return G.nodes[v].get(weight, 1) / len(neighborhood - dom_set)\n    vertices = set(G)\n    neighborhoods = {v: {v} | set(G[v]) for v in G}\n    while vertices:\n        dom_node, min_set = min(neighborhoods.items(), key=_cost)\n        dom_set.add(dom_node)\n        del neighborhoods[dom_node]\n        vertices -= min_set\n    return dom_set"
 },
 {
  "docstring": "Returns minimum cardinality edge dominating set.\n\nParameters\n----------\nG : NetworkX graph\n  Undirected graph\n\nReturns\n-------\nmin_edge_dominating_set : set\n  Returns a set of dominating edges whose size is no more than 2 * OPT.\n\n",
  "code": "@nx._dispatch\ndef min_edge_dominating_set(G):\n    if not G:\n        raise ValueError('Expected non-empty NetworkX graph!')\n    return maximal_matching(G)"
 },
 {
  "docstring": "Returns the cost-effectiveness of greedily choosing the given\nnode.\n\n`node_and_neighborhood` is a two-tuple comprising a node and its\nclosed neighborhood.",
  "code": "def _cost(node_and_neighborhood):\n    v, neighborhood = node_and_neighborhood\n    return G.nodes[v].get(weight, 1) / len(neighborhood - dom_set)"
 },
 {
  "docstring": "Returns the approximate k-component structure of a graph G.\n\nA `k`-component is a maximal subgraph of a graph G that has, at least,\nnode connectivity `k`: we need to remove at least `k` nodes to break it\ninto more components. `k`-components have an inherent hierarchical\nstructure because they are nested in terms of connectivity: a connected\ngraph can contain several 2-components, each of which can contain\none or more 3-components, and so forth.\n\nThis implementation is based on the fast heuristics to approximate\nthe `k`-component structure of a graph [1]_. Which, in turn, it is based on\na fast approximation algorithm for finding good lower bounds of the number\nof node independent paths between two nodes [2]_.\n\nParameters\n----------\nG : NetworkX graph\n    Undirected graph\n\nmin_density : Float\n    Density relaxation threshold. Default value 0.95\n\nReturns\n-------\nk_components : dict\n    Dictionary with connectivity level `k` as key and a list of\n    sets of nodes that form a k-component of level `k` as values.\n\nRaises\n------\nNetworkXNotImplemented\n    If G is directed.\n\nExamples\n--------\n>>> # Petersen graph has 10 nodes and it is triconnected, thus all\n>>> # nodes are in a single component on all three connectivity levels\n>>> from networkx.algorithms import approximation as apxa\n>>> G = nx.petersen_graph()\n>>> k_components = apxa.k_components(G)\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch(name='approximate_k_components')\ndef k_components(G, min_density=0.95):\n    k_components = defaultdict(list)\n    node_connectivity = local_node_connectivity\n    k_core = nx.k_core\n    core_number = nx.core_number\n    biconnected_components = nx.biconnected_components\n    combinations = itertools.combinations\n    for component in nx.connected_components(G):\n        comp = set(component)\n        if len(comp) > 1:\n            k_components[1].append(comp)\n    for bicomponent in nx.biconnected_components(G):\n        bicomp = set(bicomponent)\n        if len(bicomp) > 2:\n            k_components[2].append(bicomp)\n    g_cnumber = core_number(G)\n    max_core = max(g_cnumber.values())\n    for k in range(3, max_core + 1):\n        C = k_core(G, k, core_number=g_cnumber)\n        for nodes in biconnected_components(C):\n            if len(nodes) < k:\n                continue\n            SG = G.subgraph(nodes)\n            H = _AntiGraph()\n            H.add_nodes_from(SG.nodes())\n            for u, v in combinations(SG, 2):\n                K = node_connectivity(SG, u, v, cutoff=k)\n                if k > K:\n                    H.add_edge(u, v)\n            for h_nodes in biconnected_components(H):\n                if len(h_nodes) <= k:\n                    continue\n                SH = H.subgraph(h_nodes)\n                for Gc in _cliques_heuristic(SG, SH, k, min_density):\n                    for k_nodes in biconnected_components(Gc):\n                        Gk = nx.k_core(SG.subgraph(k_nodes), k)\n                        if len(Gk) <= k:\n                            continue\n                        k_components[k].append(set(Gk))\n    return k_components"
 },
 {
  "docstring": "Returns a dict of neighbors of node n in the dense graph.\n\nParameters\n----------\nn : node\n   A node in the graph.\n\nReturns\n-------\nadj_dict : dictionary\n   The adjacency dictionary for nodes connected to n.",
  "code": "def __getitem__(self, n):\n    all_edge_dict = self.all_edge_dict\n    return {node: all_edge_dict for node in set(self._adj) - set(self._adj[n]) - {n}}"
 },
 {
  "docstring": "Returns an iterator over all neighbors of node n in the\ndense graph.",
  "code": "def neighbors(self, n):\n    try:\n        return iter(set(self._adj) - set(self._adj[n]) - {n})\n    except KeyError as err:\n        raise NetworkXError(f'The node {n} is not in the graph.') from err"
 },
 {
  "docstring": "This subgraph method returns a full AntiGraph. Not a View",
  "code": "def subgraph(self, nodes):\n    nodes = set(nodes)\n    G = _AntiGraph()\n    G.add_nodes_from(nodes)\n    for n in G:\n        Gnbrs = G.adjlist_inner_dict_factory()\n        G._adj[n] = Gnbrs\n        for nbr, d in self._adj[n].items():\n            if nbr in G._adj:\n                Gnbrs[nbr] = d\n                G._adj[nbr][n] = d\n    G.graph = self.graph\n    return G"
 },
 {
  "docstring": "Returns an iterator for (node, degree) and degree for single node.\n\nThe node degree is the number of edges adjacent to the node.\n\nParameters\n----------\nnbunch : iterable container, optional (default=all nodes)\n    A container of nodes.  The container will be iterated\n    through once.\n\nweight : string or None, optional (default=None)\n   The edge attribute that holds the numerical value used\n   as a weight.  If None, then each edge has weight 1.\n   The degree is the sum of the edge weights adjacent to the node.\n\nReturns\n-------\ndeg:\n    Degree of the node, if a single node is passed as argument.\nnd_iter : an iterator\n    The iterator returns two-tuples of (node, degree).\n\nSee Also\n--------\ndegree\n\nExamples\n--------\n>>> G = nx.path_graph(4)\n>>> G.degree(0)  # node 0 with degree 1\n1\n>>> list(G.degree([0, 1]))\n[(0, 1), (1, 2)]",
  "code": "@cached_property\ndef degree(self):\n    return self.AntiDegreeView(self)"
 },
 {
  "docstring": "Returns an iterator of (node, adjacency set) tuples for all nodes\n   in the dense graph.\n\nThis is the fastest way to look at every edge.\nFor directed graphs, only outgoing adjacencies are included.\n\nReturns\n-------\nadj_iter : iterator\n   An iterator of (node, adjacency set) for all nodes in\n   the graph.",
  "code": "def adjacency(self):\n    for n in self._adj:\n        yield (n, set(self._adj) - set(self._adj[n]) - {n})"
 },
 {
  "docstring": "Returns the minimum maximal matching of G. That is, out of all maximal\nmatchings of the graph G, the smallest is returned.\n\nParameters\n----------\nG : NetworkX graph\n  Undirected graph\n\nReturns\n-------\nmin_maximal_matching : set\n  Returns a set of edges such that no two edges share a common endpoint\n  and every edge not in the set shares some common endpoint in the set.\n  Cardinality will be 2*OPT in the worst case.\n\n",
  "code": "@nx._dispatch\ndef min_maximal_matching(G):\n    return nx.maximal_matching(G)"
 },
 {
  "docstring": "Compute a random partitioning of the graph nodes and its cut value.\n\nA partitioning is calculated by observing each node\nand deciding to add it to the partition with probability `p`,\nreturning a random cut and its corresponding value (the\nsum of weights of edges connecting different partitions).\n\nParameters\n----------\nG : NetworkX graph\n\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\np : scalar\n    Probability for each node to be part of the first partition.\n    Should be in [0,1]\n\nweight : object\n    Edge attribute key to use as weight. If not specified, edges\n    have weight one.\n\nReturns\n-------\ncut_size : scalar\n    Value of the minimum cut.\n\npartition : pair of node sets\n    A partitioning of the nodes that defines a minimum cut.",
  "code": "@not_implemented_for('directed', 'multigraph')\n@py_random_state(1)\n@nx._dispatch(edge_attrs='weight')\ndef randomized_partitioning(G, seed=None, p=0.5, weight=None):\n    cut = {node for node in G.nodes() if seed.random() < p}\n    cut_size = nx.algorithms.cut_size(G, cut, weight=weight)\n    partition = (cut, G.nodes - cut)\n    return (cut_size, partition)"
 },
 {
  "docstring": "Compute a partitioning of the graphs nodes and the corresponding cut value.\n\nUse a greedy one exchange strategy to find a locally maximal cut\nand its value, it works by finding the best node (one that gives\nthe highest gain to the cut value) to add to the current cut\nand repeats this process until no improvement can be made.\n\nParameters\n----------\nG : networkx Graph\n    Graph to find a maximum cut for.\n\ninitial_cut : set\n    Cut to use as a starting point. If not supplied the algorithm\n    starts with an empty cut.\n\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\nweight : object\n    Edge attribute key to use as weight. If not specified, edges\n    have weight one.\n\nReturns\n-------\ncut_value : scalar\n    Value of the maximum cut.\n\npartition : pair of node sets\n    A partitioning of the nodes that defines a maximum cut.",
  "code": "@not_implemented_for('directed', 'multigraph')\n@py_random_state(2)\n@nx._dispatch(edge_attrs='weight')\ndef one_exchange(G, initial_cut=None, seed=None, weight=None):\n    if initial_cut is None:\n        initial_cut = set()\n    cut = set(initial_cut)\n    current_cut_size = nx.algorithms.cut_size(G, cut, weight=weight)\n    while True:\n        nodes = list(G.nodes())\n        seed.shuffle(nodes)\n        best_node_to_swap = max(nodes, key=lambda v: nx.algorithms.cut_size(G, _swap_node_partition(cut, v), weight=weight), default=None)\n        potential_cut = _swap_node_partition(cut, best_node_to_swap)\n        potential_cut_size = nx.algorithms.cut_size(G, potential_cut, weight=weight)\n        if potential_cut_size > current_cut_size:\n            cut = potential_cut\n            current_cut_size = potential_cut_size\n        else:\n            break\n    partition = (cut, G.nodes - cut)\n    return (current_cut_size, partition)"
 },
 {
  "docstring": "Compute the largest clique and largest independent set in `G`.\n\nThis can be used to estimate bounds for the 2-color\nRamsey number `R(2;s,t)` for `G`.\n\nThis is a recursive implementation which could run into trouble\nfor large recursions. Note that self-loop edges are ignored.\n\nParameters\n----------\nG : NetworkX graph\n    Undirected graph\n\nReturns\n-------\nmax_pair : (set, set) tuple\n    Maximum clique, Maximum independent set.\n\nRaises\n------\nNetworkXNotImplemented\n    If the graph is directed or is a multigraph.",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef ramsey_R2(G):\n    if not G:\n        return (set(), set())\n    node = arbitrary_element(G)\n    nbrs = (nbr for nbr in nx.all_neighbors(G, node) if nbr != node)\n    nnbrs = nx.non_neighbors(G, node)\n    c_1, i_1 = ramsey_R2(G.subgraph(nbrs).copy())\n    c_2, i_2 = ramsey_R2(G.subgraph(nnbrs).copy())\n    c_1.add(node)\n    i_2.add(node)\n    return (max(c_1, c_2, key=len), max(i_1, i_2, key=len))"
 },
 {
  "docstring": "Return the metric closure of a graph.\n\nThe metric closure of a graph *G* is the complete graph in which each edge\nis weighted by the shortest path distance between the nodes in *G* .\n\nParameters\n----------\nG : NetworkX graph\n\nReturns\n-------\nNetworkX graph\n    Metric closure of the graph `G`.",
  "code": "@not_implemented_for('directed')\n@nx._dispatch(edge_attrs='weight')\ndef metric_closure(G, weight='weight'):\n    M = nx.Graph()\n    Gnodes = set(G)\n    all_paths_iter = nx.all_pairs_dijkstra(G, weight=weight)\n    u, (distance, path) = next(all_paths_iter)\n    if Gnodes - set(distance):\n        msg = 'G is not a connected graph. metric_closure is not defined.'\n        raise nx.NetworkXError(msg)\n    Gnodes.remove(u)\n    for v in Gnodes:\n        M.add_edge(u, v, distance=distance[v], path=path[v])\n    for u, (distance, path) in all_paths_iter:\n        Gnodes.remove(u)\n        for v in Gnodes:\n            M.add_edge(u, v, distance=distance[v], path=path[v])\n    return M"
 },
 {
  "docstring": "Return an approximation to the minimum Steiner tree of a graph.\n\nThe minimum Steiner tree of `G` w.r.t a set of `terminal_nodes` (also *S*)\nis a tree within `G` that spans those nodes and has minimum size (sum of\nedge weights) among all such trees.\n\nThe approximation algorithm is specified with the `method` keyword\nargument. All three available algorithms produce a tree whose weight is\nwithin a ``(2 - (2 / l))`` factor of the weight of the optimal Steiner tree,\nwhere ``l`` is the minimum number of leaf nodes across all possible Steiner\ntrees.\n\n* ``\"kou\"`` [2]_ (runtime $O(|S| |V|^2)$) computes the minimum spanning tree of\n  the subgraph of the metric closure of *G* induced by the terminal nodes,\n  where the metric closure of *G* is the complete graph in which each edge is\n  weighted by the shortest path distance between the nodes in *G*.\n\n* ``\"mehlhorn\"`` [3]_ (runtime $O(|E|+|V|\\log|V|)$) modifies Kou et al.'s\n  algorithm, beginning by finding the closest terminal node for each\n  non-terminal. This data is used to create a complete graph containing only\n  the terminal nodes, in which edge is weighted with the shortest path\n  distance between them. The algorithm then proceeds in the same way as Kou\n  et al..\n\nParameters\n----------\nG : NetworkX graph\n\nterminal_nodes : list\n     A list of terminal nodes for which minimum steiner tree is\n     to be found.\n\nweight : string (default = 'weight')\n    Use the edge attribute specified by this string as the edge weight.\n    Any edge attribute not present defaults to 1.\n\nmethod : string, optional (default = 'kou')\n    The algorithm to use to approximate the Steiner tree.\n    Supported options: 'kou', 'mehlhorn'.\n    Other inputs produce a ValueError.\n\nReturns\n-------\nNetworkX graph\n    Approximation to the minimum steiner tree of `G` induced by\n    `terminal_nodes` .\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch(edge_attrs='weight')\ndef steiner_tree(G, terminal_nodes, weight='weight', method=None):\n    if method is None:\n        import warnings\n        msg = \"steiner_tree will change default method from 'kou' to 'mehlhorn' in version 3.2.\\nSet the `method` kwarg to remove this warning.\"\n        warnings.warn(msg, FutureWarning, stacklevel=4)\n        method = 'kou'\n    try:\n        algo = ALGORITHMS[method]\n    except KeyError as e:\n        msg = f'{method} is not a valid choice for an algorithm.'\n        raise ValueError(msg) from e\n    edges = algo(G, terminal_nodes, weight)\n    if G.is_multigraph():\n        edges = ((u, v, min(G[u][v], key=lambda k: G[u][v][k][weight])) for u, v in edges)\n    T = G.edge_subgraph(edges)\n    return T"
 },
 {
  "docstring": "Swap two nodes in `soln` to give a neighbor solution.\n\nParameters\n----------\nsoln : list of nodes\n    Current cycle of nodes\n\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\nReturns\n-------\nlist\n    The solution after move is applied. (A neighbor solution.)\n\n",
  "code": "def swap_two_nodes(soln, seed):\n    a, b = seed.sample(range(1, len(soln) - 1), k=2)\n    soln[a], soln[b] = (soln[b], soln[a])\n    return soln"
 },
 {
  "docstring": "Move one node to another position to give a neighbor solution.\n\nThe node to move and the position to move to are chosen randomly.\nThe first and last nodes are left untouched as soln must be a cycle\nstarting at that node.\n\nParameters\n----------\nsoln : list of nodes\n    Current cycle of nodes\n\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\nReturns\n-------\nlist\n    The solution after move is applied. (A neighbor solution.)\n\n",
  "code": "def move_one_node(soln, seed):\n    a, b = seed.sample(range(1, len(soln) - 1), k=2)\n    soln.insert(b, soln.pop(a))\n    return soln"
 },
 {
  "docstring": "Approximate a solution of the traveling salesman problem\n\nCompute a 3/2-approximation of the traveling salesman problem\nin a complete undirected graph using Christofides [1]_ algorithm.\n\nParameters\n----------\nG : Graph\n    `G` should be a complete weighted undirected graph.\n    The distance between all pairs of nodes should be included.\n\nweight : string, optional (default=\"weight\")\n    Edge data key corresponding to the edge weight.\n    If any edge does not have this attribute the weight is set to 1.\n\ntree : NetworkX graph or None (default: None)\n    A minimum spanning tree of G. Or, if None, the minimum spanning\n    tree is computed using :func:`networkx.minimum_spanning_tree`\n\nReturns\n-------\nlist\n    List of nodes in `G` along a cycle with a 3/2-approximation of\n    the minimal Hamiltonian cycle.\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch(edge_attrs='weight')\ndef christofides(G, weight='weight', tree=None):\n    loop_nodes = nx.nodes_with_selfloops(G)\n    try:\n        node = next(loop_nodes)\n    except StopIteration:\n        pass\n    else:\n        G = G.copy()\n        G.remove_edge(node, node)\n        G.remove_edges_from(((n, n) for n in loop_nodes))\n    N = len(G) - 1\n    if any((len(nbrdict) != N for n, nbrdict in G.adj.items())):\n        raise nx.NetworkXError('G must be a complete graph.')\n    if tree is None:\n        tree = nx.minimum_spanning_tree(G, weight=weight)\n    L = G.copy()\n    L.remove_nodes_from([v for v, degree in tree.degree if not degree % 2])\n    MG = nx.MultiGraph()\n    MG.add_edges_from(tree.edges)\n    edges = nx.min_weight_matching(L, weight=weight)\n    MG.add_edges_from(edges)\n    return _shortcutting(nx.eulerian_circuit(MG))"
 },
 {
  "docstring": "Remove duplicate nodes in the path",
  "code": "def _shortcutting(circuit):\n    nodes = []\n    for u, v in circuit:\n        if v in nodes:\n            continue\n        if not nodes:\n            nodes.append(u)\n        nodes.append(v)\n    nodes.append(nodes[0])\n    return nodes"
 },
 {
  "docstring": "Find the shortest path in `G` connecting specified nodes\n\nThis function allows approximate solution to the traveling salesman\nproblem on networks that are not complete graphs and/or where the\nsalesman does not need to visit all nodes.\n\nThis function proceeds in two steps. First, it creates a complete\ngraph using the all-pairs shortest_paths between nodes in `nodes`.\nEdge weights in the new graph are the lengths of the paths\nbetween each pair of nodes in the original graph.\nSecond, an algorithm (default: `christofides` for undirected and\n`asadpour_atsp` for directed) is used to approximate the minimal Hamiltonian\ncycle on this new graph. The available algorithms are:\n\n - christofides\n - greedy_tsp\n - simulated_annealing_tsp\n - threshold_accepting_tsp\n - asadpour_atsp\n\nOnce the Hamiltonian Cycle is found, this function post-processes to\naccommodate the structure of the original graph. If `cycle` is ``False``,\nthe biggest weight edge is removed to make a Hamiltonian path.\nThen each edge on the new complete graph used for that analysis is\nreplaced by the shortest_path between those nodes on the original graph.\nIf the input graph `G` includes edges with weights that do not adhere to\nthe triangle inequality, such as when `G` is not a complete graph (i.e\nlength of non-existent edges is infinity), then the returned path may\ncontain some repeating nodes (other than the starting node).\n\nParameters\n----------\nG : NetworkX graph\n    A possibly weighted graph\n\nnodes : collection of nodes (default=G.nodes)\n    collection (list, set, etc.) of nodes to visit\n\nweight : string, optional (default=\"weight\")\n    Edge data key corresponding to the edge weight.\n    If any edge does not have this attribute the weight is set to 1.\n\ncycle : bool (default: True)\n    Indicates whether a cycle should be returned, or a path.\n    Note: the cycle is the approximate minimal cycle.\n    The path simply removes the biggest edge in that cycle.\n\nmethod : function (default: None)\n    A function that returns a cycle on all nodes and approximates\n    the solution to the traveling salesman problem on a complete\n    graph. The returned cycle is then used to find a corresponding\n    solution on `G`. `method` should be callable; take inputs\n    `G`, and `weight`; and return a list of nodes along the cycle.\n\n    Provided options include :func:`christofides`, :func:`greedy_tsp`,\n    :func:`simulated_annealing_tsp` and :func:`threshold_accepting_tsp`.\n\n    If `method is None`: use :func:`christofides` for undirected `G` and\n    :func:`threshold_accepting_tsp` for directed `G`.\n\n    To specify parameters for these provided functions, construct lambda\n    functions that state the specific value. `method` must have 2 inputs.\n    (See examples).\n\nReturns\n-------\nlist\n    List of nodes in `G` along a path with an approximation of the minimal\n    path through `nodes`.\n\nRaises\n------\nNetworkXError\n    If `G` is a directed graph it has to be strongly connected or the\n    complete version cannot be generated.\n\nExamples\n--------\n>>> tsp = nx.approximation.traveling_salesman_problem\n>>> G = nx.cycle_graph(9)\n>>> G[4][5][\"weight\"] = 5  # all other weights are 1\n>>> tsp(G, nodes=[3, 6])\n[3, 2, 1, 0, 8, 7, 6, 7, 8, 0, 1, 2, 3]\n>>> path = tsp(G, cycle=False)\n>>> path in ([4, 3, 2, 1, 0, 8, 7, 6, 5], [5, 6, 7, 8, 0, 1, 2, 3, 4])\nTrue\n\nBuild (curry) your own function to provide parameter values to the methods.\n\n>>> SA_tsp = nx.approximation.simulated_annealing_tsp\n>>> method = lambda G, wt: SA_tsp(G, \"greedy\", weight=wt, temp=500)\n>>> path = tsp(G, cycle=False, method=method)\n>>> path in ([4, 3, 2, 1, 0, 8, 7, 6, 5], [5, 6, 7, 8, 0, 1, 2, 3, 4])\nTrue",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef traveling_salesman_problem(G, weight='weight', nodes=None, cycle=True, method=None):\n    if method is None:\n        if G.is_directed():\n            method = asadpour_atsp\n        else:\n            method = christofides\n    if nodes is None:\n        nodes = list(G.nodes)\n    dist = {}\n    path = {}\n    for n, (d, p) in nx.all_pairs_dijkstra(G, weight=weight):\n        dist[n] = d\n        path[n] = p\n    if G.is_directed():\n        if not nx.is_strongly_connected(G):\n            raise nx.NetworkXError('G is not strongly connected')\n        GG = nx.DiGraph()\n    else:\n        GG = nx.Graph()\n    for u in nodes:\n        for v in nodes:\n            if u == v:\n                continue\n            GG.add_edge(u, v, weight=dist[u][v])\n    best_GG = method(GG, weight)\n    if not cycle:\n        u, v = max(pairwise(best_GG), key=lambda x: dist[x[0]][x[1]])\n        pos = best_GG.index(u) + 1\n        while best_GG[pos] != v:\n            pos = best_GG[pos:].index(u) + 1\n        best_GG = best_GG[pos:-1] + best_GG[:pos]\n    best_path = []\n    for u, v in pairwise(best_GG):\n        best_path.extend(path[u][v][:-1])\n    best_path.append(v)\n    return best_path"
 },
 {
  "docstring": "Returns an approximate solution to the traveling salesman problem.\n\nThis approximate solution is one of the best known approximations for the\nasymmetric traveling salesman problem developed by Asadpour et al,\n[1]_. The algorithm first solves the Held-Karp relaxation to find a lower\nbound for the weight of the cycle. Next, it constructs an exponential\ndistribution of undirected spanning trees where the probability of an\nedge being in the tree corresponds to the weight of that edge using a\nmaximum entropy rounding scheme. Next we sample that distribution\n$2 \\lceil \\ln n \\rceil$ times and save the minimum sampled tree once the\ndirection of the arcs is added back to the edges. Finally, we augment\nthen short circuit that graph to find the approximate tour for the\nsalesman.\n\nParameters\n----------\nG : nx.DiGraph\n    The graph should be a complete weighted directed graph. The\n    distance between all paris of nodes should be included and the triangle\n    inequality should hold. That is, the direct edge between any two nodes\n    should be the path of least cost.\n\nweight : string, optional (default=\"weight\")\n    Edge data key corresponding to the edge weight.\n    If any edge does not have this attribute the weight is set to 1.\n\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\nsource : node label (default=`None`)\n    If given, return the cycle starting and ending at the given node.\n\nReturns\n-------\ncycle : list of nodes\n    Returns the cycle (list of nodes) that a salesman can follow to minimize\n    the total weight of the trip.\n\nRaises\n------\nNetworkXError\n    If `G` is not complete or has less than two nodes, the algorithm raises\n    an exception.\n\nNetworkXError\n    If `source` is not `None` and is not a node in `G`, the algorithm raises\n    an exception.\n\nNetworkXNotImplemented\n    If `G` is an undirected graph.\n\n",
  "code": "@not_implemented_for('undirected')\n@py_random_state(2)\n@nx._dispatch(edge_attrs='weight')\ndef asadpour_atsp(G, weight='weight', seed=None, source=None):\n    from math import ceil, exp\n    from math import log as ln\n    N = len(G) - 1\n    if N < 2:\n        raise nx.NetworkXError('G must have at least two nodes')\n    if any((len(nbrdict) - (n in nbrdict) != N for n, nbrdict in G.adj.items())):\n        raise nx.NetworkXError('G is not a complete DiGraph')\n    if source is not None and source not in G.nodes:\n        raise nx.NetworkXError('Given source node not in G.')\n    opt_hk, z_star = held_karp_ascent(G, weight)\n    if not isinstance(z_star, dict):\n        return _shortcutting(nx.eulerian_circuit(z_star, source=source))\n    z_support = nx.MultiGraph()\n    for u, v in z_star:\n        if (u, v) not in z_support.edges:\n            edge_weight = min(G[u][v][weight], G[v][u][weight])\n            z_support.add_edge(u, v, **{weight: edge_weight})\n    gamma = spanning_tree_distribution(z_support, z_star)\n    z_support = nx.Graph(z_support)\n    lambda_dict = {(u, v): exp(gamma[u, v]) for u, v in z_support.edges()}\n    nx.set_edge_attributes(z_support, lambda_dict, 'weight')\n    del gamma, lambda_dict\n    minimum_sampled_tree = None\n    minimum_sampled_tree_weight = math.inf\n    for _ in range(2 * ceil(ln(G.number_of_nodes()))):\n        sampled_tree = random_spanning_tree(z_support, 'weight', seed=seed)\n        sampled_tree_weight = sampled_tree.size(weight)\n        if sampled_tree_weight < minimum_sampled_tree_weight:\n            minimum_sampled_tree = sampled_tree.copy()\n            minimum_sampled_tree_weight = sampled_tree_weight\n    t_star = nx.MultiDiGraph()\n    for u, v, d in minimum_sampled_tree.edges(data=weight):\n        if d == G[u][v][weight]:\n            t_star.add_edge(u, v, **{weight: d})\n        else:\n            t_star.add_edge(v, u, **{weight: d})\n    node_demands = {n: t_star.out_degree(n) - t_star.in_degree(n) for n in t_star}\n    nx.set_node_attributes(G, node_demands, 'demand')\n    flow_dict = nx.min_cost_flow(G, 'demand')\n    for source, values in flow_dict.items():\n        for target in values:\n            if (source, target) not in t_star.edges and values[target] > 0:\n                for _ in range(values[target]):\n                    t_star.add_edge(source, target)\n    circuit = nx.eulerian_circuit(t_star, source=source)\n    return _shortcutting(circuit)"
 },
 {
  "docstring": "Minimizes the Held-Karp relaxation of the TSP for `G`\n\nSolves the Held-Karp relaxation of the input complete digraph and scales\nthe output solution for use in the Asadpour [1]_ ASTP algorithm.\n\nThe Held-Karp relaxation defines the lower bound for solutions to the\nATSP, although it does return a fractional solution. This is used in the\nAsadpour algorithm as an initial solution which is later rounded to a\nintegral tree within the spanning tree polytopes. This function solves\nthe relaxation with the branch and bound method in [2]_.\n\nParameters\n----------\nG : nx.DiGraph\n    The graph should be a complete weighted directed graph.\n    The distance between all paris of nodes should be included.\n\nweight : string, optional (default=\"weight\")\n    Edge data key corresponding to the edge weight.\n    If any edge does not have this attribute the weight is set to 1.\n\nReturns\n-------\nOPT : float\n    The cost for the optimal solution to the Held-Karp relaxation\nz : dict or nx.Graph\n    A symmetrized and scaled version of the optimal solution to the\n    Held-Karp relaxation for use in the Asadpour algorithm.\n\n    If an integral solution is found, then that is an optimal solution for\n    the ATSP problem and that is returned instead.\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef held_karp_ascent(G, weight='weight'):\n    import numpy as np\n    from scipy import optimize\n\n    def k_pi():\n        \"\"\"\n        Find the set of minimum 1-Arborescences for G at point pi.\n\n        Returns\n        -------\n        Set\n            The set of minimum 1-Arborescences\n        \"\"\"\n        G_1 = G.copy()\n        minimum_1_arborescences = set()\n        minimum_1_arborescence_weight = math.inf\n        n = next(G.__iter__())\n        G_1.remove_node(n)\n        min_root = {'node': None, weight: math.inf}\n        max_root = {'node': None, weight: -math.inf}\n        for u, v, d in G.edges(n, data=True):\n            if d[weight] < min_root[weight]:\n                min_root = {'node': v, weight: d[weight]}\n            if d[weight] > max_root[weight]:\n                max_root = {'node': v, weight: d[weight]}\n        min_in_edge = min(G.in_edges(n, data=True), key=lambda x: x[2][weight])\n        min_root[weight] = min_root[weight] + min_in_edge[2][weight]\n        max_root[weight] = max_root[weight] + min_in_edge[2][weight]\n        min_arb_weight = math.inf\n        for arb in nx.ArborescenceIterator(G_1):\n            arb_weight = arb.size(weight)\n            if min_arb_weight == math.inf:\n                min_arb_weight = arb_weight\n            elif arb_weight > min_arb_weight + max_root[weight] - min_root[weight]:\n                break\n            for N, deg in arb.in_degree:\n                if deg == 0:\n                    arb.add_edge(n, N, **{weight: G[n][N][weight]})\n                    arb_weight += G[n][N][weight]\n                    break\n            edge_data = G[N][n]\n            G.remove_edge(N, n)\n            min_weight = min(G.in_edges(n, data=weight), key=lambda x: x[2])[2]\n            min_edges = [(u, v, d) for u, v, d in G.in_edges(n, data=weight) if d == min_weight]\n            for u, v, d in min_edges:\n                new_arb = arb.copy()\n                new_arb.add_edge(u, v, **{weight: d})\n                new_arb_weight = arb_weight + d\n                if new_arb_weight < minimum_1_arborescence_weight:\n                    minimum_1_arborescences.clear()\n                    minimum_1_arborescence_weight = new_arb_weight\n                if new_arb_weight == minimum_1_arborescence_weight:\n                    minimum_1_arborescences.add(new_arb)\n            G.add_edge(N, n, **edge_data)\n        return minimum_1_arborescences\n\n    def direction_of_ascent():\n        \"\"\"\n        Find the direction of ascent at point pi.\n\n        See [1]_ for more information.\n\n        Returns\n        -------\n        dict\n            A mapping from the nodes of the graph which represents the direction\n            of ascent.\n\n        References\n        ----------\n        .. [1] M. Held, R. M. Karp, The traveling-salesman problem and minimum\n           spanning trees, Operations Research, 1970-11-01, Vol. 18 (6),\n           pp.1138-1162\n        \"\"\"\n        d = {}\n        for n in G:\n            d[n] = 0\n        del n\n        minimum_1_arborescences = k_pi()\n        while True:\n            min_k_d_weight = math.inf\n            min_k_d = None\n            for arborescence in minimum_1_arborescences:\n                weighted_cost = 0\n                for n, deg in arborescence.degree:\n                    weighted_cost += d[n] * (deg - 2)\n                if weighted_cost < min_k_d_weight:\n                    min_k_d_weight = weighted_cost\n                    min_k_d = arborescence\n            if min_k_d_weight > 0:\n                return (d, min_k_d)\n            for n, deg in min_k_d.degree:\n                d[n] += deg - 2\n            c = np.full(len(minimum_1_arborescences), -1, dtype=int)\n            a_eq = np.empty((len(G) + 1, len(minimum_1_arborescences)), dtype=int)\n            b_eq = np.zeros(len(G) + 1, dtype=int)\n            b_eq[len(G)] = 1\n            for arb_count, arborescence in enumerate(minimum_1_arborescences):\n                n_count = len(G) - 1\n                for n, deg in arborescence.degree:\n                    a_eq[n_count][arb_count] = deg - 2\n                    n_count -= 1\n                a_eq[len(G)][arb_count] = 1\n            program_result = optimize.linprog(c, A_eq=a_eq, b_eq=b_eq)\n            if program_result.success:\n                return (None, minimum_1_arborescences)\n\n    def find_epsilon(k, d):\n        \"\"\"\n        Given the direction of ascent at pi, find the maximum distance we can go\n        in that direction.\n\n        Parameters\n        ----------\n        k_xy : set\n            The set of 1-arborescences which have the minimum rate of increase\n            in the direction of ascent\n\n        d : dict\n            The direction of ascent\n\n        Returns\n        -------\n        float\n            The distance we can travel in direction `d`\n        \"\"\"\n        min_epsilon = math.inf\n        for e_u, e_v, e_w in G.edges(data=weight):\n            if (e_u, e_v) in k.edges:\n                continue\n            if len(k.in_edges(e_v, data=weight)) > 1:\n                raise Exception\n            sub_u, sub_v, sub_w = next(k.in_edges(e_v, data=weight).__iter__())\n            k.add_edge(e_u, e_v, **{weight: e_w})\n            k.remove_edge(sub_u, sub_v)\n            if max((d for n, d in k.in_degree())) <= 1 and len(G) == k.number_of_edges() and nx.is_weakly_connected(k):\n                if d[sub_u] == d[e_u] or sub_w == e_w:\n                    k.remove_edge(e_u, e_v)\n                    k.add_edge(sub_u, sub_v, **{weight: sub_w})\n                    continue\n                epsilon = (sub_w - e_w) / (d[e_u] - d[sub_u])\n                if 0 < epsilon < min_epsilon:\n                    min_epsilon = epsilon\n            k.remove_edge(e_u, e_v)\n            k.add_edge(sub_u, sub_v, **{weight: sub_w})\n        return min_epsilon\n    pi_dict = {}\n    for n in G:\n        pi_dict[n] = 0\n    del n\n    original_edge_weights = {}\n    for u, v, d in G.edges(data=True):\n        original_edge_weights[u, v] = d[weight]\n    dir_ascent, k_d = direction_of_ascent()\n    while dir_ascent is not None:\n        max_distance = find_epsilon(k_d, dir_ascent)\n        for n, v in dir_ascent.items():\n            pi_dict[n] += max_distance * v\n        for u, v, d in G.edges(data=True):\n            d[weight] = original_edge_weights[u, v] + pi_dict[u]\n        dir_ascent, k_d = direction_of_ascent()\n    k_max = k_d\n    for k in k_max:\n        if len([n for n in k if k.degree(n) == 2]) == G.order():\n            return (k.size(weight), k)\n    x_star = {}\n    size_k_max = len(k_max)\n    for u, v, d in G.edges(data=True):\n        edge_count = 0\n        d[weight] = original_edge_weights[u, v]\n        for k in k_max:\n            if (u, v) in k.edges():\n                edge_count += 1\n                k[u][v][weight] = original_edge_weights[u, v]\n        x_star[u, v] = edge_count / size_k_max\n    z_star = {}\n    scale_factor = (G.order() - 1) / G.order()\n    for u, v in x_star:\n        frequency = x_star[u, v] + x_star[v, u]\n        if frequency > 0:\n            z_star[u, v] = scale_factor * frequency\n    del x_star\n    return (next(k_max.__iter__()).size(weight), z_star)"
 },
 {
  "docstring": "Find the asadpour exponential distribution of spanning trees.\n\nSolves the Maximum Entropy Convex Program in the Asadpour algorithm [1]_\nusing the approach in section 7 to build an exponential distribution of\nundirected spanning trees.\n\nThis algorithm ensures that the probability of any edge in a spanning\ntree is proportional to the sum of the probabilities of the tress\ncontaining that edge over the sum of the probabilities of all spanning\ntrees of the graph.\n\nParameters\n----------\nG : nx.MultiGraph\n    The undirected support graph for the Held Karp relaxation\n\nz : dict\n    The output of `held_karp_ascent()`, a scaled version of the Held-Karp\n    solution.\n\nReturns\n-------\ngamma : dict\n    The probability distribution which approximately preserves the marginal\n    probabilities of `z`.",
  "code": "@nx._dispatch\ndef spanning_tree_distribution(G, z):\n    from math import exp\n    from math import log as ln\n\n    def q(e):\n        \"\"\"\n        The value of q(e) is described in the Asadpour paper is \"the\n        probability that edge e will be included in a spanning tree T that is\n        chosen with probability proportional to exp(gamma(T))\" which\n        basically means that it is the total probability of the edge appearing\n        across the whole distribution.\n\n        Parameters\n        ----------\n        e : tuple\n            The `(u, v)` tuple describing the edge we are interested in\n\n        Returns\n        -------\n        float\n            The probability that a spanning tree chosen according to the\n            current values of gamma will include edge `e`.\n        \"\"\"\n        for u, v, d in G.edges(data=True):\n            d[lambda_key] = exp(gamma[u, v])\n        G_Kirchhoff = nx.total_spanning_tree_weight(G, lambda_key)\n        G_e = nx.contracted_edge(G, e, self_loops=False)\n        G_e_Kirchhoff = nx.total_spanning_tree_weight(G_e, lambda_key)\n        return exp(gamma[e[0], e[1]]) * G_e_Kirchhoff / G_Kirchhoff\n    gamma = {}\n    for u, v, _ in G.edges:\n        gamma[u, v] = 0\n    EPSILON = 0.2\n    lambda_key = \"spanning_tree_distribution's secret attribute name for lambda\"\n    while True:\n        in_range_count = 0\n        for u, v in gamma:\n            e = (u, v)\n            q_e = q(e)\n            z_e = z[e]\n            if q_e > (1 + EPSILON) * z_e:\n                delta = ln(q_e * (1 - (1 + EPSILON / 2) * z_e) / ((1 - q_e) * (1 + EPSILON / 2) * z_e))\n                gamma[e] -= delta\n                new_q_e = q(e)\n                desired_q_e = (1 + EPSILON / 2) * z_e\n                if round(new_q_e, 8) != round(desired_q_e, 8):\n                    raise nx.NetworkXError(f'Unable to modify probability for edge ({u}, {v})')\n            else:\n                in_range_count += 1\n        if in_range_count == len(gamma):\n            break\n    for _, _, d in G.edges(data=True):\n        if lambda_key in d:\n            del d[lambda_key]\n    return gamma"
 },
 {
  "docstring": "Return a low cost cycle starting at `source` and its cost.\n\nThis approximates a solution to the traveling salesman problem.\nIt finds a cycle of all the nodes that a salesman can visit in order\nto visit many nodes while minimizing total distance.\nIt uses a simple greedy algorithm.\nIn essence, this function returns a large cycle given a source point\nfor which the total cost of the cycle is minimized.\n\nParameters\n----------\nG : Graph\n    The Graph should be a complete weighted undirected graph.\n    The distance between all pairs of nodes should be included.\n\nweight : string, optional (default=\"weight\")\n    Edge data key corresponding to the edge weight.\n    If any edge does not have this attribute the weight is set to 1.\n\nsource : node, optional (default: first node in list(G))\n    Starting node.  If None, defaults to ``next(iter(G))``\n\nReturns\n-------\ncycle : list of nodes\n    Returns the cycle (list of nodes) that a salesman\n    can follow to minimize total weight of the trip.\n\nRaises\n------\nNetworkXError\n    If `G` is not complete, the algorithm raises an exception.\n\nExamples\n--------\n>>> from networkx.algorithms import approximation as approx\n>>> G = nx.DiGraph()\n>>> G.add_weighted_edges_from({\n...     (\"A\", \"B\", 3), (\"A\", \"C\", 17), (\"A\", \"D\", 14), (\"B\", \"A\", 3),\n...     (\"B\", \"C\", 12), (\"B\", \"D\", 16), (\"C\", \"A\", 13),(\"C\", \"B\", 12),\n...     (\"C\", \"D\", 4), (\"D\", \"A\", 14), (\"D\", \"B\", 15), (\"D\", \"C\", 2)\n... })\n>>> cycle = approx.greedy_tsp(G, source=\"D\")\n>>> cost = sum(G[n][nbr][\"weight\"] for n, nbr in nx.utils.pairwise(cycle))\n>>> cycle\n['D', 'C', 'B', 'A', 'D']\n>>> cost\n31\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef greedy_tsp(G, weight='weight', source=None):\n    N = len(G) - 1\n    if any((len(nbrdict) - (n in nbrdict) != N for n, nbrdict in G.adj.items())):\n        raise nx.NetworkXError('G must be a complete graph.')\n    if source is None:\n        source = nx.utils.arbitrary_element(G)\n    if G.number_of_nodes() == 2:\n        neighbor = next(G.neighbors(source))\n        return [source, neighbor, source]\n    nodeset = set(G)\n    nodeset.remove(source)\n    cycle = [source]\n    next_node = source\n    while nodeset:\n        nbrdict = G[next_node]\n        next_node = min(nodeset, key=lambda n: nbrdict[n].get(weight, 1))\n        cycle.append(next_node)\n        nodeset.remove(next_node)\n    cycle.append(cycle[0])\n    return cycle"
 },
 {
  "docstring": "Returns an approximate solution to the traveling salesman problem.\n\nThis function uses simulated annealing to approximate the minimal cost\ncycle through the nodes. Starting from a suboptimal solution, simulated\nannealing perturbs that solution, occasionally accepting changes that make\nthe solution worse to escape from a locally optimal solution. The chance\nof accepting such changes decreases over the iterations to encourage\nan optimal result.  In summary, the function returns a cycle starting\nat `source` for which the total cost is minimized. It also returns the cost.\n\nThe chance of accepting a proposed change is related to a parameter called\nthe temperature (annealing has a physical analogue of steel hardening\nas it cools). As the temperature is reduced, the chance of moves that\nincrease cost goes down.\n\nParameters\n----------\nG : Graph\n    `G` should be a complete weighted graph.\n    The distance between all pairs of nodes should be included.\n\ninit_cycle : list of all nodes or \"greedy\"\n    The initial solution (a cycle through all nodes returning to the start).\n    This argument has no default to make you think about it.\n    If \"greedy\", use `greedy_tsp(G, weight)`.\n    Other common starting cycles are `list(G) + [next(iter(G))]` or the final\n    result of `simulated_annealing_tsp` when doing `threshold_accepting_tsp`.\n\nweight : string, optional (default=\"weight\")\n    Edge data key corresponding to the edge weight.\n    If any edge does not have this attribute the weight is set to 1.\n\nsource : node, optional (default: first node in list(G))\n    Starting node.  If None, defaults to ``next(iter(G))``\n\ntemp : int, optional (default=100)\n    The algorithm's temperature parameter. It represents the initial\n    value of temperature\n\nmove : \"1-1\" or \"1-0\" or function, optional (default=\"1-1\")\n    Indicator of what move to use when finding new trial solutions.\n    Strings indicate two special built-in moves:\n\n    - \"1-1\": 1-1 exchange which transposes the position\n      of two elements of the current solution.\n      The function called is :func:`swap_two_nodes`.\n      For example if we apply 1-1 exchange in the solution\n      ``A = [3, 2, 1, 4, 3]``\n      we can get the following by the transposition of 1 and 4 elements:\n      ``A' = [3, 2, 4, 1, 3]``\n    - \"1-0\": 1-0 exchange which moves an node in the solution\n      to a new position.\n      The function called is :func:`move_one_node`.\n      For example if we apply 1-0 exchange in the solution\n      ``A = [3, 2, 1, 4, 3]``\n      we can transfer the fourth element to the second position:\n      ``A' = [3, 4, 2, 1, 3]``\n\n    You may provide your own functions to enact a move from\n    one solution to a neighbor solution. The function must take\n    the solution as input along with a `seed` input to control\n    random number generation (see the `seed` input here).\n    Your function should maintain the solution as a cycle with\n    equal first and last node and all others appearing once.\n    Your function should return the new solution.\n\nmax_iterations : int, optional (default=10)\n    Declared done when this number of consecutive iterations of\n    the outer loop occurs without any change in the best cost solution.\n\nN_inner : int, optional (default=100)\n    The number of iterations of the inner loop.\n\nalpha : float between (0, 1), optional (default=0.01)\n    Percentage of temperature decrease in each iteration\n    of outer loop\n\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\nReturns\n-------\ncycle : list of nodes\n    Returns the cycle (list of nodes) that a salesman\n    can follow to minimize total weight of the trip.\n\nRaises\n------\nNetworkXError\n    If `G` is not complete the algorithm raises an exception.\n\nExamples\n--------\n>>> from networkx.algorithms import approximation as approx\n>>> G = nx.DiGraph()\n>>> G.add_weighted_edges_from({\n...     (\"A\", \"B\", 3), (\"A\", \"C\", 17), (\"A\", \"D\", 14), (\"B\", \"A\", 3),\n...     (\"B\", \"C\", 12), (\"B\", \"D\", 16), (\"C\", \"A\", 13),(\"C\", \"B\", 12),\n...     (\"C\", \"D\", 4), (\"D\", \"A\", 14), (\"D\", \"B\", 15), (\"D\", \"C\", 2)\n... })\n>>> cycle = approx.simulated_annealing_tsp(G, \"greedy\", source=\"D\")\n>>> cost = sum(G[n][nbr][\"weight\"] for n, nbr in nx.utils.pairwise(cycle))\n>>> cycle\n['D', 'C', 'B', 'A', 'D']\n>>> cost\n31\n>>> incycle = [\"D\", \"B\", \"A\", \"C\", \"D\"]\n>>> cycle = approx.simulated_annealing_tsp(G, incycle, source=\"D\")\n>>> cost = sum(G[n][nbr][\"weight\"] for n, nbr in nx.utils.pairwise(cycle))\n>>> cycle\n['D', 'C', 'B', 'A', 'D']\n>>> cost\n31\n\n",
  "code": "@py_random_state(9)\n@nx._dispatch(edge_attrs='weight')\ndef simulated_annealing_tsp(G, init_cycle, weight='weight', source=None, temp=100, move='1-1', max_iterations=10, N_inner=100, alpha=0.01, seed=None):\n    if move == '1-1':\n        move = swap_two_nodes\n    elif move == '1-0':\n        move = move_one_node\n    if init_cycle == 'greedy':\n        cycle = greedy_tsp(G, weight=weight, source=source)\n        if G.number_of_nodes() == 2:\n            return cycle\n    else:\n        cycle = list(init_cycle)\n        if source is None:\n            source = cycle[0]\n        elif source != cycle[0]:\n            raise nx.NetworkXError('source must be first node in init_cycle')\n        if cycle[0] != cycle[-1]:\n            raise nx.NetworkXError('init_cycle must be a cycle. (return to start)')\n        if len(cycle) - 1 != len(G) or len(set(G.nbunch_iter(cycle))) != len(G):\n            raise nx.NetworkXError('init_cycle should be a cycle over all nodes in G.')\n        N = len(G) - 1\n        if any((len(nbrdict) - (n in nbrdict) != N for n, nbrdict in G.adj.items())):\n            raise nx.NetworkXError('G must be a complete graph.')\n        if G.number_of_nodes() == 2:\n            neighbor = next(G.neighbors(source))\n            return [source, neighbor, source]\n    cost = sum((G[u][v].get(weight, 1) for u, v in pairwise(cycle)))\n    count = 0\n    best_cycle = cycle.copy()\n    best_cost = cost\n    while count <= max_iterations and temp > 0:\n        count += 1\n        for i in range(N_inner):\n            adj_sol = move(cycle, seed)\n            adj_cost = sum((G[u][v].get(weight, 1) for u, v in pairwise(adj_sol)))\n            delta = adj_cost - cost\n            if delta <= 0:\n                cycle = adj_sol\n                cost = adj_cost\n                if cost < best_cost:\n                    count = 0\n                    best_cycle = cycle.copy()\n                    best_cost = cost\n            else:\n                p = math.exp(-delta / temp)\n                if p >= seed.random():\n                    cycle = adj_sol\n                    cost = adj_cost\n        temp -= temp * alpha\n    return best_cycle"
 },
 {
  "docstring": "Returns an approximate solution to the traveling salesman problem.\n\nThis function uses threshold accepting methods to approximate the minimal cost\ncycle through the nodes. Starting from a suboptimal solution, threshold\naccepting methods perturb that solution, accepting any changes that make\nthe solution no worse than increasing by a threshold amount. Improvements\nin cost are accepted, but so are changes leading to small increases in cost.\nThis allows the solution to leave suboptimal local minima in solution space.\nThe threshold is decreased slowly as iterations proceed helping to ensure\nan optimum. In summary, the function returns a cycle starting at `source`\nfor which the total cost is minimized.\n\nParameters\n----------\nG : Graph\n    `G` should be a complete weighted graph.\n    The distance between all pairs of nodes should be included.\n\ninit_cycle : list or \"greedy\"\n    The initial solution (a cycle through all nodes returning to the start).\n    This argument has no default to make you think about it.\n    If \"greedy\", use `greedy_tsp(G, weight)`.\n    Other common starting cycles are `list(G) + [next(iter(G))]` or the final\n    result of `simulated_annealing_tsp` when doing `threshold_accepting_tsp`.\n\nweight : string, optional (default=\"weight\")\n    Edge data key corresponding to the edge weight.\n    If any edge does not have this attribute the weight is set to 1.\n\nsource : node, optional (default: first node in list(G))\n    Starting node.  If None, defaults to ``next(iter(G))``\n\nthreshold : int, optional (default=1)\n    The algorithm's threshold parameter. It represents the initial\n    threshold's value\n\nmove : \"1-1\" or \"1-0\" or function, optional (default=\"1-1\")\n    Indicator of what move to use when finding new trial solutions.\n    Strings indicate two special built-in moves:\n\n    - \"1-1\": 1-1 exchange which transposes the position\n      of two elements of the current solution.\n      The function called is :func:`swap_two_nodes`.\n      For example if we apply 1-1 exchange in the solution\n      ``A = [3, 2, 1, 4, 3]``\n      we can get the following by the transposition of 1 and 4 elements:\n      ``A' = [3, 2, 4, 1, 3]``\n    - \"1-0\": 1-0 exchange which moves an node in the solution\n      to a new position.\n      The function called is :func:`move_one_node`.\n      For example if we apply 1-0 exchange in the solution\n      ``A = [3, 2, 1, 4, 3]``\n      we can transfer the fourth element to the second position:\n      ``A' = [3, 4, 2, 1, 3]``\n\n    You may provide your own functions to enact a move from\n    one solution to a neighbor solution. The function must take\n    the solution as input along with a `seed` input to control\n    random number generation (see the `seed` input here).\n    Your function should maintain the solution as a cycle with\n    equal first and last node and all others appearing once.\n    Your function should return the new solution.\n\nmax_iterations : int, optional (default=10)\n    Declared done when this number of consecutive iterations of\n    the outer loop occurs without any change in the best cost solution.\n\nN_inner : int, optional (default=100)\n    The number of iterations of the inner loop.\n\nalpha : float between (0, 1), optional (default=0.1)\n    Percentage of threshold decrease when there is at\n    least one acceptance of a neighbor solution.\n    If no inner loop moves are accepted the threshold remains unchanged.\n\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\nReturns\n-------\ncycle : list of nodes\n    Returns the cycle (list of nodes) that a salesman\n    can follow to minimize total weight of the trip.\n\nRaises\n------\nNetworkXError\n    If `G` is not complete the algorithm raises an exception.\n\nExamples\n--------\n>>> from networkx.algorithms import approximation as approx\n>>> G = nx.DiGraph()\n>>> G.add_weighted_edges_from({\n...     (\"A\", \"B\", 3), (\"A\", \"C\", 17), (\"A\", \"D\", 14), (\"B\", \"A\", 3),\n...     (\"B\", \"C\", 12), (\"B\", \"D\", 16), (\"C\", \"A\", 13),(\"C\", \"B\", 12),\n...     (\"C\", \"D\", 4), (\"D\", \"A\", 14), (\"D\", \"B\", 15), (\"D\", \"C\", 2)\n... })\n>>> cycle = approx.threshold_accepting_tsp(G, \"greedy\", source=\"D\")\n>>> cost = sum(G[n][nbr][\"weight\"] for n, nbr in nx.utils.pairwise(cycle))\n>>> cycle\n['D', 'C', 'B', 'A', 'D']\n>>> cost\n31\n>>> incycle = [\"D\", \"B\", \"A\", \"C\", \"D\"]\n>>> cycle = approx.threshold_accepting_tsp(G, incycle, source=\"D\")\n>>> cost = sum(G[n][nbr][\"weight\"] for n, nbr in nx.utils.pairwise(cycle))\n>>> cycle\n['D', 'C', 'B', 'A', 'D']\n>>> cost\n31\n\n",
  "code": "@py_random_state(9)\n@nx._dispatch(edge_attrs='weight')\ndef threshold_accepting_tsp(G, init_cycle, weight='weight', source=None, threshold=1, move='1-1', max_iterations=10, N_inner=100, alpha=0.1, seed=None):\n    if move == '1-1':\n        move = swap_two_nodes\n    elif move == '1-0':\n        move = move_one_node\n    if init_cycle == 'greedy':\n        cycle = greedy_tsp(G, weight=weight, source=source)\n        if G.number_of_nodes() == 2:\n            return cycle\n    else:\n        cycle = list(init_cycle)\n        if source is None:\n            source = cycle[0]\n        elif source != cycle[0]:\n            raise nx.NetworkXError('source must be first node in init_cycle')\n        if cycle[0] != cycle[-1]:\n            raise nx.NetworkXError('init_cycle must be a cycle. (return to start)')\n        if len(cycle) - 1 != len(G) or len(set(G.nbunch_iter(cycle))) != len(G):\n            raise nx.NetworkXError('init_cycle is not all and only nodes.')\n        N = len(G) - 1\n        if any((len(nbrdict) - (n in nbrdict) != N for n, nbrdict in G.adj.items())):\n            raise nx.NetworkXError('G must be a complete graph.')\n        if G.number_of_nodes() == 2:\n            neighbor = list(G.neighbors(source))[0]\n            return [source, neighbor, source]\n    cost = sum((G[u][v].get(weight, 1) for u, v in pairwise(cycle)))\n    count = 0\n    best_cycle = cycle.copy()\n    best_cost = cost\n    while count <= max_iterations:\n        count += 1\n        accepted = False\n        for i in range(N_inner):\n            adj_sol = move(cycle, seed)\n            adj_cost = sum((G[u][v].get(weight, 1) for u, v in pairwise(adj_sol)))\n            delta = adj_cost - cost\n            if delta <= threshold:\n                accepted = True\n                cycle = adj_sol\n                cost = adj_cost\n                if cost < best_cost:\n                    count = 0\n                    best_cycle = cycle.copy()\n                    best_cost = cost\n        if accepted:\n            threshold -= threshold * alpha\n    return best_cycle"
 },
 {
  "docstring": "Find the set of minimum 1-Arborescences for G at point pi.\n\nReturns\n-------\nSet\n    The set of minimum 1-Arborescences",
  "code": "def k_pi():\n    G_1 = G.copy()\n    minimum_1_arborescences = set()\n    minimum_1_arborescence_weight = math.inf\n    n = next(G.__iter__())\n    G_1.remove_node(n)\n    min_root = {'node': None, weight: math.inf}\n    max_root = {'node': None, weight: -math.inf}\n    for u, v, d in G.edges(n, data=True):\n        if d[weight] < min_root[weight]:\n            min_root = {'node': v, weight: d[weight]}\n        if d[weight] > max_root[weight]:\n            max_root = {'node': v, weight: d[weight]}\n    min_in_edge = min(G.in_edges(n, data=True), key=lambda x: x[2][weight])\n    min_root[weight] = min_root[weight] + min_in_edge[2][weight]\n    max_root[weight] = max_root[weight] + min_in_edge[2][weight]\n    min_arb_weight = math.inf\n    for arb in nx.ArborescenceIterator(G_1):\n        arb_weight = arb.size(weight)\n        if min_arb_weight == math.inf:\n            min_arb_weight = arb_weight\n        elif arb_weight > min_arb_weight + max_root[weight] - min_root[weight]:\n            break\n        for N, deg in arb.in_degree:\n            if deg == 0:\n                arb.add_edge(n, N, **{weight: G[n][N][weight]})\n                arb_weight += G[n][N][weight]\n                break\n        edge_data = G[N][n]\n        G.remove_edge(N, n)\n        min_weight = min(G.in_edges(n, data=weight), key=lambda x: x[2])[2]\n        min_edges = [(u, v, d) for u, v, d in G.in_edges(n, data=weight) if d == min_weight]\n        for u, v, d in min_edges:\n            new_arb = arb.copy()\n            new_arb.add_edge(u, v, **{weight: d})\n            new_arb_weight = arb_weight + d\n            if new_arb_weight < minimum_1_arborescence_weight:\n                minimum_1_arborescences.clear()\n                minimum_1_arborescence_weight = new_arb_weight\n            if new_arb_weight == minimum_1_arborescence_weight:\n                minimum_1_arborescences.add(new_arb)\n        G.add_edge(N, n, **edge_data)\n    return minimum_1_arborescences"
 },
 {
  "docstring": "Find the direction of ascent at point pi.\n\nSee [1]_ for more information.\n\nReturns\n-------\ndict\n    A mapping from the nodes of the graph which represents the direction\n    of ascent.\n\n",
  "code": "def direction_of_ascent():\n    d = {}\n    for n in G:\n        d[n] = 0\n    del n\n    minimum_1_arborescences = k_pi()\n    while True:\n        min_k_d_weight = math.inf\n        min_k_d = None\n        for arborescence in minimum_1_arborescences:\n            weighted_cost = 0\n            for n, deg in arborescence.degree:\n                weighted_cost += d[n] * (deg - 2)\n            if weighted_cost < min_k_d_weight:\n                min_k_d_weight = weighted_cost\n                min_k_d = arborescence\n        if min_k_d_weight > 0:\n            return (d, min_k_d)\n        for n, deg in min_k_d.degree:\n            d[n] += deg - 2\n        c = np.full(len(minimum_1_arborescences), -1, dtype=int)\n        a_eq = np.empty((len(G) + 1, len(minimum_1_arborescences)), dtype=int)\n        b_eq = np.zeros(len(G) + 1, dtype=int)\n        b_eq[len(G)] = 1\n        for arb_count, arborescence in enumerate(minimum_1_arborescences):\n            n_count = len(G) - 1\n            for n, deg in arborescence.degree:\n                a_eq[n_count][arb_count] = deg - 2\n                n_count -= 1\n            a_eq[len(G)][arb_count] = 1\n        program_result = optimize.linprog(c, A_eq=a_eq, b_eq=b_eq)\n        if program_result.success:\n            return (None, minimum_1_arborescences)"
 },
 {
  "docstring": "Given the direction of ascent at pi, find the maximum distance we can go\nin that direction.\n\nParameters\n----------\nk_xy : set\n    The set of 1-arborescences which have the minimum rate of increase\n    in the direction of ascent\n\nd : dict\n    The direction of ascent\n\nReturns\n-------\nfloat\n    The distance we can travel in direction `d`",
  "code": "def find_epsilon(k, d):\n    min_epsilon = math.inf\n    for e_u, e_v, e_w in G.edges(data=weight):\n        if (e_u, e_v) in k.edges:\n            continue\n        if len(k.in_edges(e_v, data=weight)) > 1:\n            raise Exception\n        sub_u, sub_v, sub_w = next(k.in_edges(e_v, data=weight).__iter__())\n        k.add_edge(e_u, e_v, **{weight: e_w})\n        k.remove_edge(sub_u, sub_v)\n        if max((d for n, d in k.in_degree())) <= 1 and len(G) == k.number_of_edges() and nx.is_weakly_connected(k):\n            if d[sub_u] == d[e_u] or sub_w == e_w:\n                k.remove_edge(e_u, e_v)\n                k.add_edge(sub_u, sub_v, **{weight: sub_w})\n                continue\n            epsilon = (sub_w - e_w) / (d[e_u] - d[sub_u])\n            if 0 < epsilon < min_epsilon:\n                min_epsilon = epsilon\n        k.remove_edge(e_u, e_v)\n        k.add_edge(sub_u, sub_v, **{weight: sub_w})\n    return min_epsilon"
 },
 {
  "docstring": "The value of q(e) is described in the Asadpour paper is \"the\nprobability that edge e will be included in a spanning tree T that is\nchosen with probability proportional to exp(gamma(T))\" which\nbasically means that it is the total probability of the edge appearing\nacross the whole distribution.\n\nParameters\n----------\ne : tuple\n    The `(u, v)` tuple describing the edge we are interested in\n\nReturns\n-------\nfloat\n    The probability that a spanning tree chosen according to the\n    current values of gamma will include edge `e`.",
  "code": "def q(e):\n    for u, v, d in G.edges(data=True):\n        d[lambda_key] = exp(gamma[u, v])\n    G_Kirchhoff = nx.total_spanning_tree_weight(G, lambda_key)\n    G_e = nx.contracted_edge(G, e, self_loops=False)\n    G_e_Kirchhoff = nx.total_spanning_tree_weight(G_e, lambda_key)\n    return exp(gamma[e[0], e[1]]) * G_e_Kirchhoff / G_Kirchhoff"
 },
 {
  "docstring": "Returns a treewidth decomposition using the Minimum Degree heuristic.\n\nThe heuristic chooses the nodes according to their degree, i.e., first\nthe node with the lowest degree is chosen, then the graph is updated\nand the corresponding node is removed. Next, a new node with the lowest\ndegree is chosen, and so on.\n\nParameters\n----------\nG : NetworkX graph\n\nReturns\n-------\nTreewidth decomposition : (int, Graph) tuple\n      2-tuple with treewidth and the corresponding decomposed tree.",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef treewidth_min_degree(G):\n    deg_heuristic = MinDegreeHeuristic(G)\n    return treewidth_decomp(G, lambda graph: deg_heuristic.best_node(graph))"
 },
 {
  "docstring": "Returns a treewidth decomposition using the Minimum Fill-in heuristic.\n\nThe heuristic chooses a node from the graph, where the number of edges\nadded turning the neighbourhood of the chosen node into clique is as\nsmall as possible.\n\nParameters\n----------\nG : NetworkX graph\n\nReturns\n-------\nTreewidth decomposition : (int, Graph) tuple\n    2-tuple with treewidth and the corresponding decomposed tree.",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef treewidth_min_fill_in(G):\n    return treewidth_decomp(G, min_fill_in_heuristic)"
 },
 {
  "docstring": "Implements the Minimum Degree heuristic.\n\nReturns the node from the graph, where the number of edges added when\nturning the neighbourhood of the chosen node into clique is as small as\npossible. This algorithm chooses the nodes using the Minimum Fill-In\nheuristic. The running time of the algorithm is :math:`O(V^3)` and it uses\nadditional constant memory.",
  "code": "def min_fill_in_heuristic(graph):\n    if len(graph) == 0:\n        return None\n    min_fill_in_node = None\n    min_fill_in = sys.maxsize\n    nodes_by_degree = sorted(graph, key=lambda x: len(graph[x]))\n    min_degree = len(graph[nodes_by_degree[0]])\n    if min_degree == len(graph) - 1:\n        return None\n    for node in nodes_by_degree:\n        num_fill_in = 0\n        nbrs = graph[node]\n        for nbr in nbrs:\n            num_fill_in += len(nbrs - graph[nbr]) - 1\n            if num_fill_in >= 2 * min_fill_in:\n                break\n        num_fill_in /= 2\n        if num_fill_in < min_fill_in:\n            if num_fill_in == 0:\n                return node\n            min_fill_in = num_fill_in\n            min_fill_in_node = node\n    return min_fill_in_node"
 },
 {
  "docstring": "Returns a treewidth decomposition using the passed heuristic.\n\nParameters\n----------\nG : NetworkX graph\nheuristic : heuristic function\n\nReturns\n-------\nTreewidth decomposition : (int, Graph) tuple\n    2-tuple with treewidth and the corresponding decomposed tree.",
  "code": "@nx._dispatch\ndef treewidth_decomp(G, heuristic=min_fill_in_heuristic):\n    graph = {n: set(G[n]) - {n} for n in G}\n    node_stack = []\n    elim_node = heuristic(graph)\n    while elim_node is not None:\n        nbrs = graph[elim_node]\n        for u, v in itertools.permutations(nbrs, 2):\n            if v not in graph[u]:\n                graph[u].add(v)\n        node_stack.append((elim_node, nbrs))\n        for u in graph[elim_node]:\n            graph[u].remove(elim_node)\n        del graph[elim_node]\n        elim_node = heuristic(graph)\n    decomp = nx.Graph()\n    first_bag = frozenset(graph.keys())\n    decomp.add_node(first_bag)\n    treewidth = len(first_bag) - 1\n    while node_stack:\n        curr_node, nbrs = node_stack.pop()\n        old_bag = None\n        for bag in decomp.nodes:\n            if nbrs <= bag:\n                old_bag = bag\n                break\n        if old_bag is None:\n            old_bag = first_bag\n        nbrs.add(curr_node)\n        new_bag = frozenset(nbrs)\n        treewidth = max(treewidth, len(new_bag) - 1)\n        decomp.add_edge(old_bag, new_bag)\n    return (treewidth, decomp)"
 },
 {
  "docstring": "Returns an approximate minimum weighted vertex cover.\n\nThe set of nodes returned by this function is guaranteed to be a\nvertex cover, and the total weight of the set is guaranteed to be at\nmost twice the total weight of the minimum weight vertex cover. In\nother words,\n\n.. math::\n\n   w(S) \\leq 2 * w(S^*),\n\nwhere $S$ is the vertex cover returned by this function,\n$S^*$ is the vertex cover of minimum weight out of all vertex\ncovers of the graph, and $w$ is the function that computes the\nsum of the weights of each node in that given set.\n\nParameters\n----------\nG : NetworkX graph\n\nweight : string, optional (default = None)\n    If None, every node has weight 1. If a string, use this node\n    attribute as the node weight. A node without this attribute is\n    assumed to have weight 1.\n\nReturns\n-------\nmin_weighted_cover : set\n    Returns a set of nodes whose weight sum is no more than twice\n    the weight sum of the minimum weight vertex cover.\n\n",
  "code": "@nx._dispatch(node_attrs='weight')\ndef min_weighted_vertex_cover(G, weight=None):\n    cost = dict(G.nodes(data=weight, default=1))\n    cover = set()\n    for u, v in G.edges():\n        if u in cover or v in cover:\n            continue\n        if cost[u] <= cost[v]:\n            cover.add(u)\n            cost[v] -= cost[u]\n        else:\n            cover.add(v)\n            cost[u] -= cost[v]\n    return cover"
 },
 {
  "docstring": "Returns True if and only if `nodes` is a clique in `G`.\n\n`G` is a NetworkX graph. `nodes` is an iterable of nodes in\n`G`.",
  "code": "def is_independent_set(G, nodes):\n    return G.subgraph(nodes).number_of_edges() == 0"
 },
 {
  "docstring": "Returns True if and only if `nodes` is an independent set\nin `G`.\n\n`G` is an undirected simple graph. `nodes` is an iterable of\nnodes in `G`.",
  "code": "def is_clique(G, nodes):\n    H = G.subgraph(nodes)\n    n = len(H)\n    return H.number_of_edges() == n * (n - 1) // 2"
 },
 {
  "docstring": "Tests that the maximal clique is computed according to maximum\ncardinality of the sets.\n\nFor more information, see pull request #1531.",
  "code": "def test_maximal_by_cardinality(self):\n    G = nx.complete_graph(5)\n    G.add_edge(4, 5)\n    clique = max_clique(G)\n    assert len(clique) > 1\n    G = nx.lollipop_graph(30, 2)\n    clique = max_clique(G)\n    assert len(clique) > 2"
 },
 {
  "docstring": "Test empty graph.",
  "code": "def test_null_graph(self):\n    G = nx.null_graph()\n    with pytest.raises(nx.NetworkXError, match='Expected non-empty NetworkX graph!'):\n        diameter(G)"
 },
 {
  "docstring": "Test an undirected disconnected graph.",
  "code": "def test_undirected_non_connected(self):\n    graph = nx.path_graph(10)\n    graph.remove_edge(3, 4)\n    with pytest.raises(nx.NetworkXError, match='Graph not connected.'):\n        diameter(graph)"
 },
 {
  "docstring": "Test a directed non strongly connected graph.",
  "code": "def test_directed_non_strongly_connected(self):\n    graph = nx.path_graph(10, create_using=nx.DiGraph())\n    with pytest.raises(nx.NetworkXError, match='DiGraph not strongly connected.'):\n        diameter(graph)"
 },
 {
  "docstring": "Test a complete undirected graph.",
  "code": "def test_complete_undirected_graph(self):\n    graph = nx.complete_graph(10)\n    assert diameter(graph) == 1"
 },
 {
  "docstring": "Test a complete directed graph.",
  "code": "def test_complete_directed_graph(self):\n    graph = nx.complete_graph(10, create_using=nx.DiGraph())\n    assert diameter(graph) == 1"
 },
 {
  "docstring": "Test an undirected path graph with 10 nodes.",
  "code": "def test_undirected_path_graph(self):\n    graph = nx.path_graph(10)\n    assert diameter(graph) == 9"
 },
 {
  "docstring": "Test a directed path graph with 10 nodes.",
  "code": "def test_directed_path_graph(self):\n    graph = nx.path_graph(10).to_directed()\n    assert diameter(graph) == 9"
 },
 {
  "docstring": "Test a graph which contains just a node.",
  "code": "def test_single_node(self):\n    graph = nx.Graph()\n    graph.add_node(1)\n    assert diameter(graph) == 0"
 },
 {
  "docstring": "Tests that an approximate dominating set for the star graph,\neven when the center node does not have the smallest integer\nlabel, gives just the center node.\n\nFor more information, see #1527.",
  "code": "def test_star_graph(self):\n    G = nx.star_graph(10)\n    G = nx.relabel_nodes(G, {0: 9, 9: 0})\n    assert min_weighted_dominating_set(G) == {9}"
 },
 {
  "docstring": "Tests that the unique dominating set for the null graph is an empty set",
  "code": "def test_null_graph(self):\n    G = nx.Graph()\n    assert min_weighted_dominating_set(G) == set()"
 },
 {
  "docstring": "Test the Held-Karp relaxation with the ascent method",
  "code": "def test_held_karp_ascent():\n    import networkx.algorithms.approximation.traveling_salesman as tsp\n    np = pytest.importorskip('numpy')\n    pytest.importorskip('scipy')\n    G_array = np.array([[0, 97, 60, 73, 17, 52], [97, 0, 41, 52, 90, 30], [60, 41, 0, 21, 35, 41], [73, 52, 21, 0, 95, 46], [17, 90, 35, 95, 0, 81], [52, 30, 41, 46, 81, 0]])\n    solution_edges = [(1, 3), (2, 4), (3, 2), (4, 0), (5, 1), (0, 5)]\n    G = nx.from_numpy_array(G_array, create_using=nx.DiGraph)\n    opt_hk, z_star = tsp.held_karp_ascent(G)\n    assert round(opt_hk, 2) == 207.0\n    solution = nx.DiGraph()\n    solution.add_edges_from(solution_edges)\n    assert nx.utils.edges_equal(z_star.edges, solution.edges)"
 },
 {
  "docstring": "Test the ascent method using a modified version of Figure 2 on page 1140\nin 'The Traveling Salesman Problem and Minimum Spanning Trees' by Held and\nKarp",
  "code": "def test_ascent_fractional_solution():\n    import networkx.algorithms.approximation.traveling_salesman as tsp\n    np = pytest.importorskip('numpy')\n    pytest.importorskip('scipy')\n    G_array = np.array([[0, 100, 100, 100000, 100000, 1], [100, 0, 100, 100000, 1, 100000], [100, 100, 0, 1, 100000, 100000], [100000, 100000, 1, 0, 100, 100], [100000, 1, 100000, 100, 0, 100], [1, 100000, 100000, 100, 100, 0]])\n    solution_z_star = {(0, 1): 5 / 12, (0, 2): 5 / 12, (0, 5): 5 / 6, (1, 0): 5 / 12, (1, 2): 1 / 3, (1, 4): 5 / 6, (2, 0): 5 / 12, (2, 1): 1 / 3, (2, 3): 5 / 6, (3, 2): 5 / 6, (3, 4): 1 / 3, (3, 5): 1 / 2, (4, 1): 5 / 6, (4, 3): 1 / 3, (4, 5): 1 / 2, (5, 0): 5 / 6, (5, 3): 1 / 2, (5, 4): 1 / 2}\n    G = nx.from_numpy_array(G_array, create_using=nx.DiGraph)\n    opt_hk, z_star = tsp.held_karp_ascent(G)\n    assert round(opt_hk, 2) == 303.0\n    assert {key: round(z_star[key], 4) for key in z_star} == {key: round(solution_z_star[key], 4) for key in solution_z_star}"
 },
 {
  "docstring": "Tests the ascent method using a truly asymmetric graph for which the\nsolution has been brute forced",
  "code": "def test_ascent_method_asymmetric():\n    import networkx.algorithms.approximation.traveling_salesman as tsp\n    np = pytest.importorskip('numpy')\n    pytest.importorskip('scipy')\n    G_array = np.array([[0, 26, 63, 59, 69, 31, 41], [62, 0, 91, 53, 75, 87, 47], [47, 82, 0, 90, 15, 9, 18], [68, 19, 5, 0, 58, 34, 93], [11, 58, 53, 55, 0, 61, 79], [88, 75, 13, 76, 98, 0, 40], [41, 61, 55, 88, 46, 45, 0]])\n    solution_edges = [(0, 1), (1, 3), (3, 2), (2, 5), (5, 6), (4, 0), (6, 4)]\n    G = nx.from_numpy_array(G_array, create_using=nx.DiGraph)\n    opt_hk, z_star = tsp.held_karp_ascent(G)\n    assert round(opt_hk, 2) == 190.0\n    solution = nx.DiGraph()\n    solution.add_edges_from(solution_edges)\n    assert nx.utils.edges_equal(z_star.edges, solution.edges)"
 },
 {
  "docstring": "Tests the ascent method using a truly asymmetric graph for which the\nsolution has been brute forced",
  "code": "def test_ascent_method_asymmetric_2():\n    import networkx.algorithms.approximation.traveling_salesman as tsp\n    np = pytest.importorskip('numpy')\n    pytest.importorskip('scipy')\n    G_array = np.array([[0, 45, 39, 92, 29, 31], [72, 0, 4, 12, 21, 60], [81, 6, 0, 98, 70, 53], [49, 71, 59, 0, 98, 94], [74, 95, 24, 43, 0, 47], [56, 43, 3, 65, 22, 0]])\n    solution_edges = [(0, 5), (5, 4), (1, 3), (3, 0), (2, 1), (4, 2)]\n    G = nx.from_numpy_array(G_array, create_using=nx.DiGraph)\n    opt_hk, z_star = tsp.held_karp_ascent(G)\n    assert round(opt_hk, 2) == 144.0\n    solution = nx.DiGraph()\n    solution.add_edges_from(solution_edges)\n    assert nx.utils.edges_equal(z_star.edges, solution.edges)"
 },
 {
  "docstring": "Tests the ascent method using a truly asymmetric graph with a fractional\nsolution for which the solution has been brute forced.\n\nIn this graph their are two different optimal, integral solutions (which\nare also the overall atsp solutions) to the Held Karp relaxation. However,\nthis particular graph has two different tours of optimal value and the\npossible solutions in the held_karp_ascent function are not stored in an\nordered data structure.",
  "code": "def test_held_karp_ascent_asymmetric_3():\n    import networkx.algorithms.approximation.traveling_salesman as tsp\n    np = pytest.importorskip('numpy')\n    pytest.importorskip('scipy')\n    G_array = np.array([[0, 1, 5, 2, 7, 4], [7, 0, 7, 7, 1, 4], [4, 7, 0, 9, 2, 1], [7, 2, 7, 0, 4, 4], [5, 5, 4, 4, 0, 3], [3, 9, 1, 3, 4, 0]])\n    solution1_edges = [(0, 3), (1, 4), (2, 5), (3, 1), (4, 2), (5, 0)]\n    solution2_edges = [(0, 3), (3, 1), (1, 4), (4, 5), (2, 0), (5, 2)]\n    G = nx.from_numpy_array(G_array, create_using=nx.DiGraph)\n    opt_hk, z_star = tsp.held_karp_ascent(G)\n    assert round(opt_hk, 2) == 13.0\n    solution1 = nx.DiGraph()\n    solution1.add_edges_from(solution1_edges)\n    solution2 = nx.DiGraph()\n    solution2.add_edges_from(solution2_edges)\n    assert nx.utils.edges_equal(z_star.edges, solution1.edges) or nx.utils.edges_equal(z_star.edges, solution2.edges)"
 },
 {
  "docstring": "Tests the ascent method using a truly asymmetric graph with a fractional\nsolution for which the solution has been brute forced",
  "code": "def test_held_karp_ascent_fractional_asymmetric():\n    import networkx.algorithms.approximation.traveling_salesman as tsp\n    np = pytest.importorskip('numpy')\n    pytest.importorskip('scipy')\n    G_array = np.array([[0, 100, 150, 100000, 100000, 1], [150, 0, 100, 100000, 1, 100000], [100, 150, 0, 1, 100000, 100000], [100000, 100000, 1, 0, 150, 100], [100000, 2, 100000, 100, 0, 150], [2, 100000, 100000, 150, 100, 0]])\n    solution_z_star = {(0, 1): 5 / 12, (0, 2): 5 / 12, (0, 5): 5 / 6, (1, 0): 5 / 12, (1, 2): 5 / 12, (1, 4): 5 / 6, (2, 0): 5 / 12, (2, 1): 5 / 12, (2, 3): 5 / 6, (3, 2): 5 / 6, (3, 4): 5 / 12, (3, 5): 5 / 12, (4, 1): 5 / 6, (4, 3): 5 / 12, (4, 5): 5 / 12, (5, 0): 5 / 6, (5, 3): 5 / 12, (5, 4): 5 / 12}\n    G = nx.from_numpy_array(G_array, create_using=nx.DiGraph)\n    opt_hk, z_star = tsp.held_karp_ascent(G)\n    assert round(opt_hk, 2) == 304.0\n    assert {key: round(z_star[key], 4) for key in z_star} == {key: round(solution_z_star[key], 4) for key in solution_z_star}"
 },
 {
  "docstring": "Test that we can create an exponential distribution of spanning trees such\nthat the probability of each tree is proportional to the product of edge\nweights.\n\nResults of this test have been confirmed with hypothesis testing from the\ncreated distribution.\n\nThis test uses the symmetric, fractional Held Karp solution.",
  "code": "def test_spanning_tree_distribution():\n    import networkx.algorithms.approximation.traveling_salesman as tsp\n    pytest.importorskip('numpy')\n    pytest.importorskip('scipy')\n    z_star = {(0, 1): 5 / 12, (0, 2): 5 / 12, (0, 5): 5 / 6, (1, 0): 5 / 12, (1, 2): 1 / 3, (1, 4): 5 / 6, (2, 0): 5 / 12, (2, 1): 1 / 3, (2, 3): 5 / 6, (3, 2): 5 / 6, (3, 4): 1 / 3, (3, 5): 1 / 2, (4, 1): 5 / 6, (4, 3): 1 / 3, (4, 5): 1 / 2, (5, 0): 5 / 6, (5, 3): 1 / 2, (5, 4): 1 / 2}\n    solution_gamma = {(0, 1): -0.6383, (0, 2): -0.6827, (0, 5): 0, (1, 2): -1.0781, (1, 4): 0, (2, 3): 0, (5, 3): -0.282, (5, 4): -0.3327, (4, 3): -0.9927}\n    G = nx.MultiGraph()\n    for u, v in z_star:\n        if (u, v) in G.edges or (v, u) in G.edges:\n            continue\n        G.add_edge(u, v)\n    gamma = tsp.spanning_tree_distribution(G, z_star)\n    assert {key: round(gamma[key], 4) for key in gamma} == solution_gamma"
 },
 {
  "docstring": "Test the complete asadpour tsp algorithm with the fractional, symmetric\nHeld Karp solution. This test also uses an incomplete graph as input.",
  "code": "def test_asadpour_tsp():\n    pytest.importorskip('numpy')\n    pytest.importorskip('scipy')\n    edge_list = [(0, 1, 100), (0, 2, 100), (0, 5, 1), (1, 2, 100), (1, 4, 1), (2, 3, 1), (3, 4, 100), (3, 5, 100), (4, 5, 100), (1, 0, 100), (2, 0, 100), (5, 0, 1), (2, 1, 100), (4, 1, 1), (3, 2, 1), (4, 3, 100), (5, 3, 100), (5, 4, 100)]\n    G = nx.DiGraph()\n    G.add_weighted_edges_from(edge_list)\n\n    def fixed_asadpour(G, weight):\n        return nx_app.asadpour_atsp(G, weight, 19)\n    tour = nx_app.traveling_salesman_problem(G, weight='weight', method=fixed_asadpour)\n    expected_tours = [[1, 4, 5, 0, 2, 3, 2, 1], [3, 2, 0, 1, 4, 5, 3]]\n    assert tour in expected_tours"
 },
 {
  "docstring": "This test uses airline prices between the six largest cities in the US.\n\n    * New York City -> JFK\n    * Los Angeles -> LAX\n    * Chicago -> ORD\n    * Houston -> IAH\n    * Phoenix -> PHX\n    * Philadelphia -> PHL\n\nFlight prices from August 2021 using Delta or American airlines to get\nnonstop flight. The brute force solution found the optimal tour to cost $872\n\nThis test also uses the `source` keyword argument to ensure that the tour\nalways starts at city 0.",
  "code": "def test_asadpour_real_world():\n    np = pytest.importorskip('numpy')\n    pytest.importorskip('scipy')\n    G_array = np.array([[0, 243, 199, 208, 169, 183], [277, 0, 217, 123, 127, 252], [297, 197, 0, 197, 123, 177], [303, 169, 197, 0, 117, 117], [257, 127, 160, 117, 0, 319], [183, 332, 217, 117, 319, 0]])\n    node_map = {0: 'JFK', 1: 'LAX', 2: 'ORD', 3: 'IAH', 4: 'PHX', 5: 'PHL'}\n    expected_tours = [['JFK', 'LAX', 'PHX', 'ORD', 'IAH', 'PHL', 'JFK'], ['JFK', 'ORD', 'PHX', 'LAX', 'IAH', 'PHL', 'JFK']]\n    G = nx.from_numpy_array(G_array, create_using=nx.DiGraph)\n    nx.relabel_nodes(G, node_map, copy=False)\n\n    def fixed_asadpour(G, weight):\n        return nx_app.asadpour_atsp(G, weight, 37, source='JFK')\n    tour = nx_app.traveling_salesman_problem(G, weight='weight', method=fixed_asadpour)\n    assert tour in expected_tours"
 },
 {
  "docstring": "This test uses airline prices between the six largest cities in the US. This\ntime using a path, not a cycle.\n\n    * New York City -> JFK\n    * Los Angeles -> LAX\n    * Chicago -> ORD\n    * Houston -> IAH\n    * Phoenix -> PHX\n    * Philadelphia -> PHL\n\nFlight prices from August 2021 using Delta or American airlines to get\nnonstop flight. The brute force solution found the optimal tour to cost $872",
  "code": "def test_asadpour_real_world_path():\n    np = pytest.importorskip('numpy')\n    pytest.importorskip('scipy')\n    G_array = np.array([[0, 243, 199, 208, 169, 183], [277, 0, 217, 123, 127, 252], [297, 197, 0, 197, 123, 177], [303, 169, 197, 0, 117, 117], [257, 127, 160, 117, 0, 319], [183, 332, 217, 117, 319, 0]])\n    node_map = {0: 'JFK', 1: 'LAX', 2: 'ORD', 3: 'IAH', 4: 'PHX', 5: 'PHL'}\n    expected_paths = [['ORD', 'PHX', 'LAX', 'IAH', 'PHL', 'JFK'], ['JFK', 'PHL', 'IAH', 'ORD', 'PHX', 'LAX']]\n    G = nx.from_numpy_array(G_array, create_using=nx.DiGraph)\n    nx.relabel_nodes(G, node_map, copy=False)\n\n    def fixed_asadpour(G, weight):\n        return nx_app.asadpour_atsp(G, weight, 56)\n    path = nx_app.traveling_salesman_problem(G, weight='weight', cycle=False, method=fixed_asadpour)\n    assert path in expected_paths"
 },
 {
  "docstring": "Test that the proper exception is raised when asadpour_atsp is given an\ndisconnected graph.",
  "code": "def test_asadpour_disconnected_graph():\n    G = nx.complete_graph(4, create_using=nx.DiGraph)\n    nx.set_edge_attributes(G, 1, 'weight')\n    G.add_node(5)\n    pytest.raises(nx.NetworkXError, nx_app.asadpour_atsp, G)"
 },
 {
  "docstring": "Test that the proper exception is raised when asadpour_atsp is given an\nincomplete graph",
  "code": "def test_asadpour_incomplete_graph():\n    G = nx.complete_graph(4, create_using=nx.DiGraph)\n    nx.set_edge_attributes(G, 1, 'weight')\n    G.remove_edge(0, 1)\n    pytest.raises(nx.NetworkXError, nx_app.asadpour_atsp, G)"
 },
 {
  "docstring": "Test the asadpour_atsp function with an empty graph",
  "code": "def test_asadpour_empty_graph():\n    G = nx.DiGraph()\n    pytest.raises(nx.NetworkXError, nx_app.asadpour_atsp, G)"
 },
 {
  "docstring": "This test uses an integral held karp solution and the held karp function\nwill return a graph rather than a dict, bypassing most of the asadpour\nalgorithm.\n\nAt first glance, this test probably doesn't look like it ensures that we\nskip the rest of the asadpour algorithm, but it does. We are not fixing a\nsee for the random number generator, so if we sample any spanning trees\nthe approximation would be different basically every time this test is\nexecuted but it is not since held karp is deterministic and we do not\nreach the portion of the code with the dependence on random numbers.",
  "code": "@pytest.mark.slow\ndef test_asadpour_integral_held_karp():\n    np = pytest.importorskip('numpy')\n    G_array = np.array([[0, 26, 63, 59, 69, 31, 41], [62, 0, 91, 53, 75, 87, 47], [47, 82, 0, 90, 15, 9, 18], [68, 19, 5, 0, 58, 34, 93], [11, 58, 53, 55, 0, 61, 79], [88, 75, 13, 76, 98, 0, 40], [41, 61, 55, 88, 46, 45, 0]])\n    G = nx.from_numpy_array(G_array, create_using=nx.DiGraph)\n    for _ in range(2):\n        tour = nx_app.traveling_salesman_problem(G, method=nx_app.asadpour_atsp)\n        assert [1, 3, 2, 5, 2, 6, 4, 0, 1] == tour"
 },
 {
  "docstring": "Test the asadpour algorithm with a graph without a hamiltonian circuit",
  "code": "def test_directed_tsp_impossible():\n    pytest.importorskip('numpy')\n    edges = [(0, 1, 10), (0, 2, 11), (0, 3, 12), (1, 2, 4), (1, 3, 6), (2, 1, 3), (2, 3, 2), (3, 1, 5), (3, 2, 1)]\n    G = nx.DiGraph()\n    G.add_weighted_edges_from(edges)\n    pytest.raises(nx.NetworkXError, nx_app.traveling_salesman_problem, G)"
 },
 {
  "docstring": "Check if the given tree decomposition is valid.",
  "code": "def is_tree_decomp(graph, decomp):\n    for x in graph.nodes():\n        appear_once = False\n        for bag in decomp.nodes():\n            if x in bag:\n                appear_once = True\n                break\n        assert appear_once\n    for x, y in graph.edges():\n        appear_together = False\n        for bag in decomp.nodes():\n            if x in bag and y in bag:\n                appear_together = True\n                break\n        assert appear_together\n    for v in graph.nodes():\n        subset = []\n        for bag in decomp.nodes():\n            if v in bag:\n                subset.append(bag)\n        sub_graph = decomp.subgraph(subset)\n        assert nx.is_connected(sub_graph)"
 },
 {
  "docstring": "Setup for different kinds of trees",
  "code": "@classmethod\ndef setup_class(cls):\n    cls.complete = nx.Graph()\n    cls.complete.add_edge(1, 2)\n    cls.complete.add_edge(2, 3)\n    cls.complete.add_edge(1, 3)\n    cls.small_tree = nx.Graph()\n    cls.small_tree.add_edge(1, 3)\n    cls.small_tree.add_edge(4, 3)\n    cls.small_tree.add_edge(2, 3)\n    cls.small_tree.add_edge(3, 5)\n    cls.small_tree.add_edge(5, 6)\n    cls.small_tree.add_edge(5, 7)\n    cls.small_tree.add_edge(6, 7)\n    cls.deterministic_graph = nx.Graph()\n    cls.deterministic_graph.add_edge(0, 1)\n    cls.deterministic_graph.add_edge(1, 2)\n    cls.deterministic_graph.add_edge(2, 3)\n    cls.deterministic_graph.add_edge(2, 4)\n    cls.deterministic_graph.add_edge(3, 4)\n    cls.deterministic_graph.add_edge(3, 5)\n    cls.deterministic_graph.add_edge(3, 6)\n    cls.deterministic_graph.add_edge(4, 5)\n    cls.deterministic_graph.add_edge(4, 6)\n    cls.deterministic_graph.add_edge(4, 7)\n    cls.deterministic_graph.add_edge(5, 6)\n    cls.deterministic_graph.add_edge(5, 7)\n    cls.deterministic_graph.add_edge(5, 8)\n    cls.deterministic_graph.add_edge(5, 9)\n    cls.deterministic_graph.add_edge(6, 7)\n    cls.deterministic_graph.add_edge(6, 8)\n    cls.deterministic_graph.add_edge(6, 9)\n    cls.deterministic_graph.add_edge(7, 8)\n    cls.deterministic_graph.add_edge(7, 9)\n    cls.deterministic_graph.add_edge(8, 9)"
 },
 {
  "docstring": "Test Petersen graph tree decomposition result",
  "code": "def test_petersen_graph(self):\n    G = nx.petersen_graph()\n    _, decomp = treewidth_min_degree(G)\n    is_tree_decomp(G, decomp)"
 },
 {
  "docstring": "Test small tree\n\nTest if the computed treewidth of the known self.small_tree is 2.\nAs we know which value we can expect from our heuristic, values other\nthan two are regressions",
  "code": "def test_small_tree_treewidth(self):\n    G = self.small_tree\n    treewidth, _ = treewidth_min_fill_in(G)\n    assert treewidth == 2"
 },
 {
  "docstring": "Test heuristic abort condition for fully connected graph",
  "code": "def test_heuristic_abort(self):\n    graph = {}\n    for u in self.complete:\n        graph[u] = set()\n        for v in self.complete[u]:\n            if u != v:\n                graph[u].add(v)\n    deg_heuristic = MinDegreeHeuristic(graph)\n    node = deg_heuristic.best_node(graph)\n    if node is None:\n        pass\n    else:\n        assert False"
 },
 {
  "docstring": "Test empty graph",
  "code": "def test_empty_graph(self):\n    G = nx.Graph()\n    _, _ = treewidth_min_degree(G)"
 },
 {
  "docstring": "Test first steps of min_degree heuristic",
  "code": "def test_heuristic_first_steps(self):\n    graph = {n: set(self.deterministic_graph[n]) - {n} for n in self.deterministic_graph}\n    deg_heuristic = MinDegreeHeuristic(graph)\n    elim_node = deg_heuristic.best_node(graph)\n    print(f'Graph {graph}:')\n    steps = []\n    while elim_node is not None:\n        print(f'Removing {elim_node}:')\n        steps.append(elim_node)\n        nbrs = graph[elim_node]\n        for u, v in itertools.permutations(nbrs, 2):\n            if v not in graph[u]:\n                graph[u].add(v)\n        for u in graph:\n            if elim_node in graph[u]:\n                graph[u].remove(elim_node)\n        del graph[elim_node]\n        print(f'Graph {graph}:')\n        elim_node = deg_heuristic.best_node(graph)\n    assert steps[:5] == [0, 1, 2, 3, 4]"
 },
 {
  "docstring": "Setup for different kinds of trees",
  "code": "@classmethod\ndef setup_class(cls):\n    cls.complete = nx.Graph()\n    cls.complete.add_edge(1, 2)\n    cls.complete.add_edge(2, 3)\n    cls.complete.add_edge(1, 3)\n    cls.small_tree = nx.Graph()\n    cls.small_tree.add_edge(1, 2)\n    cls.small_tree.add_edge(2, 3)\n    cls.small_tree.add_edge(3, 4)\n    cls.small_tree.add_edge(1, 4)\n    cls.small_tree.add_edge(2, 4)\n    cls.small_tree.add_edge(4, 5)\n    cls.small_tree.add_edge(5, 6)\n    cls.small_tree.add_edge(5, 7)\n    cls.small_tree.add_edge(6, 7)\n    cls.deterministic_graph = nx.Graph()\n    cls.deterministic_graph.add_edge(1, 2)\n    cls.deterministic_graph.add_edge(1, 3)\n    cls.deterministic_graph.add_edge(3, 4)\n    cls.deterministic_graph.add_edge(2, 4)\n    cls.deterministic_graph.add_edge(3, 5)\n    cls.deterministic_graph.add_edge(4, 5)\n    cls.deterministic_graph.add_edge(3, 6)\n    cls.deterministic_graph.add_edge(5, 6)"
 },
 {
  "docstring": "Test Petersen graph tree decomposition result",
  "code": "def test_petersen_graph(self):\n    G = nx.petersen_graph()\n    _, decomp = treewidth_min_fill_in(G)\n    is_tree_decomp(G, decomp)"
 },
 {
  "docstring": "Test if the computed treewidth of the known self.small_tree is 2",
  "code": "def test_small_tree_treewidth(self):\n    G = self.small_tree\n    treewidth, _ = treewidth_min_fill_in(G)\n    assert treewidth == 2"
 },
 {
  "docstring": "Test if min_fill_in returns None for fully connected graph",
  "code": "def test_heuristic_abort(self):\n    graph = {}\n    for u in self.complete:\n        graph[u] = set()\n        for v in self.complete[u]:\n            if u != v:\n                graph[u].add(v)\n    next_node = min_fill_in_heuristic(graph)\n    if next_node is None:\n        pass\n    else:\n        assert False"
 },
 {
  "docstring": "Test empty graph",
  "code": "def test_empty_graph(self):\n    G = nx.Graph()\n    _, _ = treewidth_min_fill_in(G)"
 },
 {
  "docstring": "Test first steps of min_fill_in heuristic",
  "code": "def test_heuristic_first_steps(self):\n    graph = {n: set(self.deterministic_graph[n]) - {n} for n in self.deterministic_graph}\n    print(f'Graph {graph}:')\n    elim_node = min_fill_in_heuristic(graph)\n    steps = []\n    while elim_node is not None:\n        print(f'Removing {elim_node}:')\n        steps.append(elim_node)\n        nbrs = graph[elim_node]\n        for u, v in itertools.permutations(nbrs, 2):\n            if v not in graph[u]:\n                graph[u].add(v)\n        for u in graph:\n            if elim_node in graph[u]:\n                graph[u].remove(elim_node)\n        del graph[elim_node]\n        print(f'Graph {graph}:')\n        elim_node = min_fill_in_heuristic(graph)\n    assert steps[:2] == [6, 5]"
 },
 {
  "docstring": "Compute the average degree connectivity of graph.\n\nThe average degree connectivity is the average nearest neighbor degree of\nnodes with degree k. For weighted graphs, an analogous measure can\nbe computed using the weighted average neighbors degree defined in\n[1]_, for a node `i`, as\n\n.. math::\n\n    k_{nn,i}^{w} = \\frac{1}{s_i} \\sum_{j \\in N(i)} w_{ij} k_j\n\nwhere `s_i` is the weighted degree of node `i`,\n`w_{ij}` is the weight of the edge that links `i` and `j`,\nand `N(i)` are the neighbors of node `i`.\n\nParameters\n----------\nG : NetworkX graph\n\nsource :  \"in\"|\"out\"|\"in+out\" (default:\"in+out\")\n   Directed graphs only. Use \"in\"- or \"out\"-degree for source node.\n\ntarget : \"in\"|\"out\"|\"in+out\" (default:\"in+out\"\n   Directed graphs only. Use \"in\"- or \"out\"-degree for target node.\n\nnodes : list or iterable (optional)\n    Compute neighbor connectivity for these nodes. The default is all\n    nodes.\n\nweight : string or None, optional (default=None)\n   The edge attribute that holds the numerical value used as a weight.\n   If None, then each edge has weight 1.\n\nReturns\n-------\nd : dict\n   A dictionary keyed by degree k with the value of average connectivity.\n\nRaises\n------\nNetworkXError\n    If either `source` or `target` are not one of 'in',\n    'out', or 'in+out'.\n    If either `source` or `target` is passed for an undirected graph.\n\nExamples\n--------\n>>> G = nx.path_graph(4)\n>>> G.edges[1, 2][\"weight\"] = 3\n>>> nx.average_degree_connectivity(G)\n{1: 2.0, 2: 1.5}\n>>> nx.average_degree_connectivity(G, weight=\"weight\")\n{1: 2.0, 2: 1.75}\n\nSee Also\n--------\naverage_neighbor_degree\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef average_degree_connectivity(G, source='in+out', target='in+out', nodes=None, weight=None):\n    if G.is_directed():\n        if source not in ('in', 'out', 'in+out'):\n            raise nx.NetworkXError('source must be one of \"in\", \"out\", or \"in+out\"')\n        if target not in ('in', 'out', 'in+out'):\n            raise nx.NetworkXError('target must be one of \"in\", \"out\", or \"in+out\"')\n        direction = {'out': G.out_degree, 'in': G.in_degree, 'in+out': G.degree}\n        neighbor_funcs = {'out': G.successors, 'in': G.predecessors, 'in+out': G.neighbors}\n        source_degree = direction[source]\n        target_degree = direction[target]\n        neighbors = neighbor_funcs[source]\n        reverse = source == 'in'\n    else:\n        if source != 'in+out' or target != 'in+out':\n            raise nx.NetworkXError(f'source and target arguments are only supported for directed graphs')\n        source_degree = G.degree\n        target_degree = G.degree\n        neighbors = G.neighbors\n        reverse = False\n    dsum = defaultdict(int)\n    dnorm = defaultdict(int)\n    source_nodes = source_degree(nodes)\n    if nodes in G:\n        source_nodes = [(nodes, source_degree(nodes))]\n    for n, k in source_nodes:\n        nbrdeg = target_degree(neighbors(n))\n        if weight is None:\n            s = sum((d for n, d in nbrdeg))\n        elif reverse:\n            s = sum((G[nbr][n].get(weight, 1) * d for nbr, d in nbrdeg))\n        else:\n            s = sum((G[n][nbr].get(weight, 1) * d for nbr, d in nbrdeg))\n        dnorm[k] += source_degree(n, weight=weight)\n        dsum[k] += s\n    return {k: avg if dnorm[k] == 0 else avg / dnorm[k] for k, avg in dsum.items()}"
 },
 {
  "docstring": "Compute degree assortativity of graph.\n\nAssortativity measures the similarity of connections\nin the graph with respect to the node degree.\n\nParameters\n----------\nG : NetworkX graph\n\nx: string ('in','out')\n   The degree type for source node (directed graphs only).\n\ny: string ('in','out')\n   The degree type for target node (directed graphs only).\n\nweight: string or None, optional (default=None)\n   The edge attribute that holds the numerical value used\n   as a weight.  If None, then each edge has weight 1.\n   The degree is the sum of the edge weights adjacent to the node.\n\nnodes: list or iterable (optional)\n    Compute degree assortativity only for nodes in container.\n    The default is all nodes.\n\nReturns\n-------\nr : float\n   Assortativity of graph by degree.\n\nExamples\n--------\n>>> G = nx.path_graph(4)\n>>> r = nx.degree_assortativity_coefficient(G)\n>>> print(f\"{r:3.1f}\")\n-0.5\n\nSee Also\n--------\nattribute_assortativity_coefficient\nnumeric_assortativity_coefficient\ndegree_mixing_dict\ndegree_mixing_matrix\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef degree_assortativity_coefficient(G, x='out', y='in', weight=None, nodes=None):\n    if nodes is None:\n        nodes = G.nodes\n    degrees = None\n    if G.is_directed():\n        indeg = {d for _, d in G.in_degree(nodes, weight=weight)} if 'in' in (x, y) else set()\n        outdeg = {d for _, d in G.out_degree(nodes, weight=weight)} if 'out' in (x, y) else set()\n        degrees = set.union(indeg, outdeg)\n    else:\n        degrees = {d for _, d in G.degree(nodes, weight=weight)}\n    mapping = {d: i for i, d in enumerate(degrees)}\n    M = degree_mixing_matrix(G, x=x, y=y, nodes=nodes, weight=weight, mapping=mapping)\n    return _numeric_ac(M, mapping=mapping)"
 },
 {
  "docstring": "Compute degree assortativity of graph.\n\nAssortativity measures the similarity of connections\nin the graph with respect to the node degree.\n\nThis is the same as degree_assortativity_coefficient but uses the\npotentially faster scipy.stats.pearsonr function.\n\nParameters\n----------\nG : NetworkX graph\n\nx: string ('in','out')\n   The degree type for source node (directed graphs only).\n\ny: string ('in','out')\n   The degree type for target node (directed graphs only).\n\nweight: string or None, optional (default=None)\n   The edge attribute that holds the numerical value used\n   as a weight.  If None, then each edge has weight 1.\n   The degree is the sum of the edge weights adjacent to the node.\n\nnodes: list or iterable (optional)\n    Compute pearson correlation of degrees only for specified nodes.\n    The default is all nodes.\n\nReturns\n-------\nr : float\n   Assortativity of graph by degree.\n\nExamples\n--------\n>>> G = nx.path_graph(4)\n>>> r = nx.degree_pearson_correlation_coefficient(G)\n>>> print(f\"{r:3.1f}\")\n-0.5\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef degree_pearson_correlation_coefficient(G, x='out', y='in', weight=None, nodes=None):\n    import scipy as sp\n    xy = node_degree_xy(G, x=x, y=y, nodes=nodes, weight=weight)\n    x, y = zip(*xy)\n    return sp.stats.pearsonr(x, y)[0]"
 },
 {
  "docstring": "Compute assortativity for node attributes.\n\nAssortativity measures the similarity of connections\nin the graph with respect to the given attribute.\n\nParameters\n----------\nG : NetworkX graph\n\nattribute : string\n    Node attribute key\n\nnodes: list or iterable (optional)\n    Compute attribute assortativity for nodes in container.\n    The default is all nodes.\n\nReturns\n-------\nr: float\n   Assortativity of graph for given attribute\n\nExamples\n--------\n>>> G = nx.Graph()\n>>> G.add_nodes_from([0, 1], color=\"red\")\n>>> G.add_nodes_from([2, 3], color=\"blue\")\n>>> G.add_edges_from([(0, 1), (2, 3)])\n>>> print(nx.attribute_assortativity_coefficient(G, \"color\"))\n1.0\n\n",
  "code": "@nx._dispatch(node_attrs='attribute')\ndef attribute_assortativity_coefficient(G, attribute, nodes=None):\n    M = attribute_mixing_matrix(G, attribute, nodes)\n    return attribute_ac(M)"
 },
 {
  "docstring": "Compute assortativity for numerical node attributes.\n\nAssortativity measures the similarity of connections\nin the graph with respect to the given numeric attribute.\n\nParameters\n----------\nG : NetworkX graph\n\nattribute : string\n    Node attribute key.\n\nnodes: list or iterable (optional)\n    Compute numeric assortativity only for attributes of nodes in\n    container. The default is all nodes.\n\nReturns\n-------\nr: float\n   Assortativity of graph for given attribute\n\nExamples\n--------\n>>> G = nx.Graph()\n>>> G.add_nodes_from([0, 1], size=2)\n>>> G.add_nodes_from([2, 3], size=3)\n>>> G.add_edges_from([(0, 1), (2, 3)])\n>>> print(nx.numeric_assortativity_coefficient(G, \"size\"))\n1.0\n\n",
  "code": "@nx._dispatch(node_attrs='attribute')\ndef numeric_assortativity_coefficient(G, attribute, nodes=None):\n    if nodes is None:\n        nodes = G.nodes\n    vals = {G.nodes[n][attribute] for n in nodes}\n    mapping = {d: i for i, d in enumerate(vals)}\n    M = attribute_mixing_matrix(G, attribute, nodes, mapping)\n    return _numeric_ac(M, mapping)"
 },
 {
  "docstring": "Compute assortativity for attribute matrix M.\n\nParameters\n----------\nM : numpy.ndarray\n    2D ndarray representing the attribute mixing matrix.\n\n",
  "code": "def attribute_ac(M):\n    if M.sum() != 1.0:\n        M = M / M.sum()\n    s = (M @ M).sum()\n    t = M.trace()\n    r = (t - s) / (1 - s)\n    return r"
 },
 {
  "docstring": "Returns dictionary representation of mixing matrix for attribute.\n\nParameters\n----------\nG : graph\n   NetworkX graph object.\n\nattribute : string\n   Node attribute key.\n\nnodes: list or iterable (optional)\n    Unse nodes in container to build the dict. The default is all nodes.\n\nnormalized : bool (default=False)\n   Return counts if False or probabilities if True.\n\nExamples\n--------\n>>> G = nx.Graph()\n>>> G.add_nodes_from([0, 1], color=\"red\")\n>>> G.add_nodes_from([2, 3], color=\"blue\")\n>>> G.add_edge(1, 3)\n>>> d = nx.attribute_mixing_dict(G, \"color\")\n>>> print(d[\"red\"][\"blue\"])\n1\n>>> print(d[\"blue\"][\"red\"])  # d symmetric for undirected graphs\n1\n\nReturns\n-------\nd : dictionary\n   Counts or joint probability of occurrence of attribute pairs.",
  "code": "@nx._dispatch(node_attrs='attribute')\ndef attribute_mixing_dict(G, attribute, nodes=None, normalized=False):\n    xy_iter = node_attribute_xy(G, attribute, nodes)\n    return mixing_dict(xy_iter, normalized=normalized)"
 },
 {
  "docstring": "Returns mixing matrix for attribute.\n\nParameters\n----------\nG : graph\n   NetworkX graph object.\n\nattribute : string\n   Node attribute key.\n\nnodes: list or iterable (optional)\n    Use only nodes in container to build the matrix. The default is\n    all nodes.\n\nmapping : dictionary, optional\n   Mapping from node attribute to integer index in matrix.\n   If not specified, an arbitrary ordering will be used.\n\nnormalized : bool (default=True)\n   Return counts if False or probabilities if True.\n\nReturns\n-------\nm: numpy array\n   Counts or joint probability of occurrence of attribute pairs.\n\n",
  "code": "@nx._dispatch(node_attrs='attribute')\ndef attribute_mixing_matrix(G, attribute, nodes=None, mapping=None, normalized=True):\n    d = attribute_mixing_dict(G, attribute, nodes)\n    a = dict_to_numpy_array(d, mapping=mapping)\n    if normalized:\n        a = a / a.sum()\n    return a"
 },
 {
  "docstring": "Returns dictionary representation of mixing matrix for degree.\n\nParameters\n----------\nG : graph\n    NetworkX graph object.\n\nx: string ('in','out')\n   The degree type for source node (directed graphs only).\n\ny: string ('in','out')\n   The degree type for target node (directed graphs only).\n\nweight: string or None, optional (default=None)\n   The edge attribute that holds the numerical value used\n   as a weight.  If None, then each edge has weight 1.\n   The degree is the sum of the edge weights adjacent to the node.\n\nnormalized : bool (default=False)\n    Return counts if False or probabilities if True.\n\nReturns\n-------\nd: dictionary\n   Counts or joint probability of occurrence of degree pairs.",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef degree_mixing_dict(G, x='out', y='in', weight=None, nodes=None, normalized=False):\n    xy_iter = node_degree_xy(G, x=x, y=y, nodes=nodes, weight=weight)\n    return mixing_dict(xy_iter, normalized=normalized)"
 },
 {
  "docstring": "Returns mixing matrix for attribute.\n\nParameters\n----------\nG : graph\n   NetworkX graph object.\n\nx: string ('in','out')\n   The degree type for source node (directed graphs only).\n\ny: string ('in','out')\n   The degree type for target node (directed graphs only).\n\nnodes: list or iterable (optional)\n    Build the matrix using only nodes in container.\n    The default is all nodes.\n\nweight: string or None, optional (default=None)\n   The edge attribute that holds the numerical value used\n   as a weight.  If None, then each edge has weight 1.\n   The degree is the sum of the edge weights adjacent to the node.\n\nnormalized : bool (default=True)\n   Return counts if False or probabilities if True.\n\nmapping : dictionary, optional\n   Mapping from node degree to integer index in matrix.\n   If not specified, an arbitrary ordering will be used.\n\nReturns\n-------\nm: numpy array\n   Counts, or joint probability, of occurrence of node degree.\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef degree_mixing_matrix(G, x='out', y='in', weight=None, nodes=None, normalized=True, mapping=None):\n    d = degree_mixing_dict(G, x=x, y=y, nodes=nodes, weight=weight)\n    a = dict_to_numpy_array(d, mapping=mapping)\n    if normalized:\n        a = a / a.sum()\n    return a"
 },
 {
  "docstring": "Returns a dictionary representation of mixing matrix.\n\nParameters\n----------\nxy : list or container of two-tuples\n   Pairs of (x,y) items.\n\nattribute : string\n   Node attribute key\n\nnormalized : bool (default=False)\n   Return counts if False or probabilities if True.\n\nReturns\n-------\nd: dictionary\n   Counts or Joint probability of occurrence of values in xy.",
  "code": "def mixing_dict(xy, normalized=False):\n    d = {}\n    psum = 0.0\n    for x, y in xy:\n        if x not in d:\n            d[x] = {}\n        if y not in d:\n            d[y] = {}\n        v = d[x].get(y, 0)\n        d[x][y] = v + 1\n        psum += 1\n    if normalized:\n        for _, jdict in d.items():\n            for j in jdict:\n                jdict[j] /= psum\n    return d"
 },
 {
  "docstring": "Returns the average degree of the neighborhood of each node.\n\nIn an undirected graph, the neighborhood `N(i)` of node `i` contains the\nnodes that are connected to `i` by an edge.\n\nFor directed graphs, `N(i)` is defined according to the parameter `source`:\n\n    - if source is 'in', then `N(i)` consists of predecessors of node `i`.\n    - if source is 'out', then `N(i)` consists of successors of node `i`.\n    - if source is 'in+out', then `N(i)` is both predecessors and successors.\n\nThe average neighborhood degree of a node `i` is\n\n.. math::\n\n    k_{nn,i} = \\frac{1}{|N(i)|} \\sum_{j \\in N(i)} k_j\n\nwhere `N(i)` are the neighbors of node `i` and `k_j` is\nthe degree of node `j` which belongs to `N(i)`. For weighted\ngraphs, an analogous measure can be defined [1]_,\n\n.. math::\n\n    k_{nn,i}^{w} = \\frac{1}{s_i} \\sum_{j \\in N(i)} w_{ij} k_j\n\nwhere `s_i` is the weighted degree of node `i`, `w_{ij}`\nis the weight of the edge that links `i` and `j` and\n`N(i)` are the neighbors of node `i`.\n\n\nParameters\n----------\nG : NetworkX graph\n\nsource : string (\"in\"|\"out\"|\"in+out\"), optional (default=\"out\")\n   Directed graphs only.\n   Use \"in\"- or \"out\"-neighbors of source node.\n\ntarget : string (\"in\"|\"out\"|\"in+out\"), optional (default=\"out\")\n   Directed graphs only.\n   Use \"in\"- or \"out\"-degree for target node.\n\nnodes : list or iterable, optional (default=G.nodes)\n    Compute neighbor degree only for specified nodes.\n\nweight : string or None, optional (default=None)\n   The edge attribute that holds the numerical value used as a weight.\n   If None, then each edge has weight 1.\n\nReturns\n-------\nd: dict\n   A dictionary keyed by node to the average degree of its neighbors.\n\nRaises\n------\nNetworkXError\n    If either `source` or `target` are not one of 'in', 'out', or 'in+out'.\n    If either `source` or `target` is passed for an undirected graph.\n\nExamples\n--------\n>>> G = nx.path_graph(4)\n>>> G.edges[0, 1][\"weight\"] = 5\n>>> G.edges[2, 3][\"weight\"] = 3\n\n>>> nx.average_neighbor_degree(G)\n{0: 2.0, 1: 1.5, 2: 1.5, 3: 2.0}\n>>> nx.average_neighbor_degree(G, weight=\"weight\")\n{0: 2.0, 1: 1.1666666666666667, 2: 1.25, 3: 2.0}\n\n>>> G = nx.DiGraph()\n>>> nx.add_path(G, [0, 1, 2, 3])\n>>> nx.average_neighbor_degree(G, source=\"in\", target=\"in\")\n{0: 0.0, 1: 0.0, 2: 1.0, 3: 1.0}\n\n>>> nx.average_neighbor_degree(G, source=\"out\", target=\"out\")\n{0: 1.0, 1: 1.0, 2: 0.0, 3: 0.0}\n\nSee Also\n--------\naverage_degree_connectivity\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef average_neighbor_degree(G, source='out', target='out', nodes=None, weight=None):\n    if G.is_directed():\n        if source == 'in':\n            source_degree = G.in_degree\n        elif source == 'out':\n            source_degree = G.out_degree\n        elif source == 'in+out':\n            source_degree = G.degree\n        else:\n            raise nx.NetworkXError(f\"source argument {source} must be 'in', 'out' or 'in+out'\")\n        if target == 'in':\n            target_degree = G.in_degree\n        elif target == 'out':\n            target_degree = G.out_degree\n        elif target == 'in+out':\n            target_degree = G.degree\n        else:\n            raise nx.NetworkXError(f\"target argument {target} must be 'in', 'out' or 'in+out'\")\n    else:\n        if source != 'out' or target != 'out':\n            raise nx.NetworkXError(f'source and target arguments are only supported for directed graphs')\n        source_degree = target_degree = G.degree\n    t_deg = dict(target_degree())\n    G_P = G_S = {n: {} for n in G}\n    if G.is_directed():\n        if 'in' in source:\n            G_P = G.pred\n        if 'out' in source:\n            G_S = G.succ\n    else:\n        G_S = G.adj\n    avg = {}\n    for n, deg in source_degree(nodes, weight=weight):\n        if deg == 0:\n            avg[n] = 0.0\n            continue\n        if weight is None:\n            avg[n] = (sum((t_deg[nbr] for nbr in G_S[n])) + sum((t_deg[nbr] for nbr in G_P[n]))) / deg\n        else:\n            avg[n] = (sum((dd.get(weight, 1) * t_deg[nbr] for nbr, dd in G_S[n].items())) + sum((dd.get(weight, 1) * t_deg[nbr] for nbr, dd in G_P[n].items()))) / deg\n    return avg"
 },
 {
  "docstring": "Returns iterator of node-attribute pairs for all edges in G.\n\nParameters\n----------\nG: NetworkX graph\n\nattribute: key\n   The node attribute key.\n\nnodes: list or iterable (optional)\n    Use only edges that are incident to specified nodes.\n    The default is all nodes.\n\nReturns\n-------\n(x, y): 2-tuple\n    Generates 2-tuple of (attribute, attribute) values.\n\nExamples\n--------\n>>> G = nx.DiGraph()\n>>> G.add_node(1, color=\"red\")\n>>> G.add_node(2, color=\"blue\")\n>>> G.add_edge(1, 2)\n>>> list(nx.node_attribute_xy(G, \"color\"))\n[('red', 'blue')]\n\n",
  "code": "@nx._dispatch(node_attrs='attribute')\ndef node_attribute_xy(G, attribute, nodes=None):\n    if nodes is None:\n        nodes = set(G)\n    else:\n        nodes = set(nodes)\n    Gnodes = G.nodes\n    for u, nbrsdict in G.adjacency():\n        if u not in nodes:\n            continue\n        uattr = Gnodes[u].get(attribute, None)\n        if G.is_multigraph():\n            for v, keys in nbrsdict.items():\n                vattr = Gnodes[v].get(attribute, None)\n                for _ in keys:\n                    yield (uattr, vattr)\n        else:\n            for v in nbrsdict:\n                vattr = Gnodes[v].get(attribute, None)\n                yield (uattr, vattr)"
 },
 {
  "docstring": "Generate node degree-degree pairs for edges in G.\n\nParameters\n----------\nG: NetworkX graph\n\nx: string ('in','out')\n   The degree type for source node (directed graphs only).\n\ny: string ('in','out')\n   The degree type for target node (directed graphs only).\n\nweight: string or None, optional (default=None)\n   The edge attribute that holds the numerical value used\n   as a weight.  If None, then each edge has weight 1.\n   The degree is the sum of the edge weights adjacent to the node.\n\nnodes: list or iterable (optional)\n    Use only edges that are adjacency to specified nodes.\n    The default is all nodes.\n\nReturns\n-------\n(x, y): 2-tuple\n    Generates 2-tuple of (degree, degree) values.\n\n\nExamples\n--------\n>>> G = nx.DiGraph()\n>>> G.add_edge(1, 2)\n>>> list(nx.node_degree_xy(G, x=\"out\", y=\"in\"))\n[(1, 1)]\n>>> list(nx.node_degree_xy(G, x=\"in\", y=\"out\"))\n[(0, 0)]\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef node_degree_xy(G, x='out', y='in', weight=None, nodes=None):\n    nodes = set(G) if nodes is None else set(nodes)\n    if G.is_directed():\n        direction = {'out': G.out_degree, 'in': G.in_degree}\n        xdeg = direction[x]\n        ydeg = direction[y]\n    else:\n        xdeg = ydeg = G.degree\n    for u, degu in xdeg(nodes, weight=weight):\n        neighbors = (nbr for _, nbr in G.edges(u) if nbr in nodes)\n        for _, degv in ydeg(neighbors, weight=weight):\n            yield (degu, degv)"
 },
 {
  "docstring": "Test degree assortativity for a directed graph where the set of\nin/out degree does not equal the total degree.",
  "code": "def test_degree_assortativity_directed2(self):\n    r = nx.degree_assortativity_coefficient(self.D2)\n    np.testing.assert_almost_equal(r, 0.14852, decimal=4)"
 },
 {
  "docstring": "Test degree assortativity with Pearson for a directed graph where\nthe set of in/out degree does not equal the total degree.",
  "code": "def test_degree_pearson_assortativity_directed2(self):\n    r = nx.degree_pearson_correlation_coefficient(self.D2)\n    np.testing.assert_almost_equal(r, 0.14852, decimal=4)"
 },
 {
  "docstring": "Returns a two-coloring of the graph.\n\nRaises an exception if the graph is not bipartite.\n\nParameters\n----------\nG : NetworkX graph\n\nReturns\n-------\ncolor : dictionary\n    A dictionary keyed by node with a 1 or 0 as data for each node color.\n\nRaises\n------\nNetworkXError\n    If the graph is not two-colorable.\n\nExamples\n--------\n>>> from networkx.algorithms import bipartite\n>>> G = nx.path_graph(4)\n>>> c = bipartite.color(G)\n>>> print(c)\n{0: 1, 1: 0, 2: 1, 3: 0}\n\nYou can use this to set a node attribute indicating the bipartite set:\n\n>>> nx.set_node_attributes(G, c, \"bipartite\")\n>>> print(G.nodes[0][\"bipartite\"])\n1\n>>> print(G.nodes[1][\"bipartite\"])\n0",
  "code": "@nx._dispatch\ndef color(G):\n    if G.is_directed():\n        import itertools\n\n        def neighbors(v):\n            return itertools.chain.from_iterable([G.predecessors(v), G.successors(v)])\n    else:\n        neighbors = G.neighbors\n    color = {}\n    for n in G:\n        if n in color or len(G[n]) == 0:\n            continue\n        queue = [n]\n        color[n] = 1\n        while queue:\n            v = queue.pop()\n            c = 1 - color[v]\n            for w in neighbors(v):\n                if w in color:\n                    if color[w] == color[v]:\n                        raise nx.NetworkXError('Graph is not bipartite.')\n                else:\n                    color[w] = c\n                    queue.append(w)\n    color.update(dict.fromkeys(nx.isolates(G), 0))\n    return color"
 },
 {
  "docstring": "Returns True if graph G is bipartite, False if not.\n\nParameters\n----------\nG : NetworkX graph\n\nExamples\n--------\n>>> from networkx.algorithms import bipartite\n>>> G = nx.path_graph(4)\n>>> print(bipartite.is_bipartite(G))\nTrue\n\nSee Also\n--------\ncolor, is_bipartite_node_set",
  "code": "@nx._dispatch\ndef is_bipartite(G):\n    try:\n        color(G)\n        return True\n    except nx.NetworkXError:\n        return False"
 },
 {
  "docstring": "Returns True if nodes and G/nodes are a bipartition of G.\n\nParameters\n----------\nG : NetworkX graph\n\nnodes: list or container\n  Check if nodes are a one of a bipartite set.\n\nExamples\n--------\n>>> from networkx.algorithms import bipartite\n>>> G = nx.path_graph(4)\n>>> X = set([1, 3])\n>>> bipartite.is_bipartite_node_set(G, X)\nTrue\n\n",
  "code": "@nx._dispatch\ndef is_bipartite_node_set(G, nodes):\n    S = set(nodes)\n    if len(S) < len(nodes):\n        raise AmbiguousSolution('The input node set contains duplicates.\\nThis may lead to incorrect results when using it in bipartite algorithms.\\nConsider using set(nodes) as the input')\n    for CC in (G.subgraph(c).copy() for c in connected_components(G)):\n        X, Y = sets(CC)\n        if not (X.issubset(S) and Y.isdisjoint(S) or (Y.issubset(S) and X.isdisjoint(S))):\n            return False\n    return True"
 },
 {
  "docstring": "Returns bipartite node sets of graph G.\n\nRaises an exception if the graph is not bipartite or if the input\ngraph is disconnected and thus more than one valid solution exists.\nSee :mod:`bipartite documentation <networkx.algorithms.bipartite>`\nfor further details on how bipartite graphs are handled in NetworkX.\n\nParameters\n----------\nG : NetworkX graph\n\ntop_nodes : container, optional\n  Container with all nodes in one bipartite node set. If not supplied\n  it will be computed. But if more than one solution exists an exception\n  will be raised.\n\nReturns\n-------\nX : set\n  Nodes from one side of the bipartite graph.\nY : set\n  Nodes from the other side.\n\nRaises\n------\nAmbiguousSolution\n  Raised if the input bipartite graph is disconnected and no container\n  with all nodes in one bipartite set is provided. When determining\n  the nodes in each bipartite set more than one valid solution is\n  possible if the input graph is disconnected.\nNetworkXError\n  Raised if the input graph is not bipartite.\n\nExamples\n--------\n>>> from networkx.algorithms import bipartite\n>>> G = nx.path_graph(4)\n>>> X, Y = bipartite.sets(G)\n>>> list(X)\n[0, 2]\n>>> list(Y)\n[1, 3]\n\nSee Also\n--------\ncolor",
  "code": "@nx._dispatch\ndef sets(G, top_nodes=None):\n    if G.is_directed():\n        is_connected = nx.is_weakly_connected\n    else:\n        is_connected = nx.is_connected\n    if top_nodes is not None:\n        X = set(top_nodes)\n        Y = set(G) - X\n    else:\n        if not is_connected(G):\n            msg = 'Disconnected graph: Ambiguous solution for bipartite sets.'\n            raise nx.AmbiguousSolution(msg)\n        c = color(G)\n        X = {n for n, is_top in c.items() if is_top}\n        Y = {n for n, is_top in c.items() if not is_top}\n    return (X, Y)"
 },
 {
  "docstring": "Returns density of bipartite graph B.\n\nParameters\n----------\nB : NetworkX graph\n\nnodes: list or container\n  Nodes in one node set of the bipartite graph.\n\nReturns\n-------\nd : float\n   The bipartite density\n\nExamples\n--------\n>>> from networkx.algorithms import bipartite\n>>> G = nx.complete_bipartite_graph(3, 2)\n>>> X = set([0, 1, 2])\n>>> bipartite.density(G, X)\n1.0\n>>> Y = set([3, 4])\n>>> bipartite.density(G, Y)\n1.0\n\n",
  "code": "@nx._dispatch(graphs='B')\ndef density(B, nodes):\n    n = len(B)\n    m = nx.number_of_edges(B)\n    nb = len(nodes)\n    nt = n - nb\n    if m == 0:\n        d = 0.0\n    elif B.is_directed():\n        d = m / (2 * nb * nt)\n    else:\n        d = m / (nb * nt)\n    return d"
 },
 {
  "docstring": "Returns the degrees of the two node sets in the bipartite graph B.\n\nParameters\n----------\nB : NetworkX graph\n\nnodes: list or container\n  Nodes in one node set of the bipartite graph.\n\nweight : string or None, optional (default=None)\n   The edge attribute that holds the numerical value used as a weight.\n   If None, then each edge has weight 1.\n   The degree is the sum of the edge weights adjacent to the node.\n\nReturns\n-------\n(degX,degY) : tuple of dictionaries\n   The degrees of the two bipartite sets as dictionaries keyed by node.\n\nExamples\n--------\n>>> from networkx.algorithms import bipartite\n>>> G = nx.complete_bipartite_graph(3, 2)\n>>> Y = set([3, 4])\n>>> degX, degY = bipartite.degrees(G, Y)\n>>> dict(degX)\n{0: 2, 1: 2, 2: 2}\n\n",
  "code": "@nx._dispatch(graphs='B', edge_attrs='weight')\ndef degrees(B, nodes, weight=None):\n    bottom = set(nodes)\n    top = set(B) - bottom\n    return (B.degree(top, weight), B.degree(bottom, weight))"
 },
 {
  "docstring": "Compute the degree centrality for nodes in a bipartite network.\n\nThe degree centrality for a node `v` is the fraction of nodes\nconnected to it.\n\nParameters\n----------\nG : graph\n   A bipartite network\n\nnodes : list or container\n  Container with all nodes in one bipartite node set.\n\nReturns\n-------\ncentrality : dictionary\n   Dictionary keyed by node with bipartite degree centrality as the value.\n\nExamples\n--------\n>>> G = nx.wheel_graph(5)\n>>> top_nodes = {0, 1, 2}\n>>> nx.bipartite.degree_centrality(G, nodes=top_nodes)\n{0: 2.0, 1: 1.5, 2: 1.5, 3: 1.0, 4: 1.0}\n\nSee Also\n--------\nbetweenness_centrality\ncloseness_centrality\n:func:`~networkx.algorithms.bipartite.basic.sets`\n:func:`~networkx.algorithms.bipartite.basic.is_bipartite`\n\n",
  "code": "@nx._dispatch(name='bipartite_degree_centrality')\ndef degree_centrality(G, nodes):\n    top = set(nodes)\n    bottom = set(G) - top\n    s = 1.0 / len(bottom)\n    centrality = {n: d * s for n, d in G.degree(top)}\n    s = 1.0 / len(top)\n    centrality.update({n: d * s for n, d in G.degree(bottom)})\n    return centrality"
 },
 {
  "docstring": "Compute betweenness centrality for nodes in a bipartite network.\n\nBetweenness centrality of a node `v` is the sum of the\nfraction of all-pairs shortest paths that pass through `v`.\n\nValues of betweenness are normalized by the maximum possible\nvalue which for bipartite graphs is limited by the relative size\nof the two node sets [1]_.\n\nLet `n` be the number of nodes in the node set `U` and\n`m` be the number of nodes in the node set `V`, then\nnodes in `U` are normalized by dividing by\n\n.. math::\n\n   \\frac{1}{2} [m^2 (s + 1)^2 + m (s + 1)(2t - s - 1) - t (2s - t + 3)] ,\n\nwhere\n\n.. math::\n\n    s = (n - 1) \\div m , t = (n - 1) \\mod m ,\n\nand nodes in `V` are normalized by dividing by\n\n.. math::\n\n    \\frac{1}{2} [n^2 (p + 1)^2 + n (p + 1)(2r - p - 1) - r (2p - r + 3)] ,\n\nwhere,\n\n.. math::\n\n    p = (m - 1) \\div n , r = (m - 1) \\mod n .\n\nParameters\n----------\nG : graph\n    A bipartite graph\n\nnodes : list or container\n    Container with all nodes in one bipartite node set.\n\nReturns\n-------\nbetweenness : dictionary\n    Dictionary keyed by node with bipartite betweenness centrality\n    as the value.\n\nExamples\n--------\n>>> G = nx.cycle_graph(4)\n>>> top_nodes = {1, 2}\n>>> nx.bipartite.betweenness_centrality(G, nodes=top_nodes)\n{0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}\n\nSee Also\n--------\ndegree_centrality\ncloseness_centrality\n:func:`~networkx.algorithms.bipartite.basic.sets`\n:func:`~networkx.algorithms.bipartite.basic.is_bipartite`\n\n",
  "code": "@nx._dispatch(name='bipartite_betweenness_centrality')\ndef betweenness_centrality(G, nodes):\n    top = set(nodes)\n    bottom = set(G) - top\n    n = len(top)\n    m = len(bottom)\n    s, t = divmod(n - 1, m)\n    bet_max_top = (m ** 2 * (s + 1) ** 2 + m * (s + 1) * (2 * t - s - 1) - t * (2 * s - t + 3)) / 2.0\n    p, r = divmod(m - 1, n)\n    bet_max_bot = (n ** 2 * (p + 1) ** 2 + n * (p + 1) * (2 * r - p - 1) - r * (2 * p - r + 3)) / 2.0\n    betweenness = nx.betweenness_centrality(G, normalized=False, weight=None)\n    for node in top:\n        betweenness[node] /= bet_max_top\n    for node in bottom:\n        betweenness[node] /= bet_max_bot\n    return betweenness"
 },
 {
  "docstring": "Compute the closeness centrality for nodes in a bipartite network.\n\nThe closeness of a node is the distance to all other nodes in the\ngraph or in the case that the graph is not connected to all other nodes\nin the connected component containing that node.\n\nParameters\n----------\nG : graph\n    A bipartite network\n\nnodes : list or container\n    Container with all nodes in one bipartite node set.\n\nnormalized : bool, optional\n  If True (default) normalize by connected component size.\n\nReturns\n-------\ncloseness : dictionary\n    Dictionary keyed by node with bipartite closeness centrality\n    as the value.\n\nExamples\n--------\n>>> G = nx.wheel_graph(5)\n>>> top_nodes = {0, 1, 2}\n>>> nx.bipartite.closeness_centrality(G, nodes=top_nodes)\n{0: 1.5, 1: 1.2, 2: 1.2, 3: 1.0, 4: 1.0}\n\nSee Also\n--------\nbetweenness_centrality\ndegree_centrality\n:func:`~networkx.algorithms.bipartite.basic.sets`\n:func:`~networkx.algorithms.bipartite.basic.is_bipartite`\n\n",
  "code": "@nx._dispatch(name='bipartite_closeness_centrality')\ndef closeness_centrality(G, nodes, normalized=True):\n    closeness = {}\n    path_length = nx.single_source_shortest_path_length\n    top = set(nodes)\n    bottom = set(G) - top\n    n = len(top)\n    m = len(bottom)\n    for node in top:\n        sp = dict(path_length(G, node))\n        totsp = sum(sp.values())\n        if totsp > 0.0 and len(G) > 1:\n            closeness[node] = (m + 2 * (n - 1)) / totsp\n            if normalized:\n                s = (len(sp) - 1) / (len(G) - 1)\n                closeness[node] *= s\n        else:\n            closeness[node] = 0.0\n    for node in bottom:\n        sp = dict(path_length(G, node))\n        totsp = sum(sp.values())\n        if totsp > 0.0 and len(G) > 1:\n            closeness[node] = (n + 2 * (m - 1)) / totsp\n            if normalized:\n                s = (len(sp) - 1) / (len(G) - 1)\n                closeness[node] *= s\n        else:\n            closeness[node] = 0.0\n    return closeness"
 },
 {
  "docstring": "Compute a bipartite clustering coefficient for nodes.\n\nThe bipartite clustering coefficient is a measure of local density\nof connections defined as [1]_:\n\n.. math::\n\n   c_u = \\frac{\\sum_{v \\in N(N(u))} c_{uv} }{|N(N(u))|}\n\nwhere `N(N(u))` are the second order neighbors of `u` in `G` excluding `u`,\nand `c_{uv}` is the pairwise clustering coefficient between nodes\n`u` and `v`.\n\nThe mode selects the function for `c_{uv}` which can be:\n\n`dot`:\n\n.. math::\n\n   c_{uv}=\\frac{|N(u)\\cap N(v)|}{|N(u) \\cup N(v)|}\n\n`min`:\n\n.. math::\n\n   c_{uv}=\\frac{|N(u)\\cap N(v)|}{min(|N(u)|,|N(v)|)}\n\n`max`:\n\n.. math::\n\n   c_{uv}=\\frac{|N(u)\\cap N(v)|}{max(|N(u)|,|N(v)|)}\n\n\nParameters\n----------\nG : graph\n    A bipartite graph\n\nnodes : list or iterable (optional)\n    Compute bipartite clustering for these nodes. The default\n    is all nodes in G.\n\nmode : string\n    The pairwise bipartite clustering method to be used in the computation.\n    It must be \"dot\", \"max\", or \"min\".\n\nReturns\n-------\nclustering : dictionary\n    A dictionary keyed by node with the clustering coefficient value.\n\n\nExamples\n--------\n>>> from networkx.algorithms import bipartite\n>>> G = nx.path_graph(4)  # path graphs are bipartite\n>>> c = bipartite.clustering(G)\n>>> c[0]\n0.5\n>>> c = bipartite.clustering(G, mode=\"min\")\n>>> c[0]\n1.0\n\nSee Also\n--------\nrobins_alexander_clustering\naverage_clustering\nnetworkx.algorithms.cluster.square_clustering\n\n",
  "code": "@nx._dispatch\ndef latapy_clustering(G, nodes=None, mode='dot'):\n    if not nx.algorithms.bipartite.is_bipartite(G):\n        raise nx.NetworkXError('Graph is not bipartite')\n    try:\n        cc_func = modes[mode]\n    except KeyError as err:\n        raise nx.NetworkXError('Mode for bipartite clustering must be: dot, min or max') from err\n    if nodes is None:\n        nodes = G\n    ccs = {}\n    for v in nodes:\n        cc = 0.0\n        nbrs2 = {u for nbr in G[v] for u in G[nbr]} - {v}\n        for u in nbrs2:\n            cc += cc_func(set(G[u]), set(G[v]))\n        if cc > 0.0:\n            cc /= len(nbrs2)\n        ccs[v] = cc\n    return ccs"
 },
 {
  "docstring": "Compute the average bipartite clustering coefficient.\n\nA clustering coefficient for the whole graph is the average,\n\n.. math::\n\n   C = \\frac{1}{n}\\sum_{v \\in G} c_v,\n\nwhere `n` is the number of nodes in `G`.\n\nSimilar measures for the two bipartite sets can be defined [1]_\n\n.. math::\n\n   C_X = \\frac{1}{|X|}\\sum_{v \\in X} c_v,\n\nwhere `X` is a bipartite set of `G`.\n\nParameters\n----------\nG : graph\n    a bipartite graph\n\nnodes : list or iterable, optional\n    A container of nodes to use in computing the average.\n    The nodes should be either the entire graph (the default) or one of the\n    bipartite sets.\n\nmode : string\n    The pairwise bipartite clustering method.\n    It must be \"dot\", \"max\", or \"min\"\n\nReturns\n-------\nclustering : float\n   The average bipartite clustering for the given set of nodes or the\n   entire graph if no nodes are specified.\n\nExamples\n--------\n>>> from networkx.algorithms import bipartite\n>>> G = nx.star_graph(3)  # star graphs are bipartite\n>>> bipartite.average_clustering(G)\n0.75\n>>> X, Y = bipartite.sets(G)\n>>> bipartite.average_clustering(G, X)\n0.0\n>>> bipartite.average_clustering(G, Y)\n1.0\n\nSee Also\n--------\nclustering\n\n",
  "code": "@nx._dispatch(name='bipartite_average_clustering')\ndef average_clustering(G, nodes=None, mode='dot'):\n    if nodes is None:\n        nodes = G\n    ccs = latapy_clustering(G, nodes=nodes, mode=mode)\n    return sum((ccs[v] for v in nodes)) / len(nodes)"
 },
 {
  "docstring": "Compute the bipartite clustering of G.\n\nRobins and Alexander [1]_ defined bipartite clustering coefficient as\nfour times the number of four cycles `C_4` divided by the number of\nthree paths `L_3` in a bipartite graph:\n\n.. math::\n\n   CC_4 = \\frac{4 * C_4}{L_3}\n\nParameters\n----------\nG : graph\n    a bipartite graph\n\nReturns\n-------\nclustering : float\n   The Robins and Alexander bipartite clustering for the input graph.\n\nExamples\n--------\n>>> from networkx.algorithms import bipartite\n>>> G = nx.davis_southern_women_graph()\n>>> print(round(bipartite.robins_alexander_clustering(G), 3))\n0.468\n\nSee Also\n--------\nlatapy_clustering\nnetworkx.algorithms.cluster.square_clustering\n\n",
  "code": "@nx._dispatch\ndef robins_alexander_clustering(G):\n    if G.order() < 4 or G.size() < 3:\n        return 0\n    L_3 = _threepaths(G)\n    if L_3 == 0:\n        return 0\n    C_4 = _four_cycles(G)\n    return 4.0 * C_4 / L_3"
 },
 {
  "docstring": "Returns a set of edges which constitutes\nthe minimum edge cover of the graph.\n\nThe smallest edge cover can be found in polynomial time by finding\na maximum matching and extending it greedily so that all nodes\nare covered.\n\nParameters\n----------\nG : NetworkX graph\n    An undirected bipartite graph.\n\nmatching_algorithm : function\n    A function that returns a maximum cardinality matching in a\n    given bipartite graph. The function must take one input, the\n    graph ``G``, and return a dictionary mapping each node to its\n    mate. If not specified,\n    :func:`~networkx.algorithms.bipartite.matching.hopcroft_karp_matching`\n    will be used. Other possibilities include\n    :func:`~networkx.algorithms.bipartite.matching.eppstein_matching`,\n\nReturns\n-------\nset\n    A set of the edges in a minimum edge cover of the graph, given as\n    pairs of nodes. It contains both the edges `(u, v)` and `(v, u)`\n    for given nodes `u` and `v` among the edges of minimum edge cover.\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch(name='bipartite_min_edge_cover')\ndef min_edge_cover(G, matching_algorithm=None):\n    if G.order() == 0:\n        return set()\n    if matching_algorithm is None:\n        matching_algorithm = hopcroft_karp_matching\n    return _min_edge_cover(G, matching_algorithm=matching_algorithm)"
 },
 {
  "docstring": "Write a bipartite graph as a list of edges.\n\nParameters\n----------\nG : Graph\n   A NetworkX bipartite graph\npath : file or string\n   File or filename to write. If a file is provided, it must be\n   opened in 'wb' mode. Filenames ending in .gz or .bz2 will be compressed.\ncomments : string, optional\n   The character used to indicate the start of a comment\ndelimiter : string, optional\n   The string used to separate values.  The default is whitespace.\ndata : bool or list, optional\n   If False write no edge data.\n   If True write a string representation of the edge data dictionary..\n   If a list (or other iterable) is provided, write the  keys specified\n   in the list.\nencoding: string, optional\n   Specify which encoding to use when writing file.\n\nExamples\n--------\n>>> G = nx.path_graph(4)\n>>> G.add_nodes_from([0, 2], bipartite=0)\n>>> G.add_nodes_from([1, 3], bipartite=1)\n>>> nx.write_edgelist(G, \"test.edgelist\")\n>>> fh = open(\"test.edgelist\", \"wb\")\n>>> nx.write_edgelist(G, fh)\n>>> nx.write_edgelist(G, \"test.edgelist.gz\")\n>>> nx.write_edgelist(G, \"test.edgelist.gz\", data=False)\n\n>>> G = nx.Graph()\n>>> G.add_edge(1, 2, weight=7, color=\"red\")\n>>> nx.write_edgelist(G, \"test.edgelist\", data=False)\n>>> nx.write_edgelist(G, \"test.edgelist\", data=[\"color\"])\n>>> nx.write_edgelist(G, \"test.edgelist\", data=[\"color\", \"weight\"])\n\nSee Also\n--------\nwrite_edgelist\ngenerate_edgelist",
  "code": "@open_file(1, mode='wb')\ndef write_edgelist(G, path, comments='#', delimiter=' ', data=True, encoding='utf-8'):\n    for line in generate_edgelist(G, delimiter, data):\n        line += '\\n'\n        path.write(line.encode(encoding))"
 },
 {
  "docstring": "Generate a single line of the bipartite graph G in edge list format.\n\nParameters\n----------\nG : NetworkX graph\n   The graph is assumed to have node attribute `part` set to 0,1 representing\n   the two graph parts\n\ndelimiter : string, optional\n   Separator for node labels\n\ndata : bool or list of keys\n   If False generate no edge data.  If True use a dictionary\n   representation of edge data.  If a list of keys use a list of data\n   values corresponding to the keys.\n\nReturns\n-------\nlines : string\n    Lines of data in adjlist format.\n\nExamples\n--------\n>>> from networkx.algorithms import bipartite\n>>> G = nx.path_graph(4)\n>>> G.add_nodes_from([0, 2], bipartite=0)\n>>> G.add_nodes_from([1, 3], bipartite=1)\n>>> G[1][2][\"weight\"] = 3\n>>> G[2][3][\"capacity\"] = 12\n>>> for line in bipartite.generate_edgelist(G, data=False):\n...     print(line)\n0 1\n2 1\n2 3\n\n>>> for line in bipartite.generate_edgelist(G):\n...     print(line)\n0 1 {}\n2 1 {'weight': 3}\n2 3 {'capacity': 12}\n\n>>> for line in bipartite.generate_edgelist(G, data=[\"weight\"]):\n...     print(line)\n0 1\n2 1 3\n2 3",
  "code": "@not_implemented_for('directed')\ndef generate_edgelist(G, delimiter=' ', data=True):\n    try:\n        part0 = [n for n, d in G.nodes.items() if d['bipartite'] == 0]\n    except BaseException as err:\n        raise AttributeError('Missing node attribute `bipartite`') from err\n    if data is True or data is False:\n        for n in part0:\n            for edge in G.edges(n, data=data):\n                yield delimiter.join(map(str, edge))\n    else:\n        for n in part0:\n            for u, v, d in G.edges(n, data=True):\n                edge = [u, v]\n                try:\n                    edge.extend((d[k] for k in data))\n                except KeyError:\n                    pass\n                yield delimiter.join(map(str, edge))"
 },
 {
  "docstring": "Parse lines of an edge list representation of a bipartite graph.\n\nParameters\n----------\nlines : list or iterator of strings\n    Input data in edgelist format\ncomments : string, optional\n   Marker for comment lines\ndelimiter : string, optional\n   Separator for node labels\ncreate_using: NetworkX graph container, optional\n   Use given NetworkX graph for holding nodes or edges.\nnodetype : Python type, optional\n   Convert nodes to this type.\ndata : bool or list of (label,type) tuples\n   If False generate no edge data or if True use a dictionary\n   representation of edge data or a list tuples specifying dictionary\n   key names and types for edge data.\n\nReturns\n-------\nG: NetworkX Graph\n    The bipartite graph corresponding to lines\n\nExamples\n--------\nEdgelist with no data:\n\n>>> from networkx.algorithms import bipartite\n>>> lines = [\"1 2\", \"2 3\", \"3 4\"]\n>>> G = bipartite.parse_edgelist(lines, nodetype=int)\n>>> sorted(G.nodes())\n[1, 2, 3, 4]\n>>> sorted(G.nodes(data=True))\n[(1, {'bipartite': 0}), (2, {'bipartite': 0}), (3, {'bipartite': 0}), (4, {'bipartite': 1})]\n>>> sorted(G.edges())\n[(1, 2), (2, 3), (3, 4)]\n\nEdgelist with data in Python dictionary representation:\n\n>>> lines = [\"1 2 {'weight':3}\", \"2 3 {'weight':27}\", \"3 4 {'weight':3.0}\"]\n>>> G = bipartite.parse_edgelist(lines, nodetype=int)\n>>> sorted(G.nodes())\n[1, 2, 3, 4]\n>>> sorted(G.edges(data=True))\n[(1, 2, {'weight': 3}), (2, 3, {'weight': 27}), (3, 4, {'weight': 3.0})]\n\nEdgelist with data in a list:\n\n>>> lines = [\"1 2 3\", \"2 3 27\", \"3 4 3.0\"]\n>>> G = bipartite.parse_edgelist(lines, nodetype=int, data=((\"weight\", float),))\n>>> sorted(G.nodes())\n[1, 2, 3, 4]\n>>> sorted(G.edges(data=True))\n[(1, 2, {'weight': 3.0}), (2, 3, {'weight': 27.0}), (3, 4, {'weight': 3.0})]\n\nSee Also\n--------",
  "code": "@nx._dispatch(name='bipartite_parse_edgelist', graphs=None)\ndef parse_edgelist(lines, comments='#', delimiter=None, create_using=None, nodetype=None, data=True):\n    from ast import literal_eval\n    G = nx.empty_graph(0, create_using)\n    for line in lines:\n        p = line.find(comments)\n        if p >= 0:\n            line = line[:p]\n        if not len(line):\n            continue\n        s = line.strip().split(delimiter)\n        if len(s) < 2:\n            continue\n        u = s.pop(0)\n        v = s.pop(0)\n        d = s\n        if nodetype is not None:\n            try:\n                u = nodetype(u)\n                v = nodetype(v)\n            except BaseException as err:\n                raise TypeError(f'Failed to convert nodes {u},{v} to type {nodetype}.') from err\n        if len(d) == 0 or data is False:\n            edgedata = {}\n        elif data is True:\n            try:\n                edgedata = dict(literal_eval(' '.join(d)))\n            except BaseException as err:\n                raise TypeError(f'Failed to convert edge data ({d}) to dictionary.') from err\n        else:\n            if len(d) != len(data):\n                raise IndexError(f'Edge data {d} and data_keys {data} are not the same length')\n            edgedata = {}\n            for (edge_key, edge_type), edge_value in zip(data, d):\n                try:\n                    edge_value = edge_type(edge_value)\n                except BaseException as err:\n                    raise TypeError(f'Failed to convert {edge_key} data {edge_value} to type {edge_type}.') from err\n                edgedata.update({edge_key: edge_value})\n        G.add_node(u, bipartite=0)\n        G.add_node(v, bipartite=1)\n        G.add_edge(u, v, **edgedata)\n    return G"
 },
 {
  "docstring": "Read a bipartite graph from a list of edges.\n\nParameters\n----------\npath : file or string\n   File or filename to read. If a file is provided, it must be\n   opened in 'rb' mode.\n   Filenames ending in .gz or .bz2 will be uncompressed.\ncomments : string, optional\n   The character used to indicate the start of a comment.\ndelimiter : string, optional\n   The string used to separate values.  The default is whitespace.\ncreate_using : Graph container, optional,\n   Use specified container to build graph.  The default is networkx.Graph,\n   an undirected graph.\nnodetype : int, float, str, Python type, optional\n   Convert node data from strings to specified type\ndata : bool or list of (label,type) tuples\n   Tuples specifying dictionary key names and types for edge data\nedgetype : int, float, str, Python type, optional OBSOLETE\n   Convert edge data from strings to specified type and use as 'weight'\nencoding: string, optional\n   Specify which encoding to use when reading file.\n\nReturns\n-------\nG : graph\n   A networkx Graph or other type specified with create_using\n\nExamples\n--------\n>>> from networkx.algorithms import bipartite\n>>> G = nx.path_graph(4)\n>>> G.add_nodes_from([0, 2], bipartite=0)\n>>> G.add_nodes_from([1, 3], bipartite=1)\n>>> bipartite.write_edgelist(G, \"test.edgelist\")\n>>> G = bipartite.read_edgelist(\"test.edgelist\")\n\n>>> fh = open(\"test.edgelist\", \"rb\")\n>>> G = bipartite.read_edgelist(fh)\n>>> fh.close()\n\n>>> G = bipartite.read_edgelist(\"test.edgelist\", nodetype=int)\n\nEdgelist with data in a list:\n\n>>> textline = \"1 2 3\"\n>>> fh = open(\"test.edgelist\", \"w\")\n>>> d = fh.write(textline)\n>>> fh.close()\n>>> G = bipartite.read_edgelist(\n...     \"test.edgelist\", nodetype=int, data=((\"weight\", float),)\n... )\n>>> list(G)\n[1, 2]\n>>> list(G.edges(data=True))\n[(1, 2, {'weight': 3.0})]\n\nSee parse_edgelist() for more examples of formatting.\n\nSee Also\n--------\nparse_edgelist\n\n",
  "code": "@open_file(0, mode='rb')\n@nx._dispatch(name='bipartite_read_edgelist', graphs=None)\ndef read_edgelist(path, comments='#', delimiter=None, create_using=None, nodetype=None, data=True, edgetype=None, encoding='utf-8'):\n    lines = (line.decode(encoding) for line in path)\n    return parse_edgelist(lines, comments=comments, delimiter=delimiter, create_using=create_using, nodetype=nodetype, data=data)"
 },
 {
  "docstring": "Computes the extendability of a graph.\n\nThe extendability of a graph is defined as the maximum $k$ for which `G`\nis $k$-extendable. Graph `G` is $k$-extendable if and only if `G` has a\nperfect matching and every set of $k$ independent edges can be extended\nto a perfect matching in `G`.\n\nParameters\n----------\nG : NetworkX Graph\n    A fully-connected bipartite graph without self-loops\n\nReturns\n-------\nextendability : int\n\nRaises\n------\nNetworkXError\n   If the graph `G` is disconnected.\n   If the graph `G` is not bipartite.\n   If the graph `G` does not contain a perfect matching.\n   If the residual graph of `G` is not strongly connected.\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\ndef maximal_extendability(G):\n    if not nx.is_connected(G):\n        raise nx.NetworkXError('Graph G is not connected')\n    if not nx.bipartite.is_bipartite(G):\n        raise nx.NetworkXError('Graph G is not bipartite')\n    U, V = nx.bipartite.sets(G)\n    maximum_matching = nx.bipartite.hopcroft_karp_matching(G)\n    if not nx.is_perfect_matching(G, maximum_matching):\n        raise nx.NetworkXError('Graph G does not contain a perfect matching')\n    pm = [(node, maximum_matching[node]) for node in V & maximum_matching.keys()]\n    directed_edges = [(x, y) if x in V and (x, y) in pm or (x in U and (y, x) not in pm) else (y, x) for x, y in G.edges]\n    residual_G = nx.DiGraph()\n    residual_G.add_nodes_from(G)\n    residual_G.add_edges_from(directed_edges)\n    if not nx.is_strongly_connected(residual_G):\n        raise nx.NetworkXError('The residual graph of G is not strongly connected')\n    k = float('Inf')\n    for u in U:\n        for v in V:\n            num_paths = sum((1 for _ in nx.node_disjoint_paths(residual_G, u, v)))\n            k = k if k < num_paths else num_paths\n    return k"
 },
 {
  "docstring": "Returns the complete bipartite graph `K_{n_1,n_2}`.\n\nThe graph is composed of two partitions with nodes 0 to (n1 - 1)\nin the first and nodes n1 to (n1 + n2 - 1) in the second.\nEach node in the first is connected to each node in the second.\n\nParameters\n----------\nn1, n2 : integer or iterable container of nodes\n    If integers, nodes are from `range(n1)` and `range(n1, n1 + n2)`.\n    If a container, the elements are the nodes.\ncreate_using : NetworkX graph instance, (default: nx.Graph)\n   Return graph of this type.\n\n",
  "code": "@nx._dispatch(graphs=None)\n@nodes_or_number([0, 1])\ndef complete_bipartite_graph(n1, n2, create_using=None):\n    G = nx.empty_graph(0, create_using)\n    if G.is_directed():\n        raise nx.NetworkXError('Directed Graph not supported')\n    n1, top = n1\n    n2, bottom = n2\n    if isinstance(n1, numbers.Integral) and isinstance(n2, numbers.Integral):\n        bottom = [n1 + i for i in bottom]\n    G.add_nodes_from(top, bipartite=0)\n    G.add_nodes_from(bottom, bipartite=1)\n    if len(G) != len(top) + len(bottom):\n        raise nx.NetworkXError('Inputs n1 and n2 must contain distinct nodes')\n    G.add_edges_from(((u, v) for u in top for v in bottom))\n    G.graph['name'] = f'complete_bipartite_graph({n1}, {n2})'\n    return G"
 },
 {
  "docstring": "Returns a random bipartite graph from two given degree sequences.\n\nParameters\n----------\naseq : list\n   Degree sequence for node set A.\nbseq : list\n   Degree sequence for node set B.\ncreate_using : NetworkX graph instance, optional\n   Return graph of this type.\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\nThe graph is composed of two partitions. Set A has nodes 0 to\n(len(aseq) - 1) and set B has nodes len(aseq) to (len(bseq) - 1).\nNodes from set A are connected to nodes in set B by choosing\nrandomly from the possible free stubs, one in A and one in B.\n\n",
  "code": "@py_random_state(3)\n@nx._dispatch(name='bipartite_configuration_model', graphs=None)\ndef configuration_model(aseq, bseq, create_using=None, seed=None):\n    G = nx.empty_graph(0, create_using, default=nx.MultiGraph)\n    if G.is_directed():\n        raise nx.NetworkXError('Directed Graph not supported')\n    lena = len(aseq)\n    lenb = len(bseq)\n    suma = sum(aseq)\n    sumb = sum(bseq)\n    if not suma == sumb:\n        raise nx.NetworkXError(f'invalid degree sequences, sum(aseq)!=sum(bseq),{suma},{sumb}')\n    G = _add_nodes_with_bipartite_label(G, lena, lenb)\n    if len(aseq) == 0 or max(aseq) == 0:\n        return G\n    stubs = [[v] * aseq[v] for v in range(lena)]\n    astubs = [x for subseq in stubs for x in subseq]\n    stubs = [[v] * bseq[v - lena] for v in range(lena, lena + lenb)]\n    bstubs = [x for subseq in stubs for x in subseq]\n    seed.shuffle(astubs)\n    seed.shuffle(bstubs)\n    G.add_edges_from(([astubs[i], bstubs[i]] for i in range(suma)))\n    G.name = 'bipartite_configuration_model'\n    return G"
 },
 {
  "docstring": "Returns a bipartite graph from two given degree sequences using a\nHavel-Hakimi style construction.\n\nThe graph is composed of two partitions. Set A has nodes 0 to\n(len(aseq) - 1) and set B has nodes len(aseq) to (len(bseq) - 1).\nNodes from the set A are connected to nodes in the set B by\nconnecting the highest degree nodes in set A to the highest degree\nnodes in set B until all stubs are connected.\n\nParameters\n----------\naseq : list\n   Degree sequence for node set A.\nbseq : list\n   Degree sequence for node set B.\ncreate_using : NetworkX graph instance, optional\n   Return graph of this type.\n\n",
  "code": "@nx._dispatch(name='bipartite_havel_hakimi_graph', graphs=None)\ndef havel_hakimi_graph(aseq, bseq, create_using=None):\n    G = nx.empty_graph(0, create_using, default=nx.MultiGraph)\n    if G.is_directed():\n        raise nx.NetworkXError('Directed Graph not supported')\n    naseq = len(aseq)\n    nbseq = len(bseq)\n    suma = sum(aseq)\n    sumb = sum(bseq)\n    if not suma == sumb:\n        raise nx.NetworkXError(f'invalid degree sequences, sum(aseq)!=sum(bseq),{suma},{sumb}')\n    G = _add_nodes_with_bipartite_label(G, naseq, nbseq)\n    if len(aseq) == 0 or max(aseq) == 0:\n        return G\n    astubs = [[aseq[v], v] for v in range(naseq)]\n    bstubs = [[bseq[v - naseq], v] for v in range(naseq, naseq + nbseq)]\n    astubs.sort()\n    while astubs:\n        degree, u = astubs.pop()\n        if degree == 0:\n            break\n        bstubs.sort()\n        for target in bstubs[-degree:]:\n            v = target[1]\n            G.add_edge(u, v)\n            target[0] -= 1\n            if target[0] == 0:\n                bstubs.remove(target)\n    G.name = 'bipartite_havel_hakimi_graph'\n    return G"
 },
 {
  "docstring": "Returns a bipartite graph from two given degree sequences using a\nHavel-Hakimi style construction.\n\nThe graph is composed of two partitions. Set A has nodes 0 to\n(len(aseq) - 1) and set B has nodes len(aseq) to (len(bseq) - 1).\nNodes from set A are connected to nodes in the set B by connecting\nthe highest degree nodes in set A to the lowest degree nodes in\nset B until all stubs are connected.\n\nParameters\n----------\naseq : list\n   Degree sequence for node set A.\nbseq : list\n   Degree sequence for node set B.\ncreate_using : NetworkX graph instance, optional\n   Return graph of this type.\n\n",
  "code": "@nx._dispatch(graphs=None)\ndef reverse_havel_hakimi_graph(aseq, bseq, create_using=None):\n    G = nx.empty_graph(0, create_using, default=nx.MultiGraph)\n    if G.is_directed():\n        raise nx.NetworkXError('Directed Graph not supported')\n    lena = len(aseq)\n    lenb = len(bseq)\n    suma = sum(aseq)\n    sumb = sum(bseq)\n    if not suma == sumb:\n        raise nx.NetworkXError(f'invalid degree sequences, sum(aseq)!=sum(bseq),{suma},{sumb}')\n    G = _add_nodes_with_bipartite_label(G, lena, lenb)\n    if len(aseq) == 0 or max(aseq) == 0:\n        return G\n    astubs = [[aseq[v], v] for v in range(lena)]\n    bstubs = [[bseq[v - lena], v] for v in range(lena, lena + lenb)]\n    astubs.sort()\n    bstubs.sort()\n    while astubs:\n        degree, u = astubs.pop()\n        if degree == 0:\n            break\n        for target in bstubs[0:degree]:\n            v = target[1]\n            G.add_edge(u, v)\n            target[0] -= 1\n            if target[0] == 0:\n                bstubs.remove(target)\n    G.name = 'bipartite_reverse_havel_hakimi_graph'\n    return G"
 },
 {
  "docstring": "Returns a bipartite graph from two given degree sequences using\nan alternating Havel-Hakimi style construction.\n\nThe graph is composed of two partitions. Set A has nodes 0 to\n(len(aseq) - 1) and set B has nodes len(aseq) to (len(bseq) - 1).\nNodes from the set A are connected to nodes in the set B by\nconnecting the highest degree nodes in set A to alternatively the\nhighest and the lowest degree nodes in set B until all stubs are\nconnected.\n\nParameters\n----------\naseq : list\n   Degree sequence for node set A.\nbseq : list\n   Degree sequence for node set B.\ncreate_using : NetworkX graph instance, optional\n   Return graph of this type.\n\n",
  "code": "@nx._dispatch(graphs=None)\ndef alternating_havel_hakimi_graph(aseq, bseq, create_using=None):\n    G = nx.empty_graph(0, create_using, default=nx.MultiGraph)\n    if G.is_directed():\n        raise nx.NetworkXError('Directed Graph not supported')\n    naseq = len(aseq)\n    nbseq = len(bseq)\n    suma = sum(aseq)\n    sumb = sum(bseq)\n    if not suma == sumb:\n        raise nx.NetworkXError(f'invalid degree sequences, sum(aseq)!=sum(bseq),{suma},{sumb}')\n    G = _add_nodes_with_bipartite_label(G, naseq, nbseq)\n    if len(aseq) == 0 or max(aseq) == 0:\n        return G\n    astubs = [[aseq[v], v] for v in range(naseq)]\n    bstubs = [[bseq[v - naseq], v] for v in range(naseq, naseq + nbseq)]\n    while astubs:\n        astubs.sort()\n        degree, u = astubs.pop()\n        if degree == 0:\n            break\n        bstubs.sort()\n        small = bstubs[0:degree // 2]\n        large = bstubs[-degree + degree // 2:]\n        stubs = [x for z in zip(large, small) for x in z]\n        if len(stubs) < len(small) + len(large):\n            stubs.append(large.pop())\n        for target in stubs:\n            v = target[1]\n            G.add_edge(u, v)\n            target[0] -= 1\n            if target[0] == 0:\n                bstubs.remove(target)\n    G.name = 'bipartite_alternating_havel_hakimi_graph'\n    return G"
 },
 {
  "docstring": "Create a bipartite graph with a preferential attachment model from\na given single degree sequence.\n\nThe graph is composed of two partitions. Set A has nodes 0 to\n(len(aseq) - 1) and set B has nodes starting with node len(aseq).\nThe number of nodes in set B is random.\n\nParameters\n----------\naseq : list\n   Degree sequence for node set A.\np :  float\n   Probability that a new bottom node is added.\ncreate_using : NetworkX graph instance, optional\n   Return graph of this type.\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\n",
  "code": "@py_random_state(3)\n@nx._dispatch(graphs=None)\ndef preferential_attachment_graph(aseq, p, create_using=None, seed=None):\n    G = nx.empty_graph(0, create_using, default=nx.MultiGraph)\n    if G.is_directed():\n        raise nx.NetworkXError('Directed Graph not supported')\n    if p > 1:\n        raise nx.NetworkXError(f'probability {p} > 1')\n    naseq = len(aseq)\n    G = _add_nodes_with_bipartite_label(G, naseq, 0)\n    vv = [[v] * aseq[v] for v in range(naseq)]\n    while vv:\n        while vv[0]:\n            source = vv[0][0]\n            vv[0].remove(source)\n            if seed.random() < p or len(G) == naseq:\n                target = len(G)\n                G.add_node(target, bipartite=1)\n                G.add_edge(source, target)\n            else:\n                bb = [[b] * G.degree(b) for b in range(naseq, len(G))]\n                bbstubs = reduce(lambda x, y: x + y, bb)\n                target = seed.choice(bbstubs)\n                G.add_node(target, bipartite=1)\n                G.add_edge(source, target)\n        vv.remove(vv[0])\n    G.name = 'bipartite_preferential_attachment_model'\n    return G"
 },
 {
  "docstring": "Returns a bipartite random graph.\n\nThis is a bipartite version of the binomial (Erd\u0151s-R\u00e9nyi) graph.\nThe graph is composed of two partitions. Set A has nodes 0 to\n(n - 1) and set B has nodes n to (n + m - 1).\n\nParameters\n----------\nn : int\n    The number of nodes in the first bipartite set.\nm : int\n    The number of nodes in the second bipartite set.\np : float\n    Probability for edge creation.\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\ndirected : bool, optional (default=False)\n    If True return a directed graph\n\n",
  "code": "@py_random_state(3)\n@nx._dispatch(graphs=None)\ndef random_graph(n, m, p, seed=None, directed=False):\n    G = nx.Graph()\n    G = _add_nodes_with_bipartite_label(G, n, m)\n    if directed:\n        G = nx.DiGraph(G)\n    G.name = f'fast_gnp_random_graph({n},{m},{p})'\n    if p <= 0:\n        return G\n    if p >= 1:\n        return nx.complete_bipartite_graph(n, m)\n    lp = math.log(1.0 - p)\n    v = 0\n    w = -1\n    while v < n:\n        lr = math.log(1.0 - seed.random())\n        w = w + 1 + int(lr / lp)\n        while w >= m and v < n:\n            w = w - m\n            v = v + 1\n        if v < n:\n            G.add_edge(v, n + w)\n    if directed:\n        v = 0\n        w = -1\n        while v < n:\n            lr = math.log(1.0 - seed.random())\n            w = w + 1 + int(lr / lp)\n            while w >= m and v < n:\n                w = w - m\n                v = v + 1\n            if v < n:\n                G.add_edge(n + w, v)\n    return G"
 },
 {
  "docstring": "Returns a random bipartite graph G_{n,m,k}.\n\nProduces a bipartite graph chosen randomly out of the set of all graphs\nwith n top nodes, m bottom nodes, and k edges.\nThe graph is composed of two sets of nodes.\nSet A has nodes 0 to (n - 1) and set B has nodes n to (n + m - 1).\n\nParameters\n----------\nn : int\n    The number of nodes in the first bipartite set.\nm : int\n    The number of nodes in the second bipartite set.\nk : int\n    The number of edges\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\ndirected : bool, optional (default=False)\n    If True return a directed graph\n\nExamples\n--------\nfrom nx.algorithms import bipartite\nG = bipartite.gnmk_random_graph(10,20,50)\n\nSee Also\n--------\ngnm_random_graph\n\n",
  "code": "@py_random_state(3)\n@nx._dispatch(graphs=None)\ndef gnmk_random_graph(n, m, k, seed=None, directed=False):\n    G = nx.Graph()\n    G = _add_nodes_with_bipartite_label(G, n, m)\n    if directed:\n        G = nx.DiGraph(G)\n    G.name = f'bipartite_gnm_random_graph({n},{m},{k})'\n    if n == 1 or m == 1:\n        return G\n    max_edges = n * m\n    if k >= max_edges:\n        return nx.complete_bipartite_graph(n, m, create_using=G)\n    top = [n for n, d in G.nodes(data=True) if d['bipartite'] == 0]\n    bottom = list(set(G) - set(top))\n    edge_count = 0\n    while edge_count < k:\n        u = seed.choice(top)\n        v = seed.choice(bottom)\n        if v in G[u]:\n            continue\n        else:\n            G.add_edge(u, v)\n            edge_count += 1\n    return G"
 },
 {
  "docstring": "Returns the maximum cardinality matching of the bipartite graph `G`.\n\nA matching is a set of edges that do not share any nodes. A maximum\ncardinality matching is a matching with the most edges possible. It\nis not always unique. Finding a matching in a bipartite graph can be\ntreated as a networkx flow problem.\n\nThe functions ``hopcroft_karp_matching`` and ``maximum_matching``\nare aliases of the same function.\n\nParameters\n----------\nG : NetworkX graph\n\n  Undirected bipartite graph\n\ntop_nodes : container of nodes\n\n  Container with all nodes in one bipartite node set. If not supplied\n  it will be computed. But if more than one solution exists an exception\n  will be raised.\n\nReturns\n-------\nmatches : dictionary\n\n  The matching is returned as a dictionary, `matches`, such that\n  ``matches[v] == w`` if node `v` is matched to node `w`. Unmatched\n  nodes do not occur as a key in `matches`.\n\nRaises\n------\nAmbiguousSolution\n  Raised if the input bipartite graph is disconnected and no container\n  with all nodes in one bipartite set is provided. When determining\n  the nodes in each bipartite set more than one valid solution is\n  possible if the input graph is disconnected.\n\n",
  "code": "@nx._dispatch\ndef hopcroft_karp_matching(G, top_nodes=None):\n\n    def breadth_first_search():\n        for v in left:\n            if leftmatches[v] is None:\n                distances[v] = 0\n                queue.append(v)\n            else:\n                distances[v] = INFINITY\n        distances[None] = INFINITY\n        while queue:\n            v = queue.popleft()\n            if distances[v] < distances[None]:\n                for u in G[v]:\n                    if distances[rightmatches[u]] is INFINITY:\n                        distances[rightmatches[u]] = distances[v] + 1\n                        queue.append(rightmatches[u])\n        return distances[None] is not INFINITY\n\n    def depth_first_search(v):\n        if v is not None:\n            for u in G[v]:\n                if distances[rightmatches[u]] == distances[v] + 1:\n                    if depth_first_search(rightmatches[u]):\n                        rightmatches[u] = v\n                        leftmatches[v] = u\n                        return True\n            distances[v] = INFINITY\n            return False\n        return True\n    left, right = bipartite_sets(G, top_nodes)\n    leftmatches = {v: None for v in left}\n    rightmatches = {v: None for v in right}\n    distances = {}\n    queue = collections.deque()\n    num_matched_pairs = 0\n    while breadth_first_search():\n        for v in left:\n            if leftmatches[v] is None:\n                if depth_first_search(v):\n                    num_matched_pairs += 1\n    leftmatches = {k: v for k, v in leftmatches.items() if v is not None}\n    rightmatches = {k: v for k, v in rightmatches.items() if v is not None}\n    return dict(itertools.chain(leftmatches.items(), rightmatches.items()))"
 },
 {
  "docstring": "Returns the maximum cardinality matching of the bipartite graph `G`.\n\nParameters\n----------\nG : NetworkX graph\n\n  Undirected bipartite graph\n\ntop_nodes : container\n\n  Container with all nodes in one bipartite node set. If not supplied\n  it will be computed. But if more than one solution exists an exception\n  will be raised.\n\nReturns\n-------\nmatches : dictionary\n\n  The matching is returned as a dictionary, `matching`, such that\n  ``matching[v] == w`` if node `v` is matched to node `w`. Unmatched\n  nodes do not occur as a key in `matching`.\n\nRaises\n------\nAmbiguousSolution\n  Raised if the input bipartite graph is disconnected and no container\n  with all nodes in one bipartite set is provided. When determining\n  the nodes in each bipartite set more than one valid solution is\n  possible if the input graph is disconnected.\n\n",
  "code": "@nx._dispatch\ndef eppstein_matching(G, top_nodes=None):\n    left, right = bipartite_sets(G, top_nodes)\n    G = nx.DiGraph(G.edges(left))\n    matching = {}\n    for u in G:\n        for v in G[u]:\n            if v not in matching:\n                matching[v] = u\n                break\n    while True:\n        preds = {}\n        unmatched = []\n        pred = {u: unmatched for u in G}\n        for v in matching:\n            del pred[matching[v]]\n        layer = list(pred)\n        while layer and (not unmatched):\n            newLayer = {}\n            for u in layer:\n                for v in G[u]:\n                    if v not in preds:\n                        newLayer.setdefault(v, []).append(u)\n            layer = []\n            for v in newLayer:\n                preds[v] = newLayer[v]\n                if v in matching:\n                    layer.append(matching[v])\n                    pred[matching[v]] = v\n                else:\n                    unmatched.append(v)\n        if not unmatched:\n            for key in matching.copy():\n                matching[matching[key]] = key\n            return matching\n\n        def recurse(v):\n            if v in preds:\n                L = preds.pop(v)\n                for u in L:\n                    if u in pred:\n                        pu = pred.pop(u)\n                        if pu is unmatched or recurse(pu):\n                            matching[v] = u\n                            return True\n            return False\n        for v in unmatched:\n            recurse(v)"
 },
 {
  "docstring": "Returns True if and only if the vertex `v` is connected to one of\nthe target vertices by an alternating path in `G`.\n\nAn *alternating path* is a path in which every other edge is in the\nspecified maximum matching (and the remaining edges in the path are not in\nthe matching). An alternating path may have matched edges in the even\npositions or in the odd positions, as long as the edges alternate between\n'matched' and 'unmatched'.\n\n`G` is an undirected bipartite NetworkX graph.\n\n`v` is a vertex in `G`.\n\n`matched_edges` is a set of edges present in a maximum matching in `G`.\n\n`unmatched_edges` is a set of edges not present in a maximum\nmatching in `G`.\n\n`targets` is a set of vertices.",
  "code": "def _is_connected_by_alternating_path(G, v, matched_edges, unmatched_edges, targets):\n\n    def _alternating_dfs(u, along_matched=True):\n        \"\"\"Returns True if and only if `u` is connected to one of the\n        targets by an alternating path.\n\n        `u` is a vertex in the graph `G`.\n\n        If `along_matched` is True, this step of the depth-first search\n        will continue only through edges in the given matching. Otherwise, it\n        will continue only through edges *not* in the given matching.\n\n        \"\"\"\n        visited = set()\n        initial_depth = 0 if along_matched else 1\n        stack = [(u, iter(G[u]), initial_depth)]\n        while stack:\n            parent, children, depth = stack[-1]\n            valid_edges = matched_edges if depth % 2 else unmatched_edges\n            try:\n                child = next(children)\n                if child not in visited:\n                    if (parent, child) in valid_edges or (child, parent) in valid_edges:\n                        if child in targets:\n                            return True\n                        visited.add(child)\n                        stack.append((child, iter(G[child]), depth + 1))\n            except StopIteration:\n                stack.pop()\n        return False\n    return _alternating_dfs(v, along_matched=True) or _alternating_dfs(v, along_matched=False)"
 },
 {
  "docstring": "Returns the set of vertices that are connected to one of the target\nvertices by an alternating path in `G` or are themselves a target.\n\nAn *alternating path* is a path in which every other edge is in the\nspecified maximum matching (and the remaining edges in the path are not in\nthe matching). An alternating path may have matched edges in the even\npositions or in the odd positions, as long as the edges alternate between\n'matched' and 'unmatched'.\n\n`G` is an undirected bipartite NetworkX graph.\n\n`matching` is a dictionary representing a maximum matching in `G`, as\nreturned by, for example, :func:`maximum_matching`.\n\n`targets` is a set of vertices.",
  "code": "def _connected_by_alternating_paths(G, matching, targets):\n    edge_sets = {frozenset((u, v)) for u, v in matching.items()}\n    matched_edges = {tuple(edge) for edge in edge_sets}\n    unmatched_edges = {(u, v) for u, v in G.edges() if frozenset((u, v)) not in edge_sets}\n    return {v for v in G if v in targets or _is_connected_by_alternating_path(G, v, matched_edges, unmatched_edges, targets)}"
 },
 {
  "docstring": "Returns the minimum vertex cover corresponding to the given maximum\nmatching of the bipartite graph `G`.\n\nParameters\n----------\nG : NetworkX graph\n\n  Undirected bipartite graph\n\nmatching : dictionary\n\n  A dictionary whose keys are vertices in `G` and whose values are the\n  distinct neighbors comprising the maximum matching for `G`, as returned\n  by, for example, :func:`maximum_matching`. The dictionary *must*\n  represent the maximum matching.\n\ntop_nodes : container\n\n  Container with all nodes in one bipartite node set. If not supplied\n  it will be computed. But if more than one solution exists an exception\n  will be raised.\n\nReturns\n-------\nvertex_cover : :class:`set`\n\n  The minimum vertex cover in `G`.\n\nRaises\n------\nAmbiguousSolution\n  Raised if the input bipartite graph is disconnected and no container\n  with all nodes in one bipartite set is provided. When determining\n  the nodes in each bipartite set more than one valid solution is\n  possible if the input graph is disconnected.\n\n",
  "code": "@nx._dispatch\ndef to_vertex_cover(G, matching, top_nodes=None):\n    L, R = bipartite_sets(G, top_nodes)\n    unmatched_vertices = set(G) - set(matching)\n    U = unmatched_vertices & L\n    Z = _connected_by_alternating_paths(G, matching, U)\n    return L - Z | R & Z"
 },
 {
  "docstring": "Returns a minimum weight full matching of the bipartite graph `G`.\n\nLet :math:`G = ((U, V), E)` be a weighted bipartite graph with real weights\n:math:`w : E \\to \\mathbb{R}`. This function then produces a matching\n:math:`M \\subseteq E` with cardinality\n\n.. math::\n   \\lvert M \\rvert = \\min(\\lvert U \\rvert, \\lvert V \\rvert),\n\nwhich minimizes the sum of the weights of the edges included in the\nmatching, :math:`\\sum_{e \\in M} w(e)`, or raises an error if no such\nmatching exists.\n\nWhen :math:`\\lvert U \\rvert = \\lvert V \\rvert`, this is commonly\nreferred to as a perfect matching; here, since we allow\n:math:`\\lvert U \\rvert` and :math:`\\lvert V \\rvert` to differ, we\nfollow Karp [1]_ and refer to the matching as *full*.\n\nParameters\n----------\nG : NetworkX graph\n\n  Undirected bipartite graph\n\ntop_nodes : container\n\n  Container with all nodes in one bipartite node set. If not supplied\n  it will be computed.\n\nweight : string, optional (default='weight')\n\n   The edge data key used to provide each value in the matrix.\n   If None, then each edge has weight 1.\n\nReturns\n-------\nmatches : dictionary\n\n  The matching is returned as a dictionary, `matches`, such that\n  ``matches[v] == w`` if node `v` is matched to node `w`. Unmatched\n  nodes do not occur as a key in `matches`.\n\nRaises\n------\nValueError\n  Raised if no full matching exists.\n\nImportError\n  Raised if SciPy is not available.\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef minimum_weight_full_matching(G, top_nodes=None, weight='weight'):\n    import numpy as np\n    import scipy as sp\n    left, right = nx.bipartite.sets(G, top_nodes)\n    U = list(left)\n    V = list(right)\n    weights_sparse = biadjacency_matrix(G, row_order=U, column_order=V, weight=weight, format='coo')\n    weights = np.full(weights_sparse.shape, np.inf)\n    weights[weights_sparse.row, weights_sparse.col] = weights_sparse.data\n    left_matches = sp.optimize.linear_sum_assignment(weights)\n    d = {U[u]: V[v] for u, v in zip(*left_matches)}\n    d.update({v: u for u, v in d.items()})\n    return d"
 },
 {
  "docstring": "Returns True if and only if `u` is connected to one of the\ntargets by an alternating path.\n\n`u` is a vertex in the graph `G`.\n\nIf `along_matched` is True, this step of the depth-first search\nwill continue only through edges in the given matching. Otherwise, it\nwill continue only through edges *not* in the given matching.",
  "code": "def _alternating_dfs(u, along_matched=True):\n    visited = set()\n    initial_depth = 0 if along_matched else 1\n    stack = [(u, iter(G[u]), initial_depth)]\n    while stack:\n        parent, children, depth = stack[-1]\n        valid_edges = matched_edges if depth % 2 else unmatched_edges\n        try:\n            child = next(children)\n            if child not in visited:\n                if (parent, child) in valid_edges or (child, parent) in valid_edges:\n                    if child in targets:\n                        return True\n                    visited.add(child)\n                    stack.append((child, iter(G[child]), depth + 1))\n        except StopIteration:\n            stack.pop()\n    return False"
 },
 {
  "docstring": "Returns the biadjacency matrix of the bipartite graph G.\n\nLet `G = (U, V, E)` be a bipartite graph with node sets\n`U = u_{1},...,u_{r}` and `V = v_{1},...,v_{s}`. The biadjacency\nmatrix [1]_ is the `r` x `s` matrix `B` in which `b_{i,j} = 1`\nif, and only if, `(u_i, v_j) \\in E`. If the parameter `weight` is\nnot `None` and matches the name of an edge attribute, its value is\nused instead of 1.\n\nParameters\n----------\nG : graph\n   A NetworkX graph\n\nrow_order : list of nodes\n   The rows of the matrix are ordered according to the list of nodes.\n\ncolumn_order : list, optional\n   The columns of the matrix are ordered according to the list of nodes.\n   If column_order is None, then the ordering of columns is arbitrary.\n\ndtype : NumPy data-type, optional\n    A valid NumPy dtype used to initialize the array. If None, then the\n    NumPy default is used.\n\nweight : string or None, optional (default='weight')\n   The edge data key used to provide each value in the matrix.\n   If None, then each edge has weight 1.\n\nformat : str in {'bsr', 'csr', 'csc', 'coo', 'lil', 'dia', 'dok'}\n    The type of the matrix to be returned (default 'csr').  For\n    some algorithms different implementations of sparse matrices\n    can perform better.  See [2]_ for details.\n\nReturns\n-------\nM : SciPy sparse array\n    Biadjacency matrix representation of the bipartite graph G.\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef biadjacency_matrix(G, row_order, column_order=None, dtype=None, weight='weight', format='csr'):\n    import scipy as sp\n    nlen = len(row_order)\n    if nlen == 0:\n        raise nx.NetworkXError('row_order is empty list')\n    if len(row_order) != len(set(row_order)):\n        msg = 'Ambiguous ordering: `row_order` contained duplicates.'\n        raise nx.NetworkXError(msg)\n    if column_order is None:\n        column_order = list(set(G) - set(row_order))\n    mlen = len(column_order)\n    if len(column_order) != len(set(column_order)):\n        msg = 'Ambiguous ordering: `column_order` contained duplicates.'\n        raise nx.NetworkXError(msg)\n    row_index = dict(zip(row_order, itertools.count()))\n    col_index = dict(zip(column_order, itertools.count()))\n    if G.number_of_edges() == 0:\n        row, col, data = ([], [], [])\n    else:\n        row, col, data = zip(*((row_index[u], col_index[v], d.get(weight, 1)) for u, v, d in G.edges(row_order, data=True) if u in row_index and v in col_index))\n    A = sp.sparse.coo_array((data, (row, col)), shape=(nlen, mlen), dtype=dtype)\n    try:\n        return A.asformat(format)\n    except ValueError as err:\n        raise nx.NetworkXError(f'Unknown sparse array format: {format}') from err"
 },
 {
  "docstring": "Creates a new bipartite graph from a biadjacency matrix given as a\nSciPy sparse array.\n\nParameters\n----------\nA: scipy sparse array\n  A biadjacency matrix representation of a graph\n\ncreate_using: NetworkX graph\n   Use specified graph for result.  The default is Graph()\n\nedge_attribute: string\n   Name of edge attribute to store matrix numeric value. The data will\n   have the same type as the matrix entry (int, float, (real,imag)).\n\n",
  "code": "@nx._dispatch(graphs=None)\ndef from_biadjacency_matrix(A, create_using=None, edge_attribute='weight'):\n    G = nx.empty_graph(0, create_using)\n    n, m = A.shape\n    G.add_nodes_from(range(n), bipartite=0)\n    G.add_nodes_from(range(n, n + m), bipartite=1)\n    triples = ((u, n + v, d) for u, v, d in _generate_weighted_edges(A))\n    if A.dtype.kind in ('i', 'u') and G.is_multigraph():\n        chain = itertools.chain.from_iterable\n        triples = chain((((u, v, 1) for d in range(w)) for u, v, w in triples))\n    G.add_weighted_edges_from(triples, weight=edge_attribute)\n    return G"
 },
 {
  "docstring": "Returns the projection of B onto one of its node sets.\n\nReturns the graph G that is the projection of the bipartite graph B\nonto the specified nodes. They retain their attributes and are connected\nin G if they have a common neighbor in B.\n\nParameters\n----------\nB : NetworkX graph\n  The input graph should be bipartite.\n\nnodes : list or iterable\n  Nodes to project onto (the \"bottom\" nodes).\n\nmultigraph: bool (default=False)\n   If True return a multigraph where the multiple edges represent multiple\n   shared neighbors.  They edge key in the multigraph is assigned to the\n   label of the neighbor.\n\nReturns\n-------\nGraph : NetworkX graph or multigraph\n   A graph that is the projection onto the given nodes.\n\nExamples\n--------\n>>> from networkx.algorithms import bipartite\n>>> B = nx.path_graph(4)\n>>> G = bipartite.projected_graph(B, [1, 3])\n>>> list(G)\n[1, 3]\n>>> list(G.edges())\n[(1, 3)]\n\nIf nodes `a`, and `b` are connected through both nodes 1 and 2 then\nbuilding a multigraph results in two edges in the projection onto\n[`a`, `b`]:\n\n>>> B = nx.Graph()\n>>> B.add_edges_from([(\"a\", 1), (\"b\", 1), (\"a\", 2), (\"b\", 2)])\n>>> G = bipartite.projected_graph(B, [\"a\", \"b\"], multigraph=True)\n>>> print([sorted((u, v)) for u, v in G.edges()])\n[['a', 'b'], ['a', 'b']]\n\n",
  "code": "@nx._dispatch(graphs='B', preserve_node_attrs=True, preserve_graph_attrs=True)\ndef projected_graph(B, nodes, multigraph=False):\n    if B.is_multigraph():\n        raise nx.NetworkXError('not defined for multigraphs')\n    if B.is_directed():\n        directed = True\n        if multigraph:\n            G = nx.MultiDiGraph()\n        else:\n            G = nx.DiGraph()\n    else:\n        directed = False\n        if multigraph:\n            G = nx.MultiGraph()\n        else:\n            G = nx.Graph()\n    G.graph.update(B.graph)\n    G.add_nodes_from(((n, B.nodes[n]) for n in nodes))\n    for u in nodes:\n        nbrs2 = {v for nbr in B[u] for v in B[nbr] if v != u}\n        if multigraph:\n            for n in nbrs2:\n                if directed:\n                    links = set(B[u]) & set(B.pred[n])\n                else:\n                    links = set(B[u]) & set(B[n])\n                for l in links:\n                    if not G.has_edge(u, n, l):\n                        G.add_edge(u, n, key=l)\n        else:\n            G.add_edges_from(((u, n) for n in nbrs2))\n    return G"
 },
 {
  "docstring": "Returns a weighted projection of B onto one of its node sets.\n\nThe weighted projected graph is the projection of the bipartite\nnetwork B onto the specified nodes with weights representing the\nnumber of shared neighbors or the ratio between actual shared\nneighbors and possible shared neighbors if ``ratio is True`` [1]_.\nThe nodes retain their attributes and are connected in the resulting\ngraph if they have an edge to a common node in the original graph.\n\nParameters\n----------\nB : NetworkX graph\n    The input graph should be bipartite.\n\nnodes : list or iterable\n    Distinct nodes to project onto (the \"bottom\" nodes).\n\nratio: Bool (default=False)\n    If True, edge weight is the ratio between actual shared neighbors\n    and maximum possible shared neighbors (i.e., the size of the other\n    node set). If False, edges weight is the number of shared neighbors.\n\nReturns\n-------\nGraph : NetworkX graph\n   A graph that is the projection onto the given nodes.\n\nExamples\n--------\n>>> from networkx.algorithms import bipartite\n>>> B = nx.path_graph(4)\n>>> G = bipartite.weighted_projected_graph(B, [1, 3])\n>>> list(G)\n[1, 3]\n>>> list(G.edges(data=True))\n[(1, 3, {'weight': 1})]\n>>> G = bipartite.weighted_projected_graph(B, [1, 3], ratio=True)\n>>> list(G.edges(data=True))\n[(1, 3, {'weight': 0.5})]\n\n",
  "code": "@not_implemented_for('multigraph')\n@nx._dispatch(graphs='B')\ndef weighted_projected_graph(B, nodes, ratio=False):\n    if B.is_directed():\n        pred = B.pred\n        G = nx.DiGraph()\n    else:\n        pred = B.adj\n        G = nx.Graph()\n    G.graph.update(B.graph)\n    G.add_nodes_from(((n, B.nodes[n]) for n in nodes))\n    n_top = len(B) - len(nodes)\n    if n_top < 1:\n        raise NetworkXAlgorithmError(f'the size of the nodes to project onto ({len(nodes)}) is >= the graph size ({len(B)}).\\nThey are either not a valid bipartite partition or contain duplicates')\n    for u in nodes:\n        unbrs = set(B[u])\n        nbrs2 = {n for nbr in unbrs for n in B[nbr]} - {u}\n        for v in nbrs2:\n            vnbrs = set(pred[v])\n            common = unbrs & vnbrs\n            if not ratio:\n                weight = len(common)\n            else:\n                weight = len(common) / n_top\n            G.add_edge(u, v, weight=weight)\n    return G"
 },
 {
  "docstring": "Newman's weighted projection of B onto one of its node sets.\n\nThe collaboration weighted projection is the projection of the\nbipartite network B onto the specified nodes with weights assigned\nusing Newman's collaboration model [1]_:\n\n.. math::\n\n    w_{u, v} = \\sum_k \\frac{\\delta_{u}^{k} \\delta_{v}^{k}}{d_k - 1}\n\nwhere `u` and `v` are nodes from the bottom bipartite node set,\nand `k` is a node of the top node set.\nThe value `d_k` is the degree of node `k` in the bipartite\nnetwork and `\\delta_{u}^{k}` is 1 if node `u` is\nlinked to node `k` in the original bipartite graph or 0 otherwise.\n\nThe nodes retain their attributes and are connected in the resulting\ngraph if have an edge to a common node in the original bipartite\ngraph.\n\nParameters\n----------\nB : NetworkX graph\n  The input graph should be bipartite.\n\nnodes : list or iterable\n  Nodes to project onto (the \"bottom\" nodes).\n\nReturns\n-------\nGraph : NetworkX graph\n   A graph that is the projection onto the given nodes.\n\nExamples\n--------\n>>> from networkx.algorithms import bipartite\n>>> B = nx.path_graph(5)\n>>> B.add_edge(1, 5)\n>>> G = bipartite.collaboration_weighted_projected_graph(B, [0, 2, 4, 5])\n>>> list(G)\n[0, 2, 4, 5]\n>>> for edge in sorted(G.edges(data=True)):\n...     print(edge)\n...\n(0, 2, {'weight': 0.5})\n(0, 5, {'weight': 0.5})\n(2, 4, {'weight': 1.0})\n(2, 5, {'weight': 0.5})\n\n",
  "code": "@not_implemented_for('multigraph')\n@nx._dispatch(graphs='B')\ndef collaboration_weighted_projected_graph(B, nodes):\n    if B.is_directed():\n        pred = B.pred\n        G = nx.DiGraph()\n    else:\n        pred = B.adj\n        G = nx.Graph()\n    G.graph.update(B.graph)\n    G.add_nodes_from(((n, B.nodes[n]) for n in nodes))\n    for u in nodes:\n        unbrs = set(B[u])\n        nbrs2 = {n for nbr in unbrs for n in B[nbr] if n != u}\n        for v in nbrs2:\n            vnbrs = set(pred[v])\n            common_degree = (len(B[n]) for n in unbrs & vnbrs)\n            weight = sum((1.0 / (deg - 1) for deg in common_degree if deg > 1))\n            G.add_edge(u, v, weight=weight)\n    return G"
 },
 {
  "docstring": "Overlap weighted projection of B onto one of its node sets.\n\nThe overlap weighted projection is the projection of the bipartite\nnetwork B onto the specified nodes with weights representing\nthe Jaccard index between the neighborhoods of the two nodes in the\noriginal bipartite network [1]_:\n\n.. math::\n\n    w_{v, u} = \\frac{|N(u) \\cap N(v)|}{|N(u) \\cup N(v)|}\n\nor if the parameter 'jaccard' is False, the fraction of common\nneighbors by minimum of both nodes degree in the original\nbipartite graph [1]_:\n\n.. math::\n\n    w_{v, u} = \\frac{|N(u) \\cap N(v)|}{min(|N(u)|, |N(v)|)}\n\nThe nodes retain their attributes and are connected in the resulting\ngraph if have an edge to a common node in the original bipartite graph.\n\nParameters\n----------\nB : NetworkX graph\n    The input graph should be bipartite.\n\nnodes : list or iterable\n    Nodes to project onto (the \"bottom\" nodes).\n\njaccard: Bool (default=True)\n\nReturns\n-------\nGraph : NetworkX graph\n   A graph that is the projection onto the given nodes.\n\nExamples\n--------\n>>> from networkx.algorithms import bipartite\n>>> B = nx.path_graph(5)\n>>> nodes = [0, 2, 4]\n>>> G = bipartite.overlap_weighted_projected_graph(B, nodes)\n>>> list(G)\n[0, 2, 4]\n>>> list(G.edges(data=True))\n[(0, 2, {'weight': 0.5}), (2, 4, {'weight': 0.5})]\n>>> G = bipartite.overlap_weighted_projected_graph(B, nodes, jaccard=False)\n>>> list(G.edges(data=True))\n[(0, 2, {'weight': 1.0}), (2, 4, {'weight': 1.0})]\n\n",
  "code": "@not_implemented_for('multigraph')\n@nx._dispatch(graphs='B')\ndef overlap_weighted_projected_graph(B, nodes, jaccard=True):\n    if B.is_directed():\n        pred = B.pred\n        G = nx.DiGraph()\n    else:\n        pred = B.adj\n        G = nx.Graph()\n    G.graph.update(B.graph)\n    G.add_nodes_from(((n, B.nodes[n]) for n in nodes))\n    for u in nodes:\n        unbrs = set(B[u])\n        nbrs2 = {n for nbr in unbrs for n in B[nbr]} - {u}\n        for v in nbrs2:\n            vnbrs = set(pred[v])\n            if jaccard:\n                wt = len(unbrs & vnbrs) / len(unbrs | vnbrs)\n            else:\n                wt = len(unbrs & vnbrs) / min(len(unbrs), len(vnbrs))\n            G.add_edge(u, v, weight=wt)\n    return G"
 },
 {
  "docstring": "Weighted projection of B with a user-specified weight function.\n\nThe bipartite network B is projected on to the specified nodes\nwith weights computed by a user-specified function.  This function\nmust accept as a parameter the neighborhood sets of two nodes and\nreturn an integer or a float.\n\nThe nodes retain their attributes and are connected in the resulting graph\nif they have an edge to a common node in the original graph.\n\nParameters\n----------\nB : NetworkX graph\n    The input graph should be bipartite.\n\nnodes : list or iterable\n    Nodes to project onto (the \"bottom\" nodes).\n\nweight_function : function\n    This function must accept as parameters the same input graph\n    that this function, and two nodes; and return an integer or a float.\n    The default function computes the number of shared neighbors.\n\nReturns\n-------\nGraph : NetworkX graph\n   A graph that is the projection onto the given nodes.\n\nExamples\n--------\n>>> from networkx.algorithms import bipartite\n>>> # Define some custom weight functions\n>>> def jaccard(G, u, v):\n...     unbrs = set(G[u])\n...     vnbrs = set(G[v])\n...     return float(len(unbrs & vnbrs)) / len(unbrs | vnbrs)\n...\n>>> def my_weight(G, u, v, weight=\"weight\"):\n...     w = 0\n...     for nbr in set(G[u]) & set(G[v]):\n...         w += G[u][nbr].get(weight, 1) + G[v][nbr].get(weight, 1)\n...     return w\n...\n>>> # A complete bipartite graph with 4 nodes and 4 edges\n>>> B = nx.complete_bipartite_graph(2, 2)\n>>> # Add some arbitrary weight to the edges\n>>> for i, (u, v) in enumerate(B.edges()):\n...     B.edges[u, v][\"weight\"] = i + 1\n...\n>>> for edge in B.edges(data=True):\n...     print(edge)\n...\n(0, 2, {'weight': 1})\n(0, 3, {'weight': 2})\n(1, 2, {'weight': 3})\n(1, 3, {'weight': 4})\n>>> # By default, the weight is the number of shared neighbors\n>>> G = bipartite.generic_weighted_projected_graph(B, [0, 1])\n>>> print(list(G.edges(data=True)))\n[(0, 1, {'weight': 2})]\n>>> # To specify a custom weight function use the weight_function parameter\n>>> G = bipartite.generic_weighted_projected_graph(\n...     B, [0, 1], weight_function=jaccard\n... )\n>>> print(list(G.edges(data=True)))\n[(0, 1, {'weight': 1.0})]\n>>> G = bipartite.generic_weighted_projected_graph(\n...     B, [0, 1], weight_function=my_weight\n... )\n>>> print(list(G.edges(data=True)))\n[(0, 1, {'weight': 10})]\n\n",
  "code": "@not_implemented_for('multigraph')\n@nx._dispatch(graphs='B', preserve_all_attrs=True)\ndef generic_weighted_projected_graph(B, nodes, weight_function=None):\n    if B.is_directed():\n        pred = B.pred\n        G = nx.DiGraph()\n    else:\n        pred = B.adj\n        G = nx.Graph()\n    if weight_function is None:\n\n        def weight_function(G, u, v):\n            return len(set(G[u]) & set(pred[v]))\n    G.graph.update(B.graph)\n    G.add_nodes_from(((n, B.nodes[n]) for n in nodes))\n    for u in nodes:\n        nbrs2 = {n for nbr in set(B[u]) for n in B[nbr]} - {u}\n        for v in nbrs2:\n            weight = weight_function(B, u, v)\n            G.add_edge(u, v, weight=weight)\n    return G"
 },
 {
  "docstring": "Computes the node redundancy coefficients for the nodes in the bipartite\ngraph `G`.\n\nThe redundancy coefficient of a node `v` is the fraction of pairs of\nneighbors of `v` that are both linked to other nodes. In a one-mode\nprojection these nodes would be linked together even if `v` were\nnot there.\n\nMore formally, for any vertex `v`, the *redundancy coefficient of `v`* is\ndefined by\n\n.. math::\n\n    rc(v) = \\frac{|\\{\\{u, w\\} \\subseteq N(v),\n    \\: \\exists v' \\neq  v,\\: (v',u) \\in E\\:\n    \\mathrm{and}\\: (v',w) \\in E\\}|}{ \\frac{|N(v)|(|N(v)|-1)}{2}},\n\nwhere `N(v)` is the set of neighbors of `v` in `G`.\n\nParameters\n----------\nG : graph\n    A bipartite graph\n\nnodes : list or iterable (optional)\n    Compute redundancy for these nodes. The default is all nodes in G.\n\nReturns\n-------\nredundancy : dictionary\n    A dictionary keyed by node with the node redundancy value.\n\nExamples\n--------\nCompute the redundancy coefficient of each node in a graph::\n\n    >>> from networkx.algorithms import bipartite\n    >>> G = nx.cycle_graph(4)\n    >>> rc = bipartite.node_redundancy(G)\n    >>> rc[0]\n    1.0\n\nCompute the average redundancy for the graph::\n\n    >>> from networkx.algorithms import bipartite\n    >>> G = nx.cycle_graph(4)\n    >>> rc = bipartite.node_redundancy(G)\n    >>> sum(rc.values()) / len(G)\n    1.0\n\nCompute the average redundancy for a set of nodes::\n\n    >>> from networkx.algorithms import bipartite\n    >>> G = nx.cycle_graph(4)\n    >>> rc = bipartite.node_redundancy(G)\n    >>> nodes = [0, 2]\n    >>> sum(rc[n] for n in nodes) / len(nodes)\n    1.0\n\nRaises\n------\nNetworkXError\n    If any of the nodes in the graph (or in `nodes`, if specified) has\n    (out-)degree less than two (which would result in division by zero,\n    according to the definition of the redundancy coefficient).\n\n",
  "code": "@nx._dispatch\ndef node_redundancy(G, nodes=None):\n    if nodes is None:\n        nodes = G\n    if any((len(G[v]) < 2 for v in nodes)):\n        raise NetworkXError('Cannot compute redundancy coefficient for a node that has fewer than two neighbors.')\n    return {v: _node_redundancy(G, v) for v in nodes}"
 },
 {
  "docstring": "Returns the redundancy of the node `v` in the bipartite graph `G`.\n\nIf `G` is a graph with `n` nodes, the redundancy of a node is the ratio\nof the \"overlap\" of `v` to the maximum possible overlap of `v`\naccording to its degree. The overlap of `v` is the number of pairs of\nneighbors that have mutual neighbors themselves, other than `v`.\n\n`v` must have at least two neighbors in `G`.",
  "code": "def _node_redundancy(G, v):\n    n = len(G[v])\n    overlap = sum((1 for u, w in combinations(G[v], 2) if (set(G[u]) & set(G[w])) - {v}))\n    return 2 * overlap / (n * (n - 1))"
 },
 {
  "docstring": "Returns the spectral bipartivity.\n\nParameters\n----------\nG : NetworkX graph\n\nnodes : list or container  optional(default is all nodes)\n  Nodes to return value of spectral bipartivity contribution.\n\nweight : string or None  optional (default = 'weight')\n  Edge data key to use for edge weights. If None, weights set to 1.\n\nReturns\n-------\nsb : float or dict\n   A single number if the keyword nodes is not specified, or\n   a dictionary keyed by node with the spectral bipartivity contribution\n   of that node as the value.\n\nExamples\n--------\n>>> from networkx.algorithms import bipartite\n>>> G = nx.path_graph(4)\n>>> bipartite.spectral_bipartivity(G)\n1.0\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef spectral_bipartivity(G, nodes=None, weight='weight'):\n    import scipy as sp\n    nodelist = list(G)\n    A = nx.to_numpy_array(G, nodelist, weight=weight)\n    expA = sp.linalg.expm(A)\n    expmA = sp.linalg.expm(-A)\n    coshA = 0.5 * (expA + expmA)\n    if nodes is None:\n        return coshA.diagonal().sum() / expA.diagonal().sum()\n    else:\n        index = dict(zip(nodelist, range(len(nodelist))))\n        sb = {}\n        for n in nodes:\n            i = index[n]\n            sb[n] = coshA[i, i] / expA[i, i]\n        return sb"
 },
 {
  "docstring": "Tests for conditions specific to\nparse_edge_list method",
  "code": "def test_parse_edgelist(self):\n    lines = ['1 2', '2 3', '3 1', '4', ' ']\n    G = bipartite.parse_edgelist(lines, nodetype=int)\n    assert list(G.nodes) == [1, 2, 3]\n    with pytest.raises(TypeError, match='.*Failed to convert nodes'):\n        lines = ['a b', 'b c', 'c a']\n        G = bipartite.parse_edgelist(lines, nodetype=int)\n    with pytest.raises(TypeError, match='.*Failed to convert edge data'):\n        lines = ['1 2 3', '2 3 4', '3 1 2']\n        G = bipartite.parse_edgelist(lines, nodetype=int)\n    with pytest.raises(IndexError):\n        lines = ['1 2 3 4', '2 3 4']\n        G = bipartite.parse_edgelist(lines, nodetype=int, data=[('weight', int), ('key', int)])\n    with pytest.raises(TypeError, match='.*Failed to convert key data'):\n        lines = ['1 2 3 a', '2 3 4 b']\n        G = bipartite.parse_edgelist(lines, nodetype=int, data=[('weight', int), ('key', int)])"
 },
 {
  "docstring": "Test in accordance to issue #1927",
  "code": "def test_eppstein_matching():\n    G = nx.Graph()\n    G.add_nodes_from(['a', 2, 3, 4], bipartite=0)\n    G.add_nodes_from([1, 'b', 'c'], bipartite=1)\n    G.add_edges_from([('a', 1), ('a', 'b'), (2, 'b'), (2, 'c'), (3, 'c'), (4, 1)])\n    matching = eppstein_matching(G)\n    assert len(matching) == len(maximum_matching(G))\n    assert all((x in set(matching.keys()) for x in set(matching.values())))"
 },
 {
  "docstring": "Creates a bipartite graph for use in testing matching algorithms.\n\nThe bipartite graph has a maximum cardinality matching that leaves\nvertex 1 and vertex 10 unmatched. The first six numbers are the left\nvertices and the next six numbers are the right vertices.",
  "code": "def setup_method(self):\n    self.simple_graph = nx.complete_bipartite_graph(2, 3)\n    self.simple_solution = {0: 2, 1: 3, 2: 0, 3: 1}\n    edges = [(0, 7), (0, 8), (2, 6), (2, 9), (3, 8), (4, 8), (4, 9), (5, 11)]\n    self.top_nodes = set(range(6))\n    self.graph = nx.Graph()\n    self.graph.add_nodes_from(range(12))\n    self.graph.add_edges_from(edges)\n    G = nx.Graph()\n    G.add_nodes_from([(1, 'C'), (1, 'B'), (0, 'G'), (1, 'F'), (1, 'E'), (0, 'C'), (1, 'D'), (1, 'I'), (0, 'A'), (0, 'D'), (0, 'F'), (0, 'E'), (0, 'H'), (1, 'G'), (1, 'A'), (0, 'I'), (0, 'B'), (1, 'H')])\n    G.add_edge((1, 'C'), (0, 'A'))\n    G.add_edge((1, 'B'), (0, 'A'))\n    G.add_edge((0, 'G'), (1, 'I'))\n    G.add_edge((0, 'G'), (1, 'H'))\n    G.add_edge((1, 'F'), (0, 'A'))\n    G.add_edge((1, 'F'), (0, 'C'))\n    G.add_edge((1, 'F'), (0, 'E'))\n    G.add_edge((1, 'E'), (0, 'A'))\n    G.add_edge((1, 'E'), (0, 'C'))\n    G.add_edge((0, 'C'), (1, 'D'))\n    G.add_edge((0, 'C'), (1, 'I'))\n    G.add_edge((0, 'C'), (1, 'G'))\n    G.add_edge((0, 'C'), (1, 'H'))\n    G.add_edge((1, 'D'), (0, 'A'))\n    G.add_edge((1, 'I'), (0, 'A'))\n    G.add_edge((1, 'I'), (0, 'E'))\n    G.add_edge((0, 'A'), (1, 'G'))\n    G.add_edge((0, 'A'), (1, 'H'))\n    G.add_edge((0, 'E'), (1, 'G'))\n    G.add_edge((0, 'E'), (1, 'H'))\n    self.disconnected_graph = G"
 },
 {
  "docstring": "Asserts that the matching is what we expect from the bipartite graph\nconstructed in the :meth:`setup` fixture.",
  "code": "def check_match(self, matching):\n    M = matching\n    matched_vertices = frozenset(itertools.chain(*M.items()))\n    assert matched_vertices == frozenset(range(12)) - {1, 10}\n    assert all((u == M[M[u]] for u in range(12) if u in M))"
 },
 {
  "docstring": "Asserts that the given set of vertices is the vertex cover we\nexpected from the bipartite graph constructed in the :meth:`setup`\nfixture.",
  "code": "def check_vertex_cover(self, vertices):\n    assert len(vertices) == 5\n    for u, v in self.graph.edges():\n        assert u in vertices or v in vertices"
 },
 {
  "docstring": "Tests that David Eppstein's implementation of the Hopcroft--Karp\nalgorithm produces a maximum cardinality matching.",
  "code": "def test_eppstein_matching(self):\n    self.check_match(eppstein_matching(self.graph, self.top_nodes))"
 },
 {
  "docstring": "Tests that the Hopcroft--Karp algorithm produces a maximum\ncardinality matching in a bipartite graph.",
  "code": "def test_hopcroft_karp_matching(self):\n    self.check_match(hopcroft_karp_matching(self.graph, self.top_nodes))"
 },
 {
  "docstring": "Test for converting a maximum matching to a minimum vertex cover.",
  "code": "def test_to_vertex_cover(self):\n    matching = maximum_matching(self.graph, self.top_nodes)\n    vertex_cover = to_vertex_cover(self.graph, matching, self.top_nodes)\n    self.check_vertex_cover(vertex_cover)"
 },
 {
  "docstring": "Test from issue 2127",
  "code": "def test_issue_2127(self):\n    G = nx.DiGraph()\n    G.add_edge('A', 'C')\n    G.add_edge('A', 'B')\n    G.add_edge('C', 'E')\n    G.add_edge('C', 'D')\n    G.add_edge('E', 'G')\n    G.add_edge('E', 'F')\n    G.add_edge('G', 'I')\n    G.add_edge('G', 'H')\n    tc = nx.transitive_closure(G)\n    btc = nx.Graph()\n    for v in tc.nodes():\n        btc.add_node((0, v))\n        btc.add_node((1, v))\n    for u, v in tc.edges():\n        btc.add_edge((0, u), (1, v))\n    top_nodes = {n for n in btc if n[0] == 0}\n    matching = hopcroft_karp_matching(btc, top_nodes)\n    vertex_cover = to_vertex_cover(btc, matching, top_nodes)\n    independent_set = set(G) - {v for _, v in vertex_cover}\n    assert {'B', 'D', 'F', 'I', 'H'} == independent_set"
 },
 {
  "docstring": "Compute the shortest-path betweenness centrality for nodes.\n\nBetweenness centrality of a node $v$ is the sum of the\nfraction of all-pairs shortest paths that pass through $v$\n\n.. math::\n\n   c_B(v) =\\sum_{s,t \\in V} \\frac{\\sigma(s, t|v)}{\\sigma(s, t)}\n\nwhere $V$ is the set of nodes, $\\sigma(s, t)$ is the number of\nshortest $(s, t)$-paths,  and $\\sigma(s, t|v)$ is the number of\nthose paths  passing through some  node $v$ other than $s, t$.\nIf $s = t$, $\\sigma(s, t) = 1$, and if $v \\in {s, t}$,\n$\\sigma(s, t|v) = 0$ [2]_.\n\nParameters\n----------\nG : graph\n  A NetworkX graph.\n\nk : int, optional (default=None)\n  If k is not None use k node samples to estimate betweenness.\n  The value of k <= n where n is the number of nodes in the graph.\n  Higher values give better approximation.\n\nnormalized : bool, optional\n  If True the betweenness values are normalized by `2/((n-1)(n-2))`\n  for graphs, and `1/((n-1)(n-2))` for directed graphs where `n`\n  is the number of nodes in G.\n\nweight : None or string, optional (default=None)\n  If None, all edge weights are considered equal.\n  Otherwise holds the name of the edge attribute used as weight.\n  Weights are used to calculate weighted shortest paths, so they are\n  interpreted as distances.\n\nendpoints : bool, optional\n  If True include the endpoints in the shortest path counts.\n\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n    Note that this is only used if k is not None.\n\nReturns\n-------\nnodes : dictionary\n   Dictionary of nodes with betweenness centrality as the value.\n\nSee Also\n--------\nedge_betweenness_centrality\nload_centrality\n\n",
  "code": "@py_random_state(5)\n@nx._dispatch(edge_attrs='weight')\ndef betweenness_centrality(G, k=None, normalized=True, weight=None, endpoints=False, seed=None):\n    betweenness = dict.fromkeys(G, 0.0)\n    if k is None:\n        nodes = G\n    else:\n        nodes = seed.sample(list(G.nodes()), k)\n    for s in nodes:\n        if weight is None:\n            S, P, sigma, _ = _single_source_shortest_path_basic(G, s)\n        else:\n            S, P, sigma, _ = _single_source_dijkstra_path_basic(G, s, weight)\n        if endpoints:\n            betweenness, _ = _accumulate_endpoints(betweenness, S, P, sigma, s)\n        else:\n            betweenness, _ = _accumulate_basic(betweenness, S, P, sigma, s)\n    betweenness = _rescale(betweenness, len(G), normalized=normalized, directed=G.is_directed(), k=k, endpoints=endpoints)\n    return betweenness"
 },
 {
  "docstring": "Compute betweenness centrality for edges.\n\nBetweenness centrality of an edge $e$ is the sum of the\nfraction of all-pairs shortest paths that pass through $e$\n\n.. math::\n\n   c_B(e) =\\sum_{s,t \\in V} \\frac{\\sigma(s, t|e)}{\\sigma(s, t)}\n\nwhere $V$ is the set of nodes, $\\sigma(s, t)$ is the number of\nshortest $(s, t)$-paths, and $\\sigma(s, t|e)$ is the number of\nthose paths passing through edge $e$ [2]_.\n\nParameters\n----------\nG : graph\n  A NetworkX graph.\n\nk : int, optional (default=None)\n  If k is not None use k node samples to estimate betweenness.\n  The value of k <= n where n is the number of nodes in the graph.\n  Higher values give better approximation.\n\nnormalized : bool, optional\n  If True the betweenness values are normalized by $2/(n(n-1))$\n  for graphs, and $1/(n(n-1))$ for directed graphs where $n$\n  is the number of nodes in G.\n\nweight : None or string, optional (default=None)\n  If None, all edge weights are considered equal.\n  Otherwise holds the name of the edge attribute used as weight.\n  Weights are used to calculate weighted shortest paths, so they are\n  interpreted as distances.\n\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n    Note that this is only used if k is not None.\n\nReturns\n-------\nedges : dictionary\n   Dictionary of edges with betweenness centrality as the value.\n\nSee Also\n--------\nbetweenness_centrality\nedge_load\n\n",
  "code": "@py_random_state(4)\n@nx._dispatch(edge_attrs='weight')\ndef edge_betweenness_centrality(G, k=None, normalized=True, weight=None, seed=None):\n    betweenness = dict.fromkeys(G, 0.0)\n    betweenness.update(dict.fromkeys(G.edges(), 0.0))\n    if k is None:\n        nodes = G\n    else:\n        nodes = seed.sample(list(G.nodes()), k)\n    for s in nodes:\n        if weight is None:\n            S, P, sigma, _ = _single_source_shortest_path_basic(G, s)\n        else:\n            S, P, sigma, _ = _single_source_dijkstra_path_basic(G, s, weight)\n        betweenness = _accumulate_edges(betweenness, S, P, sigma, s)\n    for n in G:\n        del betweenness[n]\n    betweenness = _rescale_e(betweenness, len(G), normalized=normalized, directed=G.is_directed())\n    if G.is_multigraph():\n        betweenness = _add_edge_keys(G, betweenness, weight=weight)\n    return betweenness"
 },
 {
  "docstring": "Adds the corrected betweenness centrality (BC) values for multigraphs.\n\nParameters\n----------\nG : NetworkX graph.\n\nbetweenness : dictionary\n    Dictionary mapping adjacent node tuples to betweenness centrality values.\n\nweight : string or function\n    See `_weight_function` for details. Defaults to `None`.\n\nReturns\n-------\nedges : dictionary\n    The parameter `betweenness` including edges with keys and their\n    betweenness centrality values.\n\nThe BC value is divided among edges of equal weight.",
  "code": "@not_implemented_for('graph')\ndef _add_edge_keys(G, betweenness, weight=None):\n    _weight = _weight_function(G, weight)\n    edge_bc = dict.fromkeys(G.edges, 0.0)\n    for u, v in betweenness:\n        d = G[u][v]\n        wt = _weight(u, v, d)\n        keys = [k for k in d if _weight(u, v, {k: d[k]}) == wt]\n        bc = betweenness[u, v] / len(keys)\n        for k in keys:\n            edge_bc[u, v, k] = bc\n    return edge_bc"
 },
 {
  "docstring": "Compute betweenness centrality for a subset of nodes.\n\n.. math::\n\n   c_B(v) =\\sum_{s\\in S, t \\in T} \\frac{\\sigma(s, t|v)}{\\sigma(s, t)}\n\nwhere $S$ is the set of sources, $T$ is the set of targets,\n$\\sigma(s, t)$ is the number of shortest $(s, t)$-paths,\nand $\\sigma(s, t|v)$ is the number of those paths\npassing through some  node $v$ other than $s, t$.\nIf $s = t$, $\\sigma(s, t) = 1$,\nand if $v \\in {s, t}$, $\\sigma(s, t|v) = 0$ [2]_.\n\n\nParameters\n----------\nG : graph\n  A NetworkX graph.\n\nsources: list of nodes\n  Nodes to use as sources for shortest paths in betweenness\n\ntargets: list of nodes\n  Nodes to use as targets for shortest paths in betweenness\n\nnormalized : bool, optional\n  If True the betweenness values are normalized by $2/((n-1)(n-2))$\n  for graphs, and $1/((n-1)(n-2))$ for directed graphs where $n$\n  is the number of nodes in G.\n\nweight : None or string, optional (default=None)\n  If None, all edge weights are considered equal.\n  Otherwise holds the name of the edge attribute used as weight.\n  Weights are used to calculate weighted shortest paths, so they are\n  interpreted as distances.\n\nReturns\n-------\nnodes : dictionary\n   Dictionary of nodes with betweenness centrality as the value.\n\nSee Also\n--------\nedge_betweenness_centrality\nload_centrality\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef betweenness_centrality_subset(G, sources, targets, normalized=False, weight=None):\n    b = dict.fromkeys(G, 0.0)\n    for s in sources:\n        if weight is None:\n            S, P, sigma, _ = shortest_path(G, s)\n        else:\n            S, P, sigma, _ = dijkstra(G, s, weight)\n        b = _accumulate_subset(b, S, P, sigma, s, targets)\n    b = _rescale(b, len(G), normalized=normalized, directed=G.is_directed())\n    return b"
 },
 {
  "docstring": "Compute betweenness centrality for edges for a subset of nodes.\n\n.. math::\n\n   c_B(v) =\\sum_{s\\in S,t \\in T} \\frac{\\sigma(s, t|e)}{\\sigma(s, t)}\n\nwhere $S$ is the set of sources, $T$ is the set of targets,\n$\\sigma(s, t)$ is the number of shortest $(s, t)$-paths,\nand $\\sigma(s, t|e)$ is the number of those paths\npassing through edge $e$ [2]_.\n\nParameters\n----------\nG : graph\n  A networkx graph.\n\nsources: list of nodes\n  Nodes to use as sources for shortest paths in betweenness\n\ntargets: list of nodes\n  Nodes to use as targets for shortest paths in betweenness\n\nnormalized : bool, optional\n  If True the betweenness values are normalized by `2/(n(n-1))`\n  for graphs, and `1/(n(n-1))` for directed graphs where `n`\n  is the number of nodes in G.\n\nweight : None or string, optional (default=None)\n  If None, all edge weights are considered equal.\n  Otherwise holds the name of the edge attribute used as weight.\n  Weights are used to calculate weighted shortest paths, so they are\n  interpreted as distances.\n\nReturns\n-------\nedges : dictionary\n   Dictionary of edges with Betweenness centrality as the value.\n\nSee Also\n--------\nbetweenness_centrality\nedge_load\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef edge_betweenness_centrality_subset(G, sources, targets, normalized=False, weight=None):\n    b = dict.fromkeys(G, 0.0)\n    b.update(dict.fromkeys(G.edges(), 0.0))\n    for s in sources:\n        if weight is None:\n            S, P, sigma, _ = shortest_path(G, s)\n        else:\n            S, P, sigma, _ = dijkstra(G, s, weight)\n        b = _accumulate_edges_subset(b, S, P, sigma, s, targets)\n    for n in G:\n        del b[n]\n    b = _rescale_e(b, len(G), normalized=normalized, directed=G.is_directed())\n    if G.is_multigraph():\n        b = _add_edge_keys(G, b, weight=weight)\n    return b"
 },
 {
  "docstring": "edge_betweenness_centrality_subset helper.",
  "code": "def _accumulate_edges_subset(betweenness, S, P, sigma, s, targets):\n    delta = dict.fromkeys(S, 0)\n    target_set = set(targets)\n    while S:\n        w = S.pop()\n        for v in P[w]:\n            if w in target_set:\n                c = sigma[v] / sigma[w] * (1.0 + delta[w])\n            else:\n                c = delta[w] / len(P[w])\n            if (v, w) not in betweenness:\n                betweenness[w, v] += c\n            else:\n                betweenness[v, w] += c\n            delta[v] += c\n        if w != s:\n            betweenness[w] += delta[w]\n    return betweenness"
 },
 {
  "docstring": "betweenness_centrality_subset helper.",
  "code": "def _rescale(betweenness, n, normalized, directed=False):\n    if normalized:\n        if n <= 2:\n            scale = None\n        else:\n            scale = 1.0 / ((n - 1) * (n - 2))\n    elif not directed:\n        scale = 0.5\n    else:\n        scale = None\n    if scale is not None:\n        for v in betweenness:\n            betweenness[v] *= scale\n    return betweenness"
 },
 {
  "docstring": "edge_betweenness_centrality_subset helper.",
  "code": "def _rescale_e(betweenness, n, normalized, directed=False):\n    if normalized:\n        if n <= 1:\n            scale = None\n        else:\n            scale = 1.0 / (n * (n - 1))\n    elif not directed:\n        scale = 0.5\n    else:\n        scale = None\n    if scale is not None:\n        for v in betweenness:\n            betweenness[v] *= scale\n    return betweenness"
 },
 {
  "docstring": "Compute closeness centrality for nodes.\n\nCloseness centrality [1]_ of a node `u` is the reciprocal of the\naverage shortest path distance to `u` over all `n-1` reachable nodes.\n\n.. math::\n\n    C(u) = \\frac{n - 1}{\\sum_{v=1}^{n-1} d(v, u)},\n\nwhere `d(v, u)` is the shortest-path distance between `v` and `u`,\nand `n-1` is the number of nodes reachable from `u`. Notice that the\ncloseness distance function computes the incoming distance to `u`\nfor directed graphs. To use outward distance, act on `G.reverse()`.\n\nNotice that higher values of closeness indicate higher centrality.\n\nWasserman and Faust propose an improved formula for graphs with\nmore than one connected component. The result is \"a ratio of the\nfraction of actors in the group who are reachable, to the average\ndistance\" from the reachable actors [2]_. You might think this\nscale factor is inverted but it is not. As is, nodes from small\ncomponents receive a smaller closeness value. Letting `N` denote\nthe number of nodes in the graph,\n\n.. math::\n\n    C_{WF}(u) = \\frac{n-1}{N-1} \\frac{n - 1}{\\sum_{v=1}^{n-1} d(v, u)},\n\nParameters\n----------\nG : graph\n  A NetworkX graph\n\nu : node, optional\n  Return only the value for node u\n\ndistance : edge attribute key, optional (default=None)\n  Use the specified edge attribute as the edge distance in shortest\n  path calculations.  If `None` (the default) all edges have a distance of 1.\n  Absent edge attributes are assigned a distance of 1. Note that no check\n  is performed to ensure that edges have the provided attribute.\n\nwf_improved : bool, optional (default=True)\n  If True, scale by the fraction of nodes reachable. This gives the\n  Wasserman and Faust improved formula. For single component graphs\n  it is the same as the original formula.\n\nReturns\n-------\nnodes : dictionary\n  Dictionary of nodes with closeness centrality as the value.\n\nExamples\n--------\n>>> G = nx.Graph([(0, 1), (0, 2), (0, 3), (1, 2), (1, 3)])\n>>> nx.closeness_centrality(G)\n{0: 1.0, 1: 1.0, 2: 0.75, 3: 0.75}\n\nSee Also\n--------\nbetweenness_centrality, load_centrality, eigenvector_centrality,\ndegree_centrality, incremental_closeness_centrality\n\n",
  "code": "@nx._dispatch(edge_attrs='distance')\ndef closeness_centrality(G, u=None, distance=None, wf_improved=True):\n    if G.is_directed():\n        G = G.reverse()\n    if distance is not None:\n        path_length = functools.partial(nx.single_source_dijkstra_path_length, weight=distance)\n    else:\n        path_length = nx.single_source_shortest_path_length\n    if u is None:\n        nodes = G.nodes\n    else:\n        nodes = [u]\n    closeness_dict = {}\n    for n in nodes:\n        sp = path_length(G, n)\n        totsp = sum(sp.values())\n        len_G = len(G)\n        _closeness_centrality = 0.0\n        if totsp > 0.0 and len_G > 1:\n            _closeness_centrality = (len(sp) - 1.0) / totsp\n            if wf_improved:\n                s = (len(sp) - 1.0) / (len_G - 1)\n                _closeness_centrality *= s\n        closeness_dict[n] = _closeness_centrality\n    if u is not None:\n        return closeness_dict[u]\n    return closeness_dict"
 },
 {
  "docstring": "Incremental closeness centrality for nodes.\n\nCompute closeness centrality for nodes using level-based work filtering\nas described in Incremental Algorithms for Closeness Centrality by Sariyuce et al.\n\nLevel-based work filtering detects unnecessary updates to the closeness\ncentrality and filters them out.\n\n---\nFrom \"Incremental Algorithms for Closeness Centrality\":\n\nTheorem 1: Let :math:`G = (V, E)` be a graph and u and v be two vertices in V\nsuch that there is no edge (u, v) in E. Let :math:`G' = (V, E \\cup uv)`\nThen :math:`cc[s] = cc'[s]` if and only if :math:`\\left|dG(s, u) - dG(s, v)\\right| \\leq 1`.\n\nWhere :math:`dG(u, v)` denotes the length of the shortest path between\ntwo vertices u, v in a graph G, cc[s] is the closeness centrality for a\nvertex s in V, and cc'[s] is the closeness centrality for a\nvertex s in V, with the (u, v) edge added.\n---\n\nWe use Theorem 1 to filter out updates when adding or removing an edge.\nWhen adding an edge (u, v), we compute the shortest path lengths from all\nother nodes to u and to v before the node is added. When removing an edge,\nwe compute the shortest path lengths after the edge is removed. Then we\napply Theorem 1 to use previously computed closeness centrality for nodes\nwhere :math:`\\left|dG(s, u) - dG(s, v)\\right| \\leq 1`. This works only for\nundirected, unweighted graphs; the distance argument is not supported.\n\nCloseness centrality [1]_ of a node `u` is the reciprocal of the\nsum of the shortest path distances from `u` to all `n-1` other nodes.\nSince the sum of distances depends on the number of nodes in the\ngraph, closeness is normalized by the sum of minimum possible\ndistances `n-1`.\n\n.. math::\n\n    C(u) = \\frac{n - 1}{\\sum_{v=1}^{n-1} d(v, u)},\n\nwhere `d(v, u)` is the shortest-path distance between `v` and `u`,\nand `n` is the number of nodes in the graph.\n\nNotice that higher values of closeness indicate higher centrality.\n\nParameters\n----------\nG : graph\n  A NetworkX graph\n\nedge : tuple\n  The modified edge (u, v) in the graph.\n\nprev_cc : dictionary\n  The previous closeness centrality for all nodes in the graph.\n\ninsertion : bool, optional\n  If True (default) the edge was inserted, otherwise it was deleted from the graph.\n\nwf_improved : bool, optional (default=True)\n  If True, scale by the fraction of nodes reachable. This gives the\n  Wasserman and Faust improved formula. For single component graphs\n  it is the same as the original formula.\n\nReturns\n-------\nnodes : dictionary\n  Dictionary of nodes with closeness centrality as the value.\n\nSee Also\n--------\nbetweenness_centrality, load_centrality, eigenvector_centrality,\ndegree_centrality, closeness_centrality\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch\ndef incremental_closeness_centrality(G, edge, prev_cc=None, insertion=True, wf_improved=True):\n    if prev_cc is not None and set(prev_cc.keys()) != set(G.nodes()):\n        raise NetworkXError('prev_cc and G do not have the same nodes')\n    u, v = edge\n    path_length = nx.single_source_shortest_path_length\n    if insertion:\n        du = path_length(G, u)\n        dv = path_length(G, v)\n        G.add_edge(u, v)\n    else:\n        G.remove_edge(u, v)\n        du = path_length(G, u)\n        dv = path_length(G, v)\n    if prev_cc is None:\n        return nx.closeness_centrality(G)\n    nodes = G.nodes()\n    closeness_dict = {}\n    for n in nodes:\n        if n in du and n in dv and (abs(du[n] - dv[n]) <= 1):\n            closeness_dict[n] = prev_cc[n]\n        else:\n            sp = path_length(G, n)\n            totsp = sum(sp.values())\n            len_G = len(G)\n            _closeness_centrality = 0.0\n            if totsp > 0.0 and len_G > 1:\n                _closeness_centrality = (len(sp) - 1.0) / totsp\n                if wf_improved:\n                    s = (len(sp) - 1.0) / (len_G - 1)\n                    _closeness_centrality *= s\n            closeness_dict[n] = _closeness_centrality\n    if insertion:\n        G.remove_edge(u, v)\n    else:\n        G.add_edge(u, v)\n    return closeness_dict"
 },
 {
  "docstring": "Compute the approximate current-flow betweenness centrality for nodes.\n\nApproximates the current-flow betweenness centrality within absolute\nerror of epsilon with high probability [1]_.\n\n\nParameters\n----------\nG : graph\n  A NetworkX graph\n\nnormalized : bool, optional (default=True)\n  If True the betweenness values are normalized by 2/[(n-1)(n-2)] where\n  n is the number of nodes in G.\n\nweight : string or None, optional (default=None)\n  Key for edge data used as the edge weight.\n  If None, then use 1 as each edge weight.\n  The weight reflects the capacity or the strength of the\n  edge.\n\ndtype : data type (float)\n  Default data type for internal matrices.\n  Set to np.float32 for lower memory consumption.\n\nsolver : string (default='full')\n   Type of linear solver to use for computing the flow matrix.\n   Options are \"full\" (uses most memory), \"lu\" (recommended), and\n   \"cg\" (uses least memory).\n\nepsilon: float\n    Absolute error tolerance.\n\nkmax: int\n   Maximum number of sample node pairs to use for approximation.\n\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\nReturns\n-------\nnodes : dictionary\n   Dictionary of nodes with betweenness centrality as the value.\n\nSee Also\n--------\ncurrent_flow_betweenness_centrality\n\n",
  "code": "@not_implemented_for('directed')\n@py_random_state(7)\n@nx._dispatch(edge_attrs='weight')\ndef approximate_current_flow_betweenness_centrality(G, normalized=True, weight=None, dtype=float, solver='full', epsilon=0.5, kmax=10000, seed=None):\n    import numpy as np\n    if not nx.is_connected(G):\n        raise nx.NetworkXError('Graph not connected.')\n    solvername = {'full': FullInverseLaplacian, 'lu': SuperLUInverseLaplacian, 'cg': CGInverseLaplacian}\n    n = G.number_of_nodes()\n    ordering = list(reverse_cuthill_mckee_ordering(G))\n    H = nx.relabel_nodes(G, dict(zip(ordering, range(n))))\n    L = nx.laplacian_matrix(H, nodelist=range(n), weight=weight).asformat('csc')\n    L = L.astype(dtype)\n    C = solvername[solver](L, dtype=dtype)\n    betweenness = dict.fromkeys(H, 0.0)\n    nb = (n - 1.0) * (n - 2.0)\n    cstar = n * (n - 1) / nb\n    l = 1\n    k = l * int(np.ceil((cstar / epsilon) ** 2 * np.log(n)))\n    if k > kmax:\n        msg = f'Number random pairs k>kmax ({k}>{kmax}) '\n        raise nx.NetworkXError(msg, 'Increase kmax or epsilon')\n    cstar2k = cstar / (2 * k)\n    for _ in range(k):\n        s, t = pair = seed.sample(range(n), 2)\n        b = np.zeros(n, dtype=dtype)\n        b[s] = 1\n        b[t] = -1\n        p = C.solve(b)\n        for v in H:\n            if v in pair:\n                continue\n            for nbr in H[v]:\n                w = H[v][nbr].get(weight, 1.0)\n                betweenness[v] += w * np.abs(p[v] - p[nbr]) * cstar2k\n    if normalized:\n        factor = 1.0\n    else:\n        factor = nb / 2.0\n    return {ordering[k]: v * factor for k, v in betweenness.items()}"
 },
 {
  "docstring": "Compute current-flow betweenness centrality for nodes.\n\nCurrent-flow betweenness centrality uses an electrical current\nmodel for information spreading in contrast to betweenness\ncentrality which uses shortest paths.\n\nCurrent-flow betweenness centrality is also known as\nrandom-walk betweenness centrality [2]_.\n\nParameters\n----------\nG : graph\n  A NetworkX graph\n\nnormalized : bool, optional (default=True)\n  If True the betweenness values are normalized by 2/[(n-1)(n-2)] where\n  n is the number of nodes in G.\n\nweight : string or None, optional (default=None)\n  Key for edge data used as the edge weight.\n  If None, then use 1 as each edge weight.\n  The weight reflects the capacity or the strength of the\n  edge.\n\ndtype : data type (float)\n  Default data type for internal matrices.\n  Set to np.float32 for lower memory consumption.\n\nsolver : string (default='full')\n   Type of linear solver to use for computing the flow matrix.\n   Options are \"full\" (uses most memory), \"lu\" (recommended), and\n   \"cg\" (uses least memory).\n\nReturns\n-------\nnodes : dictionary\n   Dictionary of nodes with betweenness centrality as the value.\n\nSee Also\n--------\napproximate_current_flow_betweenness_centrality\nbetweenness_centrality\nedge_betweenness_centrality\nedge_current_flow_betweenness_centrality\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch(edge_attrs='weight')\ndef current_flow_betweenness_centrality(G, normalized=True, weight=None, dtype=float, solver='full'):\n    if not nx.is_connected(G):\n        raise nx.NetworkXError('Graph not connected.')\n    n = G.number_of_nodes()\n    ordering = list(reverse_cuthill_mckee_ordering(G))\n    H = nx.relabel_nodes(G, dict(zip(ordering, range(n))))\n    betweenness = dict.fromkeys(H, 0.0)\n    for row, (s, t) in flow_matrix_row(H, weight=weight, dtype=dtype, solver=solver):\n        pos = dict(zip(row.argsort()[::-1], range(n)))\n        for i in range(n):\n            betweenness[s] += (i - pos[i]) * row[i]\n            betweenness[t] += (n - i - 1 - pos[i]) * row[i]\n    if normalized:\n        nb = (n - 1.0) * (n - 2.0)\n    else:\n        nb = 2.0\n    for v in H:\n        betweenness[v] = float((betweenness[v] - v) * 2.0 / nb)\n    return {ordering[k]: v for k, v in betweenness.items()}"
 },
 {
  "docstring": "Compute current-flow betweenness centrality for edges.\n\nCurrent-flow betweenness centrality uses an electrical current\nmodel for information spreading in contrast to betweenness\ncentrality which uses shortest paths.\n\nCurrent-flow betweenness centrality is also known as\nrandom-walk betweenness centrality [2]_.\n\nParameters\n----------\nG : graph\n  A NetworkX graph\n\nnormalized : bool, optional (default=True)\n  If True the betweenness values are normalized by 2/[(n-1)(n-2)] where\n  n is the number of nodes in G.\n\nweight : string or None, optional (default=None)\n  Key for edge data used as the edge weight.\n  If None, then use 1 as each edge weight.\n  The weight reflects the capacity or the strength of the\n  edge.\n\ndtype : data type (default=float)\n  Default data type for internal matrices.\n  Set to np.float32 for lower memory consumption.\n\nsolver : string (default='full')\n   Type of linear solver to use for computing the flow matrix.\n   Options are \"full\" (uses most memory), \"lu\" (recommended), and\n   \"cg\" (uses least memory).\n\nReturns\n-------\nnodes : dictionary\n   Dictionary of edge tuples with betweenness centrality as the value.\n\nRaises\n------\nNetworkXError\n    The algorithm does not support DiGraphs.\n    If the input graph is an instance of DiGraph class, NetworkXError\n    is raised.\n\nSee Also\n--------\nbetweenness_centrality\nedge_betweenness_centrality\ncurrent_flow_betweenness_centrality\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch(edge_attrs='weight')\ndef edge_current_flow_betweenness_centrality(G, normalized=True, weight=None, dtype=float, solver='full'):\n    if not nx.is_connected(G):\n        raise nx.NetworkXError('Graph not connected.')\n    n = G.number_of_nodes()\n    ordering = list(reverse_cuthill_mckee_ordering(G))\n    H = nx.relabel_nodes(G, dict(zip(ordering, range(n))))\n    edges = (tuple(sorted((u, v))) for u, v in H.edges())\n    betweenness = dict.fromkeys(edges, 0.0)\n    if normalized:\n        nb = (n - 1.0) * (n - 2.0)\n    else:\n        nb = 2.0\n    for row, e in flow_matrix_row(H, weight=weight, dtype=dtype, solver=solver):\n        pos = dict(zip(row.argsort()[::-1], range(1, n + 1)))\n        for i in range(n):\n            betweenness[e] += (i + 1 - pos[i]) * row[i]\n            betweenness[e] += (n - i - pos[i]) * row[i]\n        betweenness[e] /= nb\n    return {(ordering[s], ordering[t]): v for (s, t), v in betweenness.items()}"
 },
 {
  "docstring": "Compute current-flow betweenness centrality for subsets of nodes.\n\nCurrent-flow betweenness centrality uses an electrical current\nmodel for information spreading in contrast to betweenness\ncentrality which uses shortest paths.\n\nCurrent-flow betweenness centrality is also known as\nrandom-walk betweenness centrality [2]_.\n\nParameters\n----------\nG : graph\n  A NetworkX graph\n\nsources: list of nodes\n  Nodes to use as sources for current\n\ntargets: list of nodes\n  Nodes to use as sinks for current\n\nnormalized : bool, optional (default=True)\n  If True the betweenness values are normalized by b=b/(n-1)(n-2) where\n  n is the number of nodes in G.\n\nweight : string or None, optional (default=None)\n  Key for edge data used as the edge weight.\n  If None, then use 1 as each edge weight.\n  The weight reflects the capacity or the strength of the\n  edge.\n\ndtype: data type (float)\n  Default data type for internal matrices.\n  Set to np.float32 for lower memory consumption.\n\nsolver: string (default='lu')\n   Type of linear solver to use for computing the flow matrix.\n   Options are \"full\" (uses most memory), \"lu\" (recommended), and\n   \"cg\" (uses least memory).\n\nReturns\n-------\nnodes : dictionary\n   Dictionary of nodes with betweenness centrality as the value.\n\nSee Also\n--------\napproximate_current_flow_betweenness_centrality\nbetweenness_centrality\nedge_betweenness_centrality\nedge_current_flow_betweenness_centrality\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch(edge_attrs='weight')\ndef current_flow_betweenness_centrality_subset(G, sources, targets, normalized=True, weight=None, dtype=float, solver='lu'):\n    import numpy as np\n    from networkx.utils import reverse_cuthill_mckee_ordering\n    if not nx.is_connected(G):\n        raise nx.NetworkXError('Graph not connected.')\n    n = G.number_of_nodes()\n    ordering = list(reverse_cuthill_mckee_ordering(G))\n    mapping = dict(zip(ordering, range(n)))\n    H = nx.relabel_nodes(G, mapping)\n    betweenness = dict.fromkeys(H, 0.0)\n    for row, (s, t) in flow_matrix_row(H, weight=weight, dtype=dtype, solver=solver):\n        for ss in sources:\n            i = mapping[ss]\n            for tt in targets:\n                j = mapping[tt]\n                betweenness[s] += 0.5 * np.abs(row[i] - row[j])\n                betweenness[t] += 0.5 * np.abs(row[i] - row[j])\n    if normalized:\n        nb = (n - 1.0) * (n - 2.0)\n    else:\n        nb = 2.0\n    for v in H:\n        betweenness[v] = betweenness[v] / nb + 1.0 / (2 - n)\n    return {ordering[k]: v for k, v in betweenness.items()}"
 },
 {
  "docstring": "Compute current-flow betweenness centrality for edges using subsets\nof nodes.\n\nCurrent-flow betweenness centrality uses an electrical current\nmodel for information spreading in contrast to betweenness\ncentrality which uses shortest paths.\n\nCurrent-flow betweenness centrality is also known as\nrandom-walk betweenness centrality [2]_.\n\nParameters\n----------\nG : graph\n  A NetworkX graph\n\nsources: list of nodes\n  Nodes to use as sources for current\n\ntargets: list of nodes\n  Nodes to use as sinks for current\n\nnormalized : bool, optional (default=True)\n  If True the betweenness values are normalized by b=b/(n-1)(n-2) where\n  n is the number of nodes in G.\n\nweight : string or None, optional (default=None)\n  Key for edge data used as the edge weight.\n  If None, then use 1 as each edge weight.\n  The weight reflects the capacity or the strength of the\n  edge.\n\ndtype: data type (float)\n  Default data type for internal matrices.\n  Set to np.float32 for lower memory consumption.\n\nsolver: string (default='lu')\n   Type of linear solver to use for computing the flow matrix.\n   Options are \"full\" (uses most memory), \"lu\" (recommended), and\n   \"cg\" (uses least memory).\n\nReturns\n-------\nnodes : dict\n   Dictionary of edge tuples with betweenness centrality as the value.\n\nSee Also\n--------\nbetweenness_centrality\nedge_betweenness_centrality\ncurrent_flow_betweenness_centrality\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch(edge_attrs='weight')\ndef edge_current_flow_betweenness_centrality_subset(G, sources, targets, normalized=True, weight=None, dtype=float, solver='lu'):\n    import numpy as np\n    if not nx.is_connected(G):\n        raise nx.NetworkXError('Graph not connected.')\n    n = G.number_of_nodes()\n    ordering = list(reverse_cuthill_mckee_ordering(G))\n    mapping = dict(zip(ordering, range(n)))\n    H = nx.relabel_nodes(G, mapping)\n    edges = (tuple(sorted((u, v))) for u, v in H.edges())\n    betweenness = dict.fromkeys(edges, 0.0)\n    if normalized:\n        nb = (n - 1.0) * (n - 2.0)\n    else:\n        nb = 2.0\n    for row, e in flow_matrix_row(H, weight=weight, dtype=dtype, solver=solver):\n        for ss in sources:\n            i = mapping[ss]\n            for tt in targets:\n                j = mapping[tt]\n                betweenness[e] += 0.5 * np.abs(row[i] - row[j])\n        betweenness[e] /= nb\n    return {(ordering[s], ordering[t]): v for (s, t), v in betweenness.items()}"
 },
 {
  "docstring": "Compute current-flow closeness centrality for nodes.\n\nCurrent-flow closeness centrality is variant of closeness\ncentrality based on effective resistance between nodes in\na network. This metric is also known as information centrality.\n\nParameters\n----------\nG : graph\n  A NetworkX graph.\n\nweight : None or string, optional (default=None)\n  If None, all edge weights are considered equal.\n  Otherwise holds the name of the edge attribute used as weight.\n  The weight reflects the capacity or the strength of the\n  edge.\n\ndtype: data type (default=float)\n  Default data type for internal matrices.\n  Set to np.float32 for lower memory consumption.\n\nsolver: string (default='lu')\n   Type of linear solver to use for computing the flow matrix.\n   Options are \"full\" (uses most memory), \"lu\" (recommended), and\n   \"cg\" (uses least memory).\n\nReturns\n-------\nnodes : dictionary\n   Dictionary of nodes with current flow closeness centrality as the value.\n\nSee Also\n--------\ncloseness_centrality\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch(edge_attrs='weight')\ndef current_flow_closeness_centrality(G, weight=None, dtype=float, solver='lu'):\n    if not nx.is_connected(G):\n        raise nx.NetworkXError('Graph not connected.')\n    solvername = {'full': FullInverseLaplacian, 'lu': SuperLUInverseLaplacian, 'cg': CGInverseLaplacian}\n    n = G.number_of_nodes()\n    ordering = list(reverse_cuthill_mckee_ordering(G))\n    H = nx.relabel_nodes(G, dict(zip(ordering, range(n))))\n    betweenness = dict.fromkeys(H, 0.0)\n    n = H.number_of_nodes()\n    L = nx.laplacian_matrix(H, nodelist=range(n), weight=weight).asformat('csc')\n    L = L.astype(dtype)\n    C2 = solvername[solver](L, width=1, dtype=dtype)\n    for v in H:\n        col = C2.get_row(v)\n        for w in H:\n            betweenness[v] += col[v] - 2 * col[w]\n            betweenness[w] += col[v]\n    for v in H:\n        betweenness[v] = 1 / betweenness[v]\n    return {ordering[k]: v for k, v in betweenness.items()}"
 },
 {
  "docstring": "Compute the degree centrality for nodes.\n\nThe degree centrality for a node v is the fraction of nodes it\nis connected to.\n\nParameters\n----------\nG : graph\n  A networkx graph\n\nReturns\n-------\nnodes : dictionary\n   Dictionary of nodes with degree centrality as the value.\n\nExamples\n--------\n>>> G = nx.Graph([(0, 1), (0, 2), (0, 3), (1, 2), (1, 3)])\n>>> nx.degree_centrality(G)\n{0: 1.0, 1: 1.0, 2: 0.6666666666666666, 3: 0.6666666666666666}\n\nSee Also\n--------\nbetweenness_centrality, load_centrality, eigenvector_centrality\n\n",
  "code": "@nx._dispatch\ndef degree_centrality(G):\n    if len(G) <= 1:\n        return {n: 1 for n in G}\n    s = 1.0 / (len(G) - 1.0)\n    centrality = {n: d * s for n, d in G.degree()}\n    return centrality"
 },
 {
  "docstring": "Compute the in-degree centrality for nodes.\n\nThe in-degree centrality for a node v is the fraction of nodes its\nincoming edges are connected to.\n\nParameters\n----------\nG : graph\n    A NetworkX graph\n\nReturns\n-------\nnodes : dictionary\n    Dictionary of nodes with in-degree centrality as values.\n\nRaises\n------\nNetworkXNotImplemented\n    If G is undirected.\n\nExamples\n--------\n>>> G = nx.DiGraph([(0, 1), (0, 2), (0, 3), (1, 2), (1, 3)])\n>>> nx.in_degree_centrality(G)\n{0: 0.0, 1: 0.3333333333333333, 2: 0.6666666666666666, 3: 0.6666666666666666}\n\nSee Also\n--------\ndegree_centrality, out_degree_centrality\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef in_degree_centrality(G):\n    if len(G) <= 1:\n        return {n: 1 for n in G}\n    s = 1.0 / (len(G) - 1.0)\n    centrality = {n: d * s for n, d in G.in_degree()}\n    return centrality"
 },
 {
  "docstring": "Compute the out-degree centrality for nodes.\n\nThe out-degree centrality for a node v is the fraction of nodes its\noutgoing edges are connected to.\n\nParameters\n----------\nG : graph\n    A NetworkX graph\n\nReturns\n-------\nnodes : dictionary\n    Dictionary of nodes with out-degree centrality as values.\n\nRaises\n------\nNetworkXNotImplemented\n    If G is undirected.\n\nExamples\n--------\n>>> G = nx.DiGraph([(0, 1), (0, 2), (0, 3), (1, 2), (1, 3)])\n>>> nx.out_degree_centrality(G)\n{0: 1.0, 1: 0.6666666666666666, 2: 0.0, 3: 0.0}\n\nSee Also\n--------\ndegree_centrality, in_degree_centrality\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef out_degree_centrality(G):\n    if len(G) <= 1:\n        return {n: 1 for n in G}\n    s = 1.0 / (len(G) - 1.0)\n    centrality = {n: d * s for n, d in G.out_degree()}\n    return centrality"
 },
 {
  "docstring": "Calculate dispersion between `u` and `v` in `G`.\n\nA link between two actors (`u` and `v`) has a high dispersion when their\nmutual ties (`s` and `t`) are not well connected with each other.\n\nParameters\n----------\nG : graph\n    A NetworkX graph.\nu : node, optional\n    The source for the dispersion score (e.g. ego node of the network).\nv : node, optional\n    The target of the dispersion score if specified.\nnormalized : bool\n    If True (default) normalize by the embeddedness of the nodes (u and v).\nalpha, b, c : float\n    Parameters for the normalization procedure. When `normalized` is True,\n    the dispersion value is normalized by::\n\n        result = ((dispersion + b) ** alpha) / (embeddedness + c)\n\n    as long as the denominator is nonzero.\n\nReturns\n-------\nnodes : dictionary\n    If u (v) is specified, returns a dictionary of nodes with dispersion\n    score for all \"target\" (\"source\") nodes. If neither u nor v is\n    specified, returns a dictionary of dictionaries for all nodes 'u' in the\n    graph with a dispersion score for each node 'v'.\n\n",
  "code": "@nx._dispatch\ndef dispersion(G, u=None, v=None, normalized=True, alpha=1.0, b=0.0, c=0.0):\n\n    def _dispersion(G_u, u, v):\n        \"\"\"dispersion for all nodes 'v' in a ego network G_u of node 'u'\"\"\"\n        u_nbrs = set(G_u[u])\n        ST = {n for n in G_u[v] if n in u_nbrs}\n        set_uv = {u, v}\n        possib = combinations(ST, 2)\n        total = 0\n        for s, t in possib:\n            nbrs_s = u_nbrs.intersection(G_u[s]) - set_uv\n            if t not in nbrs_s:\n                if nbrs_s.isdisjoint(G_u[t]):\n                    total += 1\n        embeddedness = len(ST)\n        dispersion_val = total\n        if normalized:\n            dispersion_val = (total + b) ** alpha\n            if embeddedness + c != 0:\n                dispersion_val /= embeddedness + c\n        return dispersion_val\n    if u is None:\n        if v is None:\n            results = {n: {} for n in G}\n            for u in G:\n                for v in G[u]:\n                    results[u][v] = _dispersion(G, u, v)\n        else:\n            results = dict.fromkeys(G[v], {})\n            for u in G[v]:\n                results[u] = _dispersion(G, v, u)\n    elif v is None:\n        results = dict.fromkeys(G[u], {})\n        for v in G[u]:\n            results[v] = _dispersion(G, u, v)\n    else:\n        results = _dispersion(G, u, v)\n    return results"
 },
 {
  "docstring": "dispersion for all nodes 'v' in a ego network G_u of node 'u'",
  "code": "def _dispersion(G_u, u, v):\n    u_nbrs = set(G_u[u])\n    ST = {n for n in G_u[v] if n in u_nbrs}\n    set_uv = {u, v}\n    possib = combinations(ST, 2)\n    total = 0\n    for s, t in possib:\n        nbrs_s = u_nbrs.intersection(G_u[s]) - set_uv\n        if t not in nbrs_s:\n            if nbrs_s.isdisjoint(G_u[t]):\n                total += 1\n    embeddedness = len(ST)\n    dispersion_val = total\n    if normalized:\n        dispersion_val = (total + b) ** alpha\n        if embeddedness + c != 0:\n            dispersion_val /= embeddedness + c\n    return dispersion_val"
 },
 {
  "docstring": "Compute the eigenvector centrality for the graph G.\n\nEigenvector centrality computes the centrality for a node by adding\nthe centrality of its predecessors. The centrality for node $i$ is the\n$i$-th element of a left eigenvector associated with the eigenvalue $\\lambda$\nof maximum modulus that is positive. Such an eigenvector $x$ is\ndefined up to a multiplicative constant by the equation\n\n.. math::\n\n     \\lambda x^T = x^T A,\n\nwhere $A$ is the adjacency matrix of the graph G. By definition of\nrow-column product, the equation above is equivalent to\n\n.. math::\n\n    \\lambda x_i = \\sum_{j\\to i}x_j.\n\nThat is, adding the eigenvector centralities of the predecessors of\n$i$ one obtains the eigenvector centrality of $i$ multiplied by\n$\\lambda$. In the case of undirected graphs, $x$ also solves the familiar\nright-eigenvector equation $Ax = \\lambda x$.\n\nBy virtue of the Perron\u2013Frobenius theorem [1]_, if G is strongly\nconnected there is a unique eigenvector $x$, and all its entries\nare strictly positive.\n\nIf G is not strongly connected there might be several left\neigenvectors associated with $\\lambda$, and some of their elements\nmight be zero.\n\nParameters\n----------\nG : graph\n  A networkx graph.\n\nmax_iter : integer, optional (default=100)\n  Maximum number of power iterations.\n\ntol : float, optional (default=1.0e-6)\n  Error tolerance (in Euclidean norm) used to check convergence in\n  power iteration.\n\nnstart : dictionary, optional (default=None)\n  Starting value of power iteration for each node. Must have a nonzero\n  projection on the desired eigenvector for the power method to converge.\n  If None, this implementation uses an all-ones vector, which is a safe\n  choice.\n\nweight : None or string, optional (default=None)\n  If None, all edge weights are considered equal. Otherwise holds the\n  name of the edge attribute used as weight. In this measure the\n  weight is interpreted as the connection strength.\n\nReturns\n-------\nnodes : dictionary\n   Dictionary of nodes with eigenvector centrality as the value. The\n   associated vector has unit Euclidean norm and the values are\n   nonegative.\n\nExamples\n--------\n>>> G = nx.path_graph(4)\n>>> centrality = nx.eigenvector_centrality(G)\n>>> sorted((v, f\"{c:0.2f}\") for v, c in centrality.items())\n[(0, '0.37'), (1, '0.60'), (2, '0.60'), (3, '0.37')]\n\nRaises\n------\nNetworkXPointlessConcept\n    If the graph G is the null graph.\n\nNetworkXError\n    If each value in `nstart` is zero.\n\nPowerIterationFailedConvergence\n    If the algorithm fails to converge to the specified tolerance\n    within the specified number of iterations of the power iteration\n    method.\n\nSee Also\n--------\neigenvector_centrality_numpy\n:func:`~networkx.algorithms.link_analysis.pagerank_alg.pagerank`\n:func:`~networkx.algorithms.link_analysis.hits_alg.hits`\n\n",
  "code": "@not_implemented_for('multigraph')\n@nx._dispatch(edge_attrs='weight')\ndef eigenvector_centrality(G, max_iter=100, tol=1e-06, nstart=None, weight=None):\n    if len(G) == 0:\n        raise nx.NetworkXPointlessConcept('cannot compute centrality for the null graph')\n    if nstart is None:\n        nstart = {v: 1 for v in G}\n    if all((v == 0 for v in nstart.values())):\n        raise nx.NetworkXError('initial vector cannot have all zero values')\n    nstart_sum = sum(nstart.values())\n    x = {k: v / nstart_sum for k, v in nstart.items()}\n    nnodes = G.number_of_nodes()\n    for _ in range(max_iter):\n        xlast = x\n        x = xlast.copy()\n        for n in x:\n            for nbr in G[n]:\n                w = G[n][nbr].get(weight, 1) if weight else 1\n                x[nbr] += xlast[n] * w\n        norm = math.hypot(*x.values()) or 1\n        x = {k: v / norm for k, v in x.items()}\n        if sum((abs(x[n] - xlast[n]) for n in x)) < nnodes * tol:\n            return x\n    raise nx.PowerIterationFailedConvergence(max_iter)"
 },
 {
  "docstring": "Compute the eigenvector centrality for the graph G.\n\nEigenvector centrality computes the centrality for a node by adding\nthe centrality of its predecessors. The centrality for node $i$ is the\n$i$-th element of a left eigenvector associated with the eigenvalue $\\lambda$\nof maximum modulus that is positive. Such an eigenvector $x$ is\ndefined up to a multiplicative constant by the equation\n\n.. math::\n\n     \\lambda x^T = x^T A,\n\nwhere $A$ is the adjacency matrix of the graph G. By definition of\nrow-column product, the equation above is equivalent to\n\n.. math::\n\n    \\lambda x_i = \\sum_{j\\to i}x_j.\n\nThat is, adding the eigenvector centralities of the predecessors of\n$i$ one obtains the eigenvector centrality of $i$ multiplied by\n$\\lambda$. In the case of undirected graphs, $x$ also solves the familiar\nright-eigenvector equation $Ax = \\lambda x$.\n\nBy virtue of the Perron\u2013Frobenius theorem [1]_, if G is strongly\nconnected there is a unique eigenvector $x$, and all its entries\nare strictly positive.\n\nIf G is not strongly connected there might be several left\neigenvectors associated with $\\lambda$, and some of their elements\nmight be zero.\n\nParameters\n----------\nG : graph\n  A networkx graph.\n\nmax_iter : integer, optional (default=50)\n  Maximum number of Arnoldi update iterations allowed.\n\ntol : float, optional (default=0)\n  Relative accuracy for eigenvalues (stopping criterion).\n  The default value of 0 implies machine precision.\n\nweight : None or string, optional (default=None)\n  If None, all edge weights are considered equal. Otherwise holds the\n  name of the edge attribute used as weight. In this measure the\n  weight is interpreted as the connection strength.\n\nReturns\n-------\nnodes : dictionary\n   Dictionary of nodes with eigenvector centrality as the value. The\n   associated vector has unit Euclidean norm and the values are\n   nonegative.\n\nExamples\n--------\n>>> G = nx.path_graph(4)\n>>> centrality = nx.eigenvector_centrality_numpy(G)\n>>> print([f\"{node} {centrality[node]:0.2f}\" for node in centrality])\n['0 0.37', '1 0.60', '2 0.60', '3 0.37']\n\nRaises\n------\nNetworkXPointlessConcept\n    If the graph G is the null graph.\n\nArpackNoConvergence\n    When the requested convergence is not obtained. The currently\n    converged eigenvalues and eigenvectors can be found as\n    eigenvalues and eigenvectors attributes of the exception object.\n\nSee Also\n--------\n:func:`scipy.sparse.linalg.eigs`\neigenvector_centrality\n:func:`~networkx.algorithms.link_analysis.pagerank_alg.pagerank`\n:func:`~networkx.algorithms.link_analysis.hits_alg.hits`\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef eigenvector_centrality_numpy(G, weight=None, max_iter=50, tol=0):\n    import numpy as np\n    import scipy as sp\n    if len(G) == 0:\n        raise nx.NetworkXPointlessConcept('cannot compute centrality for the null graph')\n    M = nx.to_scipy_sparse_array(G, nodelist=list(G), weight=weight, dtype=float)\n    _, eigenvector = sp.sparse.linalg.eigs(M.T, k=1, which='LR', maxiter=max_iter, tol=tol)\n    largest = eigenvector.flatten().real\n    norm = np.sign(largest.sum()) * sp.linalg.norm(largest)\n    return dict(zip(G, largest / norm))"
 },
 {
  "docstring": "Compute the group betweenness centrality for a group of nodes.\n\nGroup betweenness centrality of a group of nodes $C$ is the sum of the\nfraction of all-pairs shortest paths that pass through any vertex in $C$\n\n.. math::\n\n   c_B(v) =\\sum_{s,t \\in V} \\frac{\\sigma(s, t|v)}{\\sigma(s, t)}\n\nwhere $V$ is the set of nodes, $\\sigma(s, t)$ is the number of\nshortest $(s, t)$-paths, and $\\sigma(s, t|C)$ is the number of\nthose paths passing through some node in group $C$. Note that\n$(s, t)$ are not members of the group ($V-C$ is the set of nodes\nin $V$ that are not in $C$).\n\nParameters\n----------\nG : graph\n  A NetworkX graph.\n\nC : list or set or list of lists or list of sets\n  A group or a list of groups containing nodes which belong to G, for which group betweenness\n  centrality is to be calculated.\n\nnormalized : bool, optional (default=True)\n  If True, group betweenness is normalized by `1/((|V|-|C|)(|V|-|C|-1))`\n  where `|V|` is the number of nodes in G and `|C|` is the number of nodes in C.\n\nweight : None or string, optional (default=None)\n  If None, all edge weights are considered equal.\n  Otherwise holds the name of the edge attribute used as weight.\n  The weight of an edge is treated as the length or distance between the two sides.\n\nendpoints : bool, optional (default=False)\n  If True include the endpoints in the shortest path counts.\n\nRaises\n------\nNodeNotFound\n   If node(s) in C are not present in G.\n\nReturns\n-------\nbetweenness : list of floats or float\n   If C is a single group then return a float. If C is a list with\n   several groups then return a list of group betweenness centralities.\n\nSee Also\n--------\nbetweenness_centrality\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef group_betweenness_centrality(G, C, normalized=True, weight=None, endpoints=False):\n    GBC = []\n    list_of_groups = True\n    if any((el in G for el in C)):\n        C = [C]\n        list_of_groups = False\n    set_v = {node for group in C for node in group}\n    if set_v - G.nodes:\n        raise nx.NodeNotFound(f'The node(s) {set_v - G.nodes} are in C but not in G.')\n    PB, sigma, D = _group_preprocessing(G, set_v, weight)\n    for group in C:\n        group = set(group)\n        GBC_group = 0\n        sigma_m = deepcopy(sigma)\n        PB_m = deepcopy(PB)\n        sigma_m_v = deepcopy(sigma_m)\n        PB_m_v = deepcopy(PB_m)\n        for v in group:\n            GBC_group += PB_m[v][v]\n            for x in group:\n                for y in group:\n                    dxvy = 0\n                    dxyv = 0\n                    dvxy = 0\n                    if not (sigma_m[x][y] == 0 or sigma_m[x][v] == 0 or sigma_m[v][y] == 0):\n                        if D[x][v] == D[x][y] + D[y][v]:\n                            dxyv = sigma_m[x][y] * sigma_m[y][v] / sigma_m[x][v]\n                        if D[x][y] == D[x][v] + D[v][y]:\n                            dxvy = sigma_m[x][v] * sigma_m[v][y] / sigma_m[x][y]\n                        if D[v][y] == D[v][x] + D[x][y]:\n                            dvxy = sigma_m[v][x] * sigma[x][y] / sigma[v][y]\n                    sigma_m_v[x][y] = sigma_m[x][y] * (1 - dxvy)\n                    PB_m_v[x][y] = PB_m[x][y] - PB_m[x][y] * dxvy\n                    if y != v:\n                        PB_m_v[x][y] -= PB_m[x][v] * dxyv\n                    if x != v:\n                        PB_m_v[x][y] -= PB_m[v][y] * dvxy\n            sigma_m, sigma_m_v = (sigma_m_v, sigma_m)\n            PB_m, PB_m_v = (PB_m_v, PB_m)\n        v, c = (len(G), len(group))\n        if not endpoints:\n            scale = 0\n            if nx.is_directed(G):\n                if nx.is_strongly_connected(G):\n                    scale = c * (2 * v - c - 1)\n            elif nx.is_connected(G):\n                scale = c * (2 * v - c - 1)\n            if scale == 0:\n                for group_node1 in group:\n                    for node in D[group_node1]:\n                        if node != group_node1:\n                            if node in group:\n                                scale += 1\n                            else:\n                                scale += 2\n            GBC_group -= scale\n        if normalized:\n            scale = 1 / ((v - c) * (v - c - 1))\n            GBC_group *= scale\n        elif not G.is_directed():\n            GBC_group /= 2\n        GBC.append(GBC_group)\n    if list_of_groups:\n        return GBC\n    return GBC[0]"
 },
 {
  "docstring": "Find the prominent group of size $k$ in graph $G$. The prominence of the\ngroup is evaluated by the group betweenness centrality.\n\nGroup betweenness centrality of a group of nodes $C$ is the sum of the\nfraction of all-pairs shortest paths that pass through any vertex in $C$\n\n.. math::\n\n   c_B(v) =\\sum_{s,t \\in V} \\frac{\\sigma(s, t|v)}{\\sigma(s, t)}\n\nwhere $V$ is the set of nodes, $\\sigma(s, t)$ is the number of\nshortest $(s, t)$-paths, and $\\sigma(s, t|C)$ is the number of\nthose paths passing through some node in group $C$. Note that\n$(s, t)$ are not members of the group ($V-C$ is the set of nodes\nin $V$ that are not in $C$).\n\nParameters\n----------\nG : graph\n   A NetworkX graph.\n\nk : int\n   The number of nodes in the group.\n\nnormalized : bool, optional (default=True)\n   If True, group betweenness is normalized by ``1/((|V|-|C|)(|V|-|C|-1))``\n   where ``|V|`` is the number of nodes in G and ``|C|`` is the number of\n   nodes in C.\n\nweight : None or string, optional (default=None)\n   If None, all edge weights are considered equal.\n   Otherwise holds the name of the edge attribute used as weight.\n   The weight of an edge is treated as the length or distance between the two sides.\n\nendpoints : bool, optional (default=False)\n   If True include the endpoints in the shortest path counts.\n\nC : list or set, optional (default=None)\n   list of nodes which won't be candidates of the prominent group.\n\ngreedy : bool, optional (default=False)\n   Using a naive greedy algorithm in order to find non-optimal prominent\n   group. For scale free networks the results are negligibly below the optimal\n   results.\n\nRaises\n------\nNodeNotFound\n   If node(s) in C are not present in G.\n\nReturns\n-------\nmax_GBC : float\n   The group betweenness centrality of the prominent group.\n\nmax_group : list\n    The list of nodes in the prominent group.\n\nSee Also\n--------\nbetweenness_centrality, group_betweenness_centrality\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef prominent_group(G, k, weight=None, C=None, endpoints=False, normalized=True, greedy=False):\n    import numpy as np\n    import pandas as pd\n    if C is not None:\n        C = set(C)\n        if C - G.nodes:\n            raise nx.NodeNotFound(f'The node(s) {C - G.nodes} are in C but not in G.')\n        nodes = list(G.nodes - C)\n    else:\n        nodes = list(G.nodes)\n    DF_tree = nx.Graph()\n    PB, sigma, D = _group_preprocessing(G, nodes, weight)\n    betweenness = pd.DataFrame.from_dict(PB)\n    if C is not None:\n        for node in C:\n            betweenness.drop(index=node, inplace=True)\n            betweenness.drop(columns=node, inplace=True)\n    CL = [node for _, node in sorted(zip(np.diag(betweenness), nodes), reverse=True)]\n    max_GBC = 0\n    max_group = []\n    DF_tree.add_node(1, CL=CL, betweenness=betweenness, GBC=0, GM=[], sigma=sigma, cont=dict(zip(nodes, np.diag(betweenness))))\n    DF_tree.nodes[1]['heu'] = 0\n    for i in range(k):\n        DF_tree.nodes[1]['heu'] += DF_tree.nodes[1]['cont'][DF_tree.nodes[1]['CL'][i]]\n    max_GBC, DF_tree, max_group = _dfbnb(G, k, DF_tree, max_GBC, 1, D, max_group, nodes, greedy)\n    v = len(G)\n    if not endpoints:\n        scale = 0\n        if nx.is_directed(G):\n            if nx.is_strongly_connected(G):\n                scale = k * (2 * v - k - 1)\n        elif nx.is_connected(G):\n            scale = k * (2 * v - k - 1)\n        if scale == 0:\n            for group_node1 in max_group:\n                for node in D[group_node1]:\n                    if node != group_node1:\n                        if node in max_group:\n                            scale += 1\n                        else:\n                            scale += 2\n        max_GBC -= scale\n    if normalized:\n        scale = 1 / ((v - k) * (v - k - 1))\n        max_GBC *= scale\n    elif not G.is_directed():\n        max_GBC /= 2\n    max_GBC = float('%.2f' % max_GBC)\n    return (max_GBC, max_group)"
 },
 {
  "docstring": "Compute the group closeness centrality for a group of nodes.\n\nGroup closeness centrality of a group of nodes $S$ is a measure\nof how close the group is to the other nodes in the graph.\n\n.. math::\n\n   c_{close}(S) = \\frac{|V-S|}{\\sum_{v \\in V-S} d_{S, v}}\n\n   d_{S, v} = min_{u \\in S} (d_{u, v})\n\nwhere $V$ is the set of nodes, $d_{S, v}$ is the distance of\nthe group $S$ from $v$ defined as above. ($V-S$ is the set of nodes\nin $V$ that are not in $S$).\n\nParameters\n----------\nG : graph\n   A NetworkX graph.\n\nS : list or set\n   S is a group of nodes which belong to G, for which group closeness\n   centrality is to be calculated.\n\nweight : None or string, optional (default=None)\n   If None, all edge weights are considered equal.\n   Otherwise holds the name of the edge attribute used as weight.\n   The weight of an edge is treated as the length or distance between the two sides.\n\nRaises\n------\nNodeNotFound\n   If node(s) in S are not present in G.\n\nReturns\n-------\ncloseness : float\n   Group closeness centrality of the group S.\n\nSee Also\n--------\ncloseness_centrality\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef group_closeness_centrality(G, S, weight=None):\n    if G.is_directed():\n        G = G.reverse()\n    closeness = 0\n    V = set(G)\n    S = set(S)\n    V_S = V - S\n    shortest_path_lengths = nx.multi_source_dijkstra_path_length(G, S, weight=weight)\n    for v in V_S:\n        try:\n            closeness += shortest_path_lengths[v]\n        except KeyError:\n            closeness += 0\n    try:\n        closeness = len(V_S) / closeness\n    except ZeroDivisionError:\n        closeness = 0\n    return closeness"
 },
 {
  "docstring": "Compute the group degree centrality for a group of nodes.\n\nGroup degree centrality of a group of nodes $S$ is the fraction\nof non-group members connected to group members.\n\nParameters\n----------\nG : graph\n   A NetworkX graph.\n\nS : list or set\n   S is a group of nodes which belong to G, for which group degree\n   centrality is to be calculated.\n\nRaises\n------\nNetworkXError\n   If node(s) in S are not in G.\n\nReturns\n-------\ncentrality : float\n   Group degree centrality of the group S.\n\nSee Also\n--------\ndegree_centrality\ngroup_in_degree_centrality\ngroup_out_degree_centrality\n\n",
  "code": "@nx._dispatch\ndef group_degree_centrality(G, S):\n    centrality = len(set().union(*[set(G.neighbors(i)) for i in S]) - set(S))\n    centrality /= len(G.nodes()) - len(S)\n    return centrality"
 },
 {
  "docstring": "Compute the group in-degree centrality for a group of nodes.\n\nGroup in-degree centrality of a group of nodes $S$ is the fraction\nof non-group members connected to group members by incoming edges.\n\nParameters\n----------\nG : graph\n   A NetworkX graph.\n\nS : list or set\n   S is a group of nodes which belong to G, for which group in-degree\n   centrality is to be calculated.\n\nReturns\n-------\ncentrality : float\n   Group in-degree centrality of the group S.\n\nRaises\n------\nNetworkXNotImplemented\n   If G is undirected.\n\nNodeNotFound\n   If node(s) in S are not in G.\n\nSee Also\n--------\ndegree_centrality\ngroup_degree_centrality\ngroup_out_degree_centrality\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef group_in_degree_centrality(G, S):\n    return group_degree_centrality(G.reverse(), S)"
 },
 {
  "docstring": "Compute the group out-degree centrality for a group of nodes.\n\nGroup out-degree centrality of a group of nodes $S$ is the fraction\nof non-group members connected to group members by outgoing edges.\n\nParameters\n----------\nG : graph\n   A NetworkX graph.\n\nS : list or set\n   S is a group of nodes which belong to G, for which group in-degree\n   centrality is to be calculated.\n\nReturns\n-------\ncentrality : float\n   Group out-degree centrality of the group S.\n\nRaises\n------\nNetworkXNotImplemented\n   If G is undirected.\n\nNodeNotFound\n   If node(s) in S are not in G.\n\nSee Also\n--------\ndegree_centrality\ngroup_degree_centrality\ngroup_in_degree_centrality\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef group_out_degree_centrality(G, S):\n    return group_degree_centrality(G, S)"
 },
 {
  "docstring": "Compute harmonic centrality for nodes.\n\nHarmonic centrality [1]_ of a node `u` is the sum of the reciprocal\nof the shortest path distances from all other nodes to `u`\n\n.. math::\n\n    C(u) = \\sum_{v \\neq u} \\frac{1}{d(v, u)}\n\nwhere `d(v, u)` is the shortest-path distance between `v` and `u`.\n\nIf `sources` is given as an argument, the returned harmonic centrality\nvalues are calculated as the sum of the reciprocals of the shortest\npath distances from the nodes specified in `sources` to `u` instead\nof from all nodes to `u`.\n\nNotice that higher values indicate higher centrality.\n\nParameters\n----------\nG : graph\n  A NetworkX graph\n\nnbunch : container (default: all nodes in G)\n  Container of nodes for which harmonic centrality values are calculated.\n\nsources : container (default: all nodes in G)\n  Container of nodes `v` over which reciprocal distances are computed.\n  Nodes not in `G` are silently ignored.\n\ndistance : edge attribute key, optional (default=None)\n  Use the specified edge attribute as the edge distance in shortest\n  path calculations.  If `None`, then each edge will have distance equal to 1.\n\nReturns\n-------\nnodes : dictionary\n  Dictionary of nodes with harmonic centrality as the value.\n\nSee Also\n--------\nbetweenness_centrality, load_centrality, eigenvector_centrality,\ndegree_centrality, closeness_centrality\n\n",
  "code": "@nx._dispatch(edge_attrs='distance')\ndef harmonic_centrality(G, nbunch=None, distance=None, sources=None):\n    nbunch = set(G.nbunch_iter(nbunch)) if nbunch is not None else set(G.nodes)\n    sources = set(G.nbunch_iter(sources)) if sources is not None else G.nodes\n    spl = partial(nx.shortest_path_length, G, weight=distance)\n    centrality = {u: 0 for u in nbunch}\n    for v in sources:\n        dist = spl(v)\n        for u in nbunch.intersection(dist):\n            d = dist[u]\n            if d == 0:\n                continue\n            centrality[u] += 1 / d\n    return centrality"
 },
 {
  "docstring": "Compute the Katz centrality for the nodes of the graph G.\n\nKatz centrality computes the centrality for a node based on the centrality\nof its neighbors. It is a generalization of the eigenvector centrality. The\nKatz centrality for node $i$ is\n\n.. math::\n\n    x_i = \\alpha \\sum_{j} A_{ij} x_j + \\beta,\n\nwhere $A$ is the adjacency matrix of graph G with eigenvalues $\\lambda$.\n\nThe parameter $\\beta$ controls the initial centrality and\n\n.. math::\n\n    \\alpha < \\frac{1}{\\lambda_{\\max}}.\n\nKatz centrality computes the relative influence of a node within a\nnetwork by measuring the number of the immediate neighbors (first\ndegree nodes) and also all other nodes in the network that connect\nto the node under consideration through these immediate neighbors.\n\nExtra weight can be provided to immediate neighbors through the\nparameter $\\beta$.  Connections made with distant neighbors\nare, however, penalized by an attenuation factor $\\alpha$ which\nshould be strictly less than the inverse largest eigenvalue of the\nadjacency matrix in order for the Katz centrality to be computed\ncorrectly. More information is provided in [1]_.\n\nParameters\n----------\nG : graph\n  A NetworkX graph.\n\nalpha : float, optional (default=0.1)\n  Attenuation factor\n\nbeta : scalar or dictionary, optional (default=1.0)\n  Weight attributed to the immediate neighborhood. If not a scalar, the\n  dictionary must have a value for every node.\n\nmax_iter : integer, optional (default=1000)\n  Maximum number of iterations in power method.\n\ntol : float, optional (default=1.0e-6)\n  Error tolerance used to check convergence in power method iteration.\n\nnstart : dictionary, optional\n  Starting value of Katz iteration for each node.\n\nnormalized : bool, optional (default=True)\n  If True normalize the resulting values.\n\nweight : None or string, optional (default=None)\n  If None, all edge weights are considered equal.\n  Otherwise holds the name of the edge attribute used as weight.\n  In this measure the weight is interpreted as the connection strength.\n\nReturns\n-------\nnodes : dictionary\n   Dictionary of nodes with Katz centrality as the value.\n\nRaises\n------\nNetworkXError\n   If the parameter `beta` is not a scalar but lacks a value for at least\n   one node\n\nPowerIterationFailedConvergence\n    If the algorithm fails to converge to the specified tolerance\n    within the specified number of iterations of the power iteration\n    method.\n\nExamples\n--------\n>>> import math\n>>> G = nx.path_graph(4)\n>>> phi = (1 + math.sqrt(5)) / 2.0  # largest eigenvalue of adj matrix\n>>> centrality = nx.katz_centrality(G, 1 / phi - 0.01)\n>>> for n, c in sorted(centrality.items()):\n...     print(f\"{n} {c:.2f}\")\n0 0.37\n1 0.60\n2 0.60\n3 0.37\n\nSee Also\n--------\nkatz_centrality_numpy\neigenvector_centrality\neigenvector_centrality_numpy\n:func:`~networkx.algorithms.link_analysis.pagerank_alg.pagerank`\n:func:`~networkx.algorithms.link_analysis.hits_alg.hits`\n\n",
  "code": "@not_implemented_for('multigraph')\n@nx._dispatch(edge_attrs='weight')\ndef katz_centrality(G, alpha=0.1, beta=1.0, max_iter=1000, tol=1e-06, nstart=None, normalized=True, weight=None):\n    if len(G) == 0:\n        return {}\n    nnodes = G.number_of_nodes()\n    if nstart is None:\n        x = {n: 0 for n in G}\n    else:\n        x = nstart\n    try:\n        b = dict.fromkeys(G, float(beta))\n    except (TypeError, ValueError, AttributeError) as err:\n        b = beta\n        if set(beta) != set(G):\n            raise nx.NetworkXError('beta dictionary must have a value for every node') from err\n    for _ in range(max_iter):\n        xlast = x\n        x = dict.fromkeys(xlast, 0)\n        for n in x:\n            for nbr in G[n]:\n                x[nbr] += xlast[n] * G[n][nbr].get(weight, 1)\n        for n in x:\n            x[n] = alpha * x[n] + b[n]\n        error = sum((abs(x[n] - xlast[n]) for n in x))\n        if error < nnodes * tol:\n            if normalized:\n                try:\n                    s = 1.0 / math.hypot(*x.values())\n                except ZeroDivisionError:\n                    s = 1.0\n            else:\n                s = 1\n            for n in x:\n                x[n] *= s\n            return x\n    raise nx.PowerIterationFailedConvergence(max_iter)"
 },
 {
  "docstring": "Compute the Katz centrality for the graph G.\n\nKatz centrality computes the centrality for a node based on the centrality\nof its neighbors. It is a generalization of the eigenvector centrality. The\nKatz centrality for node $i$ is\n\n.. math::\n\n    x_i = \\alpha \\sum_{j} A_{ij} x_j + \\beta,\n\nwhere $A$ is the adjacency matrix of graph G with eigenvalues $\\lambda$.\n\nThe parameter $\\beta$ controls the initial centrality and\n\n.. math::\n\n    \\alpha < \\frac{1}{\\lambda_{\\max}}.\n\nKatz centrality computes the relative influence of a node within a\nnetwork by measuring the number of the immediate neighbors (first\ndegree nodes) and also all other nodes in the network that connect\nto the node under consideration through these immediate neighbors.\n\nExtra weight can be provided to immediate neighbors through the\nparameter $\\beta$.  Connections made with distant neighbors\nare, however, penalized by an attenuation factor $\\alpha$ which\nshould be strictly less than the inverse largest eigenvalue of the\nadjacency matrix in order for the Katz centrality to be computed\ncorrectly. More information is provided in [1]_.\n\nParameters\n----------\nG : graph\n  A NetworkX graph\n\nalpha : float\n  Attenuation factor\n\nbeta : scalar or dictionary, optional (default=1.0)\n  Weight attributed to the immediate neighborhood. If not a scalar the\n  dictionary must have an value for every node.\n\nnormalized : bool\n  If True normalize the resulting values.\n\nweight : None or string, optional\n  If None, all edge weights are considered equal.\n  Otherwise holds the name of the edge attribute used as weight.\n  In this measure the weight is interpreted as the connection strength.\n\nReturns\n-------\nnodes : dictionary\n   Dictionary of nodes with Katz centrality as the value.\n\nRaises\n------\nNetworkXError\n   If the parameter `beta` is not a scalar but lacks a value for at least\n   one node\n\nExamples\n--------\n>>> import math\n>>> G = nx.path_graph(4)\n>>> phi = (1 + math.sqrt(5)) / 2.0  # largest eigenvalue of adj matrix\n>>> centrality = nx.katz_centrality_numpy(G, 1 / phi)\n>>> for n, c in sorted(centrality.items()):\n...     print(f\"{n} {c:.2f}\")\n0 0.37\n1 0.60\n2 0.60\n3 0.37\n\nSee Also\n--------\nkatz_centrality\neigenvector_centrality_numpy\neigenvector_centrality\n:func:`~networkx.algorithms.link_analysis.pagerank_alg.pagerank`\n:func:`~networkx.algorithms.link_analysis.hits_alg.hits`\n\n",
  "code": "@not_implemented_for('multigraph')\n@nx._dispatch(edge_attrs='weight')\ndef katz_centrality_numpy(G, alpha=0.1, beta=1.0, normalized=True, weight=None):\n    import numpy as np\n    if len(G) == 0:\n        return {}\n    try:\n        nodelist = beta.keys()\n        if set(nodelist) != set(G):\n            raise nx.NetworkXError('beta dictionary must have a value for every node')\n        b = np.array(list(beta.values()), dtype=float)\n    except AttributeError:\n        nodelist = list(G)\n        try:\n            b = np.ones((len(nodelist), 1)) * beta\n        except (TypeError, ValueError, AttributeError) as err:\n            raise nx.NetworkXError('beta must be a number') from err\n    A = nx.adjacency_matrix(G, nodelist=nodelist, weight=weight).todense().T\n    n = A.shape[0]\n    centrality = np.linalg.solve(np.eye(n, n) - alpha * A, b).squeeze()\n    norm = np.sign(sum(centrality)) * np.linalg.norm(centrality) if normalized else 1\n    return dict(zip(nodelist, centrality / norm))"
 },
 {
  "docstring": "Compute the Laplacian centrality for nodes in the graph `G`.\n\nThe Laplacian Centrality of a node ``i`` is measured by the drop in the\nLaplacian Energy after deleting node ``i`` from the graph. The Laplacian Energy\nis the sum of the squared eigenvalues of a graph's Laplacian matrix.\n\n.. math::\n\n    C_L(u_i,G) = \\frac{(\\Delta E)_i}{E_L (G)} = \\frac{E_L (G)-E_L (G_i)}{E_L (G)}\n\n    E_L (G) = \\sum_{i=0}^n \\lambda_i^2\n\nWhere $E_L (G)$ is the Laplacian energy of graph `G`,\nE_L (G_i) is the Laplacian energy of graph `G` after deleting node ``i``\nand $\\lambda_i$ are the eigenvalues of `G`'s Laplacian matrix.\nThis formula shows the normalized value. Without normalization,\nthe numerator on the right side is returned.\n\nParameters\n----------\nG : graph\n    A networkx graph\n\nnormalized : bool (default = True)\n    If True the centrality score is scaled so the sum over all nodes is 1.\n    If False the centrality score for each node is the drop in Laplacian\n    energy when that node is removed.\n\nnodelist : list, optional (default = None)\n    The rows and columns are ordered according to the nodes in nodelist.\n    If nodelist is None, then the ordering is produced by G.nodes().\n\nweight: string or None, optional (default=`weight`)\n    Optional parameter `weight` to compute the Laplacian matrix.\n    The edge data key used to compute each value in the matrix.\n    If None, then each edge has weight 1.\n\nwalk_type : string or None, optional (default=None)\n    Optional parameter `walk_type` used when calling\n    :func:`directed_laplacian_matrix <networkx.directed_laplacian_matrix>`.\n    If None, the transition matrix is selected depending on the properties\n    of the graph. Otherwise can be `random`, `lazy`, or `pagerank`.\n\nalpha : real (default = 0.95)\n    Optional parameter `alpha` used when calling\n    :func:`directed_laplacian_matrix <networkx.directed_laplacian_matrix>`.\n    (1 - alpha) is the teleportation probability used with pagerank.\n\nReturns\n-------\nnodes : dictionary\n   Dictionary of nodes with Laplacian centrality as the value.\n\nExamples\n--------\n>>> G = nx.Graph()\n>>> edges = [(0, 1, 4), (0, 2, 2), (2, 1, 1), (1, 3, 2), (1, 4, 2), (4, 5, 1)]\n>>> G.add_weighted_edges_from(edges)\n>>> sorted((v, f\"{c:0.2f}\") for v, c in laplacian_centrality(G).items())\n[(0, '0.70'), (1, '0.90'), (2, '0.28'), (3, '0.22'), (4, '0.26'), (5, '0.04')]\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef laplacian_centrality(G, normalized=True, nodelist=None, weight='weight', walk_type=None, alpha=0.95):\n    import numpy as np\n    import scipy as sp\n    if len(G) == 0:\n        raise nx.NetworkXPointlessConcept('null graph has no centrality defined')\n    if G.size(weight=weight) == 0:\n        if normalized:\n            raise ZeroDivisionError('graph with no edges has zero full energy')\n        return {n: 0 for n in G}\n    if nodelist is not None:\n        nodeset = set(G.nbunch_iter(nodelist))\n        if len(nodeset) != len(nodelist):\n            raise nx.NetworkXError('nodelist has duplicate nodes or nodes not in G')\n        nodes = nodelist + [n for n in G if n not in nodeset]\n    else:\n        nodelist = nodes = list(G)\n    if G.is_directed():\n        lap_matrix = nx.directed_laplacian_matrix(G, nodes, weight, walk_type, alpha)\n    else:\n        lap_matrix = nx.laplacian_matrix(G, nodes, weight).toarray()\n    full_energy = np.power(sp.linalg.eigh(lap_matrix, eigvals_only=True), 2).sum()\n    laplace_centralities_dict = {}\n    for i, node in enumerate(nodelist):\n        all_but_i = list(np.arange(lap_matrix.shape[0]))\n        all_but_i.remove(i)\n        A_2 = lap_matrix[all_but_i, :][:, all_but_i]\n        new_diag = lap_matrix.diagonal() - abs(lap_matrix[:, i])\n        np.fill_diagonal(A_2, new_diag[all_but_i])\n        if len(all_but_i) > 0:\n            new_energy = np.power(sp.linalg.eigh(A_2, eigvals_only=True), 2).sum()\n        else:\n            new_energy = 0.0\n        lapl_cent = full_energy - new_energy\n        if normalized:\n            lapl_cent = lapl_cent / full_energy\n        laplace_centralities_dict[node] = lapl_cent\n    return laplace_centralities_dict"
 },
 {
  "docstring": "Compute load centrality for nodes.\n\nThe load centrality of a node is the fraction of all shortest\npaths that pass through that node.\n\nParameters\n----------\nG : graph\n  A networkx graph.\n\nnormalized : bool, optional (default=True)\n  If True the betweenness values are normalized by b=b/(n-1)(n-2) where\n  n is the number of nodes in G.\n\nweight : None or string, optional (default=None)\n  If None, edge weights are ignored.\n  Otherwise holds the name of the edge attribute used as weight.\n  The weight of an edge is treated as the length or distance between the two sides.\n\ncutoff : bool, optional (default=None)\n  If specified, only consider paths of length <= cutoff.\n\nReturns\n-------\nnodes : dictionary\n   Dictionary of nodes with centrality as the value.\n\nSee Also\n--------\nbetweenness_centrality\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef newman_betweenness_centrality(G, v=None, cutoff=None, normalized=True, weight=None):\n    if v is not None:\n        betweenness = 0.0\n        for source in G:\n            ubetween = _node_betweenness(G, source, cutoff, False, weight)\n            betweenness += ubetween[v] if v in ubetween else 0\n        if normalized:\n            order = G.order()\n            if order <= 2:\n                return betweenness\n            betweenness *= 1.0 / ((order - 1) * (order - 2))\n    else:\n        betweenness = {}.fromkeys(G, 0.0)\n        for source in betweenness:\n            ubetween = _node_betweenness(G, source, cutoff, False, weight)\n            for vk in ubetween:\n                betweenness[vk] += ubetween[vk]\n        if normalized:\n            order = G.order()\n            if order <= 2:\n                return betweenness\n            scale = 1.0 / ((order - 1) * (order - 2))\n            for v in betweenness:\n                betweenness[v] *= scale\n    return betweenness"
 },
 {
  "docstring": "Node betweenness_centrality helper:\n\nSee betweenness_centrality for what you probably want.\nThis actually computes \"load\" and not betweenness.\nSee https://networkx.lanl.gov/ticket/103\n\nThis calculates the load of each node for paths from a single source.\n(The fraction of number of shortests paths from source that go\nthrough each node.)\n\nTo get the load for a node you need to do all-pairs shortest paths.\n\nIf weight is not None then use Dijkstra for finding shortest paths.",
  "code": "def _node_betweenness(G, source, cutoff=False, normalized=True, weight=None):\n    if weight is None:\n        pred, length = nx.predecessor(G, source, cutoff=cutoff, return_seen=True)\n    else:\n        pred, length = nx.dijkstra_predecessor_and_distance(G, source, cutoff, weight)\n    onodes = [(l, vert) for vert, l in length.items()]\n    onodes.sort()\n    onodes[:] = [vert for l, vert in onodes if l > 0]\n    between = {}.fromkeys(length, 1.0)\n    while onodes:\n        v = onodes.pop()\n        if v in pred:\n            num_paths = len(pred[v])\n            for x in pred[v]:\n                if x == source:\n                    break\n                between[x] += between[v] / num_paths\n    for v in between:\n        between[v] -= 1\n    if normalized:\n        l = len(between)\n        if l > 2:\n            scale = 1 / ((l - 1) * (l - 2))\n            for v in between:\n                between[v] *= scale\n    return between"
 },
 {
  "docstring": "Compute edge load.\n\nWARNING: This concept of edge load has not been analysed\nor discussed outside of NetworkX that we know of.\nIt is based loosely on load_centrality in the sense that\nit counts the number of shortest paths which cross each edge.\nThis function is for demonstration and testing purposes.\n\nParameters\n----------\nG : graph\n    A networkx graph\n\ncutoff : bool, optional (default=False)\n    If specified, only consider paths of length <= cutoff.\n\nReturns\n-------\nA dict keyed by edge 2-tuple to the number of shortest paths\nwhich use that edge. Where more than one path is shortest\nthe count is divided equally among paths.",
  "code": "@nx._dispatch\ndef edge_load_centrality(G, cutoff=False):\n    betweenness = {}\n    for u, v in G.edges():\n        betweenness[u, v] = 0.0\n        betweenness[v, u] = 0.0\n    for source in G:\n        ubetween = _edge_betweenness(G, source, cutoff=cutoff)\n        for e, ubetweenv in ubetween.items():\n            betweenness[e] += ubetweenv\n    return betweenness"
 },
 {
  "docstring": "Edge betweenness helper.",
  "code": "def _edge_betweenness(G, source, nodes=None, cutoff=False):\n    pred, length = nx.predecessor(G, source, cutoff=cutoff, return_seen=True)\n    onodes = [n for n, d in sorted(length.items(), key=itemgetter(1))]\n    between = {}\n    for u, v in G.edges(nodes):\n        between[u, v] = 1.0\n        between[v, u] = 1.0\n    while onodes:\n        v = onodes.pop()\n        if v in pred:\n            num_paths = len(pred[v])\n            for w in pred[v]:\n                if w in pred:\n                    num_paths = len(pred[w])\n                    for x in pred[w]:\n                        between[w, x] += between[v, w] / num_paths\n                        between[x, w] += between[w, v] / num_paths\n    return between"
 },
 {
  "docstring": "Compute the percolation centrality for nodes.\n\nPercolation centrality of a node $v$, at a given time, is defined\nas the proportion of \u2018percolated paths\u2019 that go through that node.\n\nThis measure quantifies relative impact of nodes based on their\ntopological connectivity, as well as their percolation states.\n\nPercolation states of nodes are used to depict network percolation\nscenarios (such as during infection transmission in a social network\nof individuals, spreading of computer viruses on computer networks, or\ntransmission of disease over a network of towns) over time. In this\nmeasure usually the percolation state is expressed as a decimal\nbetween 0.0 and 1.0.\n\nWhen all nodes are in the same percolated state this measure is\nequivalent to betweenness centrality.\n\nParameters\n----------\nG : graph\n  A NetworkX graph.\n\nattribute : None or string, optional (default='percolation')\n  Name of the node attribute to use for percolation state, used\n  if `states` is None. If a node does not set the attribute the\n  state of that node will be set to the default value of 1.\n  If all nodes do not have the attribute all nodes will be set to\n  1 and the centrality measure will be equivalent to betweenness centrality.\n\nstates : None or dict, optional (default=None)\n  Specify percolation states for the nodes, nodes as keys states\n  as values.\n\nweight : None or string, optional (default=None)\n  If None, all edge weights are considered equal.\n  Otherwise holds the name of the edge attribute used as weight.\n  The weight of an edge is treated as the length or distance between the two sides.\n\n\nReturns\n-------\nnodes : dictionary\n   Dictionary of nodes with percolation centrality as the value.\n\nSee Also\n--------\nbetweenness_centrality\n\n",
  "code": "@nx._dispatch(node_attrs='attribute', edge_attrs='weight')\ndef percolation_centrality(G, attribute='percolation', states=None, weight=None):\n    percolation = dict.fromkeys(G, 0.0)\n    nodes = G\n    if states is None:\n        states = nx.get_node_attributes(nodes, attribute, default=1)\n    p_sigma_x_t = 0.0\n    for v in states.values():\n        p_sigma_x_t += v\n    for s in nodes:\n        if weight is None:\n            S, P, sigma, _ = shortest_path(G, s)\n        else:\n            S, P, sigma, _ = dijkstra(G, s, weight)\n        percolation = _accumulate_percolation(percolation, S, P, sigma, s, states, p_sigma_x_t)\n    n = len(G)\n    for v in percolation:\n        percolation[v] *= 1 / (n - 2)\n    return percolation"
 },
 {
  "docstring": "Returns the average weight of an edge in a weighted path.\n\nParameters\n----------\nG : graph\n  A networkx graph.\n\npath: list\n  A list of vertices that define the path.\n\nweight : None or string, optional (default=None)\n  If None, edge weights are ignored.  Then the average weight of an edge\n  is assumed to be the multiplicative inverse of the length of the path.\n  Otherwise holds the name of the edge attribute used as weight.",
  "code": "def _average_weight(G, path, weight=None):\n    path_length = len(path) - 1\n    if path_length <= 0:\n        return 0\n    if weight is None:\n        return 1 / path_length\n    total_weight = sum((G.edges[i, j][weight] for i, j in pairwise(path)))\n    return total_weight / path_length"
 },
 {
  "docstring": "Returns the global reaching centrality of a directed graph.\n\nThe *global reaching centrality* of a weighted directed graph is the\naverage over all nodes of the difference between the local reaching\ncentrality of the node and the greatest local reaching centrality of\nany node in the graph [1]_. For more information on the local\nreaching centrality, see :func:`local_reaching_centrality`.\nInformally, the local reaching centrality is the proportion of the\ngraph that is reachable from the neighbors of the node.\n\nParameters\n----------\nG : DiGraph\n    A networkx DiGraph.\n\nweight : None or string, optional (default=None)\n    Attribute to use for edge weights. If ``None``, each edge weight\n    is assumed to be one. A higher weight implies a stronger\n    connection between nodes and a *shorter* path length.\n\nnormalized : bool, optional (default=True)\n    Whether to normalize the edge weights by the total sum of edge\n    weights.\n\nReturns\n-------\nh : float\n    The global reaching centrality of the graph.\n\nExamples\n--------\n>>> G = nx.DiGraph()\n>>> G.add_edge(1, 2)\n>>> G.add_edge(1, 3)\n>>> nx.global_reaching_centrality(G)\n1.0\n>>> G.add_edge(3, 2)\n>>> nx.global_reaching_centrality(G)\n0.75\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef global_reaching_centrality(G, weight=None, normalized=True):\n    if nx.is_negatively_weighted(G, weight=weight):\n        raise nx.NetworkXError('edge weights must be positive')\n    total_weight = G.size(weight=weight)\n    if total_weight <= 0:\n        raise nx.NetworkXError('Size of G must be positive')\n    if weight is not None:\n\n        def as_distance(u, v, d):\n            return total_weight / d.get(weight, 1)\n        shortest_paths = nx.shortest_path(G, weight=as_distance)\n    else:\n        shortest_paths = nx.shortest_path(G)\n    centrality = local_reaching_centrality\n    lrc = [centrality(G, node, paths=paths, weight=weight, normalized=normalized) for node, paths in shortest_paths.items()]\n    max_lrc = max(lrc)\n    return sum((max_lrc - c for c in lrc)) / (len(G) - 1)"
 },
 {
  "docstring": "Returns the local reaching centrality of a node in a directed\ngraph.\n\nThe *local reaching centrality* of a node in a directed graph is the\nproportion of other nodes reachable from that node [1]_.\n\nParameters\n----------\nG : DiGraph\n    A NetworkX DiGraph.\n\nv : node\n    A node in the directed graph `G`.\n\npaths : dictionary (default=None)\n    If this is not `None` it must be a dictionary representation\n    of single-source shortest paths, as computed by, for example,\n    :func:`networkx.shortest_path` with source node `v`. Use this\n    keyword argument if you intend to invoke this function many\n    times but don't want the paths to be recomputed each time.\n\nweight : None or string, optional (default=None)\n    Attribute to use for edge weights.  If `None`, each edge weight\n    is assumed to be one. A higher weight implies a stronger\n    connection between nodes and a *shorter* path length.\n\nnormalized : bool, optional (default=True)\n    Whether to normalize the edge weights by the total sum of edge\n    weights.\n\nReturns\n-------\nh : float\n    The local reaching centrality of the node ``v`` in the graph\n    ``G``.\n\nExamples\n--------\n>>> G = nx.DiGraph()\n>>> G.add_edges_from([(1, 2), (1, 3)])\n>>> nx.local_reaching_centrality(G, 3)\n0.0\n>>> G.add_edge(3, 2)\n>>> nx.local_reaching_centrality(G, 3)\n0.5\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef local_reaching_centrality(G, v, paths=None, weight=None, normalized=True):\n    if paths is None:\n        if nx.is_negatively_weighted(G, weight=weight):\n            raise nx.NetworkXError('edge weights must be positive')\n        total_weight = G.size(weight=weight)\n        if total_weight <= 0:\n            raise nx.NetworkXError('Size of G must be positive')\n        if weight is not None:\n\n            def as_distance(u, v, d):\n                return total_weight / d.get(weight, 1)\n            paths = nx.shortest_path(G, source=v, weight=as_distance)\n        else:\n            paths = nx.shortest_path(G, source=v)\n    if weight is None and G.is_directed():\n        return (len(paths) - 1) / (len(G) - 1)\n    if normalized and weight is not None:\n        norm = G.size(weight=weight) / G.size()\n    else:\n        norm = 1\n    avgw = (_average_weight(G, path, weight=weight) for path in paths.values())\n    sum_avg_weight = sum(avgw) / norm\n    return sum_avg_weight / (len(G) - 1)"
 },
 {
  "docstring": "Compute the second order centrality for nodes of G.\n\nThe second order centrality of a given node is the standard deviation of\nthe return times to that node of a perpetual random walk on G:\n\nParameters\n----------\nG : graph\n  A NetworkX connected and undirected graph.\n\nweight : string or None, optional (default=\"weight\")\n    The name of an edge attribute that holds the numerical value\n    used as a weight. If None then each edge has weight 1.\n\nReturns\n-------\nnodes : dictionary\n   Dictionary keyed by node with second order centrality as the value.\n\nExamples\n--------\n>>> G = nx.star_graph(10)\n>>> soc = nx.second_order_centrality(G)\n>>> print(sorted(soc.items(), key=lambda x: x[1])[0][0])  # pick first id\n0\n\nRaises\n------\nNetworkXException\n    If the graph G is empty, non connected or has negative weights.\n\nSee Also\n--------\nbetweenness_centrality\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch(edge_attrs='weight')\ndef second_order_centrality(G, weight='weight'):\n    import numpy as np\n    n = len(G)\n    if n == 0:\n        raise nx.NetworkXException('Empty graph.')\n    if not nx.is_connected(G):\n        raise nx.NetworkXException('Non connected graph.')\n    if any((d.get(weight, 0) < 0 for u, v, d in G.edges(data=True))):\n        raise nx.NetworkXException('Graph has negative edge weights.')\n    G = nx.DiGraph(G)\n    in_deg = dict(G.in_degree(weight=weight))\n    d_max = max(in_deg.values())\n    for i, deg in in_deg.items():\n        if deg < d_max:\n            G.add_edge(i, i, weight=d_max - deg)\n    P = nx.to_numpy_array(G)\n    P /= P.sum(axis=1)[:, np.newaxis]\n\n    def _Qj(P, j):\n        P = P.copy()\n        P[:, j] = 0\n        return P\n    M = np.empty([n, n])\n    for i in range(n):\n        M[:, i] = np.linalg.solve(np.identity(n) - _Qj(P, i), np.ones([n, 1])[:, 0])\n    return dict(zip(G.nodes, [np.sqrt(2 * np.sum(M[:, i]) - n * (n + 1)) for i in range(n)]))"
 },
 {
  "docstring": "Returns the subgraph centrality for each node of G.\n\nSubgraph centrality  of a node `n` is the sum of weighted closed\nwalks of all lengths starting and ending at node `n`. The weights\ndecrease with path length. Each closed walk is associated with a\nconnected subgraph ([1]_).\n\nParameters\n----------\nG: graph\n\nReturns\n-------\nnodes:dictionary\n    Dictionary of nodes with subgraph centrality as the value.\n\nRaises\n------\nNetworkXError\n    If the graph is not undirected and simple.\n\nSee Also\n--------\nsubgraph_centrality:\n    Alternative algorithm of the subgraph centrality for each node of G.\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef subgraph_centrality_exp(G):\n    import scipy as sp\n    nodelist = list(G)\n    A = nx.to_numpy_array(G, nodelist)\n    A[A != 0.0] = 1\n    expA = sp.linalg.expm(A)\n    sc = dict(zip(nodelist, map(float, expA.diagonal())))\n    return sc"
 },
 {
  "docstring": "Returns subgraph centrality for each node in G.\n\nSubgraph centrality  of a node `n` is the sum of weighted closed\nwalks of all lengths starting and ending at node `n`. The weights\ndecrease with path length. Each closed walk is associated with a\nconnected subgraph ([1]_).\n\nParameters\n----------\nG: graph\n\nReturns\n-------\nnodes : dictionary\n   Dictionary of nodes with subgraph centrality as the value.\n\nRaises\n------\nNetworkXError\n   If the graph is not undirected and simple.\n\nSee Also\n--------\nsubgraph_centrality_exp:\n    Alternative algorithm of the subgraph centrality for each node of G.\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef subgraph_centrality(G):\n    import numpy as np\n    nodelist = list(G)\n    A = nx.to_numpy_array(G, nodelist)\n    A[np.nonzero(A)] = 1\n    w, v = np.linalg.eigh(A)\n    vsquare = np.array(v) ** 2\n    expw = np.exp(w)\n    xg = vsquare @ expw\n    sc = dict(zip(nodelist, map(float, xg)))\n    return sc"
 },
 {
  "docstring": "Returns subgraph communicability for all pairs of nodes in G.\n\nCommunicability betweenness measure makes use of the number of walks\nconnecting every pair of nodes as the basis of a betweenness centrality\nmeasure.\n\nParameters\n----------\nG: graph\n\nReturns\n-------\nnodes : dictionary\n    Dictionary of nodes with communicability betweenness as the value.\n\nRaises\n------\nNetworkXError\n    If the graph is not undirected and simple.\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef communicability_betweenness_centrality(G):\n    import numpy as np\n    import scipy as sp\n    nodelist = list(G)\n    n = len(nodelist)\n    A = nx.to_numpy_array(G, nodelist)\n    A[np.nonzero(A)] = 1\n    expA = sp.linalg.expm(A)\n    mapping = dict(zip(nodelist, range(n)))\n    cbc = {}\n    for v in G:\n        i = mapping[v]\n        row = A[i, :].copy()\n        col = A[:, i].copy()\n        A[i, :] = 0\n        A[:, i] = 0\n        B = (expA - sp.linalg.expm(A)) / expA\n        B[i, :] = 0\n        B[:, i] = 0\n        B -= np.diag(np.diag(B))\n        cbc[v] = B.sum()\n        A[i, :] = row\n        A[:, i] = col\n    order = len(cbc)\n    if order > 2:\n        scale = 1.0 / ((order - 1.0) ** 2 - (order - 1.0))\n        for v in cbc:\n            cbc[v] *= scale\n    return cbc"
 },
 {
  "docstring": "Returns the Estrada index of a the graph G.\n\nThe Estrada Index is a topological index of folding or 3D \"compactness\" ([1]_).\n\nParameters\n----------\nG: graph\n\nReturns\n-------\nestrada index: float\n\nRaises\n------\nNetworkXError\n    If the graph is not undirected and simple.\n\n",
  "code": "@nx._dispatch\ndef estrada_index(G):\n    return sum(subgraph_centrality(G).values())"
 },
 {
  "docstring": "Compute the trophic levels of nodes.\n\nThe trophic level of a node $i$ is\n\n.. math::\n\n    s_i = 1 + \\frac{1}{k^{in}_i} \\sum_{j} a_{ij} s_j\n\nwhere $k^{in}_i$ is the in-degree of i\n\n.. math::\n\n    k^{in}_i = \\sum_{j} a_{ij}\n\nand nodes with $k^{in}_i = 0$ have $s_i = 1$ by convention.\n\nThese are calculated using the method outlined in Levine [1]_.\n\nParameters\n----------\nG : DiGraph\n    A directed networkx graph\n\nReturns\n-------\nnodes : dict\n    Dictionary of nodes with trophic level as the value.\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch(edge_attrs='weight')\ndef trophic_levels(G, weight='weight'):\n    import numpy as np\n    a = nx.adjacency_matrix(G, weight=weight).T.toarray()\n    rowsum = np.sum(a, axis=1)\n    p = a[rowsum != 0][:, rowsum != 0]\n    p = p / rowsum[rowsum != 0][:, np.newaxis]\n    nn = p.shape[0]\n    i = np.eye(nn)\n    try:\n        n = np.linalg.inv(i - p)\n    except np.linalg.LinAlgError as err:\n        msg = 'Trophic levels are only defined for graphs where every ' + 'node has a path from a basal node (basal nodes are nodes ' + 'with no incoming edges).'\n        raise nx.NetworkXError(msg) from err\n    y = n.sum(axis=1) + 1\n    levels = {}\n    zero_node_ids = (node_id for node_id, degree in G.in_degree if degree == 0)\n    for node_id in zero_node_ids:\n        levels[node_id] = 1\n    nonzero_node_ids = (node_id for node_id, degree in G.in_degree if degree != 0)\n    for i, node_id in enumerate(nonzero_node_ids):\n        levels[node_id] = y[i]\n    return levels"
 },
 {
  "docstring": "Compute the trophic differences of the edges of a directed graph.\n\nThe trophic difference $x_ij$ for each edge is defined in Johnson et al.\n[1]_ as:\n\n.. math::\n    x_ij = s_j - s_i\n\nWhere $s_i$ is the trophic level of node $i$.\n\nParameters\n----------\nG : DiGraph\n    A directed networkx graph\n\nReturns\n-------\ndiffs : dict\n    Dictionary of edges with trophic differences as the value.\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch(edge_attrs='weight')\ndef trophic_differences(G, weight='weight'):\n    levels = trophic_levels(G, weight=weight)\n    diffs = {}\n    for u, v in G.edges:\n        diffs[u, v] = levels[v] - levels[u]\n    return diffs"
 },
 {
  "docstring": "Compute the trophic incoherence parameter of a graph.\n\nTrophic coherence is defined as the homogeneity of the distribution of\ntrophic distances: the more similar, the more coherent. This is measured by\nthe standard deviation of the trophic differences and referred to as the\ntrophic incoherence parameter $q$ by [1].\n\nParameters\n----------\nG : DiGraph\n    A directed networkx graph\n\ncannibalism: Boolean\n    If set to False, self edges are not considered in the calculation\n\nReturns\n-------\ntrophic_incoherence_parameter : float\n    The trophic coherence of a graph\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch(edge_attrs='weight')\ndef trophic_incoherence_parameter(G, weight='weight', cannibalism=False):\n    import numpy as np\n    if cannibalism:\n        diffs = trophic_differences(G, weight=weight)\n    else:\n        self_loops = list(nx.selfloop_edges(G))\n        if self_loops:\n            G_2 = G.copy()\n            G_2.remove_edges_from(self_loops)\n        else:\n            G_2 = G\n        diffs = trophic_differences(G_2, weight=weight)\n    return np.std(list(diffs.values()))"
 },
 {
  "docstring": "Select a list of influential nodes in a graph using VoteRank algorithm\n\nVoteRank [1]_ computes a ranking of the nodes in a graph G based on a\nvoting scheme. With VoteRank, all nodes vote for each of its in-neighbours\nand the node with the highest votes is elected iteratively. The voting\nability of out-neighbors of elected nodes is decreased in subsequent turns.\n\nParameters\n----------\nG : graph\n    A NetworkX graph.\n\nnumber_of_nodes : integer, optional\n    Number of ranked nodes to extract (default all nodes).\n\nReturns\n-------\nvoterank : list\n    Ordered list of computed seeds.\n    Only nodes with positive number of votes are returned.\n\nExamples\n--------\n>>> G = nx.Graph([(0, 1), (0, 2), (0, 3), (1, 4)])\n>>> nx.voterank(G)\n[0, 1]\n\nThe algorithm can be used both for undirected and directed graphs.\nHowever, the directed version is different in two ways:\n(i) nodes only vote for their in-neighbors and\n(ii) only the voting ability of elected node and its out-neighbors are updated:\n\n>>> G = nx.DiGraph([(0, 1), (2, 1), (2, 3), (3, 4)])\n>>> nx.voterank(G)\n[2, 3]\n\n",
  "code": "@nx._dispatch\ndef voterank(G, number_of_nodes=None):\n    influential_nodes = []\n    vote_rank = {}\n    if len(G) == 0:\n        return influential_nodes\n    if number_of_nodes is None or number_of_nodes > len(G):\n        number_of_nodes = len(G)\n    if G.is_directed():\n        avgDegree = sum((deg for _, deg in G.out_degree())) / len(G)\n    else:\n        avgDegree = sum((deg for _, deg in G.degree())) / len(G)\n    for n in G.nodes():\n        vote_rank[n] = [0, 1]\n    for _ in range(number_of_nodes):\n        for n in G.nodes():\n            vote_rank[n][0] = 0\n        for n, nbr in G.edges():\n            vote_rank[n][0] += vote_rank[nbr][1]\n            if not G.is_directed():\n                vote_rank[nbr][0] += vote_rank[n][1]\n        for n in influential_nodes:\n            vote_rank[n][0] = 0\n        n = max(G.nodes, key=lambda x: vote_rank[x][0])\n        if vote_rank[n][0] == 0:\n            return influential_nodes\n        influential_nodes.append(n)\n        vote_rank[n] = [0, 0]\n        for _, nbr in G.edges(n):\n            vote_rank[nbr][1] -= 1 / avgDegree\n            vote_rank[nbr][1] = max(vote_rank[nbr][1], 0)\n    return influential_nodes"
 },
 {
  "docstring": "Betweenness centrality: K5",
  "code": "def test_K5(self):\n    G = nx.complete_graph(5)\n    b = nx.betweenness_centrality(G, weight=None, normalized=False)\n    b_answer = {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Betweenness centrality: K5 endpoints",
  "code": "def test_K5_endpoints(self):\n    G = nx.complete_graph(5)\n    b = nx.betweenness_centrality(G, weight=None, normalized=False, endpoints=True)\n    b_answer = {0: 4.0, 1: 4.0, 2: 4.0, 3: 4.0, 4: 4.0}\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)\n    b = nx.betweenness_centrality(G, weight=None, normalized=True, endpoints=True)\n    b_answer = {0: 0.4, 1: 0.4, 2: 0.4, 3: 0.4, 4: 0.4}\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Betweenness centrality: P3 normalized",
  "code": "def test_P3_normalized(self):\n    G = nx.path_graph(3)\n    b = nx.betweenness_centrality(G, weight=None, normalized=True)\n    b_answer = {0: 0.0, 1: 1.0, 2: 0.0}\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Betweenness centrality: P3",
  "code": "def test_P3(self):\n    G = nx.path_graph(3)\n    b_answer = {0: 0.0, 1: 1.0, 2: 0.0}\n    b = nx.betweenness_centrality(G, weight=None, normalized=False)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Betweenness centrality: P3 sample",
  "code": "def test_sample_from_P3(self):\n    G = nx.path_graph(3)\n    b_answer = {0: 0.0, 1: 1.0, 2: 0.0}\n    b = nx.betweenness_centrality(G, k=3, weight=None, normalized=False, seed=1)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)\n    b = nx.betweenness_centrality(G, k=2, weight=None, normalized=False, seed=1)\n    b_approx1 = {0: 0.0, 1: 1.5, 2: 0.0}\n    b_approx2 = {0: 0.0, 1: 0.75, 2: 0.0}\n    for n in sorted(G):\n        assert b[n] in (b_approx1[n], b_approx2[n])"
 },
 {
  "docstring": "Betweenness centrality: P3 endpoints",
  "code": "def test_P3_endpoints(self):\n    G = nx.path_graph(3)\n    b_answer = {0: 2.0, 1: 3.0, 2: 2.0}\n    b = nx.betweenness_centrality(G, weight=None, normalized=False, endpoints=True)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)\n    b_answer = {0: 2 / 3, 1: 1.0, 2: 2 / 3}\n    b = nx.betweenness_centrality(G, weight=None, normalized=True, endpoints=True)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Betweenness centrality: Krackhardt kite graph",
  "code": "def test_krackhardt_kite_graph(self):\n    G = nx.krackhardt_kite_graph()\n    b_answer = {0: 1.667, 1: 1.667, 2: 0.0, 3: 7.333, 4: 0.0, 5: 16.667, 6: 16.667, 7: 28.0, 8: 16.0, 9: 0.0}\n    for b in b_answer:\n        b_answer[b] /= 2\n    b = nx.betweenness_centrality(G, weight=None, normalized=False)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=0.001)"
 },
 {
  "docstring": "Betweenness centrality: Krackhardt kite graph normalized",
  "code": "def test_krackhardt_kite_graph_normalized(self):\n    G = nx.krackhardt_kite_graph()\n    b_answer = {0: 0.023, 1: 0.023, 2: 0.0, 3: 0.102, 4: 0.0, 5: 0.231, 6: 0.231, 7: 0.389, 8: 0.222, 9: 0.0}\n    b = nx.betweenness_centrality(G, weight=None, normalized=True)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=0.001)"
 },
 {
  "docstring": "Betweenness centrality: Florentine families graph",
  "code": "def test_florentine_families_graph(self):\n    G = nx.florentine_families_graph()\n    b_answer = {'Acciaiuoli': 0.0, 'Albizzi': 0.212, 'Barbadori': 0.093, 'Bischeri': 0.104, 'Castellani': 0.055, 'Ginori': 0.0, 'Guadagni': 0.255, 'Lamberteschi': 0.0, 'Medici': 0.522, 'Pazzi': 0.0, 'Peruzzi': 0.022, 'Ridolfi': 0.114, 'Salviati': 0.143, 'Strozzi': 0.103, 'Tornabuoni': 0.092}\n    b = nx.betweenness_centrality(G, weight=None, normalized=True)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=0.001)"
 },
 {
  "docstring": "Betweenness centrality: Les Miserables graph",
  "code": "def test_les_miserables_graph(self):\n    G = nx.les_miserables_graph()\n    b_answer = {'Napoleon': 0.0, 'Myriel': 0.177, 'MlleBaptistine': 0.0, 'MmeMagloire': 0.0, 'CountessDeLo': 0.0, 'Geborand': 0.0, 'Champtercier': 0.0, 'Cravatte': 0.0, 'Count': 0.0, 'OldMan': 0.0, 'Valjean': 0.57, 'Labarre': 0.0, 'Marguerite': 0.0, 'MmeDeR': 0.0, 'Isabeau': 0.0, 'Gervais': 0.0, 'Listolier': 0.0, 'Tholomyes': 0.041, 'Fameuil': 0.0, 'Blacheville': 0.0, 'Favourite': 0.0, 'Dahlia': 0.0, 'Zephine': 0.0, 'Fantine': 0.13, 'MmeThenardier': 0.029, 'Thenardier': 0.075, 'Cosette': 0.024, 'Javert': 0.054, 'Fauchelevent': 0.026, 'Bamatabois': 0.008, 'Perpetue': 0.0, 'Simplice': 0.009, 'Scaufflaire': 0.0, 'Woman1': 0.0, 'Judge': 0.0, 'Champmathieu': 0.0, 'Brevet': 0.0, 'Chenildieu': 0.0, 'Cochepaille': 0.0, 'Pontmercy': 0.007, 'Boulatruelle': 0.0, 'Eponine': 0.011, 'Anzelma': 0.0, 'Woman2': 0.0, 'MotherInnocent': 0.0, 'Gribier': 0.0, 'MmeBurgon': 0.026, 'Jondrette': 0.0, 'Gavroche': 0.165, 'Gillenormand': 0.02, 'Magnon': 0.0, 'MlleGillenormand': 0.048, 'MmePontmercy': 0.0, 'MlleVaubois': 0.0, 'LtGillenormand': 0.0, 'Marius': 0.132, 'BaronessT': 0.0, 'Mabeuf': 0.028, 'Enjolras': 0.043, 'Combeferre': 0.001, 'Prouvaire': 0.0, 'Feuilly': 0.001, 'Courfeyrac': 0.005, 'Bahorel': 0.002, 'Bossuet': 0.031, 'Joly': 0.002, 'Grantaire': 0.0, 'MotherPlutarch': 0.0, 'Gueulemer': 0.005, 'Babet': 0.005, 'Claquesous': 0.005, 'Montparnasse': 0.004, 'Toussaint': 0.0, 'Child1': 0.0, 'Child2': 0.0, 'Brujon': 0.0, 'MmeHucheloup': 0.0}\n    b = nx.betweenness_centrality(G, weight=None, normalized=True)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=0.001)"
 },
 {
  "docstring": "Betweenness centrality: Ladder graph",
  "code": "def test_ladder_graph(self):\n    G = nx.Graph()\n    G.add_edges_from([(0, 1), (0, 2), (1, 3), (2, 3), (2, 4), (4, 5), (3, 5)])\n    b_answer = {0: 1.667, 1: 1.667, 2: 6.667, 3: 6.667, 4: 1.667, 5: 1.667}\n    for b in b_answer:\n        b_answer[b] /= 2\n    b = nx.betweenness_centrality(G, weight=None, normalized=False)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=0.001)"
 },
 {
  "docstring": "Betweenness centrality: disconnected path",
  "code": "def test_disconnected_path(self):\n    G = nx.Graph()\n    nx.add_path(G, [0, 1, 2])\n    nx.add_path(G, [3, 4, 5, 6])\n    b_answer = {0: 0, 1: 1, 2: 0, 3: 0, 4: 2, 5: 2, 6: 0}\n    b = nx.betweenness_centrality(G, weight=None, normalized=False)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Betweenness centrality: disconnected path endpoints",
  "code": "def test_disconnected_path_endpoints(self):\n    G = nx.Graph()\n    nx.add_path(G, [0, 1, 2])\n    nx.add_path(G, [3, 4, 5, 6])\n    b_answer = {0: 2, 1: 3, 2: 2, 3: 3, 4: 5, 5: 5, 6: 3}\n    b = nx.betweenness_centrality(G, weight=None, normalized=False, endpoints=True)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)\n    b = nx.betweenness_centrality(G, weight=None, normalized=True, endpoints=True)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n] / 21, abs=1e-07)"
 },
 {
  "docstring": "Betweenness centrality: directed path",
  "code": "def test_directed_path(self):\n    G = nx.DiGraph()\n    nx.add_path(G, [0, 1, 2])\n    b = nx.betweenness_centrality(G, weight=None, normalized=False)\n    b_answer = {0: 0.0, 1: 1.0, 2: 0.0}\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Betweenness centrality: directed path normalized",
  "code": "def test_directed_path_normalized(self):\n    G = nx.DiGraph()\n    nx.add_path(G, [0, 1, 2])\n    b = nx.betweenness_centrality(G, weight=None, normalized=True)\n    b_answer = {0: 0.0, 1: 0.5, 2: 0.0}\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Weighted betweenness centrality: K5",
  "code": "def test_K5(self):\n    G = nx.complete_graph(5)\n    b = nx.betweenness_centrality(G, weight='weight', normalized=False)\n    b_answer = {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Weighted betweenness centrality: P3 normalized",
  "code": "def test_P3_normalized(self):\n    G = nx.path_graph(3)\n    b = nx.betweenness_centrality(G, weight='weight', normalized=True)\n    b_answer = {0: 0.0, 1: 1.0, 2: 0.0}\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Weighted betweenness centrality: P3",
  "code": "def test_P3(self):\n    G = nx.path_graph(3)\n    b_answer = {0: 0.0, 1: 1.0, 2: 0.0}\n    b = nx.betweenness_centrality(G, weight='weight', normalized=False)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Weighted betweenness centrality: Krackhardt kite graph",
  "code": "def test_krackhardt_kite_graph(self):\n    G = nx.krackhardt_kite_graph()\n    b_answer = {0: 1.667, 1: 1.667, 2: 0.0, 3: 7.333, 4: 0.0, 5: 16.667, 6: 16.667, 7: 28.0, 8: 16.0, 9: 0.0}\n    for b in b_answer:\n        b_answer[b] /= 2\n    b = nx.betweenness_centrality(G, weight='weight', normalized=False)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=0.001)"
 },
 {
  "docstring": "Weighted betweenness centrality:\nKrackhardt kite graph normalized",
  "code": "def test_krackhardt_kite_graph_normalized(self):\n    G = nx.krackhardt_kite_graph()\n    b_answer = {0: 0.023, 1: 0.023, 2: 0.0, 3: 0.102, 4: 0.0, 5: 0.231, 6: 0.231, 7: 0.389, 8: 0.222, 9: 0.0}\n    b = nx.betweenness_centrality(G, weight='weight', normalized=True)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=0.001)"
 },
 {
  "docstring": "Weighted betweenness centrality:\nFlorentine families graph",
  "code": "def test_florentine_families_graph(self):\n    G = nx.florentine_families_graph()\n    b_answer = {'Acciaiuoli': 0.0, 'Albizzi': 0.212, 'Barbadori': 0.093, 'Bischeri': 0.104, 'Castellani': 0.055, 'Ginori': 0.0, 'Guadagni': 0.255, 'Lamberteschi': 0.0, 'Medici': 0.522, 'Pazzi': 0.0, 'Peruzzi': 0.022, 'Ridolfi': 0.114, 'Salviati': 0.143, 'Strozzi': 0.103, 'Tornabuoni': 0.092}\n    b = nx.betweenness_centrality(G, weight='weight', normalized=True)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=0.001)"
 },
 {
  "docstring": "Weighted betweenness centrality: Les Miserables graph",
  "code": "def test_les_miserables_graph(self):\n    G = nx.les_miserables_graph()\n    b_answer = {'Napoleon': 0.0, 'Myriel': 0.177, 'MlleBaptistine': 0.0, 'MmeMagloire': 0.0, 'CountessDeLo': 0.0, 'Geborand': 0.0, 'Champtercier': 0.0, 'Cravatte': 0.0, 'Count': 0.0, 'OldMan': 0.0, 'Valjean': 0.454, 'Labarre': 0.0, 'Marguerite': 0.009, 'MmeDeR': 0.0, 'Isabeau': 0.0, 'Gervais': 0.0, 'Listolier': 0.0, 'Tholomyes': 0.066, 'Fameuil': 0.0, 'Blacheville': 0.0, 'Favourite': 0.0, 'Dahlia': 0.0, 'Zephine': 0.0, 'Fantine': 0.114, 'MmeThenardier': 0.046, 'Thenardier': 0.129, 'Cosette': 0.075, 'Javert': 0.193, 'Fauchelevent': 0.026, 'Bamatabois': 0.08, 'Perpetue': 0.0, 'Simplice': 0.001, 'Scaufflaire': 0.0, 'Woman1': 0.0, 'Judge': 0.0, 'Champmathieu': 0.0, 'Brevet': 0.0, 'Chenildieu': 0.0, 'Cochepaille': 0.0, 'Pontmercy': 0.023, 'Boulatruelle': 0.0, 'Eponine': 0.023, 'Anzelma': 0.0, 'Woman2': 0.0, 'MotherInnocent': 0.0, 'Gribier': 0.0, 'MmeBurgon': 0.026, 'Jondrette': 0.0, 'Gavroche': 0.285, 'Gillenormand': 0.024, 'Magnon': 0.005, 'MlleGillenormand': 0.036, 'MmePontmercy': 0.005, 'MlleVaubois': 0.0, 'LtGillenormand': 0.015, 'Marius': 0.072, 'BaronessT': 0.004, 'Mabeuf': 0.089, 'Enjolras': 0.003, 'Combeferre': 0.0, 'Prouvaire': 0.0, 'Feuilly': 0.004, 'Courfeyrac': 0.001, 'Bahorel': 0.007, 'Bossuet': 0.028, 'Joly': 0.0, 'Grantaire': 0.036, 'MotherPlutarch': 0.0, 'Gueulemer': 0.025, 'Babet': 0.015, 'Claquesous': 0.042, 'Montparnasse': 0.05, 'Toussaint': 0.011, 'Child1': 0.0, 'Child2': 0.0, 'Brujon': 0.002, 'MmeHucheloup': 0.034}\n    b = nx.betweenness_centrality(G, weight='weight', normalized=True)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=0.001)"
 },
 {
  "docstring": "Weighted betweenness centrality: Ladder graph",
  "code": "def test_ladder_graph(self):\n    G = nx.Graph()\n    G.add_edges_from([(0, 1), (0, 2), (1, 3), (2, 3), (2, 4), (4, 5), (3, 5)])\n    b_answer = {0: 1.667, 1: 1.667, 2: 6.667, 3: 6.667, 4: 1.667, 5: 1.667}\n    for b in b_answer:\n        b_answer[b] /= 2\n    b = nx.betweenness_centrality(G, weight='weight', normalized=False)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=0.001)"
 },
 {
  "docstring": "Weighted betweenness centrality: G",
  "code": "def test_G(self):\n    G = weighted_G()\n    b_answer = {0: 2.0, 1: 0.0, 2: 4.0, 3: 3.0, 4: 4.0, 5: 0.0}\n    b = nx.betweenness_centrality(G, weight='weight', normalized=False)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Weighted betweenness centrality: G2",
  "code": "def test_G2(self):\n    G = nx.DiGraph()\n    G.add_weighted_edges_from([('s', 'u', 10), ('s', 'x', 5), ('u', 'v', 1), ('u', 'x', 2), ('v', 'y', 1), ('x', 'u', 3), ('x', 'v', 5), ('x', 'y', 2), ('y', 's', 7), ('y', 'v', 6)])\n    b_answer = {'y': 5.0, 'x': 5.0, 's': 4.0, 'u': 2.0, 'v': 2.0}\n    b = nx.betweenness_centrality(G, weight='weight', normalized=False)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Weighted betweenness centrality: G3",
  "code": "def test_G3(self):\n    G = nx.MultiGraph(weighted_G())\n    es = list(G.edges(data=True))[::2]\n    G.add_edges_from(es)\n    b_answer = {0: 2.0, 1: 0.0, 2: 4.0, 3: 3.0, 4: 4.0, 5: 0.0}\n    b = nx.betweenness_centrality(G, weight='weight', normalized=False)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Weighted betweenness centrality: G4",
  "code": "def test_G4(self):\n    G = nx.MultiDiGraph()\n    G.add_weighted_edges_from([('s', 'u', 10), ('s', 'x', 5), ('s', 'x', 6), ('u', 'v', 1), ('u', 'x', 2), ('v', 'y', 1), ('v', 'y', 1), ('x', 'u', 3), ('x', 'v', 5), ('x', 'y', 2), ('x', 'y', 3), ('y', 's', 7), ('y', 'v', 6), ('y', 'v', 6)])\n    b_answer = {'y': 5.0, 'x': 5.0, 's': 4.0, 'u': 2.0, 'v': 2.0}\n    b = nx.betweenness_centrality(G, weight='weight', normalized=False)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Edge betweenness centrality: K5",
  "code": "def test_K5(self):\n    G = nx.complete_graph(5)\n    b = nx.edge_betweenness_centrality(G, weight=None, normalized=False)\n    b_answer = dict.fromkeys(G.edges(), 1)\n    for n in sorted(G.edges()):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Edge betweenness centrality: K5",
  "code": "def test_normalized_K5(self):\n    G = nx.complete_graph(5)\n    b = nx.edge_betweenness_centrality(G, weight=None, normalized=True)\n    b_answer = dict.fromkeys(G.edges(), 1 / 10)\n    for n in sorted(G.edges()):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Edge betweenness centrality: C4",
  "code": "def test_C4(self):\n    G = nx.cycle_graph(4)\n    b = nx.edge_betweenness_centrality(G, weight=None, normalized=True)\n    b_answer = {(0, 1): 2, (0, 3): 2, (1, 2): 2, (2, 3): 2}\n    for n in sorted(G.edges()):\n        assert b[n] == pytest.approx(b_answer[n] / 6, abs=1e-07)"
 },
 {
  "docstring": "Edge betweenness centrality: P4",
  "code": "def test_P4(self):\n    G = nx.path_graph(4)\n    b = nx.edge_betweenness_centrality(G, weight=None, normalized=False)\n    b_answer = {(0, 1): 3, (1, 2): 4, (2, 3): 3}\n    for n in sorted(G.edges()):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Edge betweenness centrality: P4",
  "code": "def test_normalized_P4(self):\n    G = nx.path_graph(4)\n    b = nx.edge_betweenness_centrality(G, weight=None, normalized=True)\n    b_answer = {(0, 1): 3, (1, 2): 4, (2, 3): 3}\n    for n in sorted(G.edges()):\n        assert b[n] == pytest.approx(b_answer[n] / 6, abs=1e-07)"
 },
 {
  "docstring": "Edge betweenness centrality: balanced tree",
  "code": "def test_balanced_tree(self):\n    G = nx.balanced_tree(r=2, h=2)\n    b = nx.edge_betweenness_centrality(G, weight=None, normalized=False)\n    b_answer = {(0, 1): 12, (0, 2): 12, (1, 3): 6, (1, 4): 6, (2, 5): 6, (2, 6): 6}\n    for n in sorted(G.edges()):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Edge betweenness centrality: K5",
  "code": "def test_K5(self):\n    G = nx.complete_graph(5)\n    b = nx.edge_betweenness_centrality(G, weight='weight', normalized=False)\n    b_answer = dict.fromkeys(G.edges(), 1)\n    for n in sorted(G.edges()):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Edge betweenness centrality: C4",
  "code": "def test_C4(self):\n    G = nx.cycle_graph(4)\n    b = nx.edge_betweenness_centrality(G, weight='weight', normalized=False)\n    b_answer = {(0, 1): 2, (0, 3): 2, (1, 2): 2, (2, 3): 2}\n    for n in sorted(G.edges()):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Edge betweenness centrality: P4",
  "code": "def test_P4(self):\n    G = nx.path_graph(4)\n    b = nx.edge_betweenness_centrality(G, weight='weight', normalized=False)\n    b_answer = {(0, 1): 3, (1, 2): 4, (2, 3): 3}\n    for n in sorted(G.edges()):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Edge betweenness centrality: balanced tree",
  "code": "def test_balanced_tree(self):\n    G = nx.balanced_tree(r=2, h=2)\n    b = nx.edge_betweenness_centrality(G, weight='weight', normalized=False)\n    b_answer = {(0, 1): 12, (0, 2): 12, (1, 3): 6, (1, 4): 6, (2, 5): 6, (2, 6): 6}\n    for n in sorted(G.edges()):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Edge betweenness centrality: weighted",
  "code": "def test_weighted_graph(self):\n    eList = [(0, 1, 5), (0, 2, 4), (0, 3, 3), (0, 4, 2), (1, 2, 4), (1, 3, 1), (1, 4, 3), (2, 4, 5), (3, 4, 4)]\n    G = nx.Graph()\n    G.add_weighted_edges_from(eList)\n    b = nx.edge_betweenness_centrality(G, weight='weight', normalized=False)\n    b_answer = {(0, 1): 0.0, (0, 2): 1.0, (0, 3): 2.0, (0, 4): 1.0, (1, 2): 2.0, (1, 3): 3.5, (1, 4): 1.5, (2, 4): 1.0, (3, 4): 0.5}\n    for n in sorted(G.edges()):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Edge betweenness centrality: normalized weighted",
  "code": "def test_normalized_weighted_graph(self):\n    eList = [(0, 1, 5), (0, 2, 4), (0, 3, 3), (0, 4, 2), (1, 2, 4), (1, 3, 1), (1, 4, 3), (2, 4, 5), (3, 4, 4)]\n    G = nx.Graph()\n    G.add_weighted_edges_from(eList)\n    b = nx.edge_betweenness_centrality(G, weight='weight', normalized=True)\n    b_answer = {(0, 1): 0.0, (0, 2): 1.0, (0, 3): 2.0, (0, 4): 1.0, (1, 2): 2.0, (1, 3): 3.5, (1, 4): 1.5, (2, 4): 1.0, (3, 4): 0.5}\n    norm = len(G) * (len(G) - 1) / 2\n    for n in sorted(G.edges()):\n        assert b[n] == pytest.approx(b_answer[n] / norm, abs=1e-07)"
 },
 {
  "docstring": "Edge betweenness centrality: weighted multigraph",
  "code": "def test_weighted_multigraph(self):\n    eList = [(0, 1, 5), (0, 1, 4), (0, 2, 4), (0, 3, 3), (0, 3, 3), (0, 4, 2), (1, 2, 4), (1, 3, 1), (1, 3, 2), (1, 4, 3), (1, 4, 4), (2, 4, 5), (3, 4, 4), (3, 4, 4)]\n    G = nx.MultiGraph()\n    G.add_weighted_edges_from(eList)\n    b = nx.edge_betweenness_centrality(G, weight='weight', normalized=False)\n    b_answer = {(0, 1, 0): 0.0, (0, 1, 1): 0.5, (0, 2, 0): 1.0, (0, 3, 0): 0.75, (0, 3, 1): 0.75, (0, 4, 0): 1.0, (1, 2, 0): 2.0, (1, 3, 0): 3.0, (1, 3, 1): 0.0, (1, 4, 0): 1.5, (1, 4, 1): 0.0, (2, 4, 0): 1.0, (3, 4, 0): 0.25, (3, 4, 1): 0.25}\n    for n in sorted(G.edges(keys=True)):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Edge betweenness centrality: normalized weighted multigraph",
  "code": "def test_normalized_weighted_multigraph(self):\n    eList = [(0, 1, 5), (0, 1, 4), (0, 2, 4), (0, 3, 3), (0, 3, 3), (0, 4, 2), (1, 2, 4), (1, 3, 1), (1, 3, 2), (1, 4, 3), (1, 4, 4), (2, 4, 5), (3, 4, 4), (3, 4, 4)]\n    G = nx.MultiGraph()\n    G.add_weighted_edges_from(eList)\n    b = nx.edge_betweenness_centrality(G, weight='weight', normalized=True)\n    b_answer = {(0, 1, 0): 0.0, (0, 1, 1): 0.5, (0, 2, 0): 1.0, (0, 3, 0): 0.75, (0, 3, 1): 0.75, (0, 4, 0): 1.0, (1, 2, 0): 2.0, (1, 3, 0): 3.0, (1, 3, 1): 0.0, (1, 4, 0): 1.5, (1, 4, 1): 0.0, (2, 4, 0): 1.0, (3, 4, 0): 0.25, (3, 4, 1): 0.25}\n    norm = len(G) * (len(G) - 1) / 2\n    for n in sorted(G.edges(keys=True)):\n        assert b[n] == pytest.approx(b_answer[n] / norm, abs=1e-07)"
 },
 {
  "docstring": "Betweenness Centrality Subset: K5",
  "code": "def test_K5(self):\n    G = nx.complete_graph(5)\n    b = nx.betweenness_centrality_subset(G, sources=[0], targets=[1, 3], weight=None)\n    b_answer = {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Betweenness Centrality Subset: P5 directed",
  "code": "def test_P5_directed(self):\n    G = nx.DiGraph()\n    nx.add_path(G, range(5))\n    b_answer = {0: 0, 1: 1, 2: 1, 3: 0, 4: 0, 5: 0}\n    b = nx.betweenness_centrality_subset(G, sources=[0], targets=[3], weight=None)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Betweenness Centrality Subset: P5",
  "code": "def test_P5(self):\n    G = nx.Graph()\n    nx.add_path(G, range(5))\n    b_answer = {0: 0, 1: 0.5, 2: 0.5, 3: 0, 4: 0, 5: 0}\n    b = nx.betweenness_centrality_subset(G, sources=[0], targets=[3], weight=None)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Betweenness Centrality Subset: P5 multiple target",
  "code": "def test_P5_multiple_target(self):\n    G = nx.Graph()\n    nx.add_path(G, range(5))\n    b_answer = {0: 0, 1: 1, 2: 1, 3: 0.5, 4: 0, 5: 0}\n    b = nx.betweenness_centrality_subset(G, sources=[0], targets=[3, 4], weight=None)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Betweenness Centrality Subset: box",
  "code": "def test_box(self):\n    G = nx.Graph()\n    G.add_edges_from([(0, 1), (0, 2), (1, 3), (2, 3)])\n    b_answer = {0: 0, 1: 0.25, 2: 0.25, 3: 0}\n    b = nx.betweenness_centrality_subset(G, sources=[0], targets=[3], weight=None)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Betweenness Centrality Subset: box and path",
  "code": "def test_box_and_path(self):\n    G = nx.Graph()\n    G.add_edges_from([(0, 1), (0, 2), (1, 3), (2, 3), (3, 4), (4, 5)])\n    b_answer = {0: 0, 1: 0.5, 2: 0.5, 3: 0.5, 4: 0, 5: 0}\n    b = nx.betweenness_centrality_subset(G, sources=[0], targets=[3, 4], weight=None)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Betweenness Centrality Subset: box and path multiple target",
  "code": "def test_box_and_path2(self):\n    G = nx.Graph()\n    G.add_edges_from([(0, 1), (1, 2), (2, 3), (1, 20), (20, 3), (3, 4)])\n    b_answer = {0: 0, 1: 1.0, 2: 0.5, 20: 0.5, 3: 0.5, 4: 0}\n    b = nx.betweenness_centrality_subset(G, sources=[0], targets=[3, 4], weight=None)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Betweenness Centrality Subset: Diamond Multi Path",
  "code": "def test_diamond_multi_path(self):\n    G = nx.Graph()\n    G.add_edges_from([(1, 2), (1, 3), (1, 4), (1, 5), (1, 10), (10, 11), (11, 12), (12, 9), (2, 6), (3, 6), (4, 6), (5, 7), (7, 8), (6, 8), (8, 9)])\n    b = nx.betweenness_centrality_subset(G, sources=[1], targets=[9], weight=None)\n    expected_b = {1: 0, 2: 1.0 / 10, 3: 1.0 / 10, 4: 1.0 / 10, 5: 1.0 / 10, 6: 3.0 / 10, 7: 1.0 / 10, 8: 4.0 / 10, 9: 0, 10: 1.0 / 10, 11: 1.0 / 10, 12: 1.0 / 10}\n    for n in sorted(G):\n        assert b[n] == pytest.approx(expected_b[n], abs=1e-07)"
 },
 {
  "docstring": "Betweenness Centrality Subset: Normalized P2\nif n <= 2:  no normalization, betweenness centrality should be 0 for all nodes.",
  "code": "def test_normalized_p2(self):\n    G = nx.Graph()\n    nx.add_path(G, range(2))\n    b_answer = {0: 0, 1: 0.0}\n    b = nx.betweenness_centrality_subset(G, sources=[0], targets=[1], normalized=True, weight=None)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Betweenness Centrality Subset: Normalized Directed P5",
  "code": "def test_normalized_P5_directed(self):\n    G = nx.DiGraph()\n    nx.add_path(G, range(5))\n    b_answer = {0: 0, 1: 1.0 / 12.0, 2: 1.0 / 12.0, 3: 0, 4: 0, 5: 0}\n    b = nx.betweenness_centrality_subset(G, sources=[0], targets=[3], normalized=True, weight=None)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Betweenness Centrality Subset: Weighted Graph",
  "code": "def test_weighted_graph(self):\n    G = nx.DiGraph()\n    G.add_edge(0, 1, weight=3)\n    G.add_edge(0, 2, weight=2)\n    G.add_edge(0, 3, weight=6)\n    G.add_edge(0, 4, weight=4)\n    G.add_edge(1, 3, weight=5)\n    G.add_edge(1, 5, weight=5)\n    G.add_edge(2, 4, weight=1)\n    G.add_edge(3, 4, weight=2)\n    G.add_edge(3, 5, weight=1)\n    G.add_edge(4, 5, weight=4)\n    b_answer = {0: 0.0, 1: 0.0, 2: 0.5, 3: 0.5, 4: 0.5, 5: 0.0}\n    b = nx.betweenness_centrality_subset(G, sources=[0], targets=[5], normalized=False, weight='weight')\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Edge betweenness subset centrality: K5",
  "code": "def test_K5(self):\n    G = nx.complete_graph(5)\n    b = nx.edge_betweenness_centrality_subset(G, sources=[0], targets=[1, 3], weight=None)\n    b_answer = dict.fromkeys(G.edges(), 0)\n    b_answer[0, 3] = b_answer[0, 1] = 0.5\n    for n in sorted(G.edges()):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Edge betweenness subset centrality: P5 directed",
  "code": "def test_P5_directed(self):\n    G = nx.DiGraph()\n    nx.add_path(G, range(5))\n    b_answer = dict.fromkeys(G.edges(), 0)\n    b_answer[0, 1] = b_answer[1, 2] = b_answer[2, 3] = 1\n    b = nx.edge_betweenness_centrality_subset(G, sources=[0], targets=[3], weight=None)\n    for n in sorted(G.edges()):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Edge betweenness subset centrality: P5",
  "code": "def test_P5(self):\n    G = nx.Graph()\n    nx.add_path(G, range(5))\n    b_answer = dict.fromkeys(G.edges(), 0)\n    b_answer[0, 1] = b_answer[1, 2] = b_answer[2, 3] = 0.5\n    b = nx.edge_betweenness_centrality_subset(G, sources=[0], targets=[3], weight=None)\n    for n in sorted(G.edges()):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Edge betweenness subset centrality: P5 multiple target",
  "code": "def test_P5_multiple_target(self):\n    G = nx.Graph()\n    nx.add_path(G, range(5))\n    b_answer = dict.fromkeys(G.edges(), 0)\n    b_answer[0, 1] = b_answer[1, 2] = b_answer[2, 3] = 1\n    b_answer[3, 4] = 0.5\n    b = nx.edge_betweenness_centrality_subset(G, sources=[0], targets=[3, 4], weight=None)\n    for n in sorted(G.edges()):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Edge betweenness subset centrality: box",
  "code": "def test_box(self):\n    G = nx.Graph()\n    G.add_edges_from([(0, 1), (0, 2), (1, 3), (2, 3)])\n    b_answer = dict.fromkeys(G.edges(), 0)\n    b_answer[0, 1] = b_answer[0, 2] = 0.25\n    b_answer[1, 3] = b_answer[2, 3] = 0.25\n    b = nx.edge_betweenness_centrality_subset(G, sources=[0], targets=[3], weight=None)\n    for n in sorted(G.edges()):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Edge betweenness subset centrality: box and path",
  "code": "def test_box_and_path(self):\n    G = nx.Graph()\n    G.add_edges_from([(0, 1), (0, 2), (1, 3), (2, 3), (3, 4), (4, 5)])\n    b_answer = dict.fromkeys(G.edges(), 0)\n    b_answer[0, 1] = b_answer[0, 2] = 0.5\n    b_answer[1, 3] = b_answer[2, 3] = 0.5\n    b_answer[3, 4] = 0.5\n    b = nx.edge_betweenness_centrality_subset(G, sources=[0], targets=[3, 4], weight=None)\n    for n in sorted(G.edges()):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Edge betweenness subset centrality: box and path multiple target",
  "code": "def test_box_and_path2(self):\n    G = nx.Graph()\n    G.add_edges_from([(0, 1), (1, 2), (2, 3), (1, 20), (20, 3), (3, 4)])\n    b_answer = dict.fromkeys(G.edges(), 0)\n    b_answer[0, 1] = 1.0\n    b_answer[1, 20] = b_answer[3, 20] = 0.5\n    b_answer[1, 2] = b_answer[2, 3] = 0.5\n    b_answer[3, 4] = 0.5\n    b = nx.edge_betweenness_centrality_subset(G, sources=[0], targets=[3, 4], weight=None)\n    for n in sorted(G.edges()):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Edge betweenness subset centrality: Diamond Multi Path",
  "code": "def test_diamond_multi_path(self):\n    G = nx.Graph()\n    G.add_edges_from([(1, 2), (1, 3), (1, 4), (1, 5), (1, 10), (10, 11), (11, 12), (12, 9), (2, 6), (3, 6), (4, 6), (5, 7), (7, 8), (6, 8), (8, 9)])\n    b_answer = dict.fromkeys(G.edges(), 0)\n    b_answer[8, 9] = 0.4\n    b_answer[6, 8] = b_answer[7, 8] = 0.2\n    b_answer[2, 6] = b_answer[3, 6] = b_answer[4, 6] = 0.2 / 3.0\n    b_answer[1, 2] = b_answer[1, 3] = b_answer[1, 4] = 0.2 / 3.0\n    b_answer[5, 7] = 0.2\n    b_answer[1, 5] = 0.2\n    b_answer[9, 12] = 0.1\n    b_answer[11, 12] = b_answer[10, 11] = b_answer[1, 10] = 0.1\n    b = nx.edge_betweenness_centrality_subset(G, sources=[1], targets=[9], weight=None)\n    for n in G.edges():\n        sort_n = tuple(sorted(n))\n        assert b[n] == pytest.approx(b_answer[sort_n], abs=1e-07)"
 },
 {
  "docstring": "Edge betweenness subset centrality: P1\nif n <= 1: no normalization b=0 for all nodes",
  "code": "def test_normalized_p1(self):\n    G = nx.Graph()\n    nx.add_path(G, range(1))\n    b_answer = dict.fromkeys(G.edges(), 0)\n    b = nx.edge_betweenness_centrality_subset(G, sources=[0], targets=[0], normalized=True, weight=None)\n    for n in G.edges():\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Edge betweenness subset centrality: Normalized Directed P5",
  "code": "def test_normalized_P5_directed(self):\n    G = nx.DiGraph()\n    nx.add_path(G, range(5))\n    b_answer = dict.fromkeys(G.edges(), 0)\n    b_answer[0, 1] = b_answer[1, 2] = b_answer[2, 3] = 0.05\n    b = nx.edge_betweenness_centrality_subset(G, sources=[0], targets=[3], normalized=True, weight=None)\n    for n in G.edges():\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Edge betweenness subset centrality: Weighted Graph",
  "code": "def test_weighted_graph(self):\n    G = nx.DiGraph()\n    G.add_edge(0, 1, weight=3)\n    G.add_edge(0, 2, weight=2)\n    G.add_edge(0, 3, weight=6)\n    G.add_edge(0, 4, weight=4)\n    G.add_edge(1, 3, weight=5)\n    G.add_edge(1, 5, weight=5)\n    G.add_edge(2, 4, weight=1)\n    G.add_edge(3, 4, weight=2)\n    G.add_edge(3, 5, weight=1)\n    G.add_edge(4, 5, weight=4)\n    b_answer = dict.fromkeys(G.edges(), 0)\n    b_answer[0, 2] = b_answer[2, 4] = b_answer[4, 5] = 0.5\n    b_answer[0, 3] = b_answer[3, 5] = 0.5\n    b = nx.edge_betweenness_centrality_subset(G, sources=[0], targets=[5], normalized=False, weight='weight')\n    for n in G.edges():\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Betweenness centrality: K4",
  "code": "def test_K4_normalized(self):\n    G = nx.complete_graph(4)\n    b = nx.current_flow_betweenness_centrality(G, normalized=True)\n    b_answer = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)\n    G.add_edge(0, 1, weight=0.5, other=0.3)\n    b = nx.current_flow_betweenness_centrality(G, normalized=True, weight=None)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)\n    wb_answer = {0: 0.2222222, 1: 0.2222222, 2: 0.30555555, 3: 0.30555555}\n    b = nx.current_flow_betweenness_centrality(G, normalized=True, weight='weight')\n    for n in sorted(G):\n        assert b[n] == pytest.approx(wb_answer[n], abs=1e-07)\n    wb_answer = {0: 0.2051282, 1: 0.2051282, 2: 0.33974358, 3: 0.33974358}\n    b = nx.current_flow_betweenness_centrality(G, normalized=True, weight='other')\n    for n in sorted(G):\n        assert b[n] == pytest.approx(wb_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Betweenness centrality: K4",
  "code": "def test_K4(self):\n    G = nx.complete_graph(4)\n    for solver in ['full', 'lu', 'cg']:\n        b = nx.current_flow_betweenness_centrality(G, normalized=False, solver=solver)\n        b_answer = {0: 0.75, 1: 0.75, 2: 0.75, 3: 0.75}\n        for n in sorted(G):\n            assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Betweenness centrality: P4 normalized",
  "code": "def test_P4_normalized(self):\n    G = nx.path_graph(4)\n    b = nx.current_flow_betweenness_centrality(G, normalized=True)\n    b_answer = {0: 0, 1: 2.0 / 3, 2: 2.0 / 3, 3: 0}\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Betweenness centrality: P4",
  "code": "def test_P4(self):\n    G = nx.path_graph(4)\n    b = nx.current_flow_betweenness_centrality(G, normalized=False)\n    b_answer = {0: 0, 1: 2, 2: 2, 3: 0}\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Betweenness centrality: star",
  "code": "def test_star(self):\n    G = nx.Graph()\n    nx.add_star(G, ['a', 'b', 'c', 'd'])\n    b = nx.current_flow_betweenness_centrality(G, normalized=True)\n    b_answer = {'a': 1.0, 'b': 0.0, 'c': 0.0, 'd': 0.0}\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Betweenness centrality: alternate solvers",
  "code": "def test_solvers2(self):\n    G = nx.complete_graph(4)\n    for solver in ['full', 'lu', 'cg']:\n        b = nx.current_flow_betweenness_centrality(G, normalized=False, solver=solver)\n        b_answer = {0: 0.75, 1: 0.75, 2: 0.75, 3: 0.75}\n        for n in sorted(G):\n            assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Approximate current-flow betweenness centrality: K4 normalized",
  "code": "def test_K4_normalized(self):\n    G = nx.complete_graph(4)\n    b = nx.current_flow_betweenness_centrality(G, normalized=True)\n    epsilon = 0.1\n    ba = approximate_cfbc(G, normalized=True, epsilon=0.5 * epsilon)\n    for n in sorted(G):\n        np.testing.assert_allclose(b[n], ba[n], atol=epsilon)"
 },
 {
  "docstring": "Approximate current-flow betweenness centrality: K4",
  "code": "def test_K4(self):\n    G = nx.complete_graph(4)\n    b = nx.current_flow_betweenness_centrality(G, normalized=False)\n    epsilon = 0.1\n    ba = approximate_cfbc(G, normalized=False, epsilon=0.5 * epsilon)\n    for n in sorted(G):\n        np.testing.assert_allclose(b[n], ba[n], atol=epsilon * len(G) ** 2)"
 },
 {
  "docstring": "Approximate current-flow betweenness centrality: star",
  "code": "def test_star(self):\n    G = nx.Graph()\n    nx.add_star(G, ['a', 'b', 'c', 'd'])\n    b = nx.current_flow_betweenness_centrality(G, normalized=True)\n    epsilon = 0.1\n    ba = approximate_cfbc(G, normalized=True, epsilon=0.5 * epsilon)\n    for n in sorted(G):\n        np.testing.assert_allclose(b[n], ba[n], atol=epsilon)"
 },
 {
  "docstring": "Approximate current-flow betweenness centrality: 2d grid",
  "code": "def test_grid(self):\n    G = nx.grid_2d_graph(4, 4)\n    b = nx.current_flow_betweenness_centrality(G, normalized=True)\n    epsilon = 0.1\n    ba = approximate_cfbc(G, normalized=True, epsilon=0.5 * epsilon)\n    for n in sorted(G):\n        np.testing.assert_allclose(b[n], ba[n], atol=epsilon)"
 },
 {
  "docstring": "Approximate current-flow betweenness centrality: solvers",
  "code": "def test_solvers(self):\n    G = nx.complete_graph(4)\n    epsilon = 0.1\n    for solver in ['full', 'lu', 'cg']:\n        b = approximate_cfbc(G, normalized=False, solver=solver, epsilon=0.5 * epsilon)\n        b_answer = {0: 0.75, 1: 0.75, 2: 0.75, 3: 0.75}\n        for n in sorted(G):\n            np.testing.assert_allclose(b[n], b_answer[n], atol=epsilon)"
 },
 {
  "docstring": "Edge flow betweenness centrality: K4",
  "code": "def test_K4(self):\n    G = nx.complete_graph(4)\n    b = edge_current_flow(G, normalized=True)\n    b_answer = dict.fromkeys(G.edges(), 0.25)\n    for (s, t), v1 in b_answer.items():\n        v2 = b.get((s, t), b.get((t, s)))\n        assert v1 == pytest.approx(v2, abs=1e-07)"
 },
 {
  "docstring": "Edge flow betweenness centrality: K4",
  "code": "def test_K4_normalized(self):\n    G = nx.complete_graph(4)\n    b = edge_current_flow(G, normalized=False)\n    b_answer = dict.fromkeys(G.edges(), 0.75)\n    for (s, t), v1 in b_answer.items():\n        v2 = b.get((s, t), b.get((t, s)))\n        assert v1 == pytest.approx(v2, abs=1e-07)"
 },
 {
  "docstring": "Edge flow betweenness centrality: C4",
  "code": "def test_C4(self):\n    G = nx.cycle_graph(4)\n    b = edge_current_flow(G, normalized=False)\n    b_answer = {(0, 1): 1.25, (0, 3): 1.25, (1, 2): 1.25, (2, 3): 1.25}\n    for (s, t), v1 in b_answer.items():\n        v2 = b.get((s, t), b.get((t, s)))\n        assert v1 == pytest.approx(v2, abs=1e-07)"
 },
 {
  "docstring": "Edge betweenness centrality: P4",
  "code": "def test_P4(self):\n    G = nx.path_graph(4)\n    b = edge_current_flow(G, normalized=False)\n    b_answer = {(0, 1): 1.5, (1, 2): 2.0, (2, 3): 1.5}\n    for (s, t), v1 in b_answer.items():\n        v2 = b.get((s, t), b.get((t, s)))\n        assert v1 == pytest.approx(v2, abs=1e-07)"
 },
 {
  "docstring": "Betweenness centrality: K4",
  "code": "def test_K4_normalized(self):\n    G = nx.complete_graph(4)\n    b = nx.current_flow_betweenness_centrality_subset(G, list(G), list(G), normalized=True)\n    b_answer = nx.current_flow_betweenness_centrality(G, normalized=True)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Betweenness centrality: K4",
  "code": "def test_K4(self):\n    G = nx.complete_graph(4)\n    b = nx.current_flow_betweenness_centrality_subset(G, list(G), list(G), normalized=True)\n    b_answer = nx.current_flow_betweenness_centrality(G, normalized=True)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)\n    G.add_edge(0, 1, weight=0.5, other=0.3)\n    b = nx.current_flow_betweenness_centrality_subset(G, list(G), list(G), normalized=True, weight=None)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)\n    b = nx.current_flow_betweenness_centrality_subset(G, list(G), list(G), normalized=True)\n    b_answer = nx.current_flow_betweenness_centrality(G, normalized=True)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)\n    b = nx.current_flow_betweenness_centrality_subset(G, list(G), list(G), normalized=True, weight='other')\n    b_answer = nx.current_flow_betweenness_centrality(G, normalized=True, weight='other')\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Betweenness centrality: P4 normalized",
  "code": "def test_P4_normalized(self):\n    G = nx.path_graph(4)\n    b = nx.current_flow_betweenness_centrality_subset(G, list(G), list(G), normalized=True)\n    b_answer = nx.current_flow_betweenness_centrality(G, normalized=True)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Betweenness centrality: P4",
  "code": "def test_P4(self):\n    G = nx.path_graph(4)\n    b = nx.current_flow_betweenness_centrality_subset(G, list(G), list(G), normalized=True)\n    b_answer = nx.current_flow_betweenness_centrality(G, normalized=True)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Betweenness centrality: star",
  "code": "def test_star(self):\n    G = nx.Graph()\n    nx.add_star(G, ['a', 'b', 'c', 'd'])\n    b = nx.current_flow_betweenness_centrality_subset(G, list(G), list(G), normalized=True)\n    b_answer = nx.current_flow_betweenness_centrality(G, normalized=True)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Betweenness centrality: K4",
  "code": "def test_K4_normalized(self):\n    G = nx.complete_graph(4)\n    b = edge_current_flow_subset(G, list(G), list(G), normalized=True)\n    b_answer = edge_current_flow(G, normalized=True)\n    for (s, t), v1 in b_answer.items():\n        v2 = b.get((s, t), b.get((t, s)))\n        assert v1 == pytest.approx(v2, abs=1e-07)"
 },
 {
  "docstring": "Betweenness centrality: K4",
  "code": "def test_K4(self):\n    G = nx.complete_graph(4)\n    b = edge_current_flow_subset(G, list(G), list(G), normalized=False)\n    b_answer = edge_current_flow(G, normalized=False)\n    for (s, t), v1 in b_answer.items():\n        v2 = b.get((s, t), b.get((t, s)))\n        assert v1 == pytest.approx(v2, abs=1e-07)\n    G.add_edge(0, 1, weight=0.5, other=0.3)\n    b = edge_current_flow_subset(G, list(G), list(G), normalized=False, weight=None)\n    for (s, t), v1 in b_answer.items():\n        v2 = b.get((s, t), b.get((t, s)))\n        assert v1 == pytest.approx(v2, abs=1e-07)\n    b = edge_current_flow_subset(G, list(G), list(G), normalized=False)\n    b_answer = edge_current_flow(G, normalized=False)\n    for (s, t), v1 in b_answer.items():\n        v2 = b.get((s, t), b.get((t, s)))\n        assert v1 == pytest.approx(v2, abs=1e-07)\n    b = edge_current_flow_subset(G, list(G), list(G), normalized=False, weight='other')\n    b_answer = edge_current_flow(G, normalized=False, weight='other')\n    for (s, t), v1 in b_answer.items():\n        v2 = b.get((s, t), b.get((t, s)))\n        assert v1 == pytest.approx(v2, abs=1e-07)"
 },
 {
  "docstring": "Edge betweenness centrality: C4",
  "code": "def test_C4(self):\n    G = nx.cycle_graph(4)\n    b = edge_current_flow_subset(G, list(G), list(G), normalized=True)\n    b_answer = edge_current_flow(G, normalized=True)\n    for (s, t), v1 in b_answer.items():\n        v2 = b.get((s, t), b.get((t, s)))\n        assert v1 == pytest.approx(v2, abs=1e-07)"
 },
 {
  "docstring": "Edge betweenness centrality: P4",
  "code": "def test_P4(self):\n    G = nx.path_graph(4)\n    b = edge_current_flow_subset(G, list(G), list(G), normalized=True)\n    b_answer = edge_current_flow(G, normalized=True)\n    for (s, t), v1 in b_answer.items():\n        v2 = b.get((s, t), b.get((t, s)))\n        assert v1 == pytest.approx(v2, abs=1e-07)"
 },
 {
  "docstring": "Closeness centrality: K4",
  "code": "def test_K4(self):\n    G = nx.complete_graph(4)\n    b = nx.current_flow_closeness_centrality(G)\n    b_answer = {0: 2.0 / 3, 1: 2.0 / 3, 2: 2.0 / 3, 3: 2.0 / 3}\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Closeness centrality: P4",
  "code": "def test_P4(self):\n    G = nx.path_graph(4)\n    b = nx.current_flow_closeness_centrality(G)\n    b_answer = {0: 1.0 / 6, 1: 1.0 / 4, 2: 1.0 / 4, 3: 1.0 / 6}\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Closeness centrality: star",
  "code": "def test_star(self):\n    G = nx.Graph()\n    nx.add_star(G, ['a', 'b', 'c', 'd'])\n    b = nx.current_flow_closeness_centrality(G)\n    b_answer = {'a': 1.0 / 3, 'b': 0.6 / 3, 'c': 0.6 / 3, 'd': 0.6 / 3}\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "The sample network from https://arxiv.org/pdf/1310.6753v1.pdf",
  "code": "def small_ego_G():\n    edges = [('a', 'b'), ('a', 'c'), ('b', 'c'), ('b', 'd'), ('b', 'e'), ('b', 'f'), ('c', 'd'), ('c', 'f'), ('c', 'h'), ('d', 'f'), ('e', 'f'), ('f', 'h'), ('h', 'j'), ('h', 'k'), ('i', 'j'), ('i', 'k'), ('j', 'k'), ('u', 'a'), ('u', 'b'), ('u', 'c'), ('u', 'd'), ('u', 'e'), ('u', 'f'), ('u', 'g'), ('u', 'h'), ('u', 'i'), ('u', 'j'), ('u', 'k')]\n    G = nx.Graph()\n    G.add_edges_from(edges)\n    return G"
 },
 {
  "docstring": "our algorithm matches article's",
  "code": "def test_article(self):\n    G = small_ego_G()\n    disp_uh = nx.dispersion(G, 'u', 'h', normalized=False)\n    disp_ub = nx.dispersion(G, 'u', 'b', normalized=False)\n    assert disp_uh == 4\n    assert disp_ub == 1"
 },
 {
  "docstring": "there is a result for every node",
  "code": "def test_results_length(self):\n    G = small_ego_G()\n    disp = nx.dispersion(G)\n    disp_Gu = nx.dispersion(G, 'u')\n    disp_uv = nx.dispersion(G, 'u', 'h')\n    assert len(disp) == len(G)\n    assert len(disp_Gu) == len(G) - 1\n    assert isinstance(disp_uv, float)"
 },
 {
  "docstring": "Eigenvector centrality: K5",
  "code": "def test_K5(self):\n    G = nx.complete_graph(5)\n    b = nx.eigenvector_centrality(G)\n    v = math.sqrt(1 / 5.0)\n    b_answer = dict.fromkeys(G, v)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)\n    nstart = {n: 1 for n in G}\n    b = nx.eigenvector_centrality(G, nstart=nstart)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)\n    b = nx.eigenvector_centrality_numpy(G)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=0.001)"
 },
 {
  "docstring": "Eigenvector centrality: P3",
  "code": "def test_P3(self):\n    G = nx.path_graph(3)\n    b_answer = {0: 0.5, 1: 0.7071, 2: 0.5}\n    b = nx.eigenvector_centrality_numpy(G)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=0.0001)\n    b = nx.eigenvector_centrality(G)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=0.0001)"
 },
 {
  "docstring": "Eigenvector centrality: P3",
  "code": "def test_P3_unweighted(self):\n    G = nx.path_graph(3)\n    b_answer = {0: 0.5, 1: 0.7071, 2: 0.5}\n    b = nx.eigenvector_centrality_numpy(G, weight=None)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=0.0001)"
 },
 {
  "docstring": "Group betweenness centrality for single node group",
  "code": "def test_group_betweenness_single_node(self):\n    G = nx.path_graph(5)\n    C = [1]\n    b = nx.group_betweenness_centrality(G, C, weight=None, normalized=False, endpoints=False)\n    b_answer = 3.0\n    assert b == b_answer"
 },
 {
  "docstring": "Group betweenness centrality for single node group",
  "code": "def test_group_betweenness_with_endpoints(self):\n    G = nx.path_graph(5)\n    C = [1]\n    b = nx.group_betweenness_centrality(G, C, weight=None, normalized=False, endpoints=True)\n    b_answer = 7.0\n    assert b == b_answer"
 },
 {
  "docstring": "Group betweenness centrality for group with more than\n1 node and normalized",
  "code": "def test_group_betweenness_normalized(self):\n    G = nx.path_graph(5)\n    C = [1, 3]\n    b = nx.group_betweenness_centrality(G, C, weight=None, normalized=True, endpoints=False)\n    b_answer = 1.0\n    assert b == b_answer"
 },
 {
  "docstring": "Group betweenness centrality value of 0",
  "code": "def test_two_group_betweenness_value_zero(self):\n    G = nx.cycle_graph(7)\n    C = [[0, 1, 6], [0, 1, 5]]\n    b = nx.group_betweenness_centrality(G, C, weight=None, normalized=False)\n    b_answer = [0.0, 3.0]\n    assert b == b_answer"
 },
 {
  "docstring": "Group betweenness centrality value of 0",
  "code": "def test_group_betweenness_value_zero(self):\n    G = nx.cycle_graph(6)\n    C = [0, 1, 5]\n    b = nx.group_betweenness_centrality(G, C, weight=None, normalized=False)\n    b_answer = 0.0\n    assert b == b_answer"
 },
 {
  "docstring": "Group betweenness centrality in a disconnected graph",
  "code": "def test_group_betweenness_disconnected_graph(self):\n    G = nx.path_graph(5)\n    G.remove_edge(0, 1)\n    C = [1]\n    b = nx.group_betweenness_centrality(G, C, weight=None, normalized=False)\n    b_answer = 0.0\n    assert b == b_answer"
 },
 {
  "docstring": "Node(s) in C not in graph, raises NodeNotFound exception",
  "code": "def test_group_betweenness_node_not_in_graph(self):\n    with pytest.raises(nx.NodeNotFound):\n        nx.group_betweenness_centrality(nx.path_graph(5), [4, 7, 8])"
 },
 {
  "docstring": "Group betweenness centrality in a directed and weighted graph",
  "code": "def test_group_betweenness_directed_weighted(self):\n    G = nx.DiGraph()\n    G.add_edge(1, 0, weight=1)\n    G.add_edge(0, 2, weight=2)\n    G.add_edge(1, 2, weight=3)\n    G.add_edge(3, 1, weight=4)\n    G.add_edge(2, 3, weight=1)\n    G.add_edge(4, 3, weight=6)\n    G.add_edge(2, 4, weight=7)\n    C = [1, 2]\n    b = nx.group_betweenness_centrality(G, C, weight='weight', normalized=False)\n    b_answer = 5.0\n    assert b == b_answer"
 },
 {
  "docstring": "Prominent group for single node",
  "code": "def test_prominent_group_single_node(self):\n    G = nx.path_graph(5)\n    k = 1\n    b, g = nx.prominent_group(G, k, normalized=False, endpoints=False)\n    b_answer, g_answer = (4.0, [2])\n    assert b == b_answer and g == g_answer"
 },
 {
  "docstring": "Prominent group without some nodes",
  "code": "def test_prominent_group_with_c(self):\n    G = nx.path_graph(5)\n    k = 1\n    b, g = nx.prominent_group(G, k, normalized=False, C=[2])\n    b_answer, g_answer = (3.0, [1])\n    assert b == b_answer and g == g_answer"
 },
 {
  "docstring": "Prominent group with normalized result, with endpoints",
  "code": "def test_prominent_group_normalized_endpoints(self):\n    G = nx.cycle_graph(7)\n    k = 2\n    b, g = nx.prominent_group(G, k, normalized=True, endpoints=True)\n    b_answer, g_answer = (1.7, [2, 5])\n    assert b == b_answer and g == g_answer"
 },
 {
  "docstring": "Prominent group of disconnected graph",
  "code": "def test_prominent_group_disconnected_graph(self):\n    G = nx.path_graph(6)\n    G.remove_edge(0, 1)\n    k = 1\n    b, g = nx.prominent_group(G, k, weight=None, normalized=False)\n    b_answer, g_answer = (4.0, [3])\n    assert b == b_answer and g == g_answer"
 },
 {
  "docstring": "Node(s) in C not in graph, raises NodeNotFound exception",
  "code": "def test_prominent_group_node_not_in_graph(self):\n    with pytest.raises(nx.NodeNotFound):\n        nx.prominent_group(nx.path_graph(5), 1, C=[10])"
 },
 {
  "docstring": "Group betweenness centrality in a directed and weighted graph",
  "code": "def test_group_betweenness_directed_weighted(self):\n    G = nx.DiGraph()\n    G.add_edge(1, 0, weight=1)\n    G.add_edge(0, 2, weight=2)\n    G.add_edge(1, 2, weight=3)\n    G.add_edge(3, 1, weight=4)\n    G.add_edge(2, 3, weight=1)\n    G.add_edge(4, 3, weight=6)\n    G.add_edge(2, 4, weight=7)\n    k = 2\n    b, g = nx.prominent_group(G, k, weight='weight', normalized=False)\n    b_answer, g_answer = (5.0, [1, 2])\n    assert b == b_answer and g == g_answer"
 },
 {
  "docstring": "Group betweenness centrality in a greedy algorithm",
  "code": "def test_prominent_group_greedy_algorithm(self):\n    G = nx.cycle_graph(7)\n    k = 2\n    b, g = nx.prominent_group(G, k, normalized=True, endpoints=True, greedy=True)\n    b_answer, g_answer = (1.7, [6, 3])\n    assert b == b_answer and g == g_answer"
 },
 {
  "docstring": "Group closeness centrality for a single node group",
  "code": "def test_group_closeness_single_node(self):\n    G = nx.path_graph(5)\n    c = nx.group_closeness_centrality(G, [1])\n    c_answer = nx.closeness_centrality(G, 1)\n    assert c == c_answer"
 },
 {
  "docstring": "Group closeness centrality for a disconnected graph",
  "code": "def test_group_closeness_disconnected(self):\n    G = nx.Graph()\n    G.add_nodes_from([1, 2, 3, 4])\n    c = nx.group_closeness_centrality(G, [1, 2])\n    c_answer = 0\n    assert c == c_answer"
 },
 {
  "docstring": "Group closeness centrality for a group with more than\n1 node",
  "code": "def test_group_closeness_multiple_node(self):\n    G = nx.path_graph(4)\n    c = nx.group_closeness_centrality(G, [1, 2])\n    c_answer = 1\n    assert c == c_answer"
 },
 {
  "docstring": "Node(s) in S not in graph, raises NodeNotFound exception",
  "code": "def test_group_closeness_node_not_in_graph(self):\n    with pytest.raises(nx.NodeNotFound):\n        nx.group_closeness_centrality(nx.path_graph(5), [6, 7, 8])"
 },
 {
  "docstring": "Group degree centrality for a single node group",
  "code": "def test_group_degree_centrality_single_node(self):\n    G = nx.path_graph(4)\n    d = nx.group_degree_centrality(G, [1])\n    d_answer = nx.degree_centrality(G)[1]\n    assert d == d_answer"
 },
 {
  "docstring": "Group degree centrality for group with more than\n1 node",
  "code": "def test_group_degree_centrality_multiple_node(self):\n    G = nx.Graph()\n    G.add_nodes_from([1, 2, 3, 4, 5, 6, 7, 8])\n    G.add_edges_from([(1, 2), (1, 3), (1, 6), (1, 7), (1, 8), (2, 3), (2, 4), (2, 5)])\n    d = nx.group_degree_centrality(G, [1, 2])\n    d_answer = 1\n    assert d == d_answer"
 },
 {
  "docstring": "Group in-degree centrality in a DiGraph",
  "code": "def test_group_in_degree_centrality(self):\n    G = nx.DiGraph()\n    G.add_nodes_from([1, 2, 3, 4, 5, 6, 7, 8])\n    G.add_edges_from([(1, 2), (1, 3), (1, 6), (1, 7), (1, 8), (2, 3), (2, 4), (2, 5)])\n    d = nx.group_in_degree_centrality(G, [1, 2])\n    d_answer = 0\n    assert d == d_answer"
 },
 {
  "docstring": "Group out-degree centrality in a DiGraph",
  "code": "def test_group_out_degree_centrality(self):\n    G = nx.DiGraph()\n    G.add_nodes_from([1, 2, 3, 4, 5, 6, 7, 8])\n    G.add_edges_from([(1, 2), (1, 3), (1, 6), (1, 7), (1, 8), (2, 3), (2, 4), (2, 5)])\n    d = nx.group_out_degree_centrality(G, [1, 2])\n    d_answer = 1\n    assert d == d_answer"
 },
 {
  "docstring": "Node(s) in S not in graph, raises NetworkXError",
  "code": "def test_group_degree_centrality_node_not_in_graph(self):\n    with pytest.raises(nx.NetworkXError):\n        nx.group_degree_centrality(nx.path_graph(5), [6, 7, 8])"
 },
 {
  "docstring": "Katz centrality: K5",
  "code": "def test_K5(self):\n    G = nx.complete_graph(5)\n    alpha = 0.1\n    b = nx.katz_centrality(G, alpha)\n    v = math.sqrt(1 / 5.0)\n    b_answer = dict.fromkeys(G, v)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)\n    nstart = {n: 1 for n in G}\n    b = nx.katz_centrality(G, alpha, nstart=nstart)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)"
 },
 {
  "docstring": "Katz centrality: P3",
  "code": "def test_P3(self):\n    alpha = 0.1\n    G = nx.path_graph(3)\n    b_answer = {0: 0.5598852584152165, 1: 0.6107839182711449, 2: 0.5598852584152162}\n    b = nx.katz_centrality(G, alpha)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=0.0001)"
 },
 {
  "docstring": "Katz centrality: K5",
  "code": "def test_K5(self):\n    G = nx.complete_graph(5)\n    alpha = 0.1\n    b = nx.katz_centrality(G, alpha)\n    v = math.sqrt(1 / 5.0)\n    b_answer = dict.fromkeys(G, v)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)\n    b = nx.eigenvector_centrality_numpy(G)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=0.001)"
 },
 {
  "docstring": "Katz centrality: P3",
  "code": "def test_P3(self):\n    alpha = 0.1\n    G = nx.path_graph(3)\n    b_answer = {0: 0.5598852584152165, 1: 0.6107839182711449, 2: 0.5598852584152162}\n    b = nx.katz_centrality_numpy(G, alpha)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=0.0001)"
 },
 {
  "docstring": "Katz centrality: K5",
  "code": "def test_K5_unweighted(self):\n    G = nx.complete_graph(5)\n    alpha = 0.1\n    b = nx.katz_centrality(G, alpha, weight=None)\n    v = math.sqrt(1 / 5.0)\n    b_answer = dict.fromkeys(G, v)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=1e-07)\n    b = nx.eigenvector_centrality_numpy(G, weight=None)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=0.001)"
 },
 {
  "docstring": "Katz centrality: P3",
  "code": "def test_P3_unweighted(self):\n    alpha = 0.1\n    G = nx.path_graph(3)\n    b_answer = {0: 0.5598852584152165, 1: 0.6107839182711449, 2: 0.5598852584152162}\n    b = nx.katz_centrality_numpy(G, alpha, weight=None)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=0.0001)"
 },
 {
  "docstring": "See gh-6571",
  "code": "def test_laplacian_centrality_single_node():\n    G = nx.empty_graph(1)\n    assert nx.laplacian_centrality(G, normalized=False) == {0: 0}\n    with pytest.raises(ZeroDivisionError):\n        nx.laplacian_centrality(G, normalized=True)"
 },
 {
  "docstring": "laplacian_centrality on a unconnected node graph should return 0\n\nFor graphs without edges, the Laplacian energy is 0 and is unchanged with\nnode removal, so::\n\n    LC(v) = LE(G) - LE(G - v) = 0 - 0 = 0",
  "code": "def test_laplacian_centrality_unconnected_nodes():\n    G = nx.empty_graph(3)\n    assert nx.laplacian_centrality(G, normalized=False) == {0: 0, 1: 0, 2: 0}"
 },
 {
  "docstring": "percolation centrality: example 1a",
  "code": "def test_percolation_example1a():\n    G = example1a_G()\n    p = nx.percolation_centrality(G)\n    p_answer = {4: 0.625, 6: 0.667}\n    for n, k in p_answer.items():\n        assert p[n] == pytest.approx(k, abs=0.001)"
 },
 {
  "docstring": "percolation centrality: example 1a",
  "code": "def test_percolation_example1b():\n    G = example1b_G()\n    p = nx.percolation_centrality(G)\n    p_answer = {4: 0.825, 6: 0.4}\n    for n, k in p_answer.items():\n        assert p[n] == pytest.approx(k, abs=0.001)"
 },
 {
  "docstring": "percolation centrality: should converge to betweenness\ncentrality when all nodes are percolated the same",
  "code": "def test_converge_to_betweenness():\n    G = nx.florentine_families_graph()\n    b_answer = {'Acciaiuoli': 0.0, 'Albizzi': 0.212, 'Barbadori': 0.093, 'Bischeri': 0.104, 'Castellani': 0.055, 'Ginori': 0.0, 'Guadagni': 0.255, 'Lamberteschi': 0.0, 'Medici': 0.522, 'Pazzi': 0.0, 'Peruzzi': 0.022, 'Ridolfi': 0.114, 'Salviati': 0.143, 'Strozzi': 0.103, 'Tornabuoni': 0.092}\n    p_answer = nx.percolation_centrality(G)\n    assert p_answer == pytest.approx(b_answer, abs=0.001)\n    p_states = {k: 0.3 for k, v in b_answer.items()}\n    p_answer = nx.percolation_centrality(G, states=p_states)\n    assert p_answer == pytest.approx(b_answer, abs=0.001)"
 },
 {
  "docstring": "Second order centrality: single node",
  "code": "def test_one_node_graph():\n    G = nx.Graph()\n    G.add_node(0)\n    G.add_edge(0, 0)\n    assert nx.second_order_centrality(G)[0] == 0"
 },
 {
  "docstring": "Second order centrality: line graph, as defined in paper",
  "code": "def test_P3():\n    G = nx.path_graph(3)\n    b_answer = {0: 3.741, 1: 1.414, 2: 3.741}\n    b = nx.second_order_centrality(G)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=0.01)"
 },
 {
  "docstring": "Second order centrality: complete graph, as defined in paper",
  "code": "def test_K3():\n    G = nx.complete_graph(3)\n    b_answer = {0: 1.414, 1: 1.414, 2: 1.414}\n    b = nx.second_order_centrality(G)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=0.01)"
 },
 {
  "docstring": "Second order centrality: ring graph, as defined in paper",
  "code": "def test_ring_graph():\n    G = nx.cycle_graph(5)\n    b_answer = {0: 4.472, 1: 4.472, 2: 4.472, 3: 4.472, 4: 4.472}\n    b = nx.second_order_centrality(G)\n    for n in sorted(G):\n        assert b[n] == pytest.approx(b_answer[n], abs=0.01)"
 },
 {
  "docstring": "Trivial example",
  "code": "def test_trophic_levels():\n    G = nx.DiGraph()\n    G.add_edge('a', 'b')\n    G.add_edge('b', 'c')\n    d = nx.trophic_levels(G)\n    assert d == {'a': 1, 'b': 2, 'c': 3}"
 },
 {
  "docstring": "Example from Figure 5 in Stephen Levine (1980) J. theor. Biol. 83,\n195-207",
  "code": "def test_trophic_levels_levine():\n    S = nx.DiGraph()\n    S.add_edge(1, 2, weight=1.0)\n    S.add_edge(1, 3, weight=0.2)\n    S.add_edge(1, 4, weight=0.8)\n    S.add_edge(2, 3, weight=0.2)\n    S.add_edge(2, 5, weight=0.3)\n    S.add_edge(4, 3, weight=0.6)\n    S.add_edge(4, 5, weight=0.7)\n    S.add_edge(5, 4, weight=0.2)\n    S2 = S.copy()\n    z = [nid for nid, d in S.in_degree if d == 0]\n    for nid in z:\n        S.remove_node(nid)\n    q = nx.linalg.graphmatrix.adjacency_matrix(S).T\n    expected_q = np.array([[0, 0, 0.0, 0], [0.2, 0, 0.6, 0], [0, 0, 0, 0.2], [0.3, 0, 0.7, 0]])\n    assert np.array_equal(q.todense(), expected_q)\n    assert len(q.shape) == 2\n    assert q.shape[0] == q.shape[1]\n    assert q.shape[0] == len(S)\n    nn = q.shape[0]\n    i = np.eye(nn)\n    n = np.linalg.inv(i - q)\n    y = np.asarray(n) @ np.ones(nn)\n    expected_y = np.array([1, 2.07906977, 1.46511628, 2.3255814])\n    assert np.allclose(y, expected_y)\n    expected_d = {1: 1, 2: 2, 3: 3.07906977, 4: 2.46511628, 5: 3.3255814}\n    d = nx.trophic_levels(S2)\n    for nid, level in d.items():\n        expected_level = expected_d[nid]\n        assert expected_level == pytest.approx(level, abs=1e-07)"
 },
 {
  "docstring": "Should raise an error with graphs with only non-basal nodes",
  "code": "def test_trophic_levels_singular_matrix():\n    matrix = np.identity(4)\n    G = nx.from_numpy_array(matrix, create_using=nx.DiGraph)\n    with pytest.raises(nx.NetworkXError) as e:\n        nx.trophic_levels(G)\n    msg = 'Trophic levels are only defined for graphs where every node ' + 'has a path from a basal node (basal nodes are nodes with no ' + 'incoming edges).'\n    assert msg in str(e.value)"
 },
 {
  "docstring": "Should fail to compute if there are any parts of the graph which are not\nreachable from any basal node (with in-degree zero).",
  "code": "def test_trophic_levels_singular_with_basal():\n    G = nx.DiGraph()\n    G.add_edge('a', 'b')\n    G.add_edge('c', 'b')\n    G.add_edge('d', 'b')\n    G.add_edge('c', 'd')\n    G.add_edge('d', 'c')\n    with pytest.raises(nx.NetworkXError) as e:\n        nx.trophic_levels(G)\n    msg = 'Trophic levels are only defined for graphs where every node ' + 'has a path from a basal node (basal nodes are nodes with no ' + 'incoming edges).'\n    assert msg in str(e.value)\n    G = nx.DiGraph()\n    G.add_edge('a', 'b')\n    G.add_edge('c', 'b')\n    G.add_edge('c', 'c')\n    with pytest.raises(nx.NetworkXError) as e:\n        nx.trophic_levels(G)\n    msg = 'Trophic levels are only defined for graphs where every node ' + 'has a path from a basal node (basal nodes are nodes with no ' + 'incoming edges).'\n    assert msg in str(e.value)"
 },
 {
  "docstring": "Determine if the coloring is a valid coloring for the graph G.",
  "code": "@nx._dispatch\ndef is_coloring(G, coloring):\n    return all((coloring[s] != coloring[d] for s, d in G.edges))"
 },
 {
  "docstring": "Determines if the coloring is valid and equitable for the graph G.",
  "code": "@nx._dispatch\ndef is_equitable(G, coloring, num_colors=None):\n    if not is_coloring(G, coloring):\n        return False\n    color_set_size = defaultdict(int)\n    for color in coloring.values():\n        color_set_size[color] += 1\n    if num_colors is not None:\n        for color in range(num_colors):\n            if color not in color_set_size:\n                color_set_size[color] = 0\n    all_set_sizes = set(color_set_size.values())\n    if len(all_set_sizes) == 0 and num_colors is None:\n        return True\n    elif len(all_set_sizes) == 1:\n        return True\n    elif len(all_set_sizes) == 2:\n        a, b = list(all_set_sizes)\n        return abs(a - b) <= 1\n    else:\n        return False"
 },
 {
  "docstring": "Change the color of 'u' from X to Y and update N, H, F, C.",
  "code": "def change_color(u, X, Y, N, H, F, C, L):\n    assert F[u] == X and X != Y\n    F[u] = Y\n    for k in C:\n        if N[u, k] == 0:\n            H[X, k] -= 1\n            H[Y, k] += 1\n    for v in L[u]:\n        N[v, X] -= 1\n        N[v, Y] += 1\n        if N[v, X] == 0:\n            H[F[v], X] += 1\n        if N[v, Y] == 1:\n            H[F[v], Y] -= 1\n    C[X].remove(u)\n    C[Y].append(u)"
 },
 {
  "docstring": "Move witness along a path from src_color to dst_color.",
  "code": "def move_witnesses(src_color, dst_color, N, H, F, C, T_cal, L):\n    X = src_color\n    while X != dst_color:\n        Y = T_cal[X]\n        w = next((x for x in C[X] if N[x, Y] == 0))\n        change_color(w, X, Y, N=N, H=H, F=F, C=C, L=L)\n        X = Y"
 },
 {
  "docstring": "Add a disconnected complete clique K_p such that the number of nodes in\nthe graph becomes a multiple of `num_colors`.\n\nAssumes that the graph's nodes are labelled using integers.\n\nReturns the number of nodes with each color.",
  "code": "@nx._dispatch\ndef pad_graph(G, num_colors):\n    n_ = len(G)\n    r = num_colors - 1\n    s = n_ // (r + 1)\n    if n_ != s * (r + 1):\n        p = r + 1 - n_ % (r + 1)\n        s += 1\n        K = nx.relabel_nodes(nx.complete_graph(p), {idx: idx + n_ for idx in range(p)})\n        G.add_edges_from(K.edges)\n    return s"
 },
 {
  "docstring": "Procedure P as described in the paper.",
  "code": "def procedure_P(V_minus, V_plus, N, H, F, C, L, excluded_colors=None):\n    if excluded_colors is None:\n        excluded_colors = set()\n    A_cal = set()\n    T_cal = {}\n    R_cal = []\n    reachable = [V_minus]\n    marked = set(reachable)\n    idx = 0\n    while idx < len(reachable):\n        pop = reachable[idx]\n        idx += 1\n        A_cal.add(pop)\n        R_cal.append(pop)\n        next_layer = []\n        for k in C:\n            if H[k, pop] > 0 and k not in A_cal and (k not in excluded_colors) and (k not in marked):\n                next_layer.append(k)\n        for dst in next_layer:\n            T_cal[dst] = pop\n        marked.update(next_layer)\n        reachable.extend(next_layer)\n    b = len(C) - len(A_cal)\n    if V_plus in A_cal:\n        move_witnesses(V_plus, V_minus, N=N, H=H, F=F, C=C, T_cal=T_cal, L=L)\n    else:\n        A_0 = set()\n        A_cal_0 = set()\n        num_terminal_sets_found = 0\n        made_equitable = False\n        for W_1 in R_cal[::-1]:\n            for v in C[W_1]:\n                X = None\n                for U in C:\n                    if N[v, U] == 0 and U in A_cal and (U != W_1):\n                        X = U\n                if X is None:\n                    continue\n                for U in C:\n                    if N[v, U] >= 1 and U not in A_cal:\n                        X_prime = U\n                        w = v\n                        try:\n                            y = next((node for node in L[w] if F[node] == X_prime and N[node, W_1] == 1))\n                        except StopIteration:\n                            pass\n                        else:\n                            W = W_1\n                            change_color(w, W, X, N=N, H=H, F=F, C=C, L=L)\n                            move_witnesses(src_color=X, dst_color=V_minus, N=N, H=H, F=F, C=C, T_cal=T_cal, L=L)\n                            change_color(y, X_prime, W, N=N, H=H, F=F, C=C, L=L)\n                            procedure_P(V_minus=X_prime, V_plus=V_plus, N=N, H=H, C=C, F=F, L=L, excluded_colors=excluded_colors.union(A_cal))\n                            made_equitable = True\n                            break\n                if made_equitable:\n                    break\n            else:\n                A_cal_0.add(W_1)\n                A_0.update(C[W_1])\n                num_terminal_sets_found += 1\n            if num_terminal_sets_found == b:\n                B_cal_prime = set()\n                T_cal_prime = {}\n                reachable = [V_plus]\n                marked = set(reachable)\n                idx = 0\n                while idx < len(reachable):\n                    pop = reachable[idx]\n                    idx += 1\n                    B_cal_prime.add(pop)\n                    next_layer = [k for k in C if H[pop, k] > 0 and k not in B_cal_prime and (k not in marked)]\n                    for dst in next_layer:\n                        T_cal_prime[pop] = dst\n                    marked.update(next_layer)\n                    reachable.extend(next_layer)\n                I_set = set()\n                I_covered = set()\n                W_covering = {}\n                B_prime = [node for k in B_cal_prime for node in C[k]]\n                for z in C[V_plus] + B_prime:\n                    if z in I_covered or F[z] not in B_cal_prime:\n                        continue\n                    I_set.add(z)\n                    I_covered.add(z)\n                    I_covered.update(list(L[z]))\n                    for w in L[z]:\n                        if F[w] in A_cal_0 and N[z, F[w]] == 1:\n                            if w not in W_covering:\n                                W_covering[w] = z\n                            else:\n                                z_1 = W_covering[w]\n                                Z = F[z_1]\n                                W = F[w]\n                                move_witnesses(W, V_minus, N=N, H=H, F=F, C=C, T_cal=T_cal, L=L)\n                                move_witnesses(V_plus, Z, N=N, H=H, F=F, C=C, T_cal=T_cal_prime, L=L)\n                                change_color(z_1, Z, W, N=N, H=H, F=F, C=C, L=L)\n                                W_plus = next((k for k in C if N[w, k] == 0 and k not in A_cal))\n                                change_color(w, W, W_plus, N=N, H=H, F=F, C=C, L=L)\n                                excluded_colors.update([k for k in C if k != W and k not in B_cal_prime])\n                                procedure_P(V_minus=W, V_plus=W_plus, N=N, H=H, C=C, F=F, L=L, excluded_colors=excluded_colors)\n                                made_equitable = True\n                                break\n                    if made_equitable:\n                        break\n                else:\n                    assert False, 'Must find a w which is the solo neighbor of two vertices in B_cal_prime.'\n            if made_equitable:\n                break"
 },
 {
  "docstring": "Provides an equitable coloring for nodes of `G`.\n\nAttempts to color a graph using `num_colors` colors, where no neighbors of\na node can have same color as the node itself and the number of nodes with\neach color differ by at most 1. `num_colors` must be greater than the\nmaximum degree of `G`. The algorithm is described in [1]_ and has\ncomplexity O(num_colors * n**2).\n\nParameters\n----------\nG : networkX graph\n   The nodes of this graph will be colored.\n\nnum_colors : number of colors to use\n   This number must be at least one more than the maximum degree of nodes\n   in the graph.\n\nReturns\n-------\nA dictionary with keys representing nodes and values representing\ncorresponding coloring.\n\nExamples\n--------\n>>> G = nx.cycle_graph(4)\n>>> nx.coloring.equitable_color(G, num_colors=3)  # doctest: +SKIP\n{0: 2, 1: 1, 2: 2, 3: 0}\n\nRaises\n------\nNetworkXAlgorithmError\n    If `num_colors` is not at least the maximum degree of the graph `G`\n\n",
  "code": "@nx._dispatch\ndef equitable_color(G, num_colors):\n    nodes_to_int = {}\n    int_to_nodes = {}\n    for idx, node in enumerate(G.nodes):\n        nodes_to_int[node] = idx\n        int_to_nodes[idx] = node\n    G = nx.relabel_nodes(G, nodes_to_int, copy=True)\n    if len(G.nodes) > 0:\n        r_ = max((G.degree(node) for node in G.nodes))\n    else:\n        r_ = 0\n    if r_ >= num_colors:\n        raise nx.NetworkXAlgorithmError(f'Graph has maximum degree {r_}, needs {r_ + 1} (> {num_colors}) colors for guaranteed coloring.')\n    pad_graph(G, num_colors)\n    L_ = {node: [] for node in G.nodes}\n    F = {node: idx % num_colors for idx, node in enumerate(G.nodes)}\n    C = make_C_from_F(F)\n    N = make_N_from_L_C(L_, C)\n    H = make_H_from_C_N(C, N)\n    edges_seen = set()\n    for u in sorted(G.nodes):\n        for v in sorted(G.neighbors(u)):\n            if (v, u) in edges_seen:\n                continue\n            edges_seen.add((u, v))\n            L_[u].append(v)\n            L_[v].append(u)\n            N[u, F[v]] += 1\n            N[v, F[u]] += 1\n            if F[u] != F[v]:\n                if N[u, F[v]] == 1:\n                    H[F[u], F[v]] -= 1\n                if N[v, F[u]] == 1:\n                    H[F[v], F[u]] -= 1\n        if N[u, F[u]] != 0:\n            Y = next((k for k in C if N[u, k] == 0))\n            X = F[u]\n            change_color(u, X, Y, N=N, H=H, F=F, C=C, L=L_)\n            procedure_P(V_minus=X, V_plus=Y, N=N, H=H, F=F, C=C, L=L_)\n    return {int_to_nodes[x]: F[x] for x in int_to_nodes}"
 },
 {
  "docstring": "Returns a list of the nodes of ``G`` in decreasing order by\ndegree.\n\n``G`` is a NetworkX graph. ``colors`` is ignored.",
  "code": "@nx._dispatch\ndef strategy_largest_first(G, colors):\n    return sorted(G, key=G.degree, reverse=True)"
 },
 {
  "docstring": "Returns a random permutation of the nodes of ``G`` as a list.\n\n``G`` is a NetworkX graph. ``colors`` is ignored.\n\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.",
  "code": "@py_random_state(2)\n@nx._dispatch\ndef strategy_random_sequential(G, colors, seed=None):\n    nodes = list(G)\n    seed.shuffle(nodes)\n    return nodes"
 },
 {
  "docstring": "Returns a deque of the nodes of ``G``, \"smallest\" last.\n\nSpecifically, the degrees of each node are tracked in a bucket queue.\nFrom this, the node of minimum degree is repeatedly popped from the\ngraph, updating its neighbors' degrees.\n\n``G`` is a NetworkX graph. ``colors`` is ignored.\n\nThis implementation of the strategy runs in $O(n + m)$ time\n(ignoring polylogarithmic factors), where $n$ is the number of nodes\nand $m$ is the number of edges.\n\nThis strategy is related to :func:`strategy_independent_set`: if we\ninterpret each node removed as an independent set of size one, then\nthis strategy chooses an independent set of size one instead of a\nmaximal independent set.",
  "code": "@nx._dispatch\ndef strategy_smallest_last(G, colors):\n    H = G.copy()\n    result = deque()\n    degrees = defaultdict(set)\n    lbound = float('inf')\n    for node, d in H.degree():\n        degrees[d].add(node)\n        lbound = min(lbound, d)\n\n    def find_min_degree():\n        return next((d for d in itertools.count(lbound) if d in degrees))\n    for _ in G:\n        min_degree = find_min_degree()\n        u = degrees[min_degree].pop()\n        if not degrees[min_degree]:\n            del degrees[min_degree]\n        result.appendleft(u)\n        for v in H[u]:\n            degree = H.degree(v)\n            degrees[degree].remove(v)\n            if not degrees[degree]:\n                del degrees[degree]\n            degrees[degree - 1].add(v)\n        H.remove_node(u)\n        lbound = min_degree - 1\n    return result"
 },
 {
  "docstring": "Returns a maximal independent set of nodes in ``G`` by repeatedly\nchoosing an independent node of minimum degree (with respect to the\nsubgraph of unchosen nodes).",
  "code": "def _maximal_independent_set(G):\n    result = set()\n    remaining = set(G)\n    while remaining:\n        G = G.subgraph(remaining)\n        v = min(remaining, key=G.degree)\n        result.add(v)\n        remaining -= set(G[v]) | {v}\n    return result"
 },
 {
  "docstring": "Uses a greedy independent set removal strategy to determine the\ncolors.\n\nThis function updates ``colors`` **in-place** and return ``None``,\nunlike the other strategy functions in this module.\n\nThis algorithm repeatedly finds and removes a maximal independent\nset, assigning each node in the set an unused color.\n\n``G`` is a NetworkX graph.\n\nThis strategy is related to :func:`strategy_smallest_last`: in that\nstrategy, an independent set of size one is chosen at each step\ninstead of a maximal independent set.",
  "code": "@nx._dispatch\ndef strategy_independent_set(G, colors):\n    remaining_nodes = set(G)\n    while len(remaining_nodes) > 0:\n        nodes = _maximal_independent_set(G.subgraph(remaining_nodes))\n        remaining_nodes -= nodes\n        yield from nodes"
 },
 {
  "docstring": "Returns an iterable over nodes in ``G`` in the order given by a\nbreadth-first traversal.\n\nThe generated sequence has the property that for each node except\nthe first, at least one neighbor appeared earlier in the sequence.\n\n``G`` is a NetworkX graph. ``colors`` is ignored.",
  "code": "@nx._dispatch\ndef strategy_connected_sequential_bfs(G, colors):\n    return strategy_connected_sequential(G, colors, 'bfs')"
 },
 {
  "docstring": "Returns an iterable over nodes in ``G`` in the order given by a\ndepth-first traversal.\n\nThe generated sequence has the property that for each node except\nthe first, at least one neighbor appeared earlier in the sequence.\n\n``G`` is a NetworkX graph. ``colors`` is ignored.",
  "code": "@nx._dispatch\ndef strategy_connected_sequential_dfs(G, colors):\n    return strategy_connected_sequential(G, colors, 'dfs')"
 },
 {
  "docstring": "Returns an iterable over nodes in ``G`` in the order given by a\nbreadth-first or depth-first traversal.\n\n``traversal`` must be one of the strings ``'dfs'`` or ``'bfs'``,\nrepresenting depth-first traversal or breadth-first traversal,\nrespectively.\n\nThe generated sequence has the property that for each node except\nthe first, at least one neighbor appeared earlier in the sequence.\n\n``G`` is a NetworkX graph. ``colors`` is ignored.",
  "code": "@nx._dispatch\ndef strategy_connected_sequential(G, colors, traversal='bfs'):\n    if traversal == 'bfs':\n        traverse = nx.bfs_edges\n    elif traversal == 'dfs':\n        traverse = nx.dfs_edges\n    else:\n        raise nx.NetworkXError(\"Please specify one of the strings 'bfs' or 'dfs' for connected sequential ordering\")\n    for component in nx.connected_components(G):\n        source = arbitrary_element(component)\n        yield source\n        for _, end in traverse(G.subgraph(component), source):\n            yield end"
 },
 {
  "docstring": "Iterates over all the nodes of ``G`` in \"saturation order\" (also\nknown as \"DSATUR\").\n\n``G`` is a NetworkX graph. ``colors`` is a dictionary mapping nodes of\n``G`` to colors, for those nodes that have already been colored.",
  "code": "@nx._dispatch\ndef strategy_saturation_largest_first(G, colors):\n    distinct_colors = {v: set() for v in G}\n    for node, color in colors.items():\n        for neighbor in G[node]:\n            distinct_colors[neighbor].add(color)\n    if len(colors) >= 2:\n        for node, color in colors.items():\n            if color in distinct_colors[node]:\n                raise nx.NetworkXError('Neighboring nodes must have different colors')\n    if not colors:\n        node = max(G, key=G.degree)\n        yield node\n        for v in G[node]:\n            distinct_colors[v].add(0)\n    while len(G) != len(colors):\n        for node, color in colors.items():\n            for neighbor in G[node]:\n                distinct_colors[neighbor].add(color)\n        saturation = {v: len(c) for v, c in distinct_colors.items() if v not in colors}\n        node = max(saturation, key=lambda v: (saturation[v], G.degree(v)))\n        yield node"
 },
 {
  "docstring": "Color a graph using various strategies of greedy graph coloring.\n\nAttempts to color a graph using as few colors as possible, where no\nneighbours of a node can have same color as the node itself. The\ngiven strategy determines the order in which nodes are colored.\n\nThe strategies are described in [1]_, and smallest-last is based on\n[2]_.\n\nParameters\n----------\nG : NetworkX graph\n\nstrategy : string or function(G, colors)\n   A function (or a string representing a function) that provides\n   the coloring strategy, by returning nodes in the ordering they\n   should be colored. ``G`` is the graph, and ``colors`` is a\n   dictionary of the currently assigned colors, keyed by nodes. The\n   function must return an iterable over all the nodes in ``G``.\n\n   If the strategy function is an iterator generator (that is, a\n   function with ``yield`` statements), keep in mind that the\n   ``colors`` dictionary will be updated after each ``yield``, since\n   this function chooses colors greedily.\n\n   If ``strategy`` is a string, it must be one of the following,\n   each of which represents one of the built-in strategy functions.\n\n   * ``'largest_first'``\n   * ``'random_sequential'``\n   * ``'smallest_last'``\n   * ``'independent_set'``\n   * ``'connected_sequential_bfs'``\n   * ``'connected_sequential_dfs'``\n   * ``'connected_sequential'`` (alias for the previous strategy)\n   * ``'saturation_largest_first'``\n   * ``'DSATUR'`` (alias for the previous strategy)\n\ninterchange: bool\n   Will use the color interchange algorithm described by [3]_ if set\n   to ``True``.\n\n   Note that ``saturation_largest_first`` and ``independent_set``\n   do not work with interchange. Furthermore, if you use\n   interchange with your own strategy function, you cannot rely\n   on the values in the ``colors`` argument.\n\nReturns\n-------\nA dictionary with keys representing nodes and values representing\ncorresponding coloring.\n\nExamples\n--------\n>>> G = nx.cycle_graph(4)\n>>> d = nx.coloring.greedy_color(G, strategy=\"largest_first\")\n>>> d in [{0: 0, 1: 1, 2: 0, 3: 1}, {0: 1, 1: 0, 2: 1, 3: 0}]\nTrue\n\nRaises\n------\nNetworkXPointlessConcept\n    If ``strategy`` is ``saturation_largest_first`` or\n    ``independent_set`` and ``interchange`` is ``True``.\n\n",
  "code": "@nx._dispatch\ndef greedy_color(G, strategy='largest_first', interchange=False):\n    if len(G) == 0:\n        return {}\n    strategy = STRATEGIES.get(strategy, strategy)\n    if not callable(strategy):\n        raise nx.NetworkXError(f'strategy must be callable or a valid string. {strategy} not valid.')\n    if interchange:\n        if strategy is strategy_independent_set:\n            msg = 'interchange cannot be used with independent_set'\n            raise nx.NetworkXPointlessConcept(msg)\n        if strategy is strategy_saturation_largest_first:\n            msg = 'interchange cannot be used with saturation_largest_first'\n            raise nx.NetworkXPointlessConcept(msg)\n    colors = {}\n    nodes = strategy(G, colors)\n    if interchange:\n        return _greedy_coloring_with_interchange(G, nodes)\n    for u in nodes:\n        neighbour_colors = {colors[v] for v in G[u] if v in colors}\n        for color in itertools.count():\n            if color not in neighbour_colors:\n                break\n        colors[u] = color\n    return colors"
 },
 {
  "docstring": "Return a coloring for `original_graph` using interchange approach\n\nThis procedure is an adaption of the algorithm described by [1]_,\nand is an implementation of coloring with interchange. Please be\nadvised, that the datastructures used are rather complex because\nthey are optimized to minimize the time spent identifying\nsubcomponents of the graph, which are possible candidates for color\ninterchange.\n\nParameters\n----------\nG : NetworkX graph\n    The graph to be colored\n\nnodes : list\n    nodes ordered using the strategy of choice\n\nReturns\n-------\ndict :\n    A dictionary keyed by node to a color value\n\n",
  "code": "def _greedy_coloring_with_interchange(G, nodes):\n    n = len(G)\n    graph = {node: _Node(node, n) for node in G}\n    for node1, node2 in G.edges():\n        adj_entry1 = _AdjEntry(node2)\n        adj_entry2 = _AdjEntry(node1)\n        adj_entry1.mate = adj_entry2\n        adj_entry2.mate = adj_entry1\n        node1_head = graph[node1].adj_list\n        adj_entry1.next = node1_head\n        graph[node1].adj_list = adj_entry1\n        node2_head = graph[node2].adj_list\n        adj_entry2.next = node2_head\n        graph[node2].adj_list = adj_entry2\n    k = 0\n    for node in nodes:\n        neighbors = graph[node].iter_neighbors()\n        col_used = {graph[adj_node.node_id].color for adj_node in neighbors}\n        col_used.discard(-1)\n        k1 = next(itertools.dropwhile(lambda x: x in col_used, itertools.count()))\n        if k1 > k:\n            connected = True\n            visited = set()\n            col1 = -1\n            col2 = -1\n            while connected and col1 < k:\n                col1 += 1\n                neighbor_cols = graph[node].iter_neighbors_color(col1)\n                col1_adj = list(neighbor_cols)\n                col2 = col1\n                while connected and col2 < k:\n                    col2 += 1\n                    visited = set(col1_adj)\n                    frontier = list(col1_adj)\n                    i = 0\n                    while i < len(frontier):\n                        search_node = frontier[i]\n                        i += 1\n                        col_opp = col2 if graph[search_node].color == col1 else col1\n                        neighbor_cols = graph[search_node].iter_neighbors_color(col_opp)\n                        for neighbor in neighbor_cols:\n                            if neighbor not in visited:\n                                visited.add(neighbor)\n                                frontier.append(neighbor)\n                    connected = len(visited.intersection(graph[node].iter_neighbors_color(col2))) > 0\n            if not connected:\n                for search_node in visited:\n                    graph[search_node].color = col2 if graph[search_node].color == col1 else col1\n                    col2_adj = graph[search_node].adj_color[col2]\n                    graph[search_node].adj_color[col2] = graph[search_node].adj_color[col1]\n                    graph[search_node].adj_color[col1] = col2_adj\n                for search_node in visited:\n                    col = graph[search_node].color\n                    col_opp = col1 if col == col2 else col2\n                    for adj_node in graph[search_node].iter_neighbors():\n                        if graph[adj_node.node_id].color != col_opp:\n                            adj_mate = adj_node.mate\n                            graph[adj_node.node_id].clear_color(adj_mate, col_opp)\n                            graph[adj_node.node_id].assign_color(adj_mate, col)\n                k1 = col1\n        graph[node].color = k1\n        k = max(k1, k)\n        for adj_node in graph[node].iter_neighbors():\n            adj_mate = adj_node.mate\n            graph[adj_node.node_id].assign_color(adj_mate, k1)\n    return {node.node_id: node.color for node in graph.values()}"
 },
 {
  "docstring": "Get the maximum degree of any node in G.",
  "code": "def max_degree(G):\n    return max((G.degree(node) for node in G.nodes)) if len(G.nodes) > 0 else 0"
 },
 {
  "docstring": "Returns {N, L, H, C} from the given graph.",
  "code": "def make_params_from_graph(G, F):\n    num_nodes = len(G)\n    L = {u: [] for u in range(num_nodes)}\n    for u, v in G.edges:\n        L[u].append(v)\n        L[v].append(u)\n    C = nx.algorithms.coloring.equitable_coloring.make_C_from_F(F)\n    N = nx.algorithms.coloring.equitable_coloring.make_N_from_L_C(L, C)\n    H = nx.algorithms.coloring.equitable_coloring.make_H_from_C_N(C, N)\n    return {'N': N, 'F': F, 'C': C, 'H': H, 'L': L}"
 },
 {
  "docstring": "Returns communities in `G` as detected by Fluid Communities algorithm.\n\nThe asynchronous fluid communities algorithm is described in\n[1]_. The algorithm is based on the simple idea of fluids interacting\nin an environment, expanding and pushing each other. Its initialization is\nrandom, so found communities may vary on different executions.\n\nThe algorithm proceeds as follows. First each of the initial k communities\nis initialized in a random vertex in the graph. Then the algorithm iterates\nover all vertices in a random order, updating the community of each vertex\nbased on its own community and the communities of its neighbours. This\nprocess is performed several times until convergence.\nAt all times, each community has a total density of 1, which is equally\ndistributed among the vertices it contains. If a vertex changes of\ncommunity, vertex densities of affected communities are adjusted\nimmediately. When a complete iteration over all vertices is done, such that\nno vertex changes the community it belongs to, the algorithm has converged\nand returns.\n\nThis is the original version of the algorithm described in [1]_.\nUnfortunately, it does not support weighted graphs yet.\n\nParameters\n----------\nG : NetworkX graph\n    Graph must be simple and undirected.\n\nk : integer\n    The number of communities to be found.\n\nmax_iter : integer\n    The number of maximum iterations allowed. By default 100.\n\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\nReturns\n-------\ncommunities : iterable\n    Iterable of communities given as sets of nodes.\n\n",
  "code": "@not_implemented_for('directed', 'multigraph')\n@py_random_state(3)\n@nx._dispatch\ndef asyn_fluidc(G, k, max_iter=100, seed=None):\n    if not isinstance(k, int):\n        raise NetworkXError('k must be an integer.')\n    if not k > 0:\n        raise NetworkXError('k must be greater than 0.')\n    if not is_connected(G):\n        raise NetworkXError('Fluid Communities require connected Graphs.')\n    if len(G) < k:\n        raise NetworkXError('k cannot be bigger than the number of nodes.')\n    max_density = 1.0\n    vertices = list(G)\n    seed.shuffle(vertices)\n    communities = {n: i for i, n in enumerate(vertices[:k])}\n    density = {}\n    com_to_numvertices = {}\n    for vertex in communities:\n        com_to_numvertices[communities[vertex]] = 1\n        density[communities[vertex]] = max_density\n    iter_count = 0\n    cont = True\n    while cont:\n        cont = False\n        iter_count += 1\n        vertices = list(G)\n        seed.shuffle(vertices)\n        for vertex in vertices:\n            com_counter = Counter()\n            try:\n                com_counter.update({communities[vertex]: density[communities[vertex]]})\n            except KeyError:\n                pass\n            for v in G[vertex]:\n                try:\n                    com_counter.update({communities[v]: density[communities[v]]})\n                except KeyError:\n                    continue\n            new_com = -1\n            if len(com_counter.keys()) > 0:\n                max_freq = max(com_counter.values())\n                best_communities = [com for com, freq in com_counter.items() if max_freq - freq < 0.0001]\n                try:\n                    if communities[vertex] in best_communities:\n                        new_com = communities[vertex]\n                except KeyError:\n                    pass\n                if new_com == -1:\n                    cont = True\n                    new_com = seed.choice(best_communities)\n                    try:\n                        com_to_numvertices[communities[vertex]] -= 1\n                        density[communities[vertex]] = max_density / com_to_numvertices[communities[vertex]]\n                    except KeyError:\n                        pass\n                    communities[vertex] = new_com\n                    com_to_numvertices[communities[vertex]] += 1\n                    density[communities[vertex]] = max_density / com_to_numvertices[communities[vertex]]\n        if iter_count > max_iter:\n            break\n    return iter(groups(communities).values())"
 },
 {
  "docstring": "Finds communities in a graph using the Girvan\u2013Newman method.\n\nParameters\n----------\nG : NetworkX graph\n\nmost_valuable_edge : function\n    Function that takes a graph as input and outputs an edge. The\n    edge returned by this function will be recomputed and removed at\n    each iteration of the algorithm.\n\n    If not specified, the edge with the highest\n    :func:`networkx.edge_betweenness_centrality` will be used.\n\nReturns\n-------\niterator\n    Iterator over tuples of sets of nodes in `G`. Each set of node\n    is a community, each tuple is a sequence of communities at a\n    particular level of the algorithm.\n\nExamples\n--------\nTo get the first pair of communities::\n\n    >>> G = nx.path_graph(10)\n    >>> comp = nx.community.girvan_newman(G)\n    >>> tuple(sorted(c) for c in next(comp))\n    ([0, 1, 2, 3, 4], [5, 6, 7, 8, 9])\n\nTo get only the first *k* tuples of communities, use\n:func:`itertools.islice`::\n\n    >>> import itertools\n    >>> G = nx.path_graph(8)\n    >>> k = 2\n    >>> comp = nx.community.girvan_newman(G)\n    >>> for communities in itertools.islice(comp, k):\n    ...     print(tuple(sorted(c) for c in communities))\n    ...\n    ([0, 1, 2, 3], [4, 5, 6, 7])\n    ([0, 1], [2, 3], [4, 5, 6, 7])\n\nTo stop getting tuples of communities once the number of communities\nis greater than *k*, use :func:`itertools.takewhile`::\n\n    >>> import itertools\n    >>> G = nx.path_graph(8)\n    >>> k = 4\n    >>> comp = nx.community.girvan_newman(G)\n    >>> limited = itertools.takewhile(lambda c: len(c) <= k, comp)\n    >>> for communities in limited:\n    ...     print(tuple(sorted(c) for c in communities))\n    ...\n    ([0, 1, 2, 3], [4, 5, 6, 7])\n    ([0, 1], [2, 3], [4, 5, 6, 7])\n    ([0, 1], [2, 3], [4, 5], [6, 7])\n\nTo just choose an edge to remove based on the weight::\n\n    >>> from operator import itemgetter\n    >>> G = nx.path_graph(10)\n    >>> edges = G.edges()\n    >>> nx.set_edge_attributes(G, {(u, v): v for u, v in edges}, \"weight\")\n    >>> def heaviest(G):\n    ...     u, v, w = max(G.edges(data=\"weight\"), key=itemgetter(2))\n    ...     return (u, v)\n    ...\n    >>> comp = nx.community.girvan_newman(G, most_valuable_edge=heaviest)\n    >>> tuple(sorted(c) for c in next(comp))\n    ([0, 1, 2, 3, 4, 5, 6, 7, 8], [9])\n\nTo utilize edge weights when choosing an edge with, for example, the\nhighest betweenness centrality::\n\n    >>> from networkx import edge_betweenness_centrality as betweenness\n    >>> def most_central_edge(G):\n    ...     centrality = betweenness(G, weight=\"weight\")\n    ...     return max(centrality, key=centrality.get)\n    ...\n    >>> G = nx.path_graph(10)\n    >>> comp = nx.community.girvan_newman(G, most_valuable_edge=most_central_edge)\n    >>> tuple(sorted(c) for c in next(comp))\n    ([0, 1, 2, 3, 4], [5, 6, 7, 8, 9])\n\nTo specify a different ranking algorithm for edges, use the\n`most_valuable_edge` keyword argument::\n\n    >>> from networkx import edge_betweenness_centrality\n    >>> from random import random\n    >>> def most_central_edge(G):\n    ...     centrality = edge_betweenness_centrality(G)\n    ...     max_cent = max(centrality.values())\n    ...     # Scale the centrality values so they are between 0 and 1,\n    ...     # and add some random noise.\n    ...     centrality = {e: c / max_cent for e, c in centrality.items()}\n    ...     # Add some random noise.\n    ...     centrality = {e: c + random() for e, c in centrality.items()}\n    ...     return max(centrality, key=centrality.get)\n    ...\n    >>> G = nx.path_graph(10)\n    >>> comp = nx.community.girvan_newman(G, most_valuable_edge=most_central_edge)\n\n",
  "code": "@nx._dispatch(preserve_edge_attrs='most_valuable_edge')\ndef girvan_newman(G, most_valuable_edge=None):\n    if G.number_of_edges() == 0:\n        yield tuple(nx.connected_components(G))\n        return\n    if most_valuable_edge is None:\n\n        def most_valuable_edge(G):\n            \"\"\"Returns the edge with the highest betweenness centrality\n            in the graph `G`.\n\n            \"\"\"\n            betweenness = nx.edge_betweenness_centrality(G)\n            return max(betweenness, key=betweenness.get)\n    g = G.copy().to_undirected()\n    g.remove_edges_from(nx.selfloop_edges(g))\n    while g.number_of_edges() > 0:\n        yield _without_most_central_edges(g, most_valuable_edge)"
 },
 {
  "docstring": "Returns the connected components of the graph that results from\nrepeatedly removing the most \"valuable\" edge in the graph.\n\n`G` must be a non-empty graph. This function modifies the graph `G`\nin-place; that is, it removes edges on the graph `G`.\n\n`most_valuable_edge` is a function that takes the graph `G` as input\n(or a subgraph with one or more edges of `G` removed) and returns an\nedge. That edge will be removed and this process will be repeated\nuntil the number of connected components in the graph increases.",
  "code": "def _without_most_central_edges(G, most_valuable_edge):\n    original_num_components = nx.number_connected_components(G)\n    num_new_components = original_num_components\n    while num_new_components <= original_num_components:\n        edge = most_valuable_edge(G)\n        G.remove_edge(*edge)\n        new_components = tuple(nx.connected_components(G))\n        num_new_components = len(new_components)\n    return new_components"
 },
 {
  "docstring": "Returns the edge with the highest betweenness centrality\nin the graph `G`.",
  "code": "def most_valuable_edge(G):\n    betweenness = nx.edge_betweenness_centrality(G)\n    return max(betweenness, key=betweenness.get)"
 },
 {
  "docstring": "Returns *True* if `communities` is a partition of the nodes of `G`.\n\nA partition of a universe set is a family of pairwise disjoint sets\nwhose union is the entire universe set.\n\nParameters\n----------\nG : NetworkX graph.\n\ncommunities : list or iterable of sets of nodes\n    If not a list, the iterable is converted internally to a list.\n    If it is an iterator it is exhausted.",
  "code": "@nx._dispatch\ndef is_partition(G, communities):\n    if not isinstance(communities, list):\n        communities = list(communities)\n    nodes = {n for c in communities for n in c if n in G}\n    return len(G) == len(nodes) == sum((len(c) for c in communities))"
 },
 {
  "docstring": "Find k-clique communities in graph using the percolation method.\n\nA k-clique community is the union of all cliques of size k that\ncan be reached through adjacent (sharing k-1 nodes) k-cliques.\n\nParameters\n----------\nG : NetworkX graph\n\nk : int\n   Size of smallest clique\n\ncliques: list or generator\n   Precomputed cliques (use networkx.find_cliques(G))\n\nReturns\n-------\nYields sets of nodes, one for each k-clique community.\n\nExamples\n--------\n>>> G = nx.complete_graph(5)\n>>> K5 = nx.convert_node_labels_to_integers(G, first_label=2)\n>>> G.add_edges_from(K5.edges())\n>>> c = list(nx.community.k_clique_communities(G, 4))\n>>> sorted(list(c[0]))\n[0, 1, 2, 3, 4, 5, 6]\n>>> list(nx.community.k_clique_communities(G, 6))\n[]\n\n",
  "code": "@nx._dispatch\ndef k_clique_communities(G, k, cliques=None):\n    if k < 2:\n        raise nx.NetworkXError(f'k={k}, k must be greater than 1.')\n    if cliques is None:\n        cliques = nx.find_cliques(G)\n    cliques = [frozenset(c) for c in cliques if len(c) >= k]\n    membership_dict = defaultdict(list)\n    for clique in cliques:\n        for node in clique:\n            membership_dict[node].append(clique)\n    perc_graph = nx.Graph()\n    perc_graph.add_nodes_from(cliques)\n    for clique in cliques:\n        for adj_clique in _get_adjacent_cliques(clique, membership_dict):\n            if len(clique.intersection(adj_clique)) >= k - 1:\n                perc_graph.add_edge(clique, adj_clique)\n    for component in nx.connected_components(perc_graph):\n        yield frozenset.union(*component)"
 },
 {
  "docstring": "This is a modified form of Kernighan-Lin, which moves single nodes at a\ntime, alternating between sides to keep the bisection balanced.  We keep\ntwo min-heaps of swap costs to make optimal-next-move selection fast.",
  "code": "def _kernighan_lin_sweep(edges, side):\n    costs0, costs1 = costs = (BinaryHeap(), BinaryHeap())\n    for u, side_u, edges_u in zip(count(), side, edges):\n        cost_u = sum((w if side[v] else -w for v, w in edges_u))\n        costs[side_u].insert(u, cost_u if side_u else -cost_u)\n\n    def _update_costs(costs_x, x):\n        for y, w in edges[x]:\n            costs_y = costs[side[y]]\n            cost_y = costs_y.get(y)\n            if cost_y is not None:\n                cost_y += 2 * (-w if costs_x is costs_y else w)\n                costs_y.insert(y, cost_y, True)\n    i = 0\n    totcost = 0\n    while costs0 and costs1:\n        u, cost_u = costs0.pop()\n        _update_costs(costs0, u)\n        v, cost_v = costs1.pop()\n        _update_costs(costs1, v)\n        totcost += cost_u + cost_v\n        i += 1\n        yield (totcost, i, (u, v))"
 },
 {
  "docstring": "Partition a graph into two blocks using the Kernighan\u2013Lin\nalgorithm.\n\nThis algorithm partitions a network into two sets by iteratively\nswapping pairs of nodes to reduce the edge cut between the two sets.  The\npairs are chosen according to a modified form of Kernighan-Lin [1]_, which\nmoves node individually, alternating between sides to keep the bisection\nbalanced.\n\nParameters\n----------\nG : NetworkX graph\n    Graph must be undirected.\n\npartition : tuple\n    Pair of iterables containing an initial partition. If not\n    specified, a random balanced partition is used.\n\nmax_iter : int\n    Maximum number of times to attempt swaps to find an\n    improvement before giving up.\n\nweight : key\n    Edge data key to use as weight. If None, the weights are all\n    set to one.\n\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n    Only used if partition is None\n\nReturns\n-------\npartition : tuple\n    A pair of sets of nodes representing the bipartition.\n\nRaises\n------\nNetworkXError\n    If partition is not a valid partition of the nodes of the graph.\n\n",
  "code": "@not_implemented_for('directed')\n@py_random_state(4)\n@nx._dispatch(edge_attrs='weight')\ndef kernighan_lin_bisection(G, partition=None, max_iter=10, weight='weight', seed=None):\n    n = len(G)\n    labels = list(G)\n    seed.shuffle(labels)\n    index = {v: i for i, v in enumerate(labels)}\n    if partition is None:\n        side = [0] * (n // 2) + [1] * ((n + 1) // 2)\n    else:\n        try:\n            A, B = partition\n        except (TypeError, ValueError) as err:\n            raise nx.NetworkXError('partition must be two sets') from err\n        if not is_partition(G, (A, B)):\n            raise nx.NetworkXError('partition invalid')\n        side = [0] * n\n        for a in A:\n            side[index[a]] = 1\n    if G.is_multigraph():\n        edges = [[(index[u], sum((e.get(weight, 1) for e in d.values()))) for u, d in G[v].items()] for v in labels]\n    else:\n        edges = [[(index[u], e.get(weight, 1)) for u, e in G[v].items()] for v in labels]\n    for i in range(max_iter):\n        costs = list(_kernighan_lin_sweep(edges, side))\n        min_cost, min_i, _ = min(costs)\n        if min_cost >= 0:\n            break\n        for _, _, (u, v) in costs[:min_i]:\n            side[u] = 1\n            side[v] = 0\n    A = {u for u, s in zip(labels, side) if s == 0}\n    B = {u for u, s in zip(labels, side) if s == 1}\n    return (A, B)"
 },
 {
  "docstring": "Returns communities in `G` as detected by fast label propagation.\n\nThe fast label propagation algorithm is described in [1]_. The algorithm is\nprobabilistic and the found communities may vary in different executions.\n\nThe algorithm operates as follows. First, the community label of each node is\nset to a unique label. The algorithm then repeatedly updates the labels of\nthe nodes to the most frequent label in their neighborhood. In case of ties,\na random label is chosen from the most frequent labels.\n\nThe algorithm maintains a queue of nodes that still need to be processed.\nInitially, all nodes are added to the queue in a random order. Then the nodes\nare removed from the queue one by one and processed. If a node updates its label,\nall its neighbors that have a different label are added to the queue (if not\nalready in the queue). The algorithm stops when the queue is empty.\n\nParameters\n----------\nG : Graph, DiGraph, MultiGraph, or MultiDiGraph\n  Any NetworkX graph.\n\nweight : string, or None (default)\n  The edge attribute representing a non-negative weight of an edge. If None,\n  each edge is assumed to have weight one. The weight of an edge is used in\n  determining the frequency with which a label appears among the neighbors of\n  a node (edge with weight `w` is equivalent to `w` unweighted edges).\n\nseed : integer, random_state, or None (default)\n  Indicator of random number generation state. See :ref:`Randomness<randomness>`.\n\nReturns\n-------\ncommunities : iterable\n  Iterable of communities given as sets of nodes.\n\n",
  "code": "@py_random_state('seed')\n@nx._dispatch(edge_attrs='weight')\ndef fast_label_propagation_communities(G, *, weight=None, seed=None):\n    nodes_queue = deque(G)\n    seed.shuffle(nodes_queue)\n    nodes_set = set(G)\n    comms = {node: i for i, node in enumerate(G)}\n    while nodes_queue:\n        node = nodes_queue.popleft()\n        nodes_set.remove(node)\n        if G.degree(node) > 0:\n            label_freqs = _fast_label_count(G, comms, node, weight)\n            max_freq = max(label_freqs.values())\n            comm = seed.choice([comm for comm in label_freqs if label_freqs[comm] == max_freq])\n            if comms[node] != comm:\n                comms[node] = comm\n                for nbr in nx.all_neighbors(G, node):\n                    if comms[nbr] != comm and nbr not in nodes_set:\n                        nodes_queue.append(nbr)\n                        nodes_set.add(nbr)\n    yield from groups(comms).values()"
 },
 {
  "docstring": "Computes the frequency of labels in the neighborhood of a node.\n\nReturns a dictionary keyed by label to the frequency of that label.",
  "code": "def _fast_label_count(G, comms, node, weight=None):\n    if weight is None:\n        if not G.is_multigraph():\n            label_freqs = Counter(map(comms.get, nx.all_neighbors(G, node)))\n        else:\n            label_freqs = defaultdict(int)\n            for nbr in G[node]:\n                label_freqs[comms[nbr]] += len(G[node][nbr])\n            if G.is_directed():\n                for nbr in G.pred[node]:\n                    label_freqs[comms[nbr]] += len(G.pred[node][nbr])\n    else:\n        label_freqs = defaultdict(float)\n        for _, nbr, w in G.edges(node, data=weight, default=1):\n            label_freqs[comms[nbr]] += w\n        if G.is_directed():\n            for nbr, _, w in G.in_edges(node, data=weight, default=1):\n                label_freqs[comms[nbr]] += w\n    return label_freqs"
 },
 {
  "docstring": "Returns communities in `G` as detected by asynchronous label\npropagation.\n\nThe asynchronous label propagation algorithm is described in\n[1]_. The algorithm is probabilistic and the found communities may\nvary on different executions.\n\nThe algorithm proceeds as follows. After initializing each node with\na unique label, the algorithm repeatedly sets the label of a node to\nbe the label that appears most frequently among that nodes\nneighbors. The algorithm halts when each node has the label that\nappears most frequently among its neighbors. The algorithm is\nasynchronous because each node is updated without waiting for\nupdates on the remaining nodes.\n\nThis generalized version of the algorithm in [1]_ accepts edge\nweights.\n\nParameters\n----------\nG : Graph\n\nweight : string\n    The edge attribute representing the weight of an edge.\n    If None, each edge is assumed to have weight one. In this\n    algorithm, the weight of an edge is used in determining the\n    frequency with which a label appears among the neighbors of a\n    node: a higher weight means the label appears more often.\n\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\nReturns\n-------\ncommunities : iterable\n    Iterable of communities given as sets of nodes.\n\n",
  "code": "@py_random_state(2)\n@nx._dispatch(edge_attrs='weight')\ndef asyn_lpa_communities(G, weight=None, seed=None):\n    labels = {n: i for i, n in enumerate(G)}\n    cont = True\n    while cont:\n        cont = False\n        nodes = list(G)\n        seed.shuffle(nodes)\n        for node in nodes:\n            if not G[node]:\n                continue\n            if weight is None:\n                label_freq = Counter(map(labels.get, G[node]))\n            else:\n                label_freq = defaultdict(float)\n                for _, v, wt in G.edges(node, data=weight, default=1):\n                    label_freq[labels[v]] += wt\n            max_freq = max(label_freq.values())\n            best_labels = [label for label, freq in label_freq.items() if freq == max_freq]\n            if labels[node] not in best_labels:\n                labels[node] = seed.choice(best_labels)\n                cont = True\n    yield from groups(labels).values()"
 },
 {
  "docstring": "Generates community sets determined by label propagation\n\nFinds communities in `G` using a semi-synchronous label propagation\nmethod [1]_. This method combines the advantages of both the synchronous\nand asynchronous models. Not implemented for directed graphs.\n\nParameters\n----------\nG : graph\n    An undirected NetworkX graph.\n\nReturns\n-------\ncommunities : iterable\n    A dict_values object that contains a set of nodes for each community.\n\nRaises\n------\nNetworkXNotImplemented\n   If the graph is directed\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch\ndef label_propagation_communities(G):\n    coloring = _color_network(G)\n    labeling = {v: k for k, v in enumerate(G)}\n    while not _labeling_complete(labeling, G):\n        for color, nodes in coloring.items():\n            for n in nodes:\n                _update_label(n, labeling, G)\n    clusters = defaultdict(set)\n    for node, label in labeling.items():\n        clusters[label].add(node)\n    return clusters.values()"
 },
 {
  "docstring": "Colors the network so that neighboring nodes all have distinct colors.\n\nReturns a dict keyed by color to a set of nodes with that color.",
  "code": "def _color_network(G):\n    coloring = {}\n    colors = nx.coloring.greedy_color(G)\n    for node, color in colors.items():\n        if color in coloring:\n            coloring[color].add(node)\n        else:\n            coloring[color] = {node}\n    return coloring"
 },
 {
  "docstring": "Determines whether or not LPA is done.\n\nLabel propagation is complete when all nodes have a label that is\nin the set of highest frequency labels amongst its neighbors.\n\nNodes with no neighbors are considered complete.",
  "code": "def _labeling_complete(labeling, G):\n    return all((labeling[v] in _most_frequent_labels(v, labeling, G) for v in G if len(G[v]) > 0))"
 },
 {
  "docstring": "Returns a set of all labels with maximum frequency in `labeling`.\n\nInput `labeling` should be a dict keyed by node to labels.",
  "code": "def _most_frequent_labels(node, labeling, G):\n    if not G[node]:\n        return {labeling[node]}\n    freqs = Counter((labeling[q] for q in G[node]))\n    max_freq = max(freqs.values())\n    return {label for label, freq in freqs.items() if freq == max_freq}"
 },
 {
  "docstring": "Updates the label of a node using the Prec-Max tie breaking algorithm\n\nThe algorithm is explained in: 'Community Detection via Semi-Synchronous\nLabel Propagation Algorithms' Cordasco and Gargano, 2011",
  "code": "def _update_label(node, labeling, G):\n    high_labels = _most_frequent_labels(node, labeling, G)\n    if len(high_labels) == 1:\n        labeling[node] = high_labels.pop()\n    elif len(high_labels) > 1:\n        if labeling[node] not in high_labels:\n            labeling[node] = max(high_labels)"
 },
 {
  "docstring": "Find the best partition of a graph using the Louvain Community Detection\nAlgorithm.\n\nLouvain Community Detection Algorithm is a simple method to extract the community\nstructure of a network. This is a heuristic method based on modularity optimization. [1]_\n\nThe algorithm works in 2 steps. On the first step it assigns every node to be\nin its own community and then for each node it tries to find the maximum positive\nmodularity gain by moving each node to all of its neighbor communities. If no positive\ngain is achieved the node remains in its original community.\n\nThe modularity gain obtained by moving an isolated node $i$ into a community $C$ can\neasily be calculated by the following formula (combining [1]_ [2]_ and some algebra):\n\n.. math::\n    \\Delta Q = \\frac{k_{i,in}}{2m} - \\gamma\\frac{ \\Sigma_{tot} \\cdot k_i}{2m^2}\n\nwhere $m$ is the size of the graph, $k_{i,in}$ is the sum of the weights of the links\nfrom $i$ to nodes in $C$, $k_i$ is the sum of the weights of the links incident to node $i$,\n$\\Sigma_{tot}$ is the sum of the weights of the links incident to nodes in $C$ and $\\gamma$\nis the resolution parameter.\n\nFor the directed case the modularity gain can be computed using this formula according to [3]_\n\n.. math::\n    \\Delta Q = \\frac{k_{i,in}}{m}\n    - \\gamma\\frac{k_i^{out} \\cdot\\Sigma_{tot}^{in} + k_i^{in} \\cdot \\Sigma_{tot}^{out}}{m^2}\n\nwhere $k_i^{out}$, $k_i^{in}$ are the outer and inner weighted degrees of node $i$ and\n$\\Sigma_{tot}^{in}$, $\\Sigma_{tot}^{out}$ are the sum of in-going and out-going links incident\nto nodes in $C$.\n\nThe first phase continues until no individual move can improve the modularity.\n\nThe second phase consists in building a new network whose nodes are now the communities\nfound in the first phase. To do so, the weights of the links between the new nodes are given by\nthe sum of the weight of the links between nodes in the corresponding two communities. Once this\nphase is complete it is possible to reapply the first phase creating bigger communities with\nincreased modularity.\n\nThe above two phases are executed until no modularity gain is achieved (or is less than\nthe `threshold`).\n\nBe careful with self-loops in the input graph. These are treated as\npreviously reduced communities -- as if the process had been started\nin the middle of the algorithm. Large self-loop edge weights thus\nrepresent strong communities and in practice may be hard to add\nother nodes to.  If your input graph edge weights for self-loops\ndo not represent already reduced communities you may want to remove\nthe self-loops before inputting that graph.\n\nParameters\n----------\nG : NetworkX graph\nweight : string or None, optional (default=\"weight\")\n    The name of an edge attribute that holds the numerical value\n    used as a weight. If None then each edge has weight 1.\nresolution : float, optional (default=1)\n    If resolution is less than 1, the algorithm favors larger communities.\n    Greater than 1 favors smaller communities\nthreshold : float, optional (default=0.0000001)\n    Modularity gain threshold for each level. If the gain of modularity\n    between 2 levels of the algorithm is less than the given threshold\n    then the algorithm stops and returns the resulting communities.\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\nReturns\n-------\nlist\n    A list of sets (partition of `G`). Each set represents one community and contains\n    all the nodes that constitute it.\n\nExamples\n--------\n>>> import networkx as nx\n>>> G = nx.petersen_graph()\n>>> nx.community.louvain_communities(G, seed=123)\n[{0, 4, 5, 7, 9}, {1, 2, 3, 6, 8}]\n\n",
  "code": "@py_random_state('seed')\n@nx._dispatch(edge_attrs='weight')\ndef louvain_communities(G, weight='weight', resolution=1, threshold=1e-07, seed=None):\n    d = louvain_partitions(G, weight, resolution, threshold, seed)\n    q = deque(d, maxlen=1)\n    return q.pop()"
 },
 {
  "docstring": "Yields partitions for each level of the Louvain Community Detection Algorithm\n\nLouvain Community Detection Algorithm is a simple method to extract the community\nstructure of a network. This is a heuristic method based on modularity optimization. [1]_\n\nThe partitions at each level (step of the algorithm) form a dendrogram of communities.\nA dendrogram is a diagram representing a tree and each level represents\na partition of the G graph. The top level contains the smallest communities\nand as you traverse to the bottom of the tree the communities get bigger\nand the overall modularity increases making the partition better.\n\nEach level is generated by executing the two phases of the Louvain Community\nDetection Algorithm.\n\nBe careful with self-loops in the input graph. These are treated as\npreviously reduced communities -- as if the process had been started\nin the middle of the algorithm. Large self-loop edge weights thus\nrepresent strong communities and in practice may be hard to add\nother nodes to.  If your input graph edge weights for self-loops\ndo not represent already reduced communities you may want to remove\nthe self-loops before inputting that graph.\n\nParameters\n----------\nG : NetworkX graph\nweight : string or None, optional (default=\"weight\")\n The name of an edge attribute that holds the numerical value\n used as a weight. If None then each edge has weight 1.\nresolution : float, optional (default=1)\n    If resolution is less than 1, the algorithm favors larger communities.\n    Greater than 1 favors smaller communities\nthreshold : float, optional (default=0.0000001)\n Modularity gain threshold for each level. If the gain of modularity\n between 2 levels of the algorithm is less than the given threshold\n then the algorithm stops and returns the resulting communities.\nseed : integer, random_state, or None (default)\n Indicator of random number generation state.\n See :ref:`Randomness<randomness>`.\n\nYields\n------\nlist\n    A list of sets (partition of `G`). Each set represents one community and contains\n    all the nodes that constitute it.\n\n",
  "code": "@py_random_state('seed')\n@nx._dispatch(edge_attrs='weight')\ndef louvain_partitions(G, weight='weight', resolution=1, threshold=1e-07, seed=None):\n    partition = [{u} for u in G.nodes()]\n    if nx.is_empty(G):\n        yield partition\n        return\n    mod = modularity(G, partition, resolution=resolution, weight=weight)\n    is_directed = G.is_directed()\n    if G.is_multigraph():\n        graph = _convert_multigraph(G, weight, is_directed)\n    else:\n        graph = G.__class__()\n        graph.add_nodes_from(G)\n        graph.add_weighted_edges_from(G.edges(data=weight, default=1))\n    m = graph.size(weight='weight')\n    partition, inner_partition, improvement = _one_level(graph, m, partition, resolution, is_directed, seed)\n    improvement = True\n    while improvement:\n        yield [s.copy() for s in partition]\n        new_mod = modularity(graph, inner_partition, resolution=resolution, weight='weight')\n        if new_mod - mod <= threshold:\n            return\n        mod = new_mod\n        graph = _gen_graph(graph, inner_partition)\n        partition, inner_partition, improvement = _one_level(graph, m, partition, resolution, is_directed, seed)"
 },
 {
  "docstring": "Calculate one level of the Louvain partitions tree\n\nParameters\n----------\nG : NetworkX Graph/DiGraph\n    The graph from which to detect communities\nm : number\n    The size of the graph `G`.\npartition : list of sets of nodes\n    A valid partition of the graph `G`\nresolution : positive number\n    The resolution parameter for computing the modularity of a partition\nis_directed : bool\n    True if `G` is a directed graph.\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.",
  "code": "def _one_level(G, m, partition, resolution=1, is_directed=False, seed=None):\n    node2com = {u: i for i, u in enumerate(G.nodes())}\n    inner_partition = [{u} for u in G.nodes()]\n    if is_directed:\n        in_degrees = dict(G.in_degree(weight='weight'))\n        out_degrees = dict(G.out_degree(weight='weight'))\n        Stot_in = list(in_degrees.values())\n        Stot_out = list(out_degrees.values())\n        nbrs = {}\n        for u in G:\n            nbrs[u] = defaultdict(float)\n            for _, n, wt in G.out_edges(u, data='weight'):\n                if u != n:\n                    nbrs[u][n] += wt\n            for n, _, wt in G.in_edges(u, data='weight'):\n                if u != n:\n                    nbrs[u][n] += wt\n    else:\n        degrees = dict(G.degree(weight='weight'))\n        Stot = list(degrees.values())\n        nbrs = {u: {v: data['weight'] for v, data in G[u].items() if v != u} for u in G}\n    rand_nodes = list(G.nodes)\n    seed.shuffle(rand_nodes)\n    nb_moves = 1\n    improvement = False\n    while nb_moves > 0:\n        nb_moves = 0\n        for u in rand_nodes:\n            best_mod = 0\n            best_com = node2com[u]\n            weights2com = _neighbor_weights(nbrs[u], node2com)\n            if is_directed:\n                in_degree = in_degrees[u]\n                out_degree = out_degrees[u]\n                Stot_in[best_com] -= in_degree\n                Stot_out[best_com] -= out_degree\n                remove_cost = -weights2com[best_com] / m + resolution * (out_degree * Stot_in[best_com] + in_degree * Stot_out[best_com]) / m ** 2\n            else:\n                degree = degrees[u]\n                Stot[best_com] -= degree\n                remove_cost = -weights2com[best_com] / m + resolution * (Stot[best_com] * degree) / (2 * m ** 2)\n            for nbr_com, wt in weights2com.items():\n                if is_directed:\n                    gain = remove_cost + wt / m - resolution * (out_degree * Stot_in[nbr_com] + in_degree * Stot_out[nbr_com]) / m ** 2\n                else:\n                    gain = remove_cost + wt / m - resolution * (Stot[nbr_com] * degree) / (2 * m ** 2)\n                if gain > best_mod:\n                    best_mod = gain\n                    best_com = nbr_com\n            if is_directed:\n                Stot_in[best_com] += in_degree\n                Stot_out[best_com] += out_degree\n            else:\n                Stot[best_com] += degree\n            if best_com != node2com[u]:\n                com = G.nodes[u].get('nodes', {u})\n                partition[node2com[u]].difference_update(com)\n                inner_partition[node2com[u]].remove(u)\n                partition[best_com].update(com)\n                inner_partition[best_com].add(u)\n                improvement = True\n                nb_moves += 1\n                node2com[u] = best_com\n    partition = list(filter(len, partition))\n    inner_partition = list(filter(len, inner_partition))\n    return (partition, inner_partition, improvement)"
 },
 {
  "docstring": "Calculate weights between node and its neighbor communities.\n\nParameters\n----------\nnbrs : dictionary\n       Dictionary with nodes' neighbours as keys and their edge weight as value.\nnode2com : dictionary\n       Dictionary with all graph's nodes as keys and their community index as value.",
  "code": "def _neighbor_weights(nbrs, node2com):\n    weights = defaultdict(float)\n    for nbr, wt in nbrs.items():\n        weights[node2com[nbr]] += wt\n    return weights"
 },
 {
  "docstring": "Generate a new graph based on the partitions of a given graph",
  "code": "def _gen_graph(G, partition):\n    H = G.__class__()\n    node2com = {}\n    for i, part in enumerate(partition):\n        nodes = set()\n        for node in part:\n            node2com[node] = i\n            nodes.update(G.nodes[node].get('nodes', {node}))\n        H.add_node(i, nodes=nodes)\n    for node1, node2, wt in G.edges(data=True):\n        wt = wt['weight']\n        com1 = node2com[node1]\n        com2 = node2com[node2]\n        temp = H.get_edge_data(com1, com2, {'weight': 0})['weight']\n        H.add_edge(com1, com2, weight=wt + temp)\n    return H"
 },
 {
  "docstring": "Convert a Multigraph to normal Graph",
  "code": "def _convert_multigraph(G, weight, is_directed):\n    if is_directed:\n        H = nx.DiGraph()\n    else:\n        H = nx.Graph()\n    H.add_nodes_from(G)\n    for u, v, wt in G.edges(data=weight, default=1):\n        if H.has_edge(u, v):\n            H[u][v]['weight'] += wt\n        else:\n            H.add_edge(u, v, weight=wt)\n    return H"
 },
 {
  "docstring": "Optimal partitioning of a weighted tree using the Lukes algorithm.\n\nThis algorithm partitions a connected, acyclic graph featuring integer\nnode weights and float edge weights. The resulting clusters are such\nthat the total weight of the nodes in each cluster does not exceed\nmax_size and that the weight of the edges that are cut by the partition\nis minimum. The algorithm is based on [1]_.\n\nParameters\n----------\nG : NetworkX graph\n\nmax_size : int\n    Maximum weight a partition can have in terms of sum of\n    node_weight for all nodes in the partition\n\nedge_weight : key\n    Edge data key to use as weight. If None, the weights are all\n    set to one.\n\nnode_weight : key\n    Node data key to use as weight. If None, the weights are all\n    set to one. The data must be int.\n\nReturns\n-------\npartition : list\n    A list of sets of nodes representing the clusters of the\n    partition.\n\nRaises\n------\nNotATree\n    If G is not a tree.\nTypeError\n    If any of the values of node_weight is not int.\n\n",
  "code": "@nx._dispatch(node_attrs='node_weight', edge_attrs='edge_weight')\ndef lukes_partitioning(G, max_size, node_weight=None, edge_weight=None):\n    if not nx.is_tree(G):\n        raise nx.NotATree('lukes_partitioning works only on trees')\n    elif nx.is_directed(G):\n        root = [n for n, d in G.in_degree() if d == 0]\n        assert len(root) == 1\n        root = root[0]\n        t_G = deepcopy(G)\n    else:\n        root = choice(list(G.nodes))\n        t_G = nx.dfs_tree(G, root)\n    if edge_weight is None or node_weight is None:\n        safe_G = deepcopy(G)\n        if edge_weight is None:\n            nx.set_edge_attributes(safe_G, D_EDGE_VALUE, D_EDGE_W)\n            edge_weight = D_EDGE_W\n        if node_weight is None:\n            nx.set_node_attributes(safe_G, D_NODE_VALUE, D_NODE_W)\n            node_weight = D_NODE_W\n    else:\n        safe_G = G\n    all_n_attr = nx.get_node_attributes(safe_G, node_weight).values()\n    for x in all_n_attr:\n        if not isinstance(x, int):\n            raise TypeError(f'lukes_partitioning needs integer values for node_weight ({node_weight})')\n\n    @not_implemented_for('undirected')\n    def _leaves(gr):\n        for x in gr.nodes:\n            if not nx.descendants(gr, x):\n                yield x\n\n    @not_implemented_for('undirected')\n    def _a_parent_of_leaves_only(gr):\n        tleaves = set(_leaves(gr))\n        for n in set(gr.nodes) - tleaves:\n            if all((x in tleaves for x in nx.descendants(gr, n))):\n                return n\n\n    @lru_cache(CLUSTER_EVAL_CACHE_SIZE)\n    def _value_of_cluster(cluster):\n        valid_edges = [e for e in safe_G.edges if e[0] in cluster and e[1] in cluster]\n        return sum((safe_G.edges[e][edge_weight] for e in valid_edges))\n\n    def _value_of_partition(partition):\n        return sum((_value_of_cluster(frozenset(c)) for c in partition))\n\n    @lru_cache(CLUSTER_EVAL_CACHE_SIZE)\n    def _weight_of_cluster(cluster):\n        return sum((safe_G.nodes[n][node_weight] for n in cluster))\n\n    def _pivot(partition, node):\n        ccx = [c for c in partition if node in c]\n        assert len(ccx) == 1\n        return ccx[0]\n\n    def _concatenate_or_merge(partition_1, partition_2, x, i, ref_weight):\n        ccx = _pivot(partition_1, x)\n        cci = _pivot(partition_2, i)\n        merged_xi = ccx.union(cci)\n        if _weight_of_cluster(frozenset(merged_xi)) <= ref_weight:\n            cp1 = list(filter(lambda x: x != ccx, partition_1))\n            cp2 = list(filter(lambda x: x != cci, partition_2))\n            option_2 = [merged_xi] + cp1 + cp2\n            return (option_2, _value_of_partition(option_2))\n        else:\n            option_1 = partition_1 + partition_2\n            return (option_1, _value_of_partition(option_1))\n    leaves = set(_leaves(t_G))\n    for lv in leaves:\n        t_G.nodes[lv][PKEY] = {}\n        slot = safe_G.nodes[lv][node_weight]\n        t_G.nodes[lv][PKEY][slot] = [{lv}]\n        t_G.nodes[lv][PKEY][0] = [{lv}]\n    for inner in [x for x in t_G.nodes if x not in leaves]:\n        t_G.nodes[inner][PKEY] = {}\n        slot = safe_G.nodes[inner][node_weight]\n        t_G.nodes[inner][PKEY][slot] = [{inner}]\n    while True:\n        x_node = _a_parent_of_leaves_only(t_G)\n        weight_of_x = safe_G.nodes[x_node][node_weight]\n        best_value = 0\n        best_partition = None\n        bp_buffer = {}\n        x_descendants = nx.descendants(t_G, x_node)\n        for i_node in x_descendants:\n            for j in range(weight_of_x, max_size + 1):\n                for a, b in _split_n_from(j, weight_of_x):\n                    if a not in t_G.nodes[x_node][PKEY] or b not in t_G.nodes[i_node][PKEY]:\n                        continue\n                    part1 = t_G.nodes[x_node][PKEY][a]\n                    part2 = t_G.nodes[i_node][PKEY][b]\n                    part, value = _concatenate_or_merge(part1, part2, x_node, i_node, j)\n                    if j not in bp_buffer or bp_buffer[j][1] < value:\n                        bp_buffer[j] = (part, value)\n                    if best_value <= value:\n                        best_value = value\n                        best_partition = part\n            for w, (best_part_for_vl, vl) in bp_buffer.items():\n                t_G.nodes[x_node][PKEY][w] = best_part_for_vl\n            bp_buffer.clear()\n        t_G.nodes[x_node][PKEY][0] = best_partition\n        t_G.remove_nodes_from(x_descendants)\n        if x_node == root:\n            return t_G.nodes[root][PKEY][0]"
 },
 {
  "docstring": "Yield community partitions of G and the modularity change at each step.\n\nThis function performs Clauset-Newman-Moore greedy modularity maximization [2]_\nAt each step of the process it yields the change in modularity that will occur in\nthe next step followed by yielding the new community partition after that step.\n\nGreedy modularity maximization begins with each node in its own community\nand repeatedly joins the pair of communities that lead to the largest\nmodularity until one community contains all nodes (the partition has one set).\n\nThis function maximizes the generalized modularity, where `resolution`\nis the resolution parameter, often expressed as $\\gamma$.\nSee :func:`~networkx.algorithms.community.quality.modularity`.\n\nParameters\n----------\nG : NetworkX graph\n\nweight : string or None, optional (default=None)\n    The name of an edge attribute that holds the numerical value used\n    as a weight.  If None, then each edge has weight 1.\n    The degree is the sum of the edge weights adjacent to the node.\n\nresolution : float (default=1)\n    If resolution is less than 1, modularity favors larger communities.\n    Greater than 1 favors smaller communities.\n\nYields\n------\nAlternating yield statements produce the following two objects:\n\ncommunities: dict_values\n    A dict_values of frozensets of nodes, one for each community.\n    This represents a partition of the nodes of the graph into communities.\n    The first yield is the partition with each node in its own community.\n\ndq: float\n    The change in modularity when merging the next two communities\n    that leads to the largest modularity.\n\nSee Also\n--------\nmodularity\n\n",
  "code": "def _greedy_modularity_communities_generator(G, weight=None, resolution=1):\n    directed = G.is_directed()\n    N = G.number_of_nodes()\n    m = G.size(weight)\n    q0 = 1 / m\n    if directed:\n        a = {node: deg_out * q0 for node, deg_out in G.out_degree(weight=weight)}\n        b = {node: deg_in * q0 for node, deg_in in G.in_degree(weight=weight)}\n    else:\n        a = b = {node: deg * q0 * 0.5 for node, deg in G.degree(weight=weight)}\n    dq_dict = defaultdict(lambda: defaultdict(float))\n    for u, v, wt in G.edges(data=weight, default=1):\n        if u == v:\n            continue\n        dq_dict[u][v] += wt\n        dq_dict[v][u] += wt\n    for u, nbrdict in dq_dict.items():\n        for v, wt in nbrdict.items():\n            dq_dict[u][v] = q0 * wt - resolution * (a[u] * b[v] + b[u] * a[v])\n    dq_heap = {u: MappedQueue({(u, v): -dq for v, dq in dq_dict[u].items()}) for u in G}\n    H = MappedQueue([dq_heap[n].heap[0] for n in G if len(dq_heap[n]) > 0])\n    communities = {n: frozenset([n]) for n in G}\n    yield communities.values()\n    while len(H) > 1:\n        try:\n            negdq, u, v = H.pop()\n        except IndexError:\n            break\n        dq = -negdq\n        yield dq\n        dq_heap[u].pop()\n        if len(dq_heap[u]) > 0:\n            H.push(dq_heap[u].heap[0])\n        if dq_heap[v].heap[0] == (v, u):\n            H.remove((v, u))\n            dq_heap[v].remove((v, u))\n            if len(dq_heap[v]) > 0:\n                H.push(dq_heap[v].heap[0])\n        else:\n            dq_heap[v].remove((v, u))\n        communities[v] = frozenset(communities[u] | communities[v])\n        del communities[u]\n        u_nbrs = set(dq_dict[u])\n        v_nbrs = set(dq_dict[v])\n        all_nbrs = (u_nbrs | v_nbrs) - {u, v}\n        both_nbrs = u_nbrs & v_nbrs\n        for w in all_nbrs:\n            if w in both_nbrs:\n                dq_vw = dq_dict[v][w] + dq_dict[u][w]\n            elif w in v_nbrs:\n                dq_vw = dq_dict[v][w] - resolution * (a[u] * b[w] + a[w] * b[u])\n            else:\n                dq_vw = dq_dict[u][w] - resolution * (a[v] * b[w] + a[w] * b[v])\n            for row, col in [(v, w), (w, v)]:\n                dq_heap_row = dq_heap[row]\n                dq_dict[row][col] = dq_vw\n                if len(dq_heap_row) > 0:\n                    d_oldmax = dq_heap_row.heap[0]\n                else:\n                    d_oldmax = None\n                d = (row, col)\n                d_negdq = -dq_vw\n                if w in v_nbrs:\n                    dq_heap_row.update(d, d, priority=d_negdq)\n                else:\n                    dq_heap_row.push(d, priority=d_negdq)\n                if d_oldmax is None:\n                    H.push(d, priority=d_negdq)\n                else:\n                    row_max = dq_heap_row.heap[0]\n                    if d_oldmax != row_max or d_oldmax.priority != row_max.priority:\n                        H.update(d_oldmax, row_max)\n        for w in dq_dict[u]:\n            dq_old = dq_dict[w][u]\n            del dq_dict[w][u]\n            if w != v:\n                for row, col in [(w, u), (u, w)]:\n                    dq_heap_row = dq_heap[row]\n                    d_old = (row, col)\n                    if dq_heap_row.heap[0] == d_old:\n                        dq_heap_row.remove(d_old)\n                        H.remove(d_old)\n                        if len(dq_heap_row) > 0:\n                            H.push(dq_heap_row.heap[0])\n                    else:\n                        dq_heap_row.remove(d_old)\n        del dq_dict[u]\n        dq_heap[u] = MappedQueue()\n        a[v] += a[u]\n        a[u] = 0\n        if directed:\n            b[v] += b[u]\n            b[u] = 0\n        yield communities.values()"
 },
 {
  "docstring": "Find communities in G using greedy modularity maximization.\n\nThis function uses Clauset-Newman-Moore greedy modularity maximization [2]_\nto find the community partition with the largest modularity.\n\nGreedy modularity maximization begins with each node in its own community\nand repeatedly joins the pair of communities that lead to the largest\nmodularity until no further increase in modularity is possible (a maximum).\nTwo keyword arguments adjust the stopping condition. `cutoff` is a lower\nlimit on the number of communities so you can stop the process before\nreaching a maximum (used to save computation time). `best_n` is an upper\nlimit on the number of communities so you can make the process continue\nuntil at most n communities remain even if the maximum modularity occurs\nfor more. To obtain exactly n communities, set both `cutoff` and `best_n` to n.\n\nThis function maximizes the generalized modularity, where `resolution`\nis the resolution parameter, often expressed as $\\gamma$.\nSee :func:`~networkx.algorithms.community.quality.modularity`.\n\nParameters\n----------\nG : NetworkX graph\n\nweight : string or None, optional (default=None)\n    The name of an edge attribute that holds the numerical value used\n    as a weight.  If None, then each edge has weight 1.\n    The degree is the sum of the edge weights adjacent to the node.\n\nresolution : float, optional (default=1)\n    If resolution is less than 1, modularity favors larger communities.\n    Greater than 1 favors smaller communities.\n\ncutoff : int, optional (default=1)\n    A minimum number of communities below which the merging process stops.\n    The process stops at this number of communities even if modularity\n    is not maximized. The goal is to let the user stop the process early.\n    The process stops before the cutoff if it finds a maximum of modularity.\n\nbest_n : int or None, optional (default=None)\n    A maximum number of communities above which the merging process will\n    not stop. This forces community merging to continue after modularity\n    starts to decrease until `best_n` communities remain.\n    If ``None``, don't force it to continue beyond a maximum.\n\nRaises\n------\nValueError : If the `cutoff` or `best_n`  value is not in the range\n    ``[1, G.number_of_nodes()]``, or if `best_n` < `cutoff`.\n\nReturns\n-------\ncommunities: list\n    A list of frozensets of nodes, one for each community.\n    Sorted by length with largest communities first.\n\nExamples\n--------\n>>> G = nx.karate_club_graph()\n>>> c = nx.community.greedy_modularity_communities(G)\n>>> sorted(c[0])\n[8, 14, 15, 18, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]\n\nSee Also\n--------\nmodularity\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef greedy_modularity_communities(G, weight=None, resolution=1, cutoff=1, best_n=None):\n    if not G.size():\n        return [{n} for n in G]\n    if cutoff < 1 or cutoff > G.number_of_nodes():\n        raise ValueError(f'cutoff must be between 1 and {len(G)}. Got {cutoff}.')\n    if best_n is not None:\n        if best_n < 1 or best_n > G.number_of_nodes():\n            raise ValueError(f'best_n must be between 1 and {len(G)}. Got {best_n}.')\n        if best_n < cutoff:\n            raise ValueError(f'Must have best_n >= cutoff. Got {best_n} < {cutoff}')\n        if best_n == 1:\n            return [set(G)]\n    else:\n        best_n = G.number_of_nodes()\n    community_gen = _greedy_modularity_communities_generator(G, weight=weight, resolution=resolution)\n    communities = next(community_gen)\n    while len(communities) > cutoff:\n        try:\n            dq = next(community_gen)\n        except StopIteration:\n            communities = sorted(communities, key=len, reverse=True)\n            while len(communities) > best_n:\n                comm1, comm2, *rest = communities\n                communities = [comm1 ^ comm2]\n                communities.extend(rest)\n            return communities\n        if dq < 0 and len(communities) <= best_n:\n            break\n        communities = next(community_gen)\n    return sorted(communities, key=len, reverse=True)"
 },
 {
  "docstring": "Find communities in G using greedy modularity maximization.\n\nThis implementation is O(n^4), much slower than alternatives, but it is\nprovided as an easy-to-understand reference implementation.\n\nGreedy modularity maximization begins with each node in its own community\nand joins the pair of communities that most increases modularity until no\nsuch pair exists.\n\nThis function maximizes the generalized modularity, where `resolution`\nis the resolution parameter, often expressed as $\\gamma$.\nSee :func:`~networkx.algorithms.community.quality.modularity`.\n\nParameters\n----------\nG : NetworkX graph\n    Graph must be simple and undirected.\n\nresolution : float (default=1)\n    If resolution is less than 1, modularity favors larger communities.\n    Greater than 1 favors smaller communities.\n\nweight : string or None, optional (default=None)\n    The name of an edge attribute that holds the numerical value used\n    as a weight.  If None, then each edge has weight 1.\n    The degree is the sum of the edge weights adjacent to the node.\n\nReturns\n-------\nlist\n    A list of sets of nodes, one for each community.\n    Sorted by length with largest communities first.\n\nExamples\n--------\n>>> G = nx.karate_club_graph()\n>>> c = nx.community.naive_greedy_modularity_communities(G)\n>>> sorted(c[0])\n[8, 14, 15, 18, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]\n\nSee Also\n--------\ngreedy_modularity_communities\nmodularity",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch(edge_attrs='weight')\ndef naive_greedy_modularity_communities(G, resolution=1, weight=None):\n    communities = [frozenset([u]) for u in G.nodes()]\n    merges = []\n    old_modularity = None\n    new_modularity = modularity(G, communities, resolution=resolution, weight=weight)\n    while old_modularity is None or new_modularity > old_modularity:\n        old_modularity = new_modularity\n        trial_communities = list(communities)\n        to_merge = None\n        for i, u in enumerate(communities):\n            for j, v in enumerate(communities):\n                if j <= i or len(u) == 0 or len(v) == 0:\n                    continue\n                trial_communities[j] = u | v\n                trial_communities[i] = frozenset([])\n                trial_modularity = modularity(G, trial_communities, resolution=resolution, weight=weight)\n                if trial_modularity >= new_modularity:\n                    if trial_modularity > new_modularity:\n                        new_modularity = trial_modularity\n                        to_merge = (i, j, new_modularity - old_modularity)\n                    elif to_merge and min(i, j) < min(to_merge[0], to_merge[1]):\n                        new_modularity = trial_modularity\n                        to_merge = (i, j, new_modularity - old_modularity)\n                trial_communities[i] = u\n                trial_communities[j] = v\n        if to_merge is not None:\n            merges.append(to_merge)\n            i, j, dq = to_merge\n            u, v = (communities[i], communities[j])\n            communities[j] = u | v\n            communities[i] = frozenset([])\n    return sorted((c for c in communities if len(c) > 0), key=len, reverse=True)"
 },
 {
  "docstring": "Decorator to check that a valid partition is input to a function\n\nRaises :exc:`networkx.NetworkXError` if the partition is not valid.\n\nThis decorator should be used on functions whose first two arguments\nare a graph and a partition of the nodes of that graph (in that\norder)::\n\n    >>> @require_partition\n    ... def foo(G, partition):\n    ...     print(\"partition is valid!\")\n    ...\n    >>> G = nx.complete_graph(5)\n    >>> partition = [{0, 1}, {2, 3}, {4}]\n    >>> foo(G, partition)\n    partition is valid!\n    >>> partition = [{0}, {2, 3}, {4}]\n    >>> foo(G, partition)\n    Traceback (most recent call last):\n      ...\n    networkx.exception.NetworkXError: `partition` is not a valid partition of the nodes of G\n    >>> partition = [{0, 1}, {1, 2, 3}, {4}]\n    >>> foo(G, partition)\n    Traceback (most recent call last):\n      ...\n    networkx.exception.NetworkXError: `partition` is not a valid partition of the nodes of G",
  "code": "def _require_partition(G, partition):\n    if is_partition(G, partition):\n        return (G, partition)\n    raise nx.NetworkXError('`partition` is not a valid partition of the nodes of G')"
 },
 {
  "docstring": "Returns the number of intra-community edges for a partition of `G`.\n\nParameters\n----------\nG : NetworkX graph.\n\npartition : iterable of sets of nodes\n    This must be a partition of the nodes of `G`.\n\nThe \"intra-community edges\" are those edges joining a pair of nodes\nin the same block of the partition.",
  "code": "@nx._dispatch\ndef intra_community_edges(G, partition):\n    return sum((G.subgraph(block).size() for block in partition))"
 },
 {
  "docstring": "Returns the number of inter-community edges for a partition of `G`.\naccording to the given\npartition of the nodes of `G`.\n\nParameters\n----------\nG : NetworkX graph.\n\npartition : iterable of sets of nodes\n    This must be a partition of the nodes of `G`.\n\nThe *inter-community edges* are those edges joining a pair of nodes\nin different blocks of the partition.\n\nImplementation note: this function creates an intermediate graph\nthat may require the same amount of memory as that of `G`.",
  "code": "@nx._dispatch\ndef inter_community_edges(G, partition):\n    MG = nx.MultiDiGraph if G.is_directed() else nx.MultiGraph\n    return nx.quotient_graph(G, partition, create_using=MG).size()"
 },
 {
  "docstring": "Returns the number of inter-community non-edges according to the\ngiven partition of the nodes of `G`.\n\nParameters\n----------\nG : NetworkX graph.\n\npartition : iterable of sets of nodes\n    This must be a partition of the nodes of `G`.\n\nA *non-edge* is a pair of nodes (undirected if `G` is undirected)\nthat are not adjacent in `G`. The *inter-community non-edges* are\nthose non-edges on a pair of nodes in different blocks of the\npartition.\n\nImplementation note: this function creates two intermediate graphs,\nwhich may require up to twice the amount of memory as required to\nstore `G`.",
  "code": "@nx._dispatch\ndef inter_community_non_edges(G, partition):\n    return inter_community_edges(nx.complement(G), partition)"
 },
 {
  "docstring": "Returns the modularity of the given partition of the graph.\n\nModularity is defined in [1]_ as\n\n.. math::\n    Q = \\frac{1}{2m} \\sum_{ij} \\left( A_{ij} - \\gamma\\frac{k_ik_j}{2m}\\right)\n        \\delta(c_i,c_j)\n\nwhere $m$ is the number of edges (or sum of all edge weights as in [5]_),\n$A$ is the adjacency matrix of `G`, $k_i$ is the (weighted) degree of $i$,\n$\\gamma$ is the resolution parameter, and $\\delta(c_i, c_j)$ is 1 if $i$ and\n$j$ are in the same community else 0.\n\nAccording to [2]_ (and verified by some algebra) this can be reduced to\n\n.. math::\n   Q = \\sum_{c=1}^{n}\n   \\left[ \\frac{L_c}{m} - \\gamma\\left( \\frac{k_c}{2m} \\right) ^2 \\right]\n\nwhere the sum iterates over all communities $c$, $m$ is the number of edges,\n$L_c$ is the number of intra-community links for community $c$,\n$k_c$ is the sum of degrees of the nodes in community $c$,\nand $\\gamma$ is the resolution parameter.\n\nThe resolution parameter sets an arbitrary tradeoff between intra-group\nedges and inter-group edges. More complex grouping patterns can be\ndiscovered by analyzing the same network with multiple values of gamma\nand then combining the results [3]_. That said, it is very common to\nsimply use gamma=1. More on the choice of gamma is in [4]_.\n\nThe second formula is the one actually used in calculation of the modularity.\nFor directed graphs the second formula replaces $k_c$ with $k^{in}_c k^{out}_c$.\n\nParameters\n----------\nG : NetworkX Graph\n\ncommunities : list or iterable of set of nodes\n    These node sets must represent a partition of G's nodes.\n\nweight : string or None, optional (default=\"weight\")\n    The edge attribute that holds the numerical value used\n    as a weight. If None or an edge does not have that attribute,\n    then that edge has weight 1.\n\nresolution : float (default=1)\n    If resolution is less than 1, modularity favors larger communities.\n    Greater than 1 favors smaller communities.\n\nReturns\n-------\nQ : float\n    The modularity of the partition.\n\nRaises\n------\nNotAPartition\n    If `communities` is not a partition of the nodes of `G`.\n\nExamples\n--------\n>>> G = nx.barbell_graph(3, 0)\n>>> nx.community.modularity(G, [{0, 1, 2}, {3, 4, 5}])\n0.35714285714285715\n>>> nx.community.modularity(G, nx.community.label_propagation_communities(G))\n0.35714285714285715\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef modularity(G, communities, weight='weight', resolution=1):\n    if not isinstance(communities, list):\n        communities = list(communities)\n    if not is_partition(G, communities):\n        raise NotAPartition(G, communities)\n    directed = G.is_directed()\n    if directed:\n        out_degree = dict(G.out_degree(weight=weight))\n        in_degree = dict(G.in_degree(weight=weight))\n        m = sum(out_degree.values())\n        norm = 1 / m ** 2\n    else:\n        out_degree = in_degree = dict(G.degree(weight=weight))\n        deg_sum = sum(out_degree.values())\n        m = deg_sum / 2\n        norm = 1 / deg_sum ** 2\n\n    def community_contribution(community):\n        comm = set(community)\n        L_c = sum((wt for u, v, wt in G.edges(comm, data=weight, default=1) if v in comm))\n        out_degree_sum = sum((out_degree[u] for u in comm))\n        in_degree_sum = sum((in_degree[u] for u in comm)) if directed else out_degree_sum\n        return L_c / m - resolution * out_degree_sum * in_degree_sum * norm\n    return sum(map(community_contribution, communities))"
 },
 {
  "docstring": "Returns the coverage and performance of a partition of G.\n\nThe *coverage* of a partition is the ratio of the number of\nintra-community edges to the total number of edges in the graph.\n\nThe *performance* of a partition is the number of\nintra-community edges plus inter-community non-edges divided by the total\nnumber of potential edges.\n\nThis algorithm has complexity $O(C^2 + L)$ where C is the number of communities and L is the number of links.\n\nParameters\n----------\nG : NetworkX graph\n\npartition : sequence\n    Partition of the nodes of `G`, represented as a sequence of\n    sets of nodes (blocks). Each block of the partition represents a\n    community.\n\nReturns\n-------\n(float, float)\n    The (coverage, performance) tuple of the partition, as defined above.\n\nRaises\n------\nNetworkXError\n    If `partition` is not a valid partition of the nodes of `G`.\n\n",
  "code": "@require_partition\n@nx._dispatch\ndef partition_quality(G, partition):\n    node_community = {}\n    for i, community in enumerate(partition):\n        for node in community:\n            node_community[node] = i\n    if not G.is_multigraph():\n        possible_inter_community_edges = sum((len(p1) * len(p2) for p1, p2 in combinations(partition, 2)))\n        if G.is_directed():\n            possible_inter_community_edges *= 2\n    else:\n        possible_inter_community_edges = 0\n    n = len(G)\n    total_pairs = n * (n - 1)\n    if not G.is_directed():\n        total_pairs //= 2\n    intra_community_edges = 0\n    inter_community_non_edges = possible_inter_community_edges\n    for e in G.edges():\n        if node_community[e[0]] == node_community[e[1]]:\n            intra_community_edges += 1\n        else:\n            inter_community_non_edges -= 1\n    coverage = intra_community_edges / len(G.edges)\n    if G.is_multigraph():\n        performance = -1.0\n    else:\n        performance = (intra_community_edges + inter_community_non_edges) / total_pairs\n    return (coverage, performance)"
 },
 {
  "docstring": "Checks that the communities computed from the given graph ``G``\nusing the :func:`~networkx.asyn_lpa_communities` function match\nthe set of nodes given in ``expected``.\n\n``expected`` must be a :class:`set` of :class:`frozenset`\ninstances, each element of which is a node in the graph.",
  "code": "def _check_communities(self, G, expected):\n    communities = nx.community.asyn_lpa_communities(G)\n    result = {frozenset(c) for c in communities}\n    assert result == expected"
 },
 {
  "docstring": "Test 2 cases that were looping infinitely\nfrom issues #5175 and #5704",
  "code": "def test_directed_partition():\n    G = nx.DiGraph()\n    H = nx.DiGraph()\n    G.add_nodes_from(range(10))\n    H.add_nodes_from([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])\n    G_edges = [(0, 2), (0, 1), (1, 0), (2, 1), (2, 0), (3, 4), (4, 3), (7, 8), (8, 7), (9, 10), (10, 9)]\n    H_edges = [(1, 2), (1, 6), (1, 9), (2, 3), (2, 4), (2, 5), (3, 4), (4, 3), (4, 5), (5, 4), (6, 7), (6, 8), (9, 10), (9, 11), (10, 11), (11, 10)]\n    G.add_edges_from(G_edges)\n    H.add_edges_from(H_edges)\n    G_expected_partition = [{0, 1, 2}, {3, 4}, {5}, {6}, {8, 7}, {9, 10}]\n    G_partition = nx.community.louvain_communities(G, seed=123, weight=None)\n    H_expected_partition = [{2, 3, 4, 5}, {8, 1, 6, 7}, {9, 10, 11}]\n    H_partition = nx.community.louvain_communities(H, seed=123, weight=None)\n    assert G_partition == G_expected_partition\n    assert H_partition == H_expected_partition"
 },
 {
  "docstring": "Tests that a poor partition has a low performance measure.",
  "code": "def test_bad_partition(self):\n    G = barbell_graph(3, 0)\n    partition = [{0, 1, 4}, {2, 3, 5}]\n    assert 8 / 15 == pytest.approx(partition_quality(G, partition)[1], abs=1e-07)"
 },
 {
  "docstring": "Tests that a good partition has a high performance measure.",
  "code": "def test_good_partition(self):\n    G = barbell_graph(3, 0)\n    partition = [{0, 1, 2}, {3, 4, 5}]\n    assert 14 / 15 == pytest.approx(partition_quality(G, partition)[1], abs=1e-07)"
 },
 {
  "docstring": "Tests that a poor partition has a low coverage measure.",
  "code": "def test_bad_partition(self):\n    G = barbell_graph(3, 0)\n    partition = [{0, 1, 4}, {2, 3, 5}]\n    assert 3 / 7 == pytest.approx(partition_quality(G, partition)[0], abs=1e-07)"
 },
 {
  "docstring": "Tests that a good partition has a high coverage measure.",
  "code": "def test_good_partition(self):\n    G = barbell_graph(3, 0)\n    partition = [{0, 1, 2}, {3, 4, 5}]\n    assert 6 / 7 == pytest.approx(partition_quality(G, partition)[0], abs=1e-07)"
 },
 {
  "docstring": "Generates the attracting components in `G`.\n\nAn attracting component in a directed graph `G` is a strongly connected\ncomponent with the property that a random walker on the graph will never\nleave the component, once it enters the component.\n\nThe nodes in attracting components can also be thought of as recurrent\nnodes.  If a random walker enters the attractor containing the node, then\nthe node will be visited infinitely often.\n\nTo obtain induced subgraphs on each component use:\n``(G.subgraph(c).copy() for c in attracting_components(G))``\n\nParameters\n----------\nG : DiGraph, MultiDiGraph\n    The graph to be analyzed.\n\nReturns\n-------\nattractors : generator of sets\n    A generator of sets of nodes, one for each attracting component of G.\n\nRaises\n------\nNetworkXNotImplemented\n    If the input graph is undirected.\n\nSee Also\n--------\nnumber_attracting_components\nis_attracting_component",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef attracting_components(G):\n    scc = list(nx.strongly_connected_components(G))\n    cG = nx.condensation(G, scc)\n    for n in cG:\n        if cG.out_degree(n) == 0:\n            yield scc[n]"
 },
 {
  "docstring": "Returns the number of attracting components in `G`.\n\nParameters\n----------\nG : DiGraph, MultiDiGraph\n    The graph to be analyzed.\n\nReturns\n-------\nn : int\n    The number of attracting components in G.\n\nRaises\n------\nNetworkXNotImplemented\n    If the input graph is undirected.\n\nSee Also\n--------\nattracting_components\nis_attracting_component",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef number_attracting_components(G):\n    return sum((1 for ac in attracting_components(G)))"
 },
 {
  "docstring": "Returns True if `G` consists of a single attracting component.\n\nParameters\n----------\nG : DiGraph, MultiDiGraph\n    The graph to be analyzed.\n\nReturns\n-------\nattracting : bool\n    True if `G` has a single attracting component. Otherwise, False.\n\nRaises\n------\nNetworkXNotImplemented\n    If the input graph is undirected.\n\nSee Also\n--------\nattracting_components\nnumber_attracting_components",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef is_attracting_component(G):\n    ac = list(attracting_components(G))\n    if len(ac) == 1:\n        return len(ac[0]) == len(G)\n    return False"
 },
 {
  "docstring": "Returns True if the graph is biconnected, False otherwise.\n\nA graph is biconnected if, and only if, it cannot be disconnected by\nremoving only one node (and all edges incident on that node).  If\nremoving a node increases the number of disconnected components\nin the graph, that node is called an articulation point, or cut\nvertex.  A biconnected graph has no articulation points.\n\nParameters\n----------\nG : NetworkX Graph\n    An undirected graph.\n\nReturns\n-------\nbiconnected : bool\n    True if the graph is biconnected, False otherwise.\n\nRaises\n------\nNetworkXNotImplemented\n    If the input graph is not undirected.\n\nExamples\n--------\n>>> G = nx.path_graph(4)\n>>> print(nx.is_biconnected(G))\nFalse\n>>> G.add_edge(0, 3)\n>>> print(nx.is_biconnected(G))\nTrue\n\nSee Also\n--------\nbiconnected_components\narticulation_points\nbiconnected_component_edges\nis_strongly_connected\nis_weakly_connected\nis_connected\nis_semiconnected\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch\ndef is_biconnected(G):\n    bccs = biconnected_components(G)\n    try:\n        bcc = next(bccs)\n    except StopIteration:\n        return False\n    try:\n        next(bccs)\n    except StopIteration:\n        return len(bcc) == len(G)\n    else:\n        return False"
 },
 {
  "docstring": "Returns a generator of lists of edges, one list for each biconnected\ncomponent of the input graph.\n\nBiconnected components are maximal subgraphs such that the removal of a\nnode (and all edges incident on that node) will not disconnect the\nsubgraph.  Note that nodes may be part of more than one biconnected\ncomponent.  Those nodes are articulation points, or cut vertices.\nHowever, each edge belongs to one, and only one, biconnected component.\n\nNotice that by convention a dyad is considered a biconnected component.\n\nParameters\n----------\nG : NetworkX Graph\n    An undirected graph.\n\nReturns\n-------\nedges : generator of lists\n    Generator of lists of edges, one list for each bicomponent.\n\nRaises\n------\nNetworkXNotImplemented\n    If the input graph is not undirected.\n\nExamples\n--------\n>>> G = nx.barbell_graph(4, 2)\n>>> print(nx.is_biconnected(G))\nFalse\n>>> bicomponents_edges = list(nx.biconnected_component_edges(G))\n>>> len(bicomponents_edges)\n5\n>>> G.add_edge(2, 8)\n>>> print(nx.is_biconnected(G))\nTrue\n>>> bicomponents_edges = list(nx.biconnected_component_edges(G))\n>>> len(bicomponents_edges)\n1\n\nSee Also\n--------\nis_biconnected,\nbiconnected_components,\narticulation_points,\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch\ndef biconnected_component_edges(G):\n    yield from _biconnected_dfs(G, components=True)"
 },
 {
  "docstring": "Returns a generator of sets of nodes, one set for each biconnected\ncomponent of the graph\n\nBiconnected components are maximal subgraphs such that the removal of a\nnode (and all edges incident on that node) will not disconnect the\nsubgraph. Note that nodes may be part of more than one biconnected\ncomponent.  Those nodes are articulation points, or cut vertices.  The\nremoval of articulation points will increase the number of connected\ncomponents of the graph.\n\nNotice that by convention a dyad is considered a biconnected component.\n\nParameters\n----------\nG : NetworkX Graph\n    An undirected graph.\n\nReturns\n-------\nnodes : generator\n    Generator of sets of nodes, one set for each biconnected component.\n\nRaises\n------\nNetworkXNotImplemented\n    If the input graph is not undirected.\n\nExamples\n--------\n>>> G = nx.lollipop_graph(5, 1)\n>>> print(nx.is_biconnected(G))\nFalse\n>>> bicomponents = list(nx.biconnected_components(G))\n>>> len(bicomponents)\n2\n>>> G.add_edge(0, 5)\n>>> print(nx.is_biconnected(G))\nTrue\n>>> bicomponents = list(nx.biconnected_components(G))\n>>> len(bicomponents)\n1\n\nYou can generate a sorted list of biconnected components, largest\nfirst, using sort.\n\n>>> G.remove_edge(0, 5)\n>>> [len(c) for c in sorted(nx.biconnected_components(G), key=len, reverse=True)]\n[5, 2]\n\nIf you only want the largest connected component, it's more\nefficient to use max instead of sort.\n\n>>> Gc = max(nx.biconnected_components(G), key=len)\n\nTo create the components as subgraphs use:\n``(G.subgraph(c).copy() for c in biconnected_components(G))``\n\nSee Also\n--------\nis_biconnected\narticulation_points\nbiconnected_component_edges\nk_components : this function is a special case where k=2\nbridge_components : similar to this function, but is defined using\n    2-edge-connectivity instead of 2-node-connectivity.\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch\ndef biconnected_components(G):\n    for comp in _biconnected_dfs(G, components=True):\n        yield set(chain.from_iterable(comp))"
 },
 {
  "docstring": "Yield the articulation points, or cut vertices, of a graph.\n\nAn articulation point or cut vertex is any node whose removal (along with\nall its incident edges) increases the number of connected components of\na graph.  An undirected connected graph without articulation points is\nbiconnected. Articulation points belong to more than one biconnected\ncomponent of a graph.\n\nNotice that by convention a dyad is considered a biconnected component.\n\nParameters\n----------\nG : NetworkX Graph\n    An undirected graph.\n\nYields\n------\nnode\n    An articulation point in the graph.\n\nRaises\n------\nNetworkXNotImplemented\n    If the input graph is not undirected.\n\nExamples\n--------\n\n>>> G = nx.barbell_graph(4, 2)\n>>> print(nx.is_biconnected(G))\nFalse\n>>> len(list(nx.articulation_points(G)))\n4\n>>> G.add_edge(2, 8)\n>>> print(nx.is_biconnected(G))\nTrue\n>>> len(list(nx.articulation_points(G)))\n0\n\nSee Also\n--------\nis_biconnected\nbiconnected_components\nbiconnected_component_edges\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch\ndef articulation_points(G):\n    seen = set()\n    for articulation in _biconnected_dfs(G, components=False):\n        if articulation not in seen:\n            seen.add(articulation)\n            yield articulation"
 },
 {
  "docstring": "Generate connected components.\n\nParameters\n----------\nG : NetworkX graph\n   An undirected graph\n\nReturns\n-------\ncomp : generator of sets\n   A generator of sets of nodes, one for each component of G.\n\nRaises\n------\nNetworkXNotImplemented\n    If G is directed.\n\nExamples\n--------\nGenerate a sorted list of connected components, largest first.\n\n>>> G = nx.path_graph(4)\n>>> nx.add_path(G, [10, 11, 12])\n>>> [len(c) for c in sorted(nx.connected_components(G), key=len, reverse=True)]\n[4, 3]\n\nIf you only want the largest connected component, it's more\nefficient to use max instead of sort.\n\n>>> largest_cc = max(nx.connected_components(G), key=len)\n\nTo create the induced subgraph of each component use:\n\n>>> S = [G.subgraph(c).copy() for c in nx.connected_components(G)]\n\nSee Also\n--------\nstrongly_connected_components\nweakly_connected_components\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch\ndef connected_components(G):\n    seen = set()\n    for v in G:\n        if v not in seen:\n            c = _plain_bfs(G, v)\n            seen.update(c)\n            yield c"
 },
 {
  "docstring": "Returns the number of connected components.\n\nParameters\n----------\nG : NetworkX graph\n   An undirected graph.\n\nReturns\n-------\nn : integer\n   Number of connected components\n\nRaises\n------\nNetworkXNotImplemented\n    If G is directed.\n\nExamples\n--------\n>>> G = nx.Graph([(0, 1), (1, 2), (5, 6), (3, 4)])\n>>> nx.number_connected_components(G)\n3\n\nSee Also\n--------\nconnected_components\nnumber_weakly_connected_components\nnumber_strongly_connected_components\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch\ndef number_connected_components(G):\n    return sum((1 for cc in connected_components(G)))"
 },
 {
  "docstring": "Returns True if the graph is connected, False otherwise.\n\nParameters\n----------\nG : NetworkX Graph\n   An undirected graph.\n\nReturns\n-------\nconnected : bool\n  True if the graph is connected, false otherwise.\n\nRaises\n------\nNetworkXNotImplemented\n    If G is directed.\n\nExamples\n--------\n>>> G = nx.path_graph(4)\n>>> print(nx.is_connected(G))\nTrue\n\nSee Also\n--------\nis_strongly_connected\nis_weakly_connected\nis_semiconnected\nis_biconnected\nconnected_components\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch\ndef is_connected(G):\n    if len(G) == 0:\n        raise nx.NetworkXPointlessConcept('Connectivity is undefined for the null graph.')\n    return sum((1 for node in _plain_bfs(G, arbitrary_element(G)))) == len(G)"
 },
 {
  "docstring": "Returns the set of nodes in the component of graph containing node n.\n\nParameters\n----------\nG : NetworkX Graph\n   An undirected graph.\n\nn : node label\n   A node in G\n\nReturns\n-------\ncomp : set\n   A set of nodes in the component of G containing node n.\n\nRaises\n------\nNetworkXNotImplemented\n    If G is directed.\n\nExamples\n--------\n>>> G = nx.Graph([(0, 1), (1, 2), (5, 6), (3, 4)])\n>>> nx.node_connected_component(G, 0)  # nodes of component that contains node 0\n{0, 1, 2}\n\nSee Also\n--------\nconnected_components\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch\ndef node_connected_component(G, n):\n    return _plain_bfs(G, n)"
 },
 {
  "docstring": "A fast BFS node generator",
  "code": "def _plain_bfs(G, source):\n    adj = G._adj\n    n = len(adj)\n    seen = {source}\n    nextlevel = [source]\n    while nextlevel:\n        thislevel = nextlevel\n        nextlevel = []\n        for v in thislevel:\n            for w in adj[v]:\n                if w not in seen:\n                    seen.add(w)\n                    nextlevel.append(w)\n            if len(seen) == n:\n                return seen\n    return seen"
 },
 {
  "docstring": "Returns True if the graph is semiconnected, False otherwise.\n\nA graph is semiconnected if and only if for any pair of nodes, either one\nis reachable from the other, or they are mutually reachable.\n\nThis function uses a theorem that states that a DAG is semiconnected\nif for any topological sort, for node $v_n$ in that sort, there is an\nedge $(v_i, v_{i+1})$. That allows us to check if a non-DAG `G` is\nsemiconnected by condensing the graph: i.e. constructing a new graph `H`\nwith nodes being the strongly connected components of `G`, and edges\n(scc_1, scc_2) if there is a edge $(v_1, v_2)$ in `G` for some\n$v_1 \\in scc_1$ and $v_2 \\in scc_2$. That results in a DAG, so we compute\nthe topological sort of `H` and check if for every $n$ there is an edge\n$(scc_n, scc_{n+1})$.\n\nParameters\n----------\nG : NetworkX graph\n    A directed graph.\n\nReturns\n-------\nsemiconnected : bool\n    True if the graph is semiconnected, False otherwise.\n\nRaises\n------\nNetworkXNotImplemented\n    If the input graph is undirected.\n\nNetworkXPointlessConcept\n    If the graph is empty.\n\nExamples\n--------\n>>> G = nx.path_graph(4, create_using=nx.DiGraph())\n>>> print(nx.is_semiconnected(G))\nTrue\n>>> G = nx.DiGraph([(1, 2), (3, 2)])\n>>> print(nx.is_semiconnected(G))\nFalse\n\nSee Also\n--------\nis_strongly_connected\nis_weakly_connected\nis_connected\nis_biconnected",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef is_semiconnected(G):\n    if len(G) == 0:\n        raise nx.NetworkXPointlessConcept('Connectivity is undefined for the null graph.')\n    if not nx.is_weakly_connected(G):\n        return False\n    H = nx.condensation(G)\n    return all((H.has_edge(u, v) for u, v in pairwise(nx.topological_sort(H))))"
 },
 {
  "docstring": "Generate nodes in strongly connected components of graph.\n\nParameters\n----------\nG : NetworkX Graph\n    A directed graph.\n\nReturns\n-------\ncomp : generator of sets\n    A generator of sets of nodes, one for each strongly connected\n    component of G.\n\nRaises\n------\nNetworkXNotImplemented\n    If G is undirected.\n\nExamples\n--------\nGenerate a sorted list of strongly connected components, largest first.\n\n>>> G = nx.cycle_graph(4, create_using=nx.DiGraph())\n>>> nx.add_cycle(G, [10, 11, 12])\n>>> [\n...     len(c)\n...     for c in sorted(nx.strongly_connected_components(G), key=len, reverse=True)\n... ]\n[4, 3]\n\nIf you only want the largest component, it's more efficient to\nuse max instead of sort.\n\n>>> largest = max(nx.strongly_connected_components(G), key=len)\n\nSee Also\n--------\nconnected_components\nweakly_connected_components\nkosaraju_strongly_connected_components\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef strongly_connected_components(G):\n    preorder = {}\n    lowlink = {}\n    scc_found = set()\n    scc_queue = []\n    i = 0\n    neighbors = {v: iter(G[v]) for v in G}\n    for source in G:\n        if source not in scc_found:\n            queue = [source]\n            while queue:\n                v = queue[-1]\n                if v not in preorder:\n                    i = i + 1\n                    preorder[v] = i\n                done = True\n                for w in neighbors[v]:\n                    if w not in preorder:\n                        queue.append(w)\n                        done = False\n                        break\n                if done:\n                    lowlink[v] = preorder[v]\n                    for w in G[v]:\n                        if w not in scc_found:\n                            if preorder[w] > preorder[v]:\n                                lowlink[v] = min([lowlink[v], lowlink[w]])\n                            else:\n                                lowlink[v] = min([lowlink[v], preorder[w]])\n                    queue.pop()\n                    if lowlink[v] == preorder[v]:\n                        scc = {v}\n                        while scc_queue and preorder[scc_queue[-1]] > preorder[v]:\n                            k = scc_queue.pop()\n                            scc.add(k)\n                        scc_found.update(scc)\n                        yield scc\n                    else:\n                        scc_queue.append(v)"
 },
 {
  "docstring": "Generate nodes in strongly connected components of graph.\n\nParameters\n----------\nG : NetworkX Graph\n    A directed graph.\n\nReturns\n-------\ncomp : generator of sets\n    A generator of sets of nodes, one for each strongly connected\n    component of G.\n\nRaises\n------\nNetworkXNotImplemented\n    If G is undirected.\n\nExamples\n--------\nGenerate a sorted list of strongly connected components, largest first.\n\n>>> G = nx.cycle_graph(4, create_using=nx.DiGraph())\n>>> nx.add_cycle(G, [10, 11, 12])\n>>> [\n...     len(c)\n...     for c in sorted(\n...         nx.kosaraju_strongly_connected_components(G), key=len, reverse=True\n...     )\n... ]\n[4, 3]\n\nIf you only want the largest component, it's more efficient to\nuse max instead of sort.\n\n>>> largest = max(nx.kosaraju_strongly_connected_components(G), key=len)\n\nSee Also\n--------\nstrongly_connected_components\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef kosaraju_strongly_connected_components(G, source=None):\n    post = list(nx.dfs_postorder_nodes(G.reverse(copy=False), source=source))\n    seen = set()\n    while post:\n        r = post.pop()\n        if r in seen:\n            continue\n        c = nx.dfs_preorder_nodes(G, r)\n        new = {v for v in c if v not in seen}\n        seen.update(new)\n        yield new"
 },
 {
  "docstring": "Generate nodes in strongly connected components of graph.\n\n.. deprecated:: 3.2\n\n   This function is deprecated and will be removed in a future version of\n   NetworkX. Use `strongly_connected_components` instead.\n\nRecursive version of algorithm.\n\nParameters\n----------\nG : NetworkX Graph\n    A directed graph.\n\nReturns\n-------\ncomp : generator of sets\n    A generator of sets of nodes, one for each strongly connected\n    component of G.\n\nRaises\n------\nNetworkXNotImplemented\n    If G is undirected.\n\nExamples\n--------\nGenerate a sorted list of strongly connected components, largest first.\n\n>>> G = nx.cycle_graph(4, create_using=nx.DiGraph())\n>>> nx.add_cycle(G, [10, 11, 12])\n>>> [\n...     len(c)\n...     for c in sorted(\n...         nx.strongly_connected_components_recursive(G), key=len, reverse=True\n...     )\n... ]\n[4, 3]\n\nIf you only want the largest component, it's more efficient to\nuse max instead of sort.\n\n>>> largest = max(nx.strongly_connected_components_recursive(G), key=len)\n\nTo create the induced subgraph of the components use:\n>>> S = [G.subgraph(c).copy() for c in nx.weakly_connected_components(G)]\n\nSee Also\n--------\nconnected_components\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef strongly_connected_components_recursive(G):\n    import warnings\n    warnings.warn('\\n\\nstrongly_connected_components_recursive is deprecated and will be\\nremoved in the future. Use strongly_connected_components instead.', category=DeprecationWarning, stacklevel=2)\n    yield from strongly_connected_components(G)"
 },
 {
  "docstring": "Returns number of strongly connected components in graph.\n\nParameters\n----------\nG : NetworkX graph\n   A directed graph.\n\nReturns\n-------\nn : integer\n   Number of strongly connected components\n\nRaises\n------\nNetworkXNotImplemented\n    If G is undirected.\n\nExamples\n--------\n>>> G = nx.DiGraph([(0, 1), (1, 2), (2, 0), (2, 3), (4, 5), (3, 4), (5, 6), (6, 3), (6, 7)])\n>>> nx.number_strongly_connected_components(G)\n3\n\nSee Also\n--------\nstrongly_connected_components\nnumber_connected_components\nnumber_weakly_connected_components\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef number_strongly_connected_components(G):\n    return sum((1 for scc in strongly_connected_components(G)))"
 },
 {
  "docstring": "Test directed graph for strong connectivity.\n\nA directed graph is strongly connected if and only if every vertex in\nthe graph is reachable from every other vertex.\n\nParameters\n----------\nG : NetworkX Graph\n   A directed graph.\n\nReturns\n-------\nconnected : bool\n  True if the graph is strongly connected, False otherwise.\n\nExamples\n--------\n>>> G = nx.DiGraph([(0, 1), (1, 2), (2, 3), (3, 0), (2, 4), (4, 2)])\n>>> nx.is_strongly_connected(G)\nTrue\n>>> G.remove_edge(2, 3)\n>>> nx.is_strongly_connected(G)\nFalse\n\nRaises\n------\nNetworkXNotImplemented\n    If G is undirected.\n\nSee Also\n--------\nis_weakly_connected\nis_semiconnected\nis_connected\nis_biconnected\nstrongly_connected_components\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef is_strongly_connected(G):\n    if len(G) == 0:\n        raise nx.NetworkXPointlessConcept('Connectivity is undefined for the null graph.')\n    return len(next(strongly_connected_components(G))) == len(G)"
 },
 {
  "docstring": "Returns the condensation of G.\n\nThe condensation of G is the graph with each of the strongly connected\ncomponents contracted into a single node.\n\nParameters\n----------\nG : NetworkX DiGraph\n   A directed graph.\n\nscc:  list or generator (optional, default=None)\n   Strongly connected components. If provided, the elements in\n   `scc` must partition the nodes in `G`. If not provided, it will be\n   calculated as scc=nx.strongly_connected_components(G).\n\nReturns\n-------\nC : NetworkX DiGraph\n   The condensation graph C of G.  The node labels are integers\n   corresponding to the index of the component in the list of\n   strongly connected components of G.  C has a graph attribute named\n   'mapping' with a dictionary mapping the original nodes to the\n   nodes in C to which they belong.  Each node in C also has a node\n   attribute 'members' with the set of original nodes in G that\n   form the SCC that the node in C represents.\n\nRaises\n------\nNetworkXNotImplemented\n    If G is undirected.\n\nExamples\n--------\nContracting two sets of strongly connected nodes into two distinct SCC\nusing the barbell graph.\n\n>>> G = nx.barbell_graph(4, 0)\n>>> G.remove_edge(3, 4)\n>>> G = nx.DiGraph(G)\n>>> H = nx.condensation(G)\n>>> H.nodes.data()\nNodeDataView({0: {'members': {0, 1, 2, 3}}, 1: {'members': {4, 5, 6, 7}}})\n>>> H.graph['mapping']\n{0: 0, 1: 0, 2: 0, 3: 0, 4: 1, 5: 1, 6: 1, 7: 1}\n\nContracting a complete graph into one single SCC.\n\n>>> G = nx.complete_graph(7, create_using=nx.DiGraph)\n>>> H = nx.condensation(G)\n>>> H.nodes\nNodeView((0,))\n>>> H.nodes.data()\nNodeDataView({0: {'members': {0, 1, 2, 3, 4, 5, 6}}})\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef condensation(G, scc=None):\n    if scc is None:\n        scc = nx.strongly_connected_components(G)\n    mapping = {}\n    members = {}\n    C = nx.DiGraph()\n    C.graph['mapping'] = mapping\n    if len(G) == 0:\n        return C\n    for i, component in enumerate(scc):\n        members[i] = component\n        mapping.update(((n, i) for n in component))\n    number_of_components = i + 1\n    C.add_nodes_from(range(number_of_components))\n    C.add_edges_from(((mapping[u], mapping[v]) for u, v in G.edges() if mapping[u] != mapping[v]))\n    nx.set_node_attributes(C, members, 'members')\n    return C"
 },
 {
  "docstring": "Generate weakly connected components of G.\n\nParameters\n----------\nG : NetworkX graph\n    A directed graph\n\nReturns\n-------\ncomp : generator of sets\n    A generator of sets of nodes, one for each weakly connected\n    component of G.\n\nRaises\n------\nNetworkXNotImplemented\n    If G is undirected.\n\nExamples\n--------\nGenerate a sorted list of weakly connected components, largest first.\n\n>>> G = nx.path_graph(4, create_using=nx.DiGraph())\n>>> nx.add_path(G, [10, 11, 12])\n>>> [\n...     len(c)\n...     for c in sorted(nx.weakly_connected_components(G), key=len, reverse=True)\n... ]\n[4, 3]\n\nIf you only want the largest component, it's more efficient to\nuse max instead of sort:\n\n>>> largest_cc = max(nx.weakly_connected_components(G), key=len)\n\nSee Also\n--------\nconnected_components\nstrongly_connected_components\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef weakly_connected_components(G):\n    seen = set()\n    for v in G:\n        if v not in seen:\n            c = set(_plain_bfs(G, v))\n            seen.update(c)\n            yield c"
 },
 {
  "docstring": "Returns the number of weakly connected components in G.\n\nParameters\n----------\nG : NetworkX graph\n    A directed graph.\n\nReturns\n-------\nn : integer\n    Number of weakly connected components\n\nRaises\n------\nNetworkXNotImplemented\n    If G is undirected.\n\nExamples\n--------\n>>> G = nx.DiGraph([(0, 1), (2, 1), (3, 4)])\n>>> nx.number_weakly_connected_components(G)\n2\n\nSee Also\n--------\nweakly_connected_components\nnumber_connected_components\nnumber_strongly_connected_components\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef number_weakly_connected_components(G):\n    return sum((1 for wcc in weakly_connected_components(G)))"
 },
 {
  "docstring": "Test directed graph for weak connectivity.\n\nA directed graph is weakly connected if and only if the graph\nis connected when the direction of the edge between nodes is ignored.\n\nNote that if a graph is strongly connected (i.e. the graph is connected\neven when we account for directionality), it is by definition weakly\nconnected as well.\n\nParameters\n----------\nG : NetworkX Graph\n    A directed graph.\n\nReturns\n-------\nconnected : bool\n    True if the graph is weakly connected, False otherwise.\n\nRaises\n------\nNetworkXNotImplemented\n    If G is undirected.\n\nExamples\n--------\n>>> G = nx.DiGraph([(0, 1), (2, 1)])\n>>> G.add_node(3)\n>>> nx.is_weakly_connected(G)  # node 3 is not connected to the graph\nFalse\n>>> G.add_edge(2, 3)\n>>> nx.is_weakly_connected(G)\nTrue\n\nSee Also\n--------\nis_strongly_connected\nis_semiconnected\nis_connected\nis_biconnected\nweakly_connected_components\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch\ndef is_weakly_connected(G):\n    if len(G) == 0:\n        raise nx.NetworkXPointlessConcept('Connectivity is undefined for the null graph.')\n    return len(next(weakly_connected_components(G))) == len(G)"
 },
 {
  "docstring": "A fast BFS node generator\n\nThe direction of the edge between nodes is ignored.\n\nFor directed graphs only.",
  "code": "def _plain_bfs(G, source):\n    n = len(G)\n    Gsucc = G._succ\n    Gpred = G._pred\n    seen = {source}\n    nextlevel = [source]\n    yield source\n    while nextlevel:\n        thislevel = nextlevel\n        nextlevel = []\n        for v in thislevel:\n            for w in Gsucc[v]:\n                if w not in seen:\n                    seen.add(w)\n                    nextlevel.append(w)\n                    yield w\n            for w in Gpred[v]:\n                if w not in seen:\n                    seen.add(w)\n                    nextlevel.append(w)\n                    yield w\n            if len(seen) == n:\n                return"
 },
 {
  "docstring": "Computes local node connectivity for nodes s and t.\n\nLocal node connectivity for two non adjacent nodes s and t is the\nminimum number of nodes that must be removed (along with their incident\nedges) to disconnect them.\n\nThis is a flow based implementation of node connectivity. We compute the\nmaximum flow on an auxiliary digraph build from the original input\ngraph (see below for details).\n\nParameters\n----------\nG : NetworkX graph\n    Undirected graph\n\ns : node\n    Source node\n\nt : node\n    Target node\n\nflow_func : function\n    A function for computing the maximum flow among a pair of nodes.\n    The function has to accept at least three parameters: a Digraph,\n    a source node, and a target node. And return a residual network\n    that follows NetworkX conventions (see :meth:`maximum_flow` for\n    details). If flow_func is None, the default maximum flow function\n    (:meth:`edmonds_karp`) is used. See below for details. The choice\n    of the default function may change from version to version and\n    should not be relied on. Default value: None.\n\nauxiliary : NetworkX DiGraph\n    Auxiliary digraph to compute flow based node connectivity. It has\n    to have a graph attribute called mapping with a dictionary mapping\n    node names in G and in the auxiliary digraph. If provided\n    it will be reused instead of recreated. Default value: None.\n\nresidual : NetworkX DiGraph\n    Residual network to compute maximum flow. If provided it will be\n    reused instead of recreated. Default value: None.\n\ncutoff : integer, float, or None (default: None)\n    If specified, the maximum flow algorithm will terminate when the\n    flow value reaches or exceeds the cutoff. This only works for flows\n    that support the cutoff parameter (most do) and is ignored otherwise.\n\nReturns\n-------\nK : integer\n    local node connectivity for nodes s and t\n\nExamples\n--------\nThis function is not imported in the base NetworkX namespace, so you\nhave to explicitly import it from the connectivity package:\n\n>>> from networkx.algorithms.connectivity import local_node_connectivity\n\nWe use in this example the platonic icosahedral graph, which has node\nconnectivity 5.\n\n>>> G = nx.icosahedral_graph()\n>>> local_node_connectivity(G, 0, 6)\n5\n\nIf you need to compute local connectivity on several pairs of\nnodes in the same graph, it is recommended that you reuse the\ndata structures that NetworkX uses in the computation: the\nauxiliary digraph for node connectivity, and the residual\nnetwork for the underlying maximum flow computation.\n\nExample of how to compute local node connectivity among\nall pairs of nodes of the platonic icosahedral graph reusing\nthe data structures.\n\n>>> import itertools\n>>> # You also have to explicitly import the function for\n>>> # building the auxiliary digraph from the connectivity package\n>>> from networkx.algorithms.connectivity import build_auxiliary_node_connectivity\n...\n>>> H = build_auxiliary_node_connectivity(G)\n>>> # And the function for building the residual network from the\n>>> # flow package\n>>> from networkx.algorithms.flow import build_residual_network\n>>> # Note that the auxiliary digraph has an edge attribute named capacity\n>>> R = build_residual_network(H, \"capacity\")\n>>> result = dict.fromkeys(G, dict())\n>>> # Reuse the auxiliary digraph and the residual network by passing them\n>>> # as parameters\n>>> for u, v in itertools.combinations(G, 2):\n...     k = local_node_connectivity(G, u, v, auxiliary=H, residual=R)\n...     result[u][v] = k\n...\n>>> all(result[u][v] == 5 for u, v in itertools.combinations(G, 2))\nTrue\n\nYou can also use alternative flow algorithms for computing node\nconnectivity. For instance, in dense networks the algorithm\n:meth:`shortest_augmenting_path` will usually perform better than\nthe default :meth:`edmonds_karp` which is faster for sparse\nnetworks with highly skewed degree distributions. Alternative flow\nfunctions have to be explicitly imported from the flow package.\n\n>>> from networkx.algorithms.flow import shortest_augmenting_path\n>>> local_node_connectivity(G, 0, 6, flow_func=shortest_augmenting_path)\n5\n\n",
  "code": "@nx._dispatch(graphs={'G': 0, 'auxiliary?': 4, 'residual?': 5}, preserve_edge_attrs={'residual': {'capacity': float('inf')}}, preserve_graph_attrs={'auxiliary', 'residual'})\ndef local_node_connectivity(G, s, t, flow_func=None, auxiliary=None, residual=None, cutoff=None):\n    if flow_func is None:\n        flow_func = default_flow_func\n    if auxiliary is None:\n        H = build_auxiliary_node_connectivity(G)\n    else:\n        H = auxiliary\n    mapping = H.graph.get('mapping', None)\n    if mapping is None:\n        raise nx.NetworkXError('Invalid auxiliary digraph.')\n    kwargs = {'flow_func': flow_func, 'residual': residual}\n    if flow_func is shortest_augmenting_path:\n        kwargs['cutoff'] = cutoff\n        kwargs['two_phase'] = True\n    elif flow_func is edmonds_karp:\n        kwargs['cutoff'] = cutoff\n    elif flow_func is dinitz:\n        kwargs['cutoff'] = cutoff\n    elif flow_func is boykov_kolmogorov:\n        kwargs['cutoff'] = cutoff\n    return nx.maximum_flow_value(H, f'{mapping[s]}B', f'{mapping[t]}A', **kwargs)"
 },
 {
  "docstring": "Returns node connectivity for a graph or digraph G.\n\nNode connectivity is equal to the minimum number of nodes that\nmust be removed to disconnect G or render it trivial. If source\nand target nodes are provided, this function returns the local node\nconnectivity: the minimum number of nodes that must be removed to break\nall paths from source to target in G.\n\nParameters\n----------\nG : NetworkX graph\n    Undirected graph\n\ns : node\n    Source node. Optional. Default value: None.\n\nt : node\n    Target node. Optional. Default value: None.\n\nflow_func : function\n    A function for computing the maximum flow among a pair of nodes.\n    The function has to accept at least three parameters: a Digraph,\n    a source node, and a target node. And return a residual network\n    that follows NetworkX conventions (see :meth:`maximum_flow` for\n    details). If flow_func is None, the default maximum flow function\n    (:meth:`edmonds_karp`) is used. See below for details. The\n    choice of the default function may change from version\n    to version and should not be relied on. Default value: None.\n\nReturns\n-------\nK : integer\n    Node connectivity of G, or local node connectivity if source\n    and target are provided.\n\nExamples\n--------\n>>> # Platonic icosahedral graph is 5-node-connected\n>>> G = nx.icosahedral_graph()\n>>> nx.node_connectivity(G)\n5\n\nYou can use alternative flow algorithms for the underlying maximum\nflow computation. In dense networks the algorithm\n:meth:`shortest_augmenting_path` will usually perform better\nthan the default :meth:`edmonds_karp`, which is faster for\nsparse networks with highly skewed degree distributions. Alternative\nflow functions have to be explicitly imported from the flow package.\n\n>>> from networkx.algorithms.flow import shortest_augmenting_path\n>>> nx.node_connectivity(G, flow_func=shortest_augmenting_path)\n5\n\nIf you specify a pair of nodes (source and target) as parameters,\nthis function returns the value of local node connectivity.\n\n>>> nx.node_connectivity(G, 3, 7)\n5\n\nIf you need to perform several local computations among different\npairs of nodes on the same graph, it is recommended that you reuse\nthe data structures used in the maximum flow computations. See\n:meth:`local_node_connectivity` for details.\n\n",
  "code": "@nx._dispatch\ndef node_connectivity(G, s=None, t=None, flow_func=None):\n    if s is not None and t is None or (s is None and t is not None):\n        raise nx.NetworkXError('Both source and target must be specified.')\n    if s is not None and t is not None:\n        if s not in G:\n            raise nx.NetworkXError(f'node {s} not in graph')\n        if t not in G:\n            raise nx.NetworkXError(f'node {t} not in graph')\n        return local_node_connectivity(G, s, t, flow_func=flow_func)\n    if G.is_directed():\n        if not nx.is_weakly_connected(G):\n            return 0\n        iter_func = itertools.permutations\n\n        def neighbors(v):\n            return itertools.chain.from_iterable([G.predecessors(v), G.successors(v)])\n    else:\n        if not nx.is_connected(G):\n            return 0\n        iter_func = itertools.combinations\n        neighbors = G.neighbors\n    H = build_auxiliary_node_connectivity(G)\n    R = build_residual_network(H, 'capacity')\n    kwargs = {'flow_func': flow_func, 'auxiliary': H, 'residual': R}\n    v, K = min(G.degree(), key=itemgetter(1))\n    for w in set(G) - set(neighbors(v)) - {v}:\n        kwargs['cutoff'] = K\n        K = min(K, local_node_connectivity(G, v, w, **kwargs))\n    for x, y in iter_func(neighbors(v), 2):\n        if y in G[x]:\n            continue\n        kwargs['cutoff'] = K\n        K = min(K, local_node_connectivity(G, x, y, **kwargs))\n    return K"
 },
 {
  "docstring": "Returns the average connectivity of a graph G.\n\nThe average connectivity `\\bar{\\kappa}` of a graph G is the average\nof local node connectivity over all pairs of nodes of G [1]_ .\n\n.. math::\n\n    \\bar{\\kappa}(G) = \\frac{\\sum_{u,v} \\kappa_{G}(u,v)}{{n \\choose 2}}\n\nParameters\n----------\n\nG : NetworkX graph\n    Undirected graph\n\nflow_func : function\n    A function for computing the maximum flow among a pair of nodes.\n    The function has to accept at least three parameters: a Digraph,\n    a source node, and a target node. And return a residual network\n    that follows NetworkX conventions (see :meth:`maximum_flow` for\n    details). If flow_func is None, the default maximum flow function\n    (:meth:`edmonds_karp`) is used. See :meth:`local_node_connectivity`\n    for details. The choice of the default function may change from\n    version to version and should not be relied on. Default value: None.\n\nReturns\n-------\nK : float\n    Average node connectivity\n\n",
  "code": "@nx._dispatch\ndef average_node_connectivity(G, flow_func=None):\n    if G.is_directed():\n        iter_func = itertools.permutations\n    else:\n        iter_func = itertools.combinations\n    H = build_auxiliary_node_connectivity(G)\n    R = build_residual_network(H, 'capacity')\n    kwargs = {'flow_func': flow_func, 'auxiliary': H, 'residual': R}\n    num, den = (0, 0)\n    for u, v in iter_func(G, 2):\n        num += local_node_connectivity(G, u, v, **kwargs)\n        den += 1\n    if den == 0:\n        return 0\n    return num / den"
 },
 {
  "docstring": "Compute node connectivity between all pairs of nodes of G.\n\nParameters\n----------\nG : NetworkX graph\n    Undirected graph\n\nnbunch: container\n    Container of nodes. If provided node connectivity will be computed\n    only over pairs of nodes in nbunch.\n\nflow_func : function\n    A function for computing the maximum flow among a pair of nodes.\n    The function has to accept at least three parameters: a Digraph,\n    a source node, and a target node. And return a residual network\n    that follows NetworkX conventions (see :meth:`maximum_flow` for\n    details). If flow_func is None, the default maximum flow function\n    (:meth:`edmonds_karp`) is used. See below for details. The\n    choice of the default function may change from version\n    to version and should not be relied on. Default value: None.\n\nReturns\n-------\nall_pairs : dict\n    A dictionary with node connectivity between all pairs of nodes\n    in G, or in nbunch if provided.\n\n",
  "code": "@nx._dispatch\ndef all_pairs_node_connectivity(G, nbunch=None, flow_func=None):\n    if nbunch is None:\n        nbunch = G\n    else:\n        nbunch = set(nbunch)\n    directed = G.is_directed()\n    if directed:\n        iter_func = itertools.permutations\n    else:\n        iter_func = itertools.combinations\n    all_pairs = {n: {} for n in nbunch}\n    H = build_auxiliary_node_connectivity(G)\n    mapping = H.graph['mapping']\n    R = build_residual_network(H, 'capacity')\n    kwargs = {'flow_func': flow_func, 'auxiliary': H, 'residual': R}\n    for u, v in iter_func(nbunch, 2):\n        K = local_node_connectivity(G, u, v, **kwargs)\n        all_pairs[u][v] = K\n        if not directed:\n            all_pairs[v][u] = K\n    return all_pairs"
 },
 {
  "docstring": "Returns local edge connectivity for nodes s and t in G.\n\nLocal edge connectivity for two nodes s and t is the minimum number\nof edges that must be removed to disconnect them.\n\nThis is a flow based implementation of edge connectivity. We compute the\nmaximum flow on an auxiliary digraph build from the original\nnetwork (see below for details). This is equal to the local edge\nconnectivity because the value of a maximum s-t-flow is equal to the\ncapacity of a minimum s-t-cut (Ford and Fulkerson theorem) [1]_ .\n\nParameters\n----------\nG : NetworkX graph\n    Undirected or directed graph\n\ns : node\n    Source node\n\nt : node\n    Target node\n\nflow_func : function\n    A function for computing the maximum flow among a pair of nodes.\n    The function has to accept at least three parameters: a Digraph,\n    a source node, and a target node. And return a residual network\n    that follows NetworkX conventions (see :meth:`maximum_flow` for\n    details). If flow_func is None, the default maximum flow function\n    (:meth:`edmonds_karp`) is used. See below for details. The\n    choice of the default function may change from version\n    to version and should not be relied on. Default value: None.\n\nauxiliary : NetworkX DiGraph\n    Auxiliary digraph for computing flow based edge connectivity. If\n    provided it will be reused instead of recreated. Default value: None.\n\nresidual : NetworkX DiGraph\n    Residual network to compute maximum flow. If provided it will be\n    reused instead of recreated. Default value: None.\n\ncutoff : integer, float, or None (default: None)\n    If specified, the maximum flow algorithm will terminate when the\n    flow value reaches or exceeds the cutoff. This only works for flows\n    that support the cutoff parameter (most do) and is ignored otherwise.\n\nReturns\n-------\nK : integer\n    local edge connectivity for nodes s and t.\n\nExamples\n--------\nThis function is not imported in the base NetworkX namespace, so you\nhave to explicitly import it from the connectivity package:\n\n>>> from networkx.algorithms.connectivity import local_edge_connectivity\n\nWe use in this example the platonic icosahedral graph, which has edge\nconnectivity 5.\n\n>>> G = nx.icosahedral_graph()\n>>> local_edge_connectivity(G, 0, 6)\n5\n\nIf you need to compute local connectivity on several pairs of\nnodes in the same graph, it is recommended that you reuse the\ndata structures that NetworkX uses in the computation: the\nauxiliary digraph for edge connectivity, and the residual\nnetwork for the underlying maximum flow computation.\n\nExample of how to compute local edge connectivity among\nall pairs of nodes of the platonic icosahedral graph reusing\nthe data structures.\n\n>>> import itertools\n>>> # You also have to explicitly import the function for\n>>> # building the auxiliary digraph from the connectivity package\n>>> from networkx.algorithms.connectivity import build_auxiliary_edge_connectivity\n>>> H = build_auxiliary_edge_connectivity(G)\n>>> # And the function for building the residual network from the\n>>> # flow package\n>>> from networkx.algorithms.flow import build_residual_network\n>>> # Note that the auxiliary digraph has an edge attribute named capacity\n>>> R = build_residual_network(H, \"capacity\")\n>>> result = dict.fromkeys(G, dict())\n>>> # Reuse the auxiliary digraph and the residual network by passing them\n>>> # as parameters\n>>> for u, v in itertools.combinations(G, 2):\n...     k = local_edge_connectivity(G, u, v, auxiliary=H, residual=R)\n...     result[u][v] = k\n>>> all(result[u][v] == 5 for u, v in itertools.combinations(G, 2))\nTrue\n\nYou can also use alternative flow algorithms for computing edge\nconnectivity. For instance, in dense networks the algorithm\n:meth:`shortest_augmenting_path` will usually perform better than\nthe default :meth:`edmonds_karp` which is faster for sparse\nnetworks with highly skewed degree distributions. Alternative flow\nfunctions have to be explicitly imported from the flow package.\n\n>>> from networkx.algorithms.flow import shortest_augmenting_path\n>>> local_edge_connectivity(G, 0, 6, flow_func=shortest_augmenting_path)\n5\n\n",
  "code": "@nx._dispatch(graphs={'G': 0, 'auxiliary?': 4, 'residual?': 5}, preserve_edge_attrs={'residual': {'capacity': float('inf')}}, preserve_graph_attrs={'residual'})\ndef local_edge_connectivity(G, s, t, flow_func=None, auxiliary=None, residual=None, cutoff=None):\n    if flow_func is None:\n        flow_func = default_flow_func\n    if auxiliary is None:\n        H = build_auxiliary_edge_connectivity(G)\n    else:\n        H = auxiliary\n    kwargs = {'flow_func': flow_func, 'residual': residual}\n    if flow_func is shortest_augmenting_path:\n        kwargs['cutoff'] = cutoff\n        kwargs['two_phase'] = True\n    elif flow_func is edmonds_karp:\n        kwargs['cutoff'] = cutoff\n    elif flow_func is dinitz:\n        kwargs['cutoff'] = cutoff\n    elif flow_func is boykov_kolmogorov:\n        kwargs['cutoff'] = cutoff\n    return nx.maximum_flow_value(H, s, t, **kwargs)"
 },
 {
  "docstring": "Returns the edge connectivity of the graph or digraph G.\n\nThe edge connectivity is equal to the minimum number of edges that\nmust be removed to disconnect G or render it trivial. If source\nand target nodes are provided, this function returns the local edge\nconnectivity: the minimum number of edges that must be removed to\nbreak all paths from source to target in G.\n\nParameters\n----------\nG : NetworkX graph\n    Undirected or directed graph\n\ns : node\n    Source node. Optional. Default value: None.\n\nt : node\n    Target node. Optional. Default value: None.\n\nflow_func : function\n    A function for computing the maximum flow among a pair of nodes.\n    The function has to accept at least three parameters: a Digraph,\n    a source node, and a target node. And return a residual network\n    that follows NetworkX conventions (see :meth:`maximum_flow` for\n    details). If flow_func is None, the default maximum flow function\n    (:meth:`edmonds_karp`) is used. See below for details. The\n    choice of the default function may change from version\n    to version and should not be relied on. Default value: None.\n\ncutoff : integer, float, or None (default: None)\n    If specified, the maximum flow algorithm will terminate when the\n    flow value reaches or exceeds the cutoff. This only works for flows\n    that support the cutoff parameter (most do) and is ignored otherwise.\n\nReturns\n-------\nK : integer\n    Edge connectivity for G, or local edge connectivity if source\n    and target were provided\n\nExamples\n--------\n>>> # Platonic icosahedral graph is 5-edge-connected\n>>> G = nx.icosahedral_graph()\n>>> nx.edge_connectivity(G)\n5\n\nYou can use alternative flow algorithms for the underlying\nmaximum flow computation. In dense networks the algorithm\n:meth:`shortest_augmenting_path` will usually perform better\nthan the default :meth:`edmonds_karp`, which is faster for\nsparse networks with highly skewed degree distributions.\nAlternative flow functions have to be explicitly imported\nfrom the flow package.\n\n>>> from networkx.algorithms.flow import shortest_augmenting_path\n>>> nx.edge_connectivity(G, flow_func=shortest_augmenting_path)\n5\n\nIf you specify a pair of nodes (source and target) as parameters,\nthis function returns the value of local edge connectivity.\n\n>>> nx.edge_connectivity(G, 3, 7)\n5\n\nIf you need to perform several local computations among different\npairs of nodes on the same graph, it is recommended that you reuse\nthe data structures used in the maximum flow computations. See\n:meth:`local_edge_connectivity` for details.\n\n",
  "code": "@nx._dispatch\ndef edge_connectivity(G, s=None, t=None, flow_func=None, cutoff=None):\n    if s is not None and t is None or (s is None and t is not None):\n        raise nx.NetworkXError('Both source and target must be specified.')\n    if s is not None and t is not None:\n        if s not in G:\n            raise nx.NetworkXError(f'node {s} not in graph')\n        if t not in G:\n            raise nx.NetworkXError(f'node {t} not in graph')\n        return local_edge_connectivity(G, s, t, flow_func=flow_func, cutoff=cutoff)\n    H = build_auxiliary_edge_connectivity(G)\n    R = build_residual_network(H, 'capacity')\n    kwargs = {'flow_func': flow_func, 'auxiliary': H, 'residual': R}\n    if G.is_directed():\n        if not nx.is_weakly_connected(G):\n            return 0\n        L = min((d for n, d in G.degree()))\n        nodes = list(G)\n        n = len(nodes)\n        if cutoff is not None:\n            L = min(cutoff, L)\n        for i in range(n):\n            kwargs['cutoff'] = L\n            try:\n                L = min(L, local_edge_connectivity(G, nodes[i], nodes[i + 1], **kwargs))\n            except IndexError:\n                L = min(L, local_edge_connectivity(G, nodes[i], nodes[0], **kwargs))\n        return L\n    else:\n        if not nx.is_connected(G):\n            return 0\n        L = min((d for n, d in G.degree()))\n        if cutoff is not None:\n            L = min(cutoff, L)\n        for node in G:\n            D = nx.dominating_set(G, start_with=node)\n            v = D.pop()\n            if D:\n                break\n        else:\n            return L\n        for w in D:\n            kwargs['cutoff'] = L\n            L = min(L, local_edge_connectivity(G, v, w, **kwargs))\n        return L"
 },
 {
  "docstring": "Returns the edges of the cut-set of a minimum (s, t)-cut.\n\nThis function returns the set of edges of minimum cardinality that,\nif removed, would destroy all paths among source and target in G.\nEdge weights are not considered. See :meth:`minimum_cut` for\ncomputing minimum cuts considering edge weights.\n\nParameters\n----------\nG : NetworkX graph\n\ns : node\n    Source node for the flow.\n\nt : node\n    Sink node for the flow.\n\nauxiliary : NetworkX DiGraph\n    Auxiliary digraph to compute flow based node connectivity. It has\n    to have a graph attribute called mapping with a dictionary mapping\n    node names in G and in the auxiliary digraph. If provided\n    it will be reused instead of recreated. Default value: None.\n\nflow_func : function\n    A function for computing the maximum flow among a pair of nodes.\n    The function has to accept at least three parameters: a Digraph,\n    a source node, and a target node. And return a residual network\n    that follows NetworkX conventions (see :meth:`maximum_flow` for\n    details). If flow_func is None, the default maximum flow function\n    (:meth:`edmonds_karp`) is used. See :meth:`node_connectivity` for\n    details. The choice of the default function may change from version\n    to version and should not be relied on. Default value: None.\n\nresidual : NetworkX DiGraph\n    Residual network to compute maximum flow. If provided it will be\n    reused instead of recreated. Default value: None.\n\nReturns\n-------\ncutset : set\n    Set of edges that, if removed from the graph, will disconnect it.\n\n",
  "code": "@nx._dispatch(graphs={'G': 0, 'auxiliary?': 4, 'residual?': 5}, preserve_edge_attrs={'auxiliary': {'capacity': float('inf')}, 'residual': {'capacity': float('inf')}}, preserve_graph_attrs={'auxiliary', 'residual'})\ndef minimum_st_edge_cut(G, s, t, flow_func=None, auxiliary=None, residual=None):\n    if flow_func is None:\n        flow_func = default_flow_func\n    if auxiliary is None:\n        H = build_auxiliary_edge_connectivity(G)\n    else:\n        H = auxiliary\n    kwargs = {'capacity': 'capacity', 'flow_func': flow_func, 'residual': residual}\n    cut_value, partition = nx.minimum_cut(H, s, t, **kwargs)\n    reachable, non_reachable = partition\n    cutset = set()\n    for u, nbrs in ((n, G[n]) for n in reachable):\n        cutset.update(((u, v) for v in nbrs if v in non_reachable))\n    return cutset"
 },
 {
  "docstring": "Returns a set of nodes of minimum cardinality that disconnect source\nfrom target in G.\n\nThis function returns the set of nodes of minimum cardinality that,\nif removed, would destroy all paths among source and target in G.\n\nParameters\n----------\nG : NetworkX graph\n\ns : node\n    Source node.\n\nt : node\n    Target node.\n\nflow_func : function\n    A function for computing the maximum flow among a pair of nodes.\n    The function has to accept at least three parameters: a Digraph,\n    a source node, and a target node. And return a residual network\n    that follows NetworkX conventions (see :meth:`maximum_flow` for\n    details). If flow_func is None, the default maximum flow function\n    (:meth:`edmonds_karp`) is used. See below for details. The choice\n    of the default function may change from version to version and\n    should not be relied on. Default value: None.\n\nauxiliary : NetworkX DiGraph\n    Auxiliary digraph to compute flow based node connectivity. It has\n    to have a graph attribute called mapping with a dictionary mapping\n    node names in G and in the auxiliary digraph. If provided\n    it will be reused instead of recreated. Default value: None.\n\nresidual : NetworkX DiGraph\n    Residual network to compute maximum flow. If provided it will be\n    reused instead of recreated. Default value: None.\n\nReturns\n-------\ncutset : set\n    Set of nodes that, if removed, would destroy all paths between\n    source and target in G.\n\nExamples\n--------\nThis function is not imported in the base NetworkX namespace, so you\nhave to explicitly import it from the connectivity package:\n\n>>> from networkx.algorithms.connectivity import minimum_st_node_cut\n\nWe use in this example the platonic icosahedral graph, which has node\nconnectivity 5.\n\n>>> G = nx.icosahedral_graph()\n>>> len(minimum_st_node_cut(G, 0, 6))\n5\n\nIf you need to compute local st cuts between several pairs of\nnodes in the same graph, it is recommended that you reuse the\ndata structures that NetworkX uses in the computation: the\nauxiliary digraph for node connectivity and node cuts, and the\nresidual network for the underlying maximum flow computation.\n\nExample of how to compute local st node cuts reusing the data\nstructures:\n\n>>> # You also have to explicitly import the function for\n>>> # building the auxiliary digraph from the connectivity package\n>>> from networkx.algorithms.connectivity import build_auxiliary_node_connectivity\n>>> H = build_auxiliary_node_connectivity(G)\n>>> # And the function for building the residual network from the\n>>> # flow package\n>>> from networkx.algorithms.flow import build_residual_network\n>>> # Note that the auxiliary digraph has an edge attribute named capacity\n>>> R = build_residual_network(H, \"capacity\")\n>>> # Reuse the auxiliary digraph and the residual network by passing them\n>>> # as parameters\n>>> len(minimum_st_node_cut(G, 0, 6, auxiliary=H, residual=R))\n5\n\nYou can also use alternative flow algorithms for computing minimum st\nnode cuts. For instance, in dense networks the algorithm\n:meth:`shortest_augmenting_path` will usually perform better than\nthe default :meth:`edmonds_karp` which is faster for sparse\nnetworks with highly skewed degree distributions. Alternative flow\nfunctions have to be explicitly imported from the flow package.\n\n>>> from networkx.algorithms.flow import shortest_augmenting_path\n>>> len(minimum_st_node_cut(G, 0, 6, flow_func=shortest_augmenting_path))\n5\n\n",
  "code": "@nx._dispatch(graphs={'G': 0, 'auxiliary?': 4, 'residual?': 5}, preserve_edge_attrs={'residual': {'capacity': float('inf')}}, preserve_node_attrs={'auxiliary': {'id': None}}, preserve_graph_attrs={'auxiliary', 'residual'})\ndef minimum_st_node_cut(G, s, t, flow_func=None, auxiliary=None, residual=None):\n    if auxiliary is None:\n        H = build_auxiliary_node_connectivity(G)\n    else:\n        H = auxiliary\n    mapping = H.graph.get('mapping', None)\n    if mapping is None:\n        raise nx.NetworkXError('Invalid auxiliary digraph.')\n    if G.has_edge(s, t) or G.has_edge(t, s):\n        return {}\n    kwargs = {'flow_func': flow_func, 'residual': residual, 'auxiliary': H}\n    edge_cut = minimum_st_edge_cut(H, f'{mapping[s]}B', f'{mapping[t]}A', **kwargs)\n    node_cut = {H.nodes[node]['id'] for edge in edge_cut for node in edge}\n    return node_cut - {s, t}"
 },
 {
  "docstring": "Returns a set of nodes of minimum cardinality that disconnects G.\n\nIf source and target nodes are provided, this function returns the\nset of nodes of minimum cardinality that, if removed, would destroy\nall paths among source and target in G. If not, it returns a set\nof nodes of minimum cardinality that disconnects G.\n\nParameters\n----------\nG : NetworkX graph\n\ns : node\n    Source node. Optional. Default value: None.\n\nt : node\n    Target node. Optional. Default value: None.\n\nflow_func : function\n    A function for computing the maximum flow among a pair of nodes.\n    The function has to accept at least three parameters: a Digraph,\n    a source node, and a target node. And return a residual network\n    that follows NetworkX conventions (see :meth:`maximum_flow` for\n    details). If flow_func is None, the default maximum flow function\n    (:meth:`edmonds_karp`) is used. See below for details. The\n    choice of the default function may change from version\n    to version and should not be relied on. Default value: None.\n\nReturns\n-------\ncutset : set\n    Set of nodes that, if removed, would disconnect G. If source\n    and target nodes are provided, the set contains the nodes that\n    if removed, would destroy all paths between source and target.\n\nExamples\n--------\n>>> # Platonic icosahedral graph has node connectivity 5\n>>> G = nx.icosahedral_graph()\n>>> node_cut = nx.minimum_node_cut(G)\n>>> len(node_cut)\n5\n\nYou can use alternative flow algorithms for the underlying maximum\nflow computation. In dense networks the algorithm\n:meth:`shortest_augmenting_path` will usually perform better\nthan the default :meth:`edmonds_karp`, which is faster for\nsparse networks with highly skewed degree distributions. Alternative\nflow functions have to be explicitly imported from the flow package.\n\n>>> from networkx.algorithms.flow import shortest_augmenting_path\n>>> node_cut == nx.minimum_node_cut(G, flow_func=shortest_augmenting_path)\nTrue\n\nIf you specify a pair of nodes (source and target) as parameters,\nthis function returns a local st node cut.\n\n>>> len(nx.minimum_node_cut(G, 3, 7))\n5\n\nIf you need to perform several local st cuts among different\npairs of nodes on the same graph, it is recommended that you reuse\nthe data structures used in the maximum flow computations. See\n:meth:`minimum_st_node_cut` for details.\n\n",
  "code": "@nx._dispatch\ndef minimum_node_cut(G, s=None, t=None, flow_func=None):\n    if s is not None and t is None or (s is None and t is not None):\n        raise nx.NetworkXError('Both source and target must be specified.')\n    if s is not None and t is not None:\n        if s not in G:\n            raise nx.NetworkXError(f'node {s} not in graph')\n        if t not in G:\n            raise nx.NetworkXError(f'node {t} not in graph')\n        return minimum_st_node_cut(G, s, t, flow_func=flow_func)\n    if G.is_directed():\n        if not nx.is_weakly_connected(G):\n            raise nx.NetworkXError('Input graph is not connected')\n        iter_func = itertools.permutations\n\n        def neighbors(v):\n            return itertools.chain.from_iterable([G.predecessors(v), G.successors(v)])\n    else:\n        if not nx.is_connected(G):\n            raise nx.NetworkXError('Input graph is not connected')\n        iter_func = itertools.combinations\n        neighbors = G.neighbors\n    H = build_auxiliary_node_connectivity(G)\n    R = build_residual_network(H, 'capacity')\n    kwargs = {'flow_func': flow_func, 'auxiliary': H, 'residual': R}\n    v = min(G, key=G.degree)\n    min_cut = set(G[v])\n    for w in set(G) - set(neighbors(v)) - {v}:\n        this_cut = minimum_st_node_cut(G, v, w, **kwargs)\n        if len(min_cut) >= len(this_cut):\n            min_cut = this_cut\n    for x, y in iter_func(neighbors(v), 2):\n        if y in G[x]:\n            continue\n        this_cut = minimum_st_node_cut(G, x, y, **kwargs)\n        if len(min_cut) >= len(this_cut):\n            min_cut = this_cut\n    return min_cut"
 },
 {
  "docstring": "Returns a set of edges of minimum cardinality that disconnects G.\n\nIf source and target nodes are provided, this function returns the\nset of edges of minimum cardinality that, if removed, would break\nall paths among source and target in G. If not, it returns a set of\nedges of minimum cardinality that disconnects G.\n\nParameters\n----------\nG : NetworkX graph\n\ns : node\n    Source node. Optional. Default value: None.\n\nt : node\n    Target node. Optional. Default value: None.\n\nflow_func : function\n    A function for computing the maximum flow among a pair of nodes.\n    The function has to accept at least three parameters: a Digraph,\n    a source node, and a target node. And return a residual network\n    that follows NetworkX conventions (see :meth:`maximum_flow` for\n    details). If flow_func is None, the default maximum flow function\n    (:meth:`edmonds_karp`) is used. See below for details. The\n    choice of the default function may change from version\n    to version and should not be relied on. Default value: None.\n\nReturns\n-------\ncutset : set\n    Set of edges that, if removed, would disconnect G. If source\n    and target nodes are provided, the set contains the edges that\n    if removed, would destroy all paths between source and target.\n\nExamples\n--------\n>>> # Platonic icosahedral graph has edge connectivity 5\n>>> G = nx.icosahedral_graph()\n>>> len(nx.minimum_edge_cut(G))\n5\n\nYou can use alternative flow algorithms for the underlying\nmaximum flow computation. In dense networks the algorithm\n:meth:`shortest_augmenting_path` will usually perform better\nthan the default :meth:`edmonds_karp`, which is faster for\nsparse networks with highly skewed degree distributions.\nAlternative flow functions have to be explicitly imported\nfrom the flow package.\n\n>>> from networkx.algorithms.flow import shortest_augmenting_path\n>>> len(nx.minimum_edge_cut(G, flow_func=shortest_augmenting_path))\n5\n\nIf you specify a pair of nodes (source and target) as parameters,\nthis function returns the value of local edge connectivity.\n\n>>> nx.edge_connectivity(G, 3, 7)\n5\n\nIf you need to perform several local computations among different\npairs of nodes on the same graph, it is recommended that you reuse\nthe data structures used in the maximum flow computations. See\n:meth:`local_edge_connectivity` for details.\n\n",
  "code": "@nx._dispatch\ndef minimum_edge_cut(G, s=None, t=None, flow_func=None):\n    if s is not None and t is None or (s is None and t is not None):\n        raise nx.NetworkXError('Both source and target must be specified.')\n    H = build_auxiliary_edge_connectivity(G)\n    R = build_residual_network(H, 'capacity')\n    kwargs = {'flow_func': flow_func, 'residual': R, 'auxiliary': H}\n    if s is not None and t is not None:\n        if s not in G:\n            raise nx.NetworkXError(f'node {s} not in graph')\n        if t not in G:\n            raise nx.NetworkXError(f'node {t} not in graph')\n        return minimum_st_edge_cut(H, s, t, **kwargs)\n    if G.is_directed():\n        if not nx.is_weakly_connected(G):\n            raise nx.NetworkXError('Input graph is not connected')\n        node = min(G, key=G.degree)\n        min_cut = set(G.edges(node))\n        nodes = list(G)\n        n = len(nodes)\n        for i in range(n):\n            try:\n                this_cut = minimum_st_edge_cut(H, nodes[i], nodes[i + 1], **kwargs)\n                if len(this_cut) <= len(min_cut):\n                    min_cut = this_cut\n            except IndexError:\n                this_cut = minimum_st_edge_cut(H, nodes[i], nodes[0], **kwargs)\n                if len(this_cut) <= len(min_cut):\n                    min_cut = this_cut\n        return min_cut\n    else:\n        if not nx.is_connected(G):\n            raise nx.NetworkXError('Input graph is not connected')\n        node = min(G, key=G.degree)\n        min_cut = set(G.edges(node))\n        for node in G:\n            D = nx.dominating_set(G, start_with=node)\n            v = D.pop()\n            if D:\n                break\n        else:\n            return min_cut\n        for w in D:\n            this_cut = minimum_st_edge_cut(H, v, w, **kwargs)\n            if len(this_cut) <= len(min_cut):\n                min_cut = this_cut\n        return min_cut"
 },
 {
  "docstring": "Returns the edges disjoint paths between source and target.\n\nEdge disjoint paths are paths that do not share any edge. The\nnumber of edge disjoint paths between source and target is equal\nto their edge connectivity.\n\nParameters\n----------\nG : NetworkX graph\n\ns : node\n    Source node for the flow.\n\nt : node\n    Sink node for the flow.\n\nflow_func : function\n    A function for computing the maximum flow among a pair of nodes.\n    The function has to accept at least three parameters: a Digraph,\n    a source node, and a target node. And return a residual network\n    that follows NetworkX conventions (see :meth:`maximum_flow` for\n    details). If flow_func is None, the default maximum flow function\n    (:meth:`edmonds_karp`) is used. The choice of the default function\n    may change from version to version and should not be relied on.\n    Default value: None.\n\ncutoff : integer or None (default: None)\n    Maximum number of paths to yield. If specified, the maximum flow\n    algorithm will terminate when the flow value reaches or exceeds the\n    cutoff. This only works for flows that support the cutoff parameter\n    (most do) and is ignored otherwise.\n\nauxiliary : NetworkX DiGraph\n    Auxiliary digraph to compute flow based edge connectivity. It has\n    to have a graph attribute called mapping with a dictionary mapping\n    node names in G and in the auxiliary digraph. If provided\n    it will be reused instead of recreated. Default value: None.\n\nresidual : NetworkX DiGraph\n    Residual network to compute maximum flow. If provided it will be\n    reused instead of recreated. Default value: None.\n\nReturns\n-------\npaths : generator\n    A generator of edge independent paths.\n\nRaises\n------\nNetworkXNoPath\n    If there is no path between source and target.\n\nNetworkXError\n    If source or target are not in the graph G.\n\n",
  "code": "@nx._dispatch(graphs={'G': 0, 'auxiliary?': 5, 'residual?': 6}, preserve_edge_attrs={'auxiliary': {'capacity': float('inf')}, 'residual': {'capacity': float('inf')}}, preserve_graph_attrs={'residual'})\ndef edge_disjoint_paths(G, s, t, flow_func=None, cutoff=None, auxiliary=None, residual=None):\n    if s not in G:\n        raise nx.NetworkXError(f'node {s} not in graph')\n    if t not in G:\n        raise nx.NetworkXError(f'node {t} not in graph')\n    if flow_func is None:\n        flow_func = default_flow_func\n    if auxiliary is None:\n        H = build_auxiliary_edge_connectivity(G)\n    else:\n        H = auxiliary\n    possible = min(H.out_degree(s), H.in_degree(t))\n    if not possible:\n        raise NetworkXNoPath\n    if cutoff is None:\n        cutoff = possible\n    else:\n        cutoff = min(cutoff, possible)\n    kwargs = {'capacity': 'capacity', 'residual': residual, 'cutoff': cutoff, 'value_only': True}\n    if flow_func is preflow_push:\n        del kwargs['cutoff']\n    if flow_func is shortest_augmenting_path:\n        kwargs['two_phase'] = True\n    R = flow_func(H, s, t, **kwargs)\n    if R.graph['flow_value'] == 0:\n        raise NetworkXNoPath\n    cutset = [(u, v) for u, v, d in R.edges(data=True) if d['capacity'] == d['flow'] and d['flow'] > 0]\n    flow_dict = {n: {} for edge in cutset for n in edge}\n    for u, v in cutset:\n        flow_dict[u][v] = 1\n    paths_found = 0\n    for v in list(flow_dict[s]):\n        if paths_found >= cutoff:\n            break\n        path = [s]\n        if v == t:\n            path.append(v)\n            yield path\n            continue\n        u = v\n        while u != t:\n            path.append(u)\n            try:\n                u, _ = flow_dict[u].popitem()\n            except KeyError:\n                break\n        else:\n            path.append(t)\n            yield path\n            paths_found += 1"
 },
 {
  "docstring": "Computes node disjoint paths between source and target.\n\nNode disjoint paths are paths that only share their first and last\nnodes. The number of node independent paths between two nodes is\nequal to their local node connectivity.\n\nParameters\n----------\nG : NetworkX graph\n\ns : node\n    Source node.\n\nt : node\n    Target node.\n\nflow_func : function\n    A function for computing the maximum flow among a pair of nodes.\n    The function has to accept at least three parameters: a Digraph,\n    a source node, and a target node. And return a residual network\n    that follows NetworkX conventions (see :meth:`maximum_flow` for\n    details). If flow_func is None, the default maximum flow function\n    (:meth:`edmonds_karp`) is used. See below for details. The choice\n    of the default function may change from version to version and\n    should not be relied on. Default value: None.\n\ncutoff : integer or None (default: None)\n    Maximum number of paths to yield. If specified, the maximum flow\n    algorithm will terminate when the flow value reaches or exceeds the\n    cutoff. This only works for flows that support the cutoff parameter\n    (most do) and is ignored otherwise.\n\nauxiliary : NetworkX DiGraph\n    Auxiliary digraph to compute flow based node connectivity. It has\n    to have a graph attribute called mapping with a dictionary mapping\n    node names in G and in the auxiliary digraph. If provided\n    it will be reused instead of recreated. Default value: None.\n\nresidual : NetworkX DiGraph\n    Residual network to compute maximum flow. If provided it will be\n    reused instead of recreated. Default value: None.\n\nReturns\n-------\npaths : generator\n    Generator of node disjoint paths.\n\nRaises\n------\nNetworkXNoPath\n    If there is no path between source and target.\n\nNetworkXError\n    If source or target are not in the graph G.\n\nExamples\n--------\nWe use in this example the platonic icosahedral graph, which has node\nconnectivity 5, thus there are 5 node disjoint paths between any pair\nof non neighbor nodes.\n\n>>> G = nx.icosahedral_graph()\n>>> len(list(nx.node_disjoint_paths(G, 0, 6)))\n5\n\nIf you need to compute node disjoint paths between several pairs of\nnodes in the same graph, it is recommended that you reuse the\ndata structures that NetworkX uses in the computation: the\nauxiliary digraph for node connectivity and node cuts, and the\nresidual network for the underlying maximum flow computation.\n\nExample of how to compute node disjoint paths reusing the data\nstructures:\n\n>>> # You also have to explicitly import the function for\n>>> # building the auxiliary digraph from the connectivity package\n>>> from networkx.algorithms.connectivity import build_auxiliary_node_connectivity\n>>> H = build_auxiliary_node_connectivity(G)\n>>> # And the function for building the residual network from the\n>>> # flow package\n>>> from networkx.algorithms.flow import build_residual_network\n>>> # Note that the auxiliary digraph has an edge attribute named capacity\n>>> R = build_residual_network(H, \"capacity\")\n>>> # Reuse the auxiliary digraph and the residual network by passing them\n>>> # as arguments\n>>> len(list(nx.node_disjoint_paths(G, 0, 6, auxiliary=H, residual=R)))\n5\n\nYou can also use alternative flow algorithms for computing node disjoint\npaths. For instance, in dense networks the algorithm\n:meth:`shortest_augmenting_path` will usually perform better than\nthe default :meth:`edmonds_karp` which is faster for sparse\nnetworks with highly skewed degree distributions. Alternative flow\nfunctions have to be explicitly imported from the flow package.\n\n>>> from networkx.algorithms.flow import shortest_augmenting_path\n>>> len(list(nx.node_disjoint_paths(G, 0, 6, flow_func=shortest_augmenting_path)))\n5\n\n",
  "code": "@nx._dispatch(graphs={'G': 0, 'auxiliary?': 5, 'residual?': 6}, preserve_edge_attrs={'residual': {'capacity': float('inf')}}, preserve_node_attrs={'auxiliary': {'id': None}}, preserve_graph_attrs={'auxiliary', 'residual'})\ndef node_disjoint_paths(G, s, t, flow_func=None, cutoff=None, auxiliary=None, residual=None):\n    if s not in G:\n        raise nx.NetworkXError(f'node {s} not in graph')\n    if t not in G:\n        raise nx.NetworkXError(f'node {t} not in graph')\n    if auxiliary is None:\n        H = build_auxiliary_node_connectivity(G)\n    else:\n        H = auxiliary\n    mapping = H.graph.get('mapping', None)\n    if mapping is None:\n        raise nx.NetworkXError('Invalid auxiliary digraph.')\n    possible = min(H.out_degree(f'{mapping[s]}B'), H.in_degree(f'{mapping[t]}A'))\n    if not possible:\n        raise NetworkXNoPath\n    if cutoff is None:\n        cutoff = possible\n    else:\n        cutoff = min(cutoff, possible)\n    kwargs = {'flow_func': flow_func, 'residual': residual, 'auxiliary': H, 'cutoff': cutoff}\n    paths_edges = edge_disjoint_paths(H, f'{mapping[s]}B', f'{mapping[t]}A', **kwargs)\n    for path in paths_edges:\n        yield list(_unique_everseen((H.nodes[node]['id'] for node in path)))"
 },
 {
  "docstring": "List unique elements, preserving order. Remember all elements ever seen.",
  "code": "def _unique_everseen(iterable):\n    seen = set()\n    seen_add = seen.add\n    for element in _filterfalse(seen.__contains__, iterable):\n        seen_add(element)\n        yield element"
 },
 {
  "docstring": "Tests to see if a graph is k-edge-connected.\n\nIs it impossible to disconnect the graph by removing fewer than k edges?\nIf so, then G is k-edge-connected.\n\nParameters\n----------\nG : NetworkX graph\n   An undirected graph.\n\nk : integer\n    edge connectivity to test for\n\nReturns\n-------\nboolean\n    True if G is k-edge-connected.\n\nSee Also\n--------\n:func:`is_locally_k_edge_connected`\n\nExamples\n--------\n>>> G = nx.barbell_graph(10, 0)\n>>> nx.is_k_edge_connected(G, k=1)\nTrue\n>>> nx.is_k_edge_connected(G, k=2)\nFalse",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef is_k_edge_connected(G, k):\n    if k < 1:\n        raise ValueError(f'k must be positive, not {k}')\n    if G.number_of_nodes() < k + 1:\n        return False\n    elif any((d < k for n, d in G.degree())):\n        return False\n    elif k == 1:\n        return nx.is_connected(G)\n    elif k == 2:\n        return nx.is_connected(G) and (not nx.has_bridges(G))\n    else:\n        return nx.edge_connectivity(G, cutoff=k) >= k"
 },
 {
  "docstring": "Tests to see if an edge in a graph is locally k-edge-connected.\n\nIs it impossible to disconnect s and t by removing fewer than k edges?\nIf so, then s and t are locally k-edge-connected in G.\n\nParameters\n----------\nG : NetworkX graph\n   An undirected graph.\n\ns : node\n    Source node\n\nt : node\n    Target node\n\nk : integer\n    local edge connectivity for nodes s and t\n\nReturns\n-------\nboolean\n    True if s and t are locally k-edge-connected in G.\n\nSee Also\n--------\n:func:`is_k_edge_connected`\n\nExamples\n--------\n>>> from networkx.algorithms.connectivity import is_locally_k_edge_connected\n>>> G = nx.barbell_graph(10, 0)\n>>> is_locally_k_edge_connected(G, 5, 15, k=1)\nTrue\n>>> is_locally_k_edge_connected(G, 5, 15, k=2)\nFalse\n>>> is_locally_k_edge_connected(G, 1, 5, k=2)\nTrue",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef is_locally_k_edge_connected(G, s, t, k):\n    if k < 1:\n        raise ValueError(f'k must be positive, not {k}')\n    if G.degree(s) < k or G.degree(t) < k:\n        return False\n    elif k == 1:\n        return nx.has_path(G, s, t)\n    else:\n        localk = nx.connectivity.local_edge_connectivity(G, s, t, cutoff=k)\n        return localk >= k"
 },
 {
  "docstring": "Finds set of edges to k-edge-connect G.\n\nAdding edges from the augmentation to G make it impossible to disconnect G\nunless k or more edges are removed. This function uses the most efficient\nfunction available (depending on the value of k and if the problem is\nweighted or unweighted) to search for a minimum weight subset of available\nedges that k-edge-connects G. In general, finding a k-edge-augmentation is\nNP-hard, so solutions are not guaranteed to be minimal. Furthermore, a\nk-edge-augmentation may not exist.\n\nParameters\n----------\nG : NetworkX graph\n   An undirected graph.\n\nk : integer\n    Desired edge connectivity\n\navail : dict or a set of 2 or 3 tuples\n    The available edges that can be used in the augmentation.\n\n    If unspecified, then all edges in the complement of G are available.\n    Otherwise, each item is an available edge (with an optional weight).\n\n    In the unweighted case, each item is an edge ``(u, v)``.\n\n    In the weighted case, each item is a 3-tuple ``(u, v, d)`` or a dict\n    with items ``(u, v): d``.  The third item, ``d``, can be a dictionary\n    or a real number.  If ``d`` is a dictionary ``d[weight]``\n    correspondings to the weight.\n\nweight : string\n    key to use to find weights if ``avail`` is a set of 3-tuples where the\n    third item in each tuple is a dictionary.\n\npartial : boolean\n    If partial is True and no feasible k-edge-augmentation exists, then all\n    a partial k-edge-augmentation is generated. Adding the edges in a\n    partial augmentation to G, minimizes the number of k-edge-connected\n    components and maximizes the edge connectivity between those\n    components. For details, see :func:`partial_k_edge_augmentation`.\n\nYields\n------\nedge : tuple\n    Edges that, once added to G, would cause G to become k-edge-connected.\n    If partial is False, an error is raised if this is not possible.\n    Otherwise, generated edges form a partial augmentation, which\n    k-edge-connects any part of G where it is possible, and maximally\n    connects the remaining parts.\n\nRaises\n------\nNetworkXUnfeasible\n    If partial is False and no k-edge-augmentation exists.\n\nNetworkXNotImplemented\n    If the input graph is directed or a multigraph.\n\nValueError:\n    If k is less than 1\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef k_edge_augmentation(G, k, avail=None, weight=None, partial=False):\n    try:\n        if k <= 0:\n            raise ValueError(f'k must be a positive integer, not {k}')\n        elif G.number_of_nodes() < k + 1:\n            msg = f'impossible to {k} connect in graph with less than {k + 1} nodes'\n            raise nx.NetworkXUnfeasible(msg)\n        elif avail is not None and len(avail) == 0:\n            if not nx.is_k_edge_connected(G, k):\n                raise nx.NetworkXUnfeasible('no available edges')\n            aug_edges = []\n        elif k == 1:\n            aug_edges = one_edge_augmentation(G, avail=avail, weight=weight, partial=partial)\n        elif k == 2:\n            aug_edges = bridge_augmentation(G, avail=avail, weight=weight)\n        else:\n            aug_edges = greedy_k_edge_augmentation(G, k=k, avail=avail, weight=weight, seed=0)\n        yield from list(aug_edges)\n    except nx.NetworkXUnfeasible:\n        if partial:\n            if avail is None:\n                aug_edges = complement_edges(G)\n            else:\n                aug_edges = partial_k_edge_augmentation(G, k=k, avail=avail, weight=weight)\n            yield from aug_edges\n        else:\n            raise"
 },
 {
  "docstring": "Finds augmentation that k-edge-connects as much of the graph as possible.\n\nWhen a k-edge-augmentation is not possible, we can still try to find a\nsmall set of edges that partially k-edge-connects as much of the graph as\npossible. All possible edges are generated between remaining parts.\nThis minimizes the number of k-edge-connected subgraphs in the resulting\ngraph and maximizes the edge connectivity between those subgraphs.\n\nParameters\n----------\nG : NetworkX graph\n   An undirected graph.\n\nk : integer\n    Desired edge connectivity\n\navail : dict or a set of 2 or 3 tuples\n    For more details, see :func:`k_edge_augmentation`.\n\nweight : string\n    key to use to find weights if ``avail`` is a set of 3-tuples.\n    For more details, see :func:`k_edge_augmentation`.\n\nYields\n------\nedge : tuple\n    Edges in the partial augmentation of G. These edges k-edge-connect any\n    part of G where it is possible, and maximally connects the remaining\n    parts. In other words, all edges from avail are generated except for\n    those within subgraphs that have already become k-edge-connected.\n\n",
  "code": "@nx._dispatch\ndef partial_k_edge_augmentation(G, k, avail, weight=None):\n\n    def _edges_between_disjoint(H, only1, only2):\n        \"\"\"finds edges between disjoint nodes\"\"\"\n        only1_adj = {u: set(H.adj[u]) for u in only1}\n        for u, neighbs in only1_adj.items():\n            neighbs12 = neighbs.intersection(only2)\n            for v in neighbs12:\n                yield (u, v)\n    avail_uv, avail_w = _unpack_available_edges(avail, weight=weight, G=G)\n    H = G.copy()\n    H.add_edges_from(((u, v, {'weight': w, 'generator': (u, v)}) for (u, v), w in zip(avail, avail_w)))\n    k_edge_subgraphs = list(nx.k_edge_subgraphs(H, k=k))\n    for nodes in k_edge_subgraphs:\n        if len(nodes) > 1:\n            C = H.subgraph(nodes).copy()\n            sub_avail = {d['generator']: d['weight'] for u, v, d in C.edges(data=True) if 'generator' in d}\n            C.remove_edges_from(sub_avail.keys())\n            yield from nx.k_edge_augmentation(C, k=k, avail=sub_avail)\n    for cc1, cc2 in it.combinations(k_edge_subgraphs, 2):\n        for u, v in _edges_between_disjoint(H, cc1, cc2):\n            d = H.get_edge_data(u, v)\n            edge = d.get('generator', None)\n            if edge is not None:\n                yield edge"
 },
 {
  "docstring": "Finds minimum weight set of edges to connect G.\n\nEquivalent to :func:`k_edge_augmentation` when k=1. Adding the resulting\nedges to G will make it 1-edge-connected. The solution is optimal for both\nweighted and non-weighted variants.\n\nParameters\n----------\nG : NetworkX graph\n   An undirected graph.\n\navail : dict or a set of 2 or 3 tuples\n    For more details, see :func:`k_edge_augmentation`.\n\nweight : string\n    key to use to find weights if ``avail`` is a set of 3-tuples.\n    For more details, see :func:`k_edge_augmentation`.\n\npartial : boolean\n    If partial is True and no feasible k-edge-augmentation exists, then the\n    augmenting edges minimize the number of connected components.\n\nYields\n------\nedge : tuple\n    Edges in the one-augmentation of G\n\nRaises\n------\nNetworkXUnfeasible\n    If partial is False and no one-edge-augmentation exists.\n\n",
  "code": "@not_implemented_for('multigraph')\n@not_implemented_for('directed')\n@nx._dispatch\ndef one_edge_augmentation(G, avail=None, weight=None, partial=False):\n    if avail is None:\n        return unconstrained_one_edge_augmentation(G)\n    else:\n        return weighted_one_edge_augmentation(G, avail=avail, weight=weight, partial=partial)"
 },
 {
  "docstring": "Finds the a set of edges that bridge connects G.\n\nEquivalent to :func:`k_edge_augmentation` when k=2, and partial=False.\nAdding the resulting edges to G will make it 2-edge-connected.  If no\nconstraints are specified the returned set of edges is minimum an optimal,\notherwise the solution is approximated.\n\nParameters\n----------\nG : NetworkX graph\n   An undirected graph.\n\navail : dict or a set of 2 or 3 tuples\n    For more details, see :func:`k_edge_augmentation`.\n\nweight : string\n    key to use to find weights if ``avail`` is a set of 3-tuples.\n    For more details, see :func:`k_edge_augmentation`.\n\nYields\n------\nedge : tuple\n    Edges in the bridge-augmentation of G\n\nRaises\n------\nNetworkXUnfeasible\n    If no bridge-augmentation exists.\n\n",
  "code": "@not_implemented_for('multigraph')\n@not_implemented_for('directed')\n@nx._dispatch\ndef bridge_augmentation(G, avail=None, weight=None):\n    if G.number_of_nodes() < 3:\n        raise nx.NetworkXUnfeasible('impossible to bridge connect less than 3 nodes')\n    if avail is None:\n        return unconstrained_bridge_augmentation(G)\n    else:\n        return weighted_bridge_augmentation(G, avail, weight=weight)"
 },
 {
  "docstring": "Returns the nodes in an undirected edge in lower-triangular order",
  "code": "def _ordered(u, v):\n    return (u, v) if u < v else (v, u)"
 },
 {
  "docstring": "Helper to separate avail into edges and corresponding weights",
  "code": "def _unpack_available_edges(avail, weight=None, G=None):\n    if weight is None:\n        weight = 'weight'\n    if isinstance(avail, dict):\n        avail_uv = list(avail.keys())\n        avail_w = list(avail.values())\n    else:\n\n        def _try_getitem(d):\n            try:\n                return d[weight]\n            except TypeError:\n                return d\n        avail_uv = [tup[0:2] for tup in avail]\n        avail_w = [1 if len(tup) == 2 else _try_getitem(tup[-1]) for tup in avail]\n    if G is not None:\n        flags = [not G.has_edge(u, v) for u, v in avail_uv]\n        avail_uv = list(it.compress(avail_uv, flags))\n        avail_w = list(it.compress(avail_w, flags))\n    return (avail_uv, avail_w)"
 },
 {
  "docstring": "Maps available edges in the original graph to edges in the metagraph.\n\nParameters\n----------\nmapping : dict\n    mapping produced by :func:`collapse`, that maps each node in the\n    original graph to a node in the meta graph\n\navail_uv : list\n    list of edges\n\navail_w : list\n    list of edge weights\n\n",
  "code": "def _lightest_meta_edges(mapping, avail_uv, avail_w):\n    grouped_wuv = defaultdict(list)\n    for w, (u, v) in zip(avail_w, avail_uv):\n        meta_uv = _ordered(mapping[u], mapping[v])\n        grouped_wuv[meta_uv].append((w, u, v))\n    for (mu, mv), choices_wuv in grouped_wuv.items():\n        if mu != mv:\n            w, u, v = min(choices_wuv)\n            yield MetaEdge((mu, mv), (u, v), w)"
 },
 {
  "docstring": "Finds the smallest set of edges to connect G.\n\nThis is a variant of the unweighted MST problem.\nIf G is not empty, a feasible solution always exists.\n\nParameters\n----------\nG : NetworkX graph\n   An undirected graph.\n\nYields\n------\nedge : tuple\n    Edges in the one-edge-augmentation of G\n\nSee Also\n--------\n:func:`one_edge_augmentation`\n:func:`k_edge_augmentation`\n\nExamples\n--------\n>>> G = nx.Graph([(1, 2), (2, 3), (4, 5)])\n>>> G.add_nodes_from([6, 7, 8])\n>>> sorted(unconstrained_one_edge_augmentation(G))\n[(1, 4), (4, 6), (6, 7), (7, 8)]",
  "code": "@nx._dispatch\ndef unconstrained_one_edge_augmentation(G):\n    ccs1 = list(nx.connected_components(G))\n    C = collapse(G, ccs1)\n    meta_nodes = list(C.nodes())\n    meta_aug = list(zip(meta_nodes, meta_nodes[1:]))\n    inverse = defaultdict(list)\n    for k, v in C.graph['mapping'].items():\n        inverse[v].append(k)\n    for mu, mv in meta_aug:\n        yield (inverse[mu][0], inverse[mv][0])"
 },
 {
  "docstring": "Finds the minimum weight set of edges to connect G if one exists.\n\nThis is a variant of the weighted MST problem.\n\nParameters\n----------\nG : NetworkX graph\n   An undirected graph.\n\navail : dict or a set of 2 or 3 tuples\n    For more details, see :func:`k_edge_augmentation`.\n\nweight : string\n    key to use to find weights if ``avail`` is a set of 3-tuples.\n    For more details, see :func:`k_edge_augmentation`.\n\npartial : boolean\n    If partial is True and no feasible k-edge-augmentation exists, then the\n    augmenting edges minimize the number of connected components.\n\nYields\n------\nedge : tuple\n    Edges in the subset of avail chosen to connect G.\n\nSee Also\n--------\n:func:`one_edge_augmentation`\n:func:`k_edge_augmentation`\n\nExamples\n--------\n>>> G = nx.Graph([(1, 2), (2, 3), (4, 5)])\n>>> G.add_nodes_from([6, 7, 8])\n>>> # any edge not in avail has an implicit weight of infinity\n>>> avail = [(1, 3), (1, 5), (4, 7), (4, 8), (6, 1), (8, 1), (8, 2)]\n>>> sorted(weighted_one_edge_augmentation(G, avail))\n[(1, 5), (4, 7), (6, 1), (8, 1)]\n>>> # find another solution by giving large weights to edges in the\n>>> # previous solution (note some of the old edges must be used)\n>>> avail = [(1, 3), (1, 5, 99), (4, 7, 9), (6, 1, 99), (8, 1, 99), (8, 2)]\n>>> sorted(weighted_one_edge_augmentation(G, avail))\n[(1, 5), (4, 7), (6, 1), (8, 2)]",
  "code": "@nx._dispatch\ndef weighted_one_edge_augmentation(G, avail, weight=None, partial=False):\n    avail_uv, avail_w = _unpack_available_edges(avail, weight=weight, G=G)\n    C = collapse(G, nx.connected_components(G))\n    mapping = C.graph['mapping']\n    candidate_mapping = _lightest_meta_edges(mapping, avail_uv, avail_w)\n    C.add_edges_from(((mu, mv, {'weight': w, 'generator': uv}) for (mu, mv), uv, w in candidate_mapping))\n    meta_mst = nx.minimum_spanning_tree(C)\n    if not partial and (not nx.is_connected(meta_mst)):\n        raise nx.NetworkXUnfeasible('Not possible to connect G with available edges')\n    for mu, mv, d in meta_mst.edges(data=True):\n        if 'generator' in d:\n            edge = d['generator']\n            yield edge"
 },
 {
  "docstring": "Finds an optimal 2-edge-augmentation of G using the fewest edges.\n\nThis is an implementation of the algorithm detailed in [1]_.\nThe basic idea is to construct a meta-graph of bridge-ccs, connect leaf\nnodes of the trees to connect the entire graph, and finally connect the\nleafs of the tree in dfs-preorder to bridge connect the entire graph.\n\nParameters\n----------\nG : NetworkX graph\n   An undirected graph.\n\nYields\n------\nedge : tuple\n    Edges in the bridge augmentation of G\n\n",
  "code": "@nx._dispatch\ndef unconstrained_bridge_augmentation(G):\n    bridge_ccs = list(nx.connectivity.bridge_components(G))\n    C = collapse(G, bridge_ccs)\n    vset1 = [tuple(cc) * 2 if len(cc) == 1 else sorted(cc, key=C.degree)[0:2] for cc in nx.connected_components(C)]\n    if len(vset1) > 1:\n        nodes1 = [vs[0] for vs in vset1]\n        nodes2 = [vs[1] for vs in vset1]\n        A1 = list(zip(nodes1[1:], nodes2))\n    else:\n        A1 = []\n    T = C.copy()\n    T.add_edges_from(A1)\n    leafs = [n for n, d in T.degree() if d == 1]\n    if len(leafs) == 1:\n        A2 = []\n    if len(leafs) == 2:\n        A2 = [tuple(leafs)]\n    else:\n        try:\n            root = next((n for n, d in T.degree() if d > 1))\n        except StopIteration:\n            return\n        v2 = [n for n in nx.dfs_preorder_nodes(T, root) if T.degree(n) == 1]\n        half = math.ceil(len(v2) / 2)\n        A2 = list(zip(v2[:half], v2[-half:]))\n    aug_tree_edges = A1 + A2\n    inverse = defaultdict(list)\n    for k, v in C.graph['mapping'].items():\n        inverse[v].append(k)\n    inverse = {mu: sorted(mapped, key=lambda u: (G.degree(u), u)) for mu, mapped in inverse.items()}\n    G2 = G.copy()\n    for mu, mv in aug_tree_edges:\n        for u, v in it.product(inverse[mu], inverse[mv]):\n            if not G2.has_edge(u, v):\n                G2.add_edge(u, v)\n                yield (u, v)\n                break"
 },
 {
  "docstring": "Finds an approximate min-weight 2-edge-augmentation of G.\n\nThis is an implementation of the approximation algorithm detailed in [1]_.\nIt chooses a set of edges from avail to add to G that renders it\n2-edge-connected if such a subset exists.  This is done by finding a\nminimum spanning arborescence of a specially constructed metagraph.\n\nParameters\n----------\nG : NetworkX graph\n   An undirected graph.\n\navail : set of 2 or 3 tuples.\n    candidate edges (with optional weights) to choose from\n\nweight : string\n    key to use to find weights if avail is a set of 3-tuples where the\n    third item in each tuple is a dictionary.\n\nYields\n------\nedge : tuple\n    Edges in the subset of avail chosen to bridge augment G.\n\n",
  "code": "@nx._dispatch\ndef weighted_bridge_augmentation(G, avail, weight=None):\n    if weight is None:\n        weight = 'weight'\n    if not nx.is_connected(G):\n        H = G.copy()\n        connectors = list(one_edge_augmentation(H, avail=avail, weight=weight))\n        H.add_edges_from(connectors)\n        yield from connectors\n    else:\n        connectors = []\n        H = G\n    if len(avail) == 0:\n        if nx.has_bridges(H):\n            raise nx.NetworkXUnfeasible('no augmentation possible')\n    avail_uv, avail_w = _unpack_available_edges(avail, weight=weight, G=H)\n    bridge_ccs = nx.connectivity.bridge_components(H)\n    C = collapse(H, bridge_ccs)\n    mapping = C.graph['mapping']\n    meta_to_wuv = {(mu, mv): (w, uv) for (mu, mv), uv, w in _lightest_meta_edges(mapping, avail_uv, avail_w)}\n    try:\n        root = next((n for n, d in C.degree() if d == 1))\n    except StopIteration:\n        return\n    TR = nx.dfs_tree(C, root)\n    D = nx.reverse(TR).copy()\n    nx.set_edge_attributes(D, name='weight', values=0)\n    lca_gen = nx.tree_all_pairs_lowest_common_ancestor(TR, root=root, pairs=meta_to_wuv.keys())\n    for (mu, mv), lca in lca_gen:\n        w, uv = meta_to_wuv[mu, mv]\n        if lca == mu:\n            D.add_edge(lca, mv, weight=w, generator=uv)\n        elif lca == mv:\n            D.add_edge(lca, mu, weight=w, generator=uv)\n        else:\n            D.add_edge(lca, mu, weight=w, generator=uv)\n            D.add_edge(lca, mv, weight=w, generator=uv)\n    try:\n        A = _minimum_rooted_branching(D, root)\n    except nx.NetworkXException as err:\n        raise nx.NetworkXUnfeasible('no 2-edge-augmentation possible') from err\n    bridge_connectors = set()\n    for mu, mv in A.edges():\n        data = D.get_edge_data(mu, mv)\n        if 'generator' in data:\n            edge = data['generator']\n            bridge_connectors.add(edge)\n    yield from bridge_connectors"
 },
 {
  "docstring": "Helper function to compute a minimum rooted branching (aka rooted\narborescence)\n\nBefore the branching can be computed, the directed graph must be rooted by\nremoving the predecessors of root.\n\nA branching / arborescence of rooted graph G is a subgraph that contains a\ndirected path from the root to every other vertex. It is the directed\nanalog of the minimum spanning tree problem.\n\n",
  "code": "def _minimum_rooted_branching(D, root):\n    rooted = D.copy()\n    rooted.remove_edges_from([(u, root) for u in D.predecessors(root)])\n    A = nx.minimum_spanning_arborescence(rooted)\n    return A"
 },
 {
  "docstring": "Collapses each group of nodes into a single node.\n\nThis is similar to condensation, but works on undirected graphs.\n\nParameters\n----------\nG : NetworkX Graph\n\ngrouped_nodes:  list or generator\n   Grouping of nodes to collapse. The grouping must be disjoint.\n   If grouped_nodes are strongly_connected_components then this is\n   equivalent to :func:`condensation`.\n\nReturns\n-------\nC : NetworkX Graph\n   The collapsed graph C of G with respect to the node grouping.  The node\n   labels are integers corresponding to the index of the component in the\n   list of grouped_nodes.  C has a graph attribute named 'mapping' with a\n   dictionary mapping the original nodes to the nodes in C to which they\n   belong.  Each node in C also has a node attribute 'members' with the set\n   of original nodes in G that form the group that the node in C\n   represents.\n\nExamples\n--------\n>>> # Collapses a graph using disjoint groups, but not necessarily connected\n>>> G = nx.Graph([(1, 0), (2, 3), (3, 1), (3, 4), (4, 5), (5, 6), (5, 7)])\n>>> G.add_node(\"A\")\n>>> grouped_nodes = [{0, 1, 2, 3}, {5, 6, 7}]\n>>> C = collapse(G, grouped_nodes)\n>>> members = nx.get_node_attributes(C, \"members\")\n>>> sorted(members.keys())\n[0, 1, 2, 3]\n>>> member_values = set(map(frozenset, members.values()))\n>>> assert {0, 1, 2, 3} in member_values\n>>> assert {4} in member_values\n>>> assert {5, 6, 7} in member_values\n>>> assert {\"A\"} in member_values",
  "code": "@nx._dispatch\ndef collapse(G, grouped_nodes):\n    mapping = {}\n    members = {}\n    C = G.__class__()\n    i = 0\n    remaining = set(G.nodes())\n    for i, group in enumerate(grouped_nodes):\n        group = set(group)\n        assert remaining.issuperset(group), 'grouped nodes must exist in G and be disjoint'\n        remaining.difference_update(group)\n        members[i] = group\n        mapping.update(((n, i) for n in group))\n    for i, node in enumerate(remaining, start=i + 1):\n        group = {node}\n        members[i] = group\n        mapping.update(((n, i) for n in group))\n    number_of_groups = i + 1\n    C.add_nodes_from(range(number_of_groups))\n    C.add_edges_from(((mapping[u], mapping[v]) for u, v in G.edges() if mapping[u] != mapping[v]))\n    nx.set_node_attributes(C, name='members', values=members)\n    C.graph['mapping'] = mapping\n    return C"
 },
 {
  "docstring": "Returns only the edges in the complement of G\n\nParameters\n----------\nG : NetworkX Graph\n\nYields\n------\nedge : tuple\n    Edges in the complement of G\n\nExamples\n--------\n>>> G = nx.path_graph((1, 2, 3, 4))\n>>> sorted(complement_edges(G))\n[(1, 3), (1, 4), (2, 4)]\n>>> G = nx.path_graph((1, 2, 3, 4), nx.DiGraph())\n>>> sorted(complement_edges(G))\n[(1, 3), (1, 4), (2, 1), (2, 4), (3, 1), (3, 2), (4, 1), (4, 2), (4, 3)]\n>>> G = nx.complete_graph(1000)\n>>> sorted(complement_edges(G))\n[]",
  "code": "@nx._dispatch\ndef complement_edges(G):\n    G_adj = G._adj\n    if G.is_directed():\n        for u, v in it.combinations(G.nodes(), 2):\n            if v not in G_adj[u]:\n                yield (u, v)\n            if u not in G_adj[v]:\n                yield (v, u)\n    else:\n        for u, v in it.combinations(G.nodes(), 2):\n            if v not in G_adj[u]:\n                yield (u, v)"
 },
 {
  "docstring": "wrapper around rng.shuffle for python 2 compatibility reasons",
  "code": "def _compat_shuffle(rng, input):\n    rng.shuffle(input)"
 },
 {
  "docstring": "Greedy algorithm for finding a k-edge-augmentation\n\nParameters\n----------\nG : NetworkX graph\n   An undirected graph.\n\nk : integer\n    Desired edge connectivity\n\navail : dict or a set of 2 or 3 tuples\n    For more details, see :func:`k_edge_augmentation`.\n\nweight : string\n    key to use to find weights if ``avail`` is a set of 3-tuples.\n    For more details, see :func:`k_edge_augmentation`.\n\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\nYields\n------\nedge : tuple\n    Edges in the greedy augmentation of G\n\n",
  "code": "@not_implemented_for('multigraph')\n@not_implemented_for('directed')\n@py_random_state(4)\n@nx._dispatch\ndef greedy_k_edge_augmentation(G, k, avail=None, weight=None, seed=None):\n    aug_edges = []\n    done = is_k_edge_connected(G, k)\n    if done:\n        return\n    if avail is None:\n        avail_uv = list(complement_edges(G))\n        avail_w = [1] * len(avail_uv)\n    else:\n        avail_uv, avail_w = _unpack_available_edges(avail, weight=weight, G=G)\n    tiebreaker = [sum(map(G.degree, uv)) for uv in avail_uv]\n    avail_wduv = sorted(zip(avail_w, tiebreaker, avail_uv))\n    avail_uv = [uv for w, d, uv in avail_wduv]\n    H = G.copy()\n    for u, v in avail_uv:\n        done = False\n        if not is_locally_k_edge_connected(H, u, v, k=k):\n            aug_edges.append((u, v))\n            H.add_edge(u, v)\n            if H.degree(u) >= k and H.degree(v) >= k:\n                done = is_k_edge_connected(H, k)\n        if done:\n            break\n    if not done:\n        raise nx.NetworkXUnfeasible('not able to k-edge-connect with available edges')\n    _compat_shuffle(seed, aug_edges)\n    for u, v in list(aug_edges):\n        if H.degree(u) <= k or H.degree(v) <= k:\n            continue\n        H.remove_edge(u, v)\n        aug_edges.remove((u, v))\n        if not is_k_edge_connected(H, k=k):\n            H.add_edge(u, v)\n            aug_edges.append((u, v))\n    yield from aug_edges"
 },
 {
  "docstring": "finds edges between disjoint nodes",
  "code": "def _edges_between_disjoint(H, only1, only2):\n    only1_adj = {u: set(H.adj[u]) for u in only1}\n    for u, neighbs in only1_adj.items():\n        neighbs12 = neighbs.intersection(only2)\n        for v in neighbs12:\n            yield (u, v)"
 },
 {
  "docstring": "Generates nodes in each maximal k-edge-connected component in G.\n\nParameters\n----------\nG : NetworkX graph\n\nk : Integer\n    Desired edge connectivity\n\nReturns\n-------\nk_edge_components : a generator of k-edge-ccs. Each set of returned nodes\n   will have k-edge-connectivity in the graph G.\n\nSee Also\n--------\n:func:`local_edge_connectivity`\n:func:`k_edge_subgraphs` : similar to this function, but the subgraph\n    defined by the nodes must also have k-edge-connectivity.\n:func:`k_components` : similar to this function, but uses node-connectivity\n    instead of edge-connectivity\n\nRaises\n------\nNetworkXNotImplemented\n    If the input graph is a multigraph.\n\nValueError:\n    If k is less than 1\n\n",
  "code": "@not_implemented_for('multigraph')\n@nx._dispatch\ndef k_edge_components(G, k):\n    if k < 1:\n        raise ValueError('k cannot be less than 1')\n    if G.is_directed():\n        if k == 1:\n            return nx.strongly_connected_components(G)\n        else:\n            aux_graph = EdgeComponentAuxGraph.construct(G)\n            return aux_graph.k_edge_components(k)\n    elif k == 1:\n        return nx.connected_components(G)\n    elif k == 2:\n        return bridge_components(G)\n    else:\n        aux_graph = EdgeComponentAuxGraph.construct(G)\n        return aux_graph.k_edge_components(k)"
 },
 {
  "docstring": "Generates nodes in each maximal k-edge-connected subgraph in G.\n\nParameters\n----------\nG : NetworkX graph\n\nk : Integer\n    Desired edge connectivity\n\nReturns\n-------\nk_edge_subgraphs : a generator of k-edge-subgraphs\n    Each k-edge-subgraph is a maximal set of nodes that defines a subgraph\n    of G that is k-edge-connected.\n\nSee Also\n--------\n:func:`edge_connectivity`\n:func:`k_edge_components` : similar to this function, but nodes only\n    need to have k-edge-connectivity within the graph G and the subgraphs\n    might not be k-edge-connected.\n\nRaises\n------\nNetworkXNotImplemented\n    If the input graph is a multigraph.\n\nValueError:\n    If k is less than 1\n\n",
  "code": "@not_implemented_for('multigraph')\n@nx._dispatch\ndef k_edge_subgraphs(G, k):\n    if k < 1:\n        raise ValueError('k cannot be less than 1')\n    if G.is_directed():\n        if k <= 1:\n            return k_edge_components(G, k)\n        else:\n            return _k_edge_subgraphs_nodes(G, k)\n    elif k <= 2:\n        return k_edge_components(G, k)\n    else:\n        return _k_edge_subgraphs_nodes(G, k)"
 },
 {
  "docstring": "Helper to get the nodes from the subgraphs.\n\nThis allows k_edge_subgraphs to return a generator.",
  "code": "def _k_edge_subgraphs_nodes(G, k):\n    for C in general_k_edge_subgraphs(G, k):\n        yield set(C.nodes())"
 },
 {
  "docstring": "Finds all bridge-connected components G.\n\nParameters\n----------\nG : NetworkX undirected graph\n\nReturns\n-------\nbridge_components : a generator of 2-edge-connected components\n\n\nSee Also\n--------\n:func:`k_edge_subgraphs` : this function is a special case for an\n    undirected graph where k=2.\n:func:`biconnected_components` : similar to this function, but is defined\n    using 2-node-connectivity instead of 2-edge-connectivity.\n\nRaises\n------\nNetworkXNotImplemented\n    If the input graph is directed or a multigraph.\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef bridge_components(G):\n    H = G.copy()\n    H.remove_edges_from(nx.bridges(G))\n    yield from nx.connected_components(H)"
 },
 {
  "docstring": "Helper for finding nodes with degree less than k.",
  "code": "def _low_degree_nodes(G, k, nbunch=None):\n    if G.is_directed():\n        seen = set()\n        for node, degree in G.out_degree(nbunch):\n            if degree < k:\n                seen.add(node)\n                yield node\n        for node, degree in G.in_degree(nbunch):\n            if node not in seen and degree < k:\n                seen.add(node)\n                yield node\n    else:\n        for node, degree in G.degree(nbunch):\n            if degree < k:\n                yield node"
 },
 {
  "docstring": "Helper for filtering components that can't be k-edge-connected.\n\nRemoves and generates each node with degree less than k.  Then generates\nremaining components where all nodes have degree at least k.",
  "code": "def _high_degree_components(G, k):\n    H = G.copy()\n    singletons = set(_low_degree_nodes(H, k))\n    while singletons:\n        nbunch = set(it.chain.from_iterable(map(H.neighbors, singletons)))\n        nbunch.difference_update(singletons)\n        H.remove_nodes_from(singletons)\n        for node in singletons:\n            yield {node}\n        singletons = set(_low_degree_nodes(H, k, nbunch))\n    if G.is_directed():\n        yield from nx.strongly_connected_components(H)\n    else:\n        yield from nx.connected_components(H)"
 },
 {
  "docstring": "General algorithm to find all maximal k-edge-connected subgraphs in G.\n\nReturns\n-------\nk_edge_subgraphs : a generator of nx.Graphs that are k-edge-subgraphs\n    Each k-edge-subgraph is a maximal set of nodes that defines a subgraph\n    of G that is k-edge-connected.\n\n",
  "code": "@nx._dispatch\ndef general_k_edge_subgraphs(G, k):\n    if k < 1:\n        raise ValueError('k cannot be less than 1')\n    find_ccs = partial(_high_degree_components, k=k)\n    if G.number_of_nodes() < k:\n        for node in G.nodes():\n            yield G.subgraph([node]).copy()\n        return\n    R0 = {G.subgraph(cc).copy() for cc in find_ccs(G)}\n    while R0:\n        G1 = R0.pop()\n        if G1.number_of_nodes() == 1:\n            yield G1\n        else:\n            cut_edges = nx.minimum_edge_cut(G1)\n            cut_value = len(cut_edges)\n            if cut_value < k:\n                G1.remove_edges_from(cut_edges)\n                for cc in find_ccs(G1):\n                    R0.add(G1.subgraph(cc).copy())\n            else:\n                yield G1"
 },
 {
  "docstring": "Builds an auxiliary graph encoding edge-connectivity between nodes.\n\n",
  "code": "@classmethod\ndef construct(EdgeComponentAuxGraph, G):\n    not_implemented_for('multigraph')(lambda G: G)(G)\n\n    def _recursive_build(H, A, source, avail):\n        if {source} == avail:\n            return\n        sink = arbitrary_element(avail - {source})\n        value, (S, T) = nx.minimum_cut(H, source, sink)\n        if H.is_directed():\n            value_, (T_, S_) = nx.minimum_cut(H, sink, source)\n            if value_ < value:\n                value, S, T = (value_, S_, T_)\n        A.add_edge(source, sink, weight=value)\n        _recursive_build(H, A, source, avail.intersection(S))\n        _recursive_build(H, A, sink, avail.intersection(T))\n    H = G.__class__()\n    H.add_nodes_from(G.nodes())\n    H.add_edges_from(G.edges(), capacity=1)\n    A = nx.Graph()\n    if H.number_of_nodes() > 0:\n        source = arbitrary_element(H.nodes())\n        avail = set(H.nodes())\n        _recursive_build(H, A, source, avail)\n    self = EdgeComponentAuxGraph()\n    self.A = A\n    self.H = H\n    return self"
 },
 {
  "docstring": "Queries the auxiliary graph for k-edge-connected components.\n\nParameters\n----------\nk : Integer\n    Desired edge connectivity\n\nReturns\n-------\nk_edge_components : a generator of k-edge-ccs\n\n",
  "code": "def k_edge_components(self, k):\n    if k < 1:\n        raise ValueError('k cannot be less than 1')\n    A = self.A\n    aux_weights = nx.get_edge_attributes(A, 'weight')\n    R = nx.Graph()\n    R.add_nodes_from(A.nodes())\n    R.add_edges_from((e for e, w in aux_weights.items() if w >= k))\n    yield from nx.connected_components(R)"
 },
 {
  "docstring": "Queries the auxiliary graph for k-edge-connected subgraphs.\n\nParameters\n----------\nk : Integer\n    Desired edge connectivity\n\nReturns\n-------\nk_edge_subgraphs : a generator of k-edge-subgraphs\n\n",
  "code": "def k_edge_subgraphs(self, k):\n    if k < 1:\n        raise ValueError('k cannot be less than 1')\n    H = self.H\n    A = self.A\n    aux_weights = nx.get_edge_attributes(A, 'weight')\n    R = nx.Graph()\n    R.add_nodes_from(A.nodes())\n    R.add_edges_from((e for e, w in aux_weights.items() if w >= k))\n    for cc in nx.connected_components(R):\n        if len(cc) < k:\n            for node in cc:\n                yield {node}\n        else:\n            C = H.subgraph(cc)\n            yield from k_edge_subgraphs(C, k)"
 },
 {
  "docstring": "Returns the k-component structure of a graph G.\n\nA `k`-component is a maximal subgraph of a graph G that has, at least,\nnode connectivity `k`: we need to remove at least `k` nodes to break it\ninto more components. `k`-components have an inherent hierarchical\nstructure because they are nested in terms of connectivity: a connected\ngraph can contain several 2-components, each of which can contain\none or more 3-components, and so forth.\n\nParameters\n----------\nG : NetworkX graph\n\nflow_func : function\n    Function to perform the underlying flow computations. Default value\n    :meth:`edmonds_karp`. This function performs better in sparse graphs with\n    right tailed degree distributions. :meth:`shortest_augmenting_path` will\n    perform better in denser graphs.\n\nReturns\n-------\nk_components : dict\n    Dictionary with all connectivity levels `k` in the input Graph as keys\n    and a list of sets of nodes that form a k-component of level `k` as\n    values.\n\nRaises\n------\nNetworkXNotImplemented\n    If the input graph is directed.\n\nExamples\n--------\n>>> # Petersen graph has 10 nodes and it is triconnected, thus all\n>>> # nodes are in a single component on all three connectivity levels\n>>> G = nx.petersen_graph()\n>>> k_components = nx.k_components(G)\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch\ndef k_components(G, flow_func=None):\n    k_components = defaultdict(list)\n    if flow_func is None:\n        flow_func = default_flow_func\n    for component in nx.connected_components(G):\n        comp = set(component)\n        if len(comp) > 1:\n            k_components[1].append(comp)\n    bicomponents = [G.subgraph(c) for c in nx.biconnected_components(G)]\n    for bicomponent in bicomponents:\n        bicomp = set(bicomponent)\n        if len(bicomp) > 2:\n            k_components[2].append(bicomp)\n    for B in bicomponents:\n        if len(B) <= 2:\n            continue\n        k = nx.node_connectivity(B, flow_func=flow_func)\n        if k > 2:\n            k_components[k].append(set(B))\n        cuts = list(nx.all_node_cuts(B, k=k, flow_func=flow_func))\n        stack = [(k, _generate_partition(B, cuts, k))]\n        while stack:\n            parent_k, partition = stack[-1]\n            try:\n                nodes = next(partition)\n                C = B.subgraph(nodes)\n                this_k = nx.node_connectivity(C, flow_func=flow_func)\n                if this_k > parent_k and this_k > 2:\n                    k_components[this_k].append(set(C))\n                cuts = list(nx.all_node_cuts(C, k=this_k, flow_func=flow_func))\n                if cuts:\n                    stack.append((this_k, _generate_partition(C, cuts, this_k)))\n            except StopIteration:\n                stack.pop()\n    return _reconstruct_k_components(k_components)"
 },
 {
  "docstring": "Merge sets that share k or more elements.\n\nSee: http://rosettacode.org/wiki/Set_consolidation\n\nThe iterative python implementation posted there is\nfaster than this because of the overhead of building a\nGraph and calling nx.connected_components, but it's not\nclear for us if we can use it in NetworkX because there\nis no licence for the code.",
  "code": "def _consolidate(sets, k):\n    G = nx.Graph()\n    nodes = dict(enumerate(sets))\n    G.add_nodes_from(nodes)\n    G.add_edges_from(((u, v) for u, v in combinations(nodes, 2) if len(nodes[u] & nodes[v]) >= k))\n    for component in nx.connected_components(G):\n        yield set.union(*[nodes[n] for n in component])"
 },
 {
  "docstring": "Returns all minimum k cutsets of an undirected graph G.\n\nThis implementation is based on Kanevsky's algorithm [1]_ for finding all\nminimum-size node cut-sets of an undirected graph G; ie the set (or sets)\nof nodes of cardinality equal to the node connectivity of G. Thus if\nremoved, would break G into two or more connected components.\n\nParameters\n----------\nG : NetworkX graph\n    Undirected graph\n\nk : Integer\n    Node connectivity of the input graph. If k is None, then it is\n    computed. Default value: None.\n\nflow_func : function\n    Function to perform the underlying flow computations. Default value is\n    :func:`~networkx.algorithms.flow.edmonds_karp`. This function performs\n    better in sparse graphs with right tailed degree distributions.\n    :func:`~networkx.algorithms.flow.shortest_augmenting_path` will\n    perform better in denser graphs.\n\n\nReturns\n-------\ncuts : a generator of node cutsets\n    Each node cutset has cardinality equal to the node connectivity of\n    the input graph.\n\nExamples\n--------\n>>> # A two-dimensional grid graph has 4 cutsets of cardinality 2\n>>> G = nx.grid_2d_graph(5, 5)\n>>> cutsets = list(nx.all_node_cuts(G))\n>>> len(cutsets)\n4\n>>> all(2 == len(cutset) for cutset in cutsets)\nTrue\n>>> nx.node_connectivity(G)\n2\n\n",
  "code": "@nx._dispatch\ndef all_node_cuts(G, k=None, flow_func=None):\n    if not nx.is_connected(G):\n        raise nx.NetworkXError('Input graph is disconnected.')\n    if nx.density(G) == 1:\n        for cut_set in combinations(G, len(G) - 1):\n            yield set(cut_set)\n        return\n    seen = []\n    H = build_auxiliary_node_connectivity(G)\n    H_nodes = H.nodes\n    mapping = H.graph['mapping']\n    original_H_pred = copy.copy(H._pred)\n    R = build_residual_network(H, 'capacity')\n    kwargs = {'capacity': 'capacity', 'residual': R}\n    if flow_func is None:\n        flow_func = default_flow_func\n    if flow_func is shortest_augmenting_path:\n        kwargs['two_phase'] = True\n    if k is None:\n        k = nx.node_connectivity(G, flow_func=flow_func)\n    X = {n for n, d in sorted(G.degree(), key=itemgetter(1), reverse=True)[:k]}\n    if _is_separating_set(G, X):\n        seen.append(X)\n        yield X\n    for x in X:\n        non_adjacent = set(G) - X - set(G[x])\n        for v in non_adjacent:\n            R = flow_func(H, f'{mapping[x]}B', f'{mapping[v]}A', **kwargs)\n            flow_value = R.graph['flow_value']\n            if flow_value == k:\n                E1 = flowed_edges = [(u, w) for u, w, d in R.edges(data=True) if d['flow'] != 0]\n                VE1 = incident_nodes = {n for edge in E1 for n in edge}\n                saturated_edges = [(u, w, d) for u, w, d in R.edges(data=True) if d['capacity'] == d['flow'] or d['capacity'] == 0]\n                R.remove_edges_from(saturated_edges)\n                R_closure = nx.transitive_closure(R)\n                L = nx.condensation(R)\n                cmap = L.graph['mapping']\n                inv_cmap = defaultdict(list)\n                for n, scc in cmap.items():\n                    inv_cmap[scc].append(n)\n                VE1 = {cmap[n] for n in VE1}\n                for antichain in nx.antichains(L):\n                    if not set(antichain).issubset(VE1):\n                        continue\n                    S = set()\n                    for scc in antichain:\n                        S.update(inv_cmap[scc])\n                    S_ancestors = set()\n                    for n in S:\n                        S_ancestors.update(R_closure._pred[n])\n                    S.update(S_ancestors)\n                    if f'{mapping[x]}B' not in S or f'{mapping[v]}A' in S:\n                        continue\n                    cutset = set()\n                    for u in S:\n                        cutset.update(((u, w) for w in original_H_pred[u] if w not in S))\n                    if any((H_nodes[u]['id'] != H_nodes[w]['id'] for u, w in cutset)):\n                        continue\n                    node_cut = {H_nodes[u]['id'] for u, _ in cutset}\n                    if len(node_cut) == k:\n                        if x in node_cut or v in node_cut:\n                            continue\n                        if node_cut not in seen:\n                            yield node_cut\n                            seen.append(node_cut)\n                H.add_edge(f'{mapping[x]}B', f'{mapping[v]}A', capacity=1)\n                H.add_edge(f'{mapping[v]}B', f'{mapping[x]}A', capacity=1)\n                R.add_edge(f'{mapping[x]}B', f'{mapping[v]}A', capacity=1)\n                R.add_edge(f'{mapping[v]}A', f'{mapping[x]}B', capacity=0)\n                R.add_edge(f'{mapping[v]}B', f'{mapping[x]}A', capacity=1)\n                R.add_edge(f'{mapping[x]}A', f'{mapping[v]}B', capacity=0)\n                R.add_edges_from(saturated_edges)"
 },
 {
  "docstring": "Assumes that the input graph is connected",
  "code": "def _is_separating_set(G, cut):\n    if len(cut) == len(G) - 1:\n        return True\n    H = nx.restricted_view(G, cut, [])\n    if nx.is_connected(H):\n        return False\n    return True"
 },
 {
  "docstring": "Returns the weighted minimum edge cut using the Stoer-Wagner algorithm.\n\nDetermine the minimum edge cut of a connected graph using the\nStoer-Wagner algorithm. In weighted cases, all weights must be\nnonnegative.\n\nThe running time of the algorithm depends on the type of heaps used:\n\n============== =============================================\nType of heap   Running time\n============== =============================================\nBinary heap    $O(n (m + n) \\log n)$\nFibonacci heap $O(nm + n^2 \\log n)$\nPairing heap   $O(2^{2 \\sqrt{\\log \\log n}} nm + n^2 \\log n)$\n============== =============================================\n\nParameters\n----------\nG : NetworkX graph\n    Edges of the graph are expected to have an attribute named by the\n    weight parameter below. If this attribute is not present, the edge is\n    considered to have unit weight.\n\nweight : string\n    Name of the weight attribute of the edges. If the attribute is not\n    present, unit weight is assumed. Default value: 'weight'.\n\nheap : class\n    Type of heap to be used in the algorithm. It should be a subclass of\n    :class:`MinHeap` or implement a compatible interface.\n\n    If a stock heap implementation is to be used, :class:`BinaryHeap` is\n    recommended over :class:`PairingHeap` for Python implementations without\n    optimized attribute accesses (e.g., CPython) despite a slower\n    asymptotic running time. For Python implementations with optimized\n    attribute accesses (e.g., PyPy), :class:`PairingHeap` provides better\n    performance. Default value: :class:`BinaryHeap`.\n\nReturns\n-------\ncut_value : integer or float\n    The sum of weights of edges in a minimum cut.\n\npartition : pair of node lists\n    A partitioning of the nodes that defines a minimum cut.\n\nRaises\n------\nNetworkXNotImplemented\n    If the graph is directed or a multigraph.\n\nNetworkXError\n    If the graph has less than two nodes, is not connected or has a\n    negative-weighted edge.\n\nExamples\n--------\n>>> G = nx.Graph()\n>>> G.add_edge(\"x\", \"a\", weight=3)\n>>> G.add_edge(\"x\", \"b\", weight=1)\n>>> G.add_edge(\"a\", \"c\", weight=3)\n>>> G.add_edge(\"b\", \"c\", weight=5)\n>>> G.add_edge(\"b\", \"d\", weight=4)\n>>> G.add_edge(\"d\", \"e\", weight=2)\n>>> G.add_edge(\"c\", \"y\", weight=2)\n>>> G.add_edge(\"e\", \"y\", weight=3)\n>>> cut_value, partition = nx.stoer_wagner(G)\n>>> cut_value\n4",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch(edge_attrs='weight')\ndef stoer_wagner(G, weight='weight', heap=BinaryHeap):\n    n = len(G)\n    if n < 2:\n        raise nx.NetworkXError('graph has less than two nodes.')\n    if not nx.is_connected(G):\n        raise nx.NetworkXError('graph is not connected.')\n    G = nx.Graph(((u, v, {'weight': e.get(weight, 1)}) for u, v, e in G.edges(data=True) if u != v))\n    for u, v, e in G.edges(data=True):\n        if e['weight'] < 0:\n            raise nx.NetworkXError('graph has a negative-weighted edge.')\n    cut_value = float('inf')\n    nodes = set(G)\n    contractions = []\n    for i in range(n - 1):\n        u = arbitrary_element(G)\n        A = {u}\n        h = heap()\n        for v, e in G[u].items():\n            h.insert(v, -e['weight'])\n        for j in range(n - i - 2):\n            u = h.pop()[0]\n            A.add(u)\n            for v, e in G[u].items():\n                if v not in A:\n                    h.insert(v, h.get(v, 0) - e['weight'])\n        v, w = h.min()\n        w = -w\n        if w < cut_value:\n            cut_value = w\n            best_phase = i\n        contractions.append((u, v))\n        for w, e in G[v].items():\n            if w != u:\n                if w not in G[u]:\n                    G.add_edge(u, w, weight=e['weight'])\n                else:\n                    G[u][w]['weight'] += e['weight']\n        G.remove_node(v)\n    G = nx.Graph(islice(contractions, best_phase))\n    v = contractions[best_phase][1]\n    G.add_node(v)\n    reachable = set(nx.single_source_shortest_path_length(G, v))\n    partition = (list(reachable), list(nodes - reachable))\n    return (cut_value, partition)"
 },
 {
  "docstring": "Creates a directed graph D from an undirected graph G to compute flow\nbased node connectivity.\n\nFor an undirected graph G having `n` nodes and `m` edges we derive a\ndirected graph D with `2n` nodes and `2m+n` arcs by replacing each\noriginal node `v` with two nodes `vA`, `vB` linked by an (internal)\narc in D. Then for each edge (`u`, `v`) in G we add two arcs (`uB`, `vA`)\nand (`vB`, `uA`) in D. Finally we set the attribute capacity = 1 for each\narc in D [1]_.\n\nFor a directed graph having `n` nodes and `m` arcs we derive a\ndirected graph D with `2n` nodes and `m+n` arcs by replacing each\noriginal node `v` with two nodes `vA`, `vB` linked by an (internal)\narc (`vA`, `vB`) in D. Then for each arc (`u`, `v`) in G we add one\narc (`uB`, `vA`) in D. Finally we set the attribute capacity = 1 for\neach arc in D.\n\nA dictionary with a mapping between nodes in the original graph and the\nauxiliary digraph is stored as a graph attribute: D.graph['mapping'].\n\n",
  "code": "@nx._dispatch\ndef build_auxiliary_node_connectivity(G):\n    directed = G.is_directed()\n    mapping = {}\n    H = nx.DiGraph()\n    for i, node in enumerate(G):\n        mapping[node] = i\n        H.add_node(f'{i}A', id=node)\n        H.add_node(f'{i}B', id=node)\n        H.add_edge(f'{i}A', f'{i}B', capacity=1)\n    edges = []\n    for source, target in G.edges():\n        edges.append((f'{mapping[source]}B', f'{mapping[target]}A'))\n        if not directed:\n            edges.append((f'{mapping[target]}B', f'{mapping[source]}A'))\n    H.add_edges_from(edges, capacity=1)\n    H.graph['mapping'] = mapping\n    return H"
 },
 {
  "docstring": "Auxiliary digraph for computing flow based edge connectivity\n\nIf the input graph is undirected, we replace each edge (`u`,`v`) with\ntwo reciprocal arcs (`u`, `v`) and (`v`, `u`) and then we set the attribute\n'capacity' for each arc to 1. If the input graph is directed we simply\nadd the 'capacity' attribute. Part of algorithm 1 in [1]_ .\n\n",
  "code": "@nx._dispatch\ndef build_auxiliary_edge_connectivity(G):\n    if G.is_directed():\n        H = nx.DiGraph()\n        H.add_nodes_from(G.nodes())\n        H.add_edges_from(G.edges(), capacity=1)\n        return H\n    else:\n        H = nx.DiGraph()\n        H.add_nodes_from(G.nodes())\n        for source, target in G.edges():\n            H.add_edges_from([(source, target), (target, source)], capacity=1)\n        return H"
 },
 {
  "docstring": "Checks that aug_edges are consistently formatted",
  "code": "def _assert_solution_properties(G, aug_edges, avail_dict=None):\n    if avail_dict is not None:\n        assert all((e in avail_dict for e in aug_edges)), 'when avail is specified aug-edges should be in avail'\n    unique_aug = set(map(tuple, map(sorted, aug_edges)))\n    unique_aug = list(map(tuple, map(sorted, aug_edges)))\n    assert len(aug_edges) == len(unique_aug), 'edges should be unique'\n    assert not any((u == v for u, v in unique_aug)), 'should be no self-edges'\n    assert not any((G.has_edge(u, v) for u, v in unique_aug)), 'aug edges and G.edges should be disjoint'"
 },
 {
  "docstring": "Does one specific augmentation and checks for properties of the result",
  "code": "def _augment_and_check(G, k, avail=None, weight=None, verbose=False, orig_k=None, max_aug_k=None):\n    if orig_k is None:\n        try:\n            orig_k = nx.edge_connectivity(G)\n        except nx.NetworkXPointlessConcept:\n            orig_k = 0\n    info = {}\n    try:\n        if avail is not None:\n            avail_dict = dict(zip(*_unpack_available_edges(avail, weight=weight)))\n        else:\n            avail_dict = None\n        try:\n            generator = nx.k_edge_augmentation(G, k=k, weight=weight, avail=avail)\n            assert not isinstance(generator, list), 'should always return an iter'\n            aug_edges = []\n            for edge in generator:\n                aug_edges.append(edge)\n        except nx.NetworkXUnfeasible:\n            infeasible = True\n            info['infeasible'] = True\n            assert len(aug_edges) == 0, 'should not generate anything if unfeasible'\n            if avail is None:\n                n_nodes = G.number_of_nodes()\n                assert n_nodes <= k, f'unconstrained cases are only unfeasible if |V| <= k. Got |V|={n_nodes} and k={k}'\n            else:\n                if max_aug_k is None:\n                    G_aug_all = G.copy()\n                    G_aug_all.add_edges_from(avail_dict.keys())\n                    try:\n                        max_aug_k = nx.edge_connectivity(G_aug_all)\n                    except nx.NetworkXPointlessConcept:\n                        max_aug_k = 0\n                assert max_aug_k < k, 'avail should only be unfeasible if using all edges does not achieve k-edge-connectivity'\n            partial_edges = list(nx.k_edge_augmentation(G, k=k, weight=weight, partial=True, avail=avail))\n            info['n_partial_edges'] = len(partial_edges)\n            if avail_dict is None:\n                assert set(partial_edges) == set(complement_edges(G)), 'unweighted partial solutions should be the complement'\n            elif len(avail_dict) > 0:\n                H = G.copy()\n                H.add_edges_from(partial_edges)\n                partial_conn = nx.edge_connectivity(H)\n                H.add_edges_from(set(avail_dict.keys()))\n                full_conn = nx.edge_connectivity(H)\n                assert partial_conn == full_conn, 'adding more edges should not increase k-conn'\n            aug_edges = partial_edges\n        else:\n            infeasible = False\n        num_edges = len(aug_edges)\n        if avail is not None:\n            total_weight = sum((avail_dict[e] for e in aug_edges))\n        else:\n            total_weight = num_edges\n        info['total_weight'] = total_weight\n        info['num_edges'] = num_edges\n        G_aug = G.copy()\n        G_aug.add_edges_from(aug_edges)\n        try:\n            aug_k = nx.edge_connectivity(G_aug)\n        except nx.NetworkXPointlessConcept:\n            aug_k = 0\n        info['aug_k'] = aug_k\n        if not infeasible and orig_k < k:\n            assert info['aug_k'] >= k, f'connectivity should increase to k={k} or more'\n        assert info['aug_k'] >= orig_k, 'augmenting should never reduce connectivity'\n        _assert_solution_properties(G, aug_edges, avail_dict)\n    except Exception:\n        info['failed'] = True\n        print(f'edges = {list(G.edges())}')\n        print(f'nodes = {list(G.nodes())}')\n        print(f'aug_edges = {list(aug_edges)}')\n        print(f'info  = {info}')\n        raise\n    else:\n        if verbose:\n            print(f'info  = {info}')\n    if infeasible:\n        aug_edges = None\n    return (aug_edges, info)"
 },
 {
  "docstring": "Helper to check weighted/unweighted cases with multiple values of k",
  "code": "def _check_augmentations(G, avail=None, max_k=None, weight=None, verbose=False):\n    try:\n        orig_k = nx.edge_connectivity(G)\n    except nx.NetworkXPointlessConcept:\n        orig_k = 0\n    if avail is not None:\n        all_aug_edges = _unpack_available_edges(avail, weight=weight)[0]\n        G_aug_all = G.copy()\n        G_aug_all.add_edges_from(all_aug_edges)\n        try:\n            max_aug_k = nx.edge_connectivity(G_aug_all)\n        except nx.NetworkXPointlessConcept:\n            max_aug_k = 0\n    else:\n        max_aug_k = G.number_of_nodes() - 1\n    if max_k is None:\n        max_k = min(4, max_aug_k)\n    avail_uniform = {e: 1 for e in complement_edges(G)}\n    if verbose:\n        print('\\n=== CHECK_AUGMENTATION ===')\n        print(f'G.number_of_nodes = {G.number_of_nodes()!r}')\n        print(f'G.number_of_edges = {G.number_of_edges()!r}')\n        print(f'max_k = {max_k!r}')\n        print(f'max_aug_k = {max_aug_k!r}')\n        print(f'orig_k = {orig_k!r}')\n    for k in range(1, max_k + 1):\n        if verbose:\n            print('---------------')\n            print(f'Checking k = {k}')\n        if verbose:\n            print('unweighted case')\n        aug_edges1, info1 = _augment_and_check(G, k=k, verbose=verbose, orig_k=orig_k)\n        if verbose:\n            print('weighted uniform case')\n        aug_edges2, info2 = _augment_and_check(G, k=k, avail=avail_uniform, verbose=verbose, orig_k=orig_k, max_aug_k=G.number_of_nodes() - 1)\n        if avail is not None:\n            if verbose:\n                print('weighted case')\n            aug_edges3, info3 = _augment_and_check(G, k=k, avail=avail, weight=weight, verbose=verbose, max_aug_k=max_aug_k, orig_k=orig_k)\n        if aug_edges1 is not None:\n            if k == 1:\n                assert info2['total_weight'] == info1['total_weight']\n            if k == 2:\n                if orig_k == 0:\n                    assert info2['total_weight'] <= info1['total_weight'] * 3\n                else:\n                    assert info2['total_weight'] <= info1['total_weight'] * 2\n                _check_unconstrained_bridge_property(G, info1)"
 },
 {
  "docstring": "allows == to be used for list of sets",
  "code": "def fset(list_of_sets):\n    return set(map(frozenset, list_of_sets))"
 },
 {
  "docstring": "tests properties of k-edge-connected subgraphs\n\nthe actual edge connectivity should be no less than k unless the cc is a\nsingle node.",
  "code": "def _assert_subgraph_edge_connectivity(G, ccs_subgraph, k):\n    for cc in ccs_subgraph:\n        C = G.subgraph(cc)\n        if len(cc) > 1:\n            connectivity = nx.edge_connectivity(C)\n            assert connectivity >= k"
 },
 {
  "docstring": "tests properties of k-edge-connected components\n\nthe local edge connectivity between each pair of nodes in the original\ngraph should be no less than k unless the cc is a single node.",
  "code": "def _assert_local_cc_edge_connectivity(G, ccs_local, k, memo):\n    for cc in ccs_local:\n        if len(cc) > 1:\n            C = G.subgraph(cc)\n            connectivity = nx.edge_connectivity(C)\n            if connectivity < k:\n                _all_pairs_connectivity(G, cc, k, memo)"
 },
 {
  "docstring": "Helper - generates all k-edge-components using the aux graph.  Checks the\nboth local and subgraph edge connectivity of each cc. Also checks that\nalternate methods of computing the k-edge-ccs generate the same result.",
  "code": "def _check_edge_connectivity(G):\n    aux_graph = EdgeComponentAuxGraph.construct(G)\n    memo = {}\n    for k in it.count(1):\n        ccs_local = fset(aux_graph.k_edge_components(k))\n        ccs_subgraph = fset(aux_graph.k_edge_subgraphs(k))\n        _assert_local_cc_edge_connectivity(G, ccs_local, k, memo)\n        _assert_subgraph_edge_connectivity(G, ccs_subgraph, k)\n        if k == 1 or (k == 2 and (not G.is_directed())):\n            assert ccs_local == ccs_subgraph, 'Subgraphs and components should be the same when k == 1 or (k == 2 and not G.directed())'\n        if G.is_directed():\n            if k == 1:\n                alt_sccs = fset(nx.strongly_connected_components(G))\n                assert alt_sccs == ccs_local, 'k=1 failed alt'\n                assert alt_sccs == ccs_subgraph, 'k=1 failed alt'\n        elif k == 1:\n            alt_ccs = fset(nx.connected_components(G))\n            assert alt_ccs == ccs_local, 'k=1 failed alt'\n            assert alt_ccs == ccs_subgraph, 'k=1 failed alt'\n        elif k == 2:\n            alt_bridge_ccs = fset(bridge_components(G))\n            assert alt_bridge_ccs == ccs_local, 'k=2 failed alt'\n            assert alt_bridge_ccs == ccs_subgraph, 'k=2 failed alt'\n        alt_subgraph_ccs = fset([set(C.nodes()) for C in general_k_edge_subgraphs(G, k=k)])\n        assert alt_subgraph_ccs == ccs_subgraph, 'alt subgraph method failed'\n        if k > 2 and all((len(cc) == 1 for cc in ccs_local)):\n            break"
 },
 {
  "docstring": "Find a maximum single-commodity flow using Boykov-Kolmogorov algorithm.\n\nThis function returns the residual network resulting after computing\nthe maximum flow. See below for details about the conventions\nNetworkX uses for defining residual networks.\n\nThis algorithm has worse case complexity $O(n^2 m |C|)$ for $n$ nodes, $m$\nedges, and $|C|$ the cost of the minimum cut [1]_. This implementation\nuses the marking heuristic defined in [2]_ which improves its running\ntime in many practical problems.\n\nParameters\n----------\nG : NetworkX graph\n    Edges of the graph are expected to have an attribute called\n    'capacity'. If this attribute is not present, the edge is\n    considered to have infinite capacity.\n\ns : node\n    Source node for the flow.\n\nt : node\n    Sink node for the flow.\n\ncapacity : string\n    Edges of the graph G are expected to have an attribute capacity\n    that indicates how much flow the edge can support. If this\n    attribute is not present, the edge is considered to have\n    infinite capacity. Default value: 'capacity'.\n\nresidual : NetworkX graph\n    Residual network on which the algorithm is to be executed. If None, a\n    new residual network is created. Default value: None.\n\nvalue_only : bool\n    If True compute only the value of the maximum flow. This parameter\n    will be ignored by this algorithm because it is not applicable.\n\ncutoff : integer, float\n    If specified, the algorithm will terminate when the flow value reaches\n    or exceeds the cutoff. In this case, it may be unable to immediately\n    determine a minimum cut. Default value: None.\n\nReturns\n-------\nR : NetworkX DiGraph\n    Residual network after computing the maximum flow.\n\nRaises\n------\nNetworkXError\n    The algorithm does not support MultiGraph and MultiDiGraph. If\n    the input graph is an instance of one of these two classes, a\n    NetworkXError is raised.\n\nNetworkXUnbounded\n    If the graph has a path of infinite capacity, the value of a\n    feasible flow on the graph is unbounded above and the function\n    raises a NetworkXUnbounded.\n\n",
  "code": "@nx._dispatch(graphs={'G': 0, 'residual?': 4}, edge_attrs={'capacity': float('inf')}, preserve_edge_attrs={'residual': {'capacity': float('inf')}}, preserve_graph_attrs={'residual'})\ndef boykov_kolmogorov(G, s, t, capacity='capacity', residual=None, value_only=False, cutoff=None):\n    R = boykov_kolmogorov_impl(G, s, t, capacity, residual, cutoff)\n    R.graph['algorithm'] = 'boykov_kolmogorov'\n    return R"
 },
 {
  "docstring": "Bidirectional breadth-first search for the growth stage.\n\nReturns a connecting edge, that is and edge that connects\na node from the source search tree with a node from the\ntarget search tree.\nThe first node in the connecting edge is always from the\nsource tree and the last node from the target tree.",
  "code": "def grow():\n    while active:\n        u = active[0]\n        if u in source_tree:\n            this_tree = source_tree\n            other_tree = target_tree\n            neighbors = R_succ\n        else:\n            this_tree = target_tree\n            other_tree = source_tree\n            neighbors = R_pred\n        for v, attr in neighbors[u].items():\n            if attr['capacity'] - attr['flow'] > 0:\n                if v not in this_tree:\n                    if v in other_tree:\n                        return (u, v) if this_tree is source_tree else (v, u)\n                    this_tree[v] = u\n                    dist[v] = dist[u] + 1\n                    timestamp[v] = timestamp[u]\n                    active.append(v)\n                elif v in this_tree and _is_closer(u, v):\n                    this_tree[v] = u\n                    dist[v] = dist[u] + 1\n                    timestamp[v] = timestamp[u]\n        _ = active.popleft()\n    return (None, None)"
 },
 {
  "docstring": "Augmentation stage.\n\nReconstruct path and determine its residual capacity.\nWe start from a connecting edge, which links a node\nfrom the source tree to a node from the target tree.\nThe connecting edge is the output of the grow function\nand the input of this function.",
  "code": "def augment(u, v):\n    attr = R_succ[u][v]\n    flow = min(INF, attr['capacity'] - attr['flow'])\n    path = [u]\n    w = u\n    while w != s:\n        n = w\n        w = source_tree[n]\n        attr = R_pred[n][w]\n        flow = min(flow, attr['capacity'] - attr['flow'])\n        path.append(w)\n    path.reverse()\n    path.append(v)\n    w = v\n    while w != t:\n        n = w\n        w = target_tree[n]\n        attr = R_succ[n][w]\n        flow = min(flow, attr['capacity'] - attr['flow'])\n        path.append(w)\n    it = iter(path)\n    u = next(it)\n    these_orphans = []\n    for v in it:\n        R_succ[u][v]['flow'] += flow\n        R_succ[v][u]['flow'] -= flow\n        if R_succ[u][v]['flow'] == R_succ[u][v]['capacity']:\n            if v in source_tree:\n                source_tree[v] = None\n                these_orphans.append(v)\n            if u in target_tree:\n                target_tree[u] = None\n                these_orphans.append(u)\n        u = v\n    orphans.extend(sorted(these_orphans, key=dist.get))\n    return flow"
 },
 {
  "docstring": "Adoption stage.\n\nReconstruct search trees by adopting or discarding orphans.\nDuring augmentation stage some edges got saturated and thus\nthe source and target search trees broke down to forests, with\norphans as roots of some of its trees. We have to reconstruct\nthe search trees rooted to source and target before we can grow\nthem again.",
  "code": "def adopt():\n    while orphans:\n        u = orphans.popleft()\n        if u in source_tree:\n            tree = source_tree\n            neighbors = R_pred\n        else:\n            tree = target_tree\n            neighbors = R_succ\n        nbrs = ((n, attr, dist[n]) for n, attr in neighbors[u].items() if n in tree)\n        for v, attr, d in sorted(nbrs, key=itemgetter(2)):\n            if attr['capacity'] - attr['flow'] > 0:\n                if _has_valid_root(v, tree):\n                    tree[u] = v\n                    dist[u] = dist[v] + 1\n                    timestamp[u] = time\n                    break\n        else:\n            nbrs = ((n, attr, dist[n]) for n, attr in neighbors[u].items() if n in tree)\n            for v, attr, d in sorted(nbrs, key=itemgetter(2)):\n                if attr['capacity'] - attr['flow'] > 0:\n                    if v not in active:\n                        active.append(v)\n                if tree[v] == u:\n                    tree[v] = None\n                    orphans.appendleft(v)\n            if u in active:\n                active.remove(u)\n            del tree[u]"
 },
 {
  "docstring": "Detect infinite-capacity negative cycles.",
  "code": "def _detect_unboundedness(R):\n    G = nx.DiGraph()\n    G.add_nodes_from(R)\n    inf = R.graph['inf']\n    f_inf = float('inf')\n    for u in R:\n        for v, e in R[u].items():\n            w = f_inf\n            for k, e in e.items():\n                if e['capacity'] == inf:\n                    w = min(w, e['weight'])\n            if w != f_inf:\n                G.add_edge(u, v, weight=w)\n    if nx.negative_edge_cycle(G):\n        raise nx.NetworkXUnbounded('Negative cost cycle of infinite capacity found. Min cost flow may be unbounded below.')"
 },
 {
  "docstring": "Build a residual network and initialize a zero flow.",
  "code": "@not_implemented_for('undirected')\ndef _build_residual_network(G, demand, capacity, weight):\n    if sum((G.nodes[u].get(demand, 0) for u in G)) != 0:\n        raise nx.NetworkXUnfeasible('Sum of the demands should be 0.')\n    R = nx.MultiDiGraph()\n    R.add_nodes_from(((u, {'excess': -G.nodes[u].get(demand, 0), 'potential': 0}) for u in G))\n    inf = float('inf')\n    for u, v, e in nx.selfloop_edges(G, data=True):\n        if e.get(weight, 0) < 0 and e.get(capacity, inf) == inf:\n            raise nx.NetworkXUnbounded('Negative cost cycle of infinite capacity found. Min cost flow may be unbounded below.')\n    if G.is_multigraph():\n        edge_list = [(u, v, k, e) for u, v, k, e in G.edges(data=True, keys=True) if u != v and e.get(capacity, inf) > 0]\n    else:\n        edge_list = [(u, v, 0, e) for u, v, e in G.edges(data=True) if u != v and e.get(capacity, inf) > 0]\n    inf = max(sum((abs(R.nodes[u]['excess']) for u in R)), 2 * sum((e[capacity] for u, v, k, e in edge_list if capacity in e and e[capacity] != inf))) or 1\n    for u, v, k, e in edge_list:\n        r = min(e.get(capacity, inf), inf)\n        w = e.get(weight, 0)\n        R.add_edge(u, v, key=(k, True), capacity=r, weight=w, flow=0)\n        R.add_edge(v, u, key=(k, False), capacity=0, weight=-w, flow=0)\n    R.graph['inf'] = inf\n    _detect_unboundedness(R)\n    return R"
 },
 {
  "docstring": "Build a flow dictionary from a residual network.",
  "code": "def _build_flow_dict(G, R, capacity, weight):\n    inf = float('inf')\n    flow_dict = {}\n    if G.is_multigraph():\n        for u in G:\n            flow_dict[u] = {}\n            for v, es in G[u].items():\n                flow_dict[u][v] = {k: 0 if u != v or e.get(capacity, inf) <= 0 or e.get(weight, 0) >= 0 else e[capacity] for k, e in es.items()}\n            for v, es in R[u].items():\n                if v in flow_dict[u]:\n                    flow_dict[u][v].update(((k[0], e['flow']) for k, e in es.items() if e['flow'] > 0))\n    else:\n        for u in G:\n            flow_dict[u] = {v: 0 if u != v or e.get(capacity, inf) <= 0 or e.get(weight, 0) >= 0 else e[capacity] for v, e in G[u].items()}\n            flow_dict[u].update(((v, e['flow']) for v, es in R[u].items() for e in es.values() if e['flow'] > 0))\n    return flow_dict"
 },
 {
  "docstring": "Find a minimum cost flow satisfying all demands in digraph G.\n\nThis is a capacity scaling successive shortest augmenting path algorithm.\n\nG is a digraph with edge costs and capacities and in which nodes\nhave demand, i.e., they want to send or receive some amount of\nflow. A negative demand means that the node wants to send flow, a\npositive demand means that the node want to receive flow. A flow on\nthe digraph G satisfies all demand if the net flow into each node\nis equal to the demand of that node.\n\nParameters\n----------\nG : NetworkX graph\n    DiGraph or MultiDiGraph on which a minimum cost flow satisfying all\n    demands is to be found.\n\ndemand : string\n    Nodes of the graph G are expected to have an attribute demand\n    that indicates how much flow a node wants to send (negative\n    demand) or receive (positive demand). Note that the sum of the\n    demands should be 0 otherwise the problem in not feasible. If\n    this attribute is not present, a node is considered to have 0\n    demand. Default value: 'demand'.\n\ncapacity : string\n    Edges of the graph G are expected to have an attribute capacity\n    that indicates how much flow the edge can support. If this\n    attribute is not present, the edge is considered to have\n    infinite capacity. Default value: 'capacity'.\n\nweight : string\n    Edges of the graph G are expected to have an attribute weight\n    that indicates the cost incurred by sending one unit of flow on\n    that edge. If not present, the weight is considered to be 0.\n    Default value: 'weight'.\n\nheap : class\n    Type of heap to be used in the algorithm. It should be a subclass of\n    :class:`MinHeap` or implement a compatible interface.\n\n    If a stock heap implementation is to be used, :class:`BinaryHeap` is\n    recommended over :class:`PairingHeap` for Python implementations without\n    optimized attribute accesses (e.g., CPython) despite a slower\n    asymptotic running time. For Python implementations with optimized\n    attribute accesses (e.g., PyPy), :class:`PairingHeap` provides better\n    performance. Default value: :class:`BinaryHeap`.\n\nReturns\n-------\nflowCost : integer\n    Cost of a minimum cost flow satisfying all demands.\n\nflowDict : dictionary\n    If G is a digraph, a dict-of-dicts keyed by nodes such that\n    flowDict[u][v] is the flow on edge (u, v).\n    If G is a MultiDiGraph, a dict-of-dicts-of-dicts keyed by nodes\n    so that flowDict[u][v][key] is the flow on edge (u, v, key).\n\nRaises\n------\nNetworkXError\n    This exception is raised if the input graph is not directed,\n    not connected.\n\nNetworkXUnfeasible\n    This exception is raised in the following situations:\n\n        * The sum of the demands is not zero. Then, there is no\n          flow satisfying all demands.\n        * There is no flow satisfying all demand.\n\nNetworkXUnbounded\n    This exception is raised if the digraph G has a cycle of\n    negative cost and infinite capacity. Then, the cost of a flow\n    satisfying all demands is unbounded below.\n\n",
  "code": "@nx._dispatch(node_attrs='demand', edge_attrs={'capacity': float('inf'), 'weight': 0})\ndef capacity_scaling(G, demand='demand', capacity='capacity', weight='weight', heap=BinaryHeap):\n    R = _build_residual_network(G, demand, capacity, weight)\n    inf = float('inf')\n    flow_cost = sum((0 if e.get(capacity, inf) <= 0 or e.get(weight, 0) >= 0 else e[capacity] * e[weight] for u, v, e in nx.selfloop_edges(G, data=True)))\n    wmax = max(chain([-inf], (e['capacity'] for u, v, e in R.edges(data=True))))\n    if wmax == -inf:\n        return (flow_cost, _build_flow_dict(G, R, capacity, weight))\n    R_nodes = R.nodes\n    R_succ = R.succ\n    delta = 2 ** int(log(wmax, 2))\n    while delta >= 1:\n        for u in R:\n            p_u = R_nodes[u]['potential']\n            for v, es in R_succ[u].items():\n                for k, e in es.items():\n                    flow = e['capacity'] - e['flow']\n                    if e['weight'] - p_u + R_nodes[v]['potential'] < 0:\n                        flow = e['capacity'] - e['flow']\n                        if flow >= delta:\n                            e['flow'] += flow\n                            R_succ[v][u][k[0], not k[1]]['flow'] -= flow\n                            R_nodes[u]['excess'] -= flow\n                            R_nodes[v]['excess'] += flow\n        S = set()\n        T = set()\n        S_add = S.add\n        S_remove = S.remove\n        T_add = T.add\n        T_remove = T.remove\n        for u in R:\n            excess = R_nodes[u]['excess']\n            if excess >= delta:\n                S_add(u)\n            elif excess <= -delta:\n                T_add(u)\n        while S and T:\n            s = arbitrary_element(S)\n            t = None\n            d = {}\n            pred = {s: None}\n            h = heap()\n            h_insert = h.insert\n            h_get = h.get\n            h_insert(s, 0)\n            while h:\n                u, d_u = h.pop()\n                d[u] = d_u\n                if u in T:\n                    t = u\n                    break\n                p_u = R_nodes[u]['potential']\n                for v, es in R_succ[u].items():\n                    if v in d:\n                        continue\n                    wmin = inf\n                    for k, e in es.items():\n                        if e['capacity'] - e['flow'] >= delta:\n                            w = e['weight']\n                            if w < wmin:\n                                wmin = w\n                                kmin = k\n                                emin = e\n                    if wmin == inf:\n                        continue\n                    d_v = d_u + wmin - p_u + R_nodes[v]['potential']\n                    if h_insert(v, d_v):\n                        pred[v] = (u, kmin, emin)\n            if t is not None:\n                while u != s:\n                    v = u\n                    u, k, e = pred[v]\n                    e['flow'] += delta\n                    R_succ[v][u][k[0], not k[1]]['flow'] -= delta\n                R_nodes[s]['excess'] -= delta\n                R_nodes[t]['excess'] += delta\n                if R_nodes[s]['excess'] < delta:\n                    S_remove(s)\n                if R_nodes[t]['excess'] > -delta:\n                    T_remove(t)\n                d_t = d[t]\n                for u, d_u in d.items():\n                    R_nodes[u]['potential'] -= d_u - d_t\n            else:\n                S_remove(s)\n        delta //= 2\n    if any((R.nodes[u]['excess'] != 0 for u in R)):\n        raise nx.NetworkXUnfeasible('No flow satisfying all demands.')\n    for u in R:\n        for v, es in R_succ[u].items():\n            for e in es.values():\n                flow = e['flow']\n                if flow > 0:\n                    flow_cost += flow * e['weight']\n    return (flow_cost, _build_flow_dict(G, R, capacity, weight))"
 },
 {
  "docstring": "Find a maximum single-commodity flow using Dinitz' algorithm.\n\nThis function returns the residual network resulting after computing\nthe maximum flow. See below for details about the conventions\nNetworkX uses for defining residual networks.\n\nThis algorithm has a running time of $O(n^2 m)$ for $n$ nodes and $m$\nedges [1]_.\n\n\nParameters\n----------\nG : NetworkX graph\n    Edges of the graph are expected to have an attribute called\n    'capacity'. If this attribute is not present, the edge is\n    considered to have infinite capacity.\n\ns : node\n    Source node for the flow.\n\nt : node\n    Sink node for the flow.\n\ncapacity : string\n    Edges of the graph G are expected to have an attribute capacity\n    that indicates how much flow the edge can support. If this\n    attribute is not present, the edge is considered to have\n    infinite capacity. Default value: 'capacity'.\n\nresidual : NetworkX graph\n    Residual network on which the algorithm is to be executed. If None, a\n    new residual network is created. Default value: None.\n\nvalue_only : bool\n    If True compute only the value of the maximum flow. This parameter\n    will be ignored by this algorithm because it is not applicable.\n\ncutoff : integer, float\n    If specified, the algorithm will terminate when the flow value reaches\n    or exceeds the cutoff. In this case, it may be unable to immediately\n    determine a minimum cut. Default value: None.\n\nReturns\n-------\nR : NetworkX DiGraph\n    Residual network after computing the maximum flow.\n\nRaises\n------\nNetworkXError\n    The algorithm does not support MultiGraph and MultiDiGraph. If\n    the input graph is an instance of one of these two classes, a\n    NetworkXError is raised.\n\nNetworkXUnbounded\n    If the graph has a path of infinite capacity, the value of a\n    feasible flow on the graph is unbounded above and the function\n    raises a NetworkXUnbounded.\n\n",
  "code": "@nx._dispatch(graphs={'G': 0, 'residual?': 4}, edge_attrs={'capacity': float('inf')}, preserve_edge_attrs={'residual': {'capacity': float('inf')}}, preserve_graph_attrs={'residual'})\ndef dinitz(G, s, t, capacity='capacity', residual=None, value_only=False, cutoff=None):\n    R = dinitz_impl(G, s, t, capacity, residual, cutoff)\n    R.graph['algorithm'] = 'dinitz'\n    return R"
 },
 {
  "docstring": "Build a path using DFS starting from the sink",
  "code": "def depth_first_search(parents):\n    total_flow = 0\n    u = t\n    path = [u]\n    while True:\n        if len(parents[u]) > 0:\n            v = parents[u][0]\n            path.append(v)\n        else:\n            path.pop()\n            if len(path) == 0:\n                break\n            v = path[-1]\n            parents[v].popleft()\n        if v == s:\n            flow = INF\n            for u, v in pairwise(path):\n                flow = min(flow, R_pred[u][v]['capacity'] - R_pred[u][v]['flow'])\n            for u, v in pairwise(reversed(path)):\n                R_pred[v][u]['flow'] += flow\n                R_pred[u][v]['flow'] -= flow\n                if R_pred[v][u]['capacity'] - R_pred[v][u]['flow'] == 0:\n                    parents[v].popleft()\n                    while path[-1] != v:\n                        path.pop()\n            total_flow += flow\n            v = path[-1]\n        u = v\n    return total_flow"
 },
 {
  "docstring": "Implementation of the Edmonds-Karp algorithm.",
  "code": "@nx._dispatch(graphs='R', preserve_edge_attrs={'R': {'capacity': float('inf'), 'flow': 0}}, preserve_graph_attrs=True)\ndef edmonds_karp_core(R, s, t, cutoff):\n    R_nodes = R.nodes\n    R_pred = R.pred\n    R_succ = R.succ\n    inf = R.graph['inf']\n\n    def augment(path):\n        \"\"\"Augment flow along a path from s to t.\"\"\"\n        flow = inf\n        it = iter(path)\n        u = next(it)\n        for v in it:\n            attr = R_succ[u][v]\n            flow = min(flow, attr['capacity'] - attr['flow'])\n            u = v\n        if flow * 2 > inf:\n            raise nx.NetworkXUnbounded('Infinite capacity path, flow unbounded above.')\n        it = iter(path)\n        u = next(it)\n        for v in it:\n            R_succ[u][v]['flow'] += flow\n            R_succ[v][u]['flow'] -= flow\n            u = v\n        return flow\n\n    def bidirectional_bfs():\n        \"\"\"Bidirectional breadth-first search for an augmenting path.\"\"\"\n        pred = {s: None}\n        q_s = [s]\n        succ = {t: None}\n        q_t = [t]\n        while True:\n            q = []\n            if len(q_s) <= len(q_t):\n                for u in q_s:\n                    for v, attr in R_succ[u].items():\n                        if v not in pred and attr['flow'] < attr['capacity']:\n                            pred[v] = u\n                            if v in succ:\n                                return (v, pred, succ)\n                            q.append(v)\n                if not q:\n                    return (None, None, None)\n                q_s = q\n            else:\n                for u in q_t:\n                    for v, attr in R_pred[u].items():\n                        if v not in succ and attr['flow'] < attr['capacity']:\n                            succ[v] = u\n                            if v in pred:\n                                return (v, pred, succ)\n                            q.append(v)\n                if not q:\n                    return (None, None, None)\n                q_t = q\n    flow_value = 0\n    while flow_value < cutoff:\n        v, pred, succ = bidirectional_bfs()\n        if pred is None:\n            break\n        path = [v]\n        u = v\n        while u != s:\n            u = pred[u]\n            path.append(u)\n        path.reverse()\n        u = v\n        while u != t:\n            u = succ[u]\n            path.append(u)\n        flow_value += augment(path)\n    return flow_value"
 },
 {
  "docstring": "Implementation of the Edmonds-Karp algorithm.",
  "code": "def edmonds_karp_impl(G, s, t, capacity, residual, cutoff):\n    if s not in G:\n        raise nx.NetworkXError(f'node {str(s)} not in graph')\n    if t not in G:\n        raise nx.NetworkXError(f'node {str(t)} not in graph')\n    if s == t:\n        raise nx.NetworkXError('source and sink are the same node')\n    if residual is None:\n        R = build_residual_network(G, capacity)\n    else:\n        R = residual\n    for u in R:\n        for e in R[u].values():\n            e['flow'] = 0\n    if cutoff is None:\n        cutoff = float('inf')\n    R.graph['flow_value'] = edmonds_karp_core(R, s, t, cutoff)\n    return R"
 },
 {
  "docstring": "Find a maximum single-commodity flow using the Edmonds-Karp algorithm.\n\nThis function returns the residual network resulting after computing\nthe maximum flow. See below for details about the conventions\nNetworkX uses for defining residual networks.\n\nThis algorithm has a running time of $O(n m^2)$ for $n$ nodes and $m$\nedges.\n\n\nParameters\n----------\nG : NetworkX graph\n    Edges of the graph are expected to have an attribute called\n    'capacity'. If this attribute is not present, the edge is\n    considered to have infinite capacity.\n\ns : node\n    Source node for the flow.\n\nt : node\n    Sink node for the flow.\n\ncapacity : string\n    Edges of the graph G are expected to have an attribute capacity\n    that indicates how much flow the edge can support. If this\n    attribute is not present, the edge is considered to have\n    infinite capacity. Default value: 'capacity'.\n\nresidual : NetworkX graph\n    Residual network on which the algorithm is to be executed. If None, a\n    new residual network is created. Default value: None.\n\nvalue_only : bool\n    If True compute only the value of the maximum flow. This parameter\n    will be ignored by this algorithm because it is not applicable.\n\ncutoff : integer, float\n    If specified, the algorithm will terminate when the flow value reaches\n    or exceeds the cutoff. In this case, it may be unable to immediately\n    determine a minimum cut. Default value: None.\n\nReturns\n-------\nR : NetworkX DiGraph\n    Residual network after computing the maximum flow.\n\nRaises\n------\nNetworkXError\n    The algorithm does not support MultiGraph and MultiDiGraph. If\n    the input graph is an instance of one of these two classes, a\n    NetworkXError is raised.\n\nNetworkXUnbounded\n    If the graph has a path of infinite capacity, the value of a\n    feasible flow on the graph is unbounded above and the function\n    raises a NetworkXUnbounded.\n\n",
  "code": "@nx._dispatch(graphs={'G': 0, 'residual?': 4}, edge_attrs={'capacity': float('inf')}, preserve_edge_attrs={'residual': {'capacity': float('inf')}}, preserve_graph_attrs={'residual'})\ndef edmonds_karp(G, s, t, capacity='capacity', residual=None, value_only=False, cutoff=None):\n    R = edmonds_karp_impl(G, s, t, capacity, residual, cutoff)\n    R.graph['algorithm'] = 'edmonds_karp'\n    return R"
 },
 {
  "docstring": "Augment flow along a path from s to t.",
  "code": "def augment(path):\n    flow = inf\n    it = iter(path)\n    u = next(it)\n    for v in it:\n        attr = R_succ[u][v]\n        flow = min(flow, attr['capacity'] - attr['flow'])\n        u = v\n    if flow * 2 > inf:\n        raise nx.NetworkXUnbounded('Infinite capacity path, flow unbounded above.')\n    it = iter(path)\n    u = next(it)\n    for v in it:\n        R_succ[u][v]['flow'] += flow\n        R_succ[v][u]['flow'] -= flow\n        u = v\n    return flow"
 },
 {
  "docstring": "Bidirectional breadth-first search for an augmenting path.",
  "code": "def bidirectional_bfs():\n    pred = {s: None}\n    q_s = [s]\n    succ = {t: None}\n    q_t = [t]\n    while True:\n        q = []\n        if len(q_s) <= len(q_t):\n            for u in q_s:\n                for v, attr in R_succ[u].items():\n                    if v not in pred and attr['flow'] < attr['capacity']:\n                        pred[v] = u\n                        if v in succ:\n                            return (v, pred, succ)\n                        q.append(v)\n            if not q:\n                return (None, None, None)\n            q_s = q\n        else:\n            for u in q_t:\n                for v, attr in R_pred[u].items():\n                    if v not in succ and attr['flow'] < attr['capacity']:\n                        succ[v] = u\n                        if v in pred:\n                            return (v, pred, succ)\n                        q.append(v)\n            if not q:\n                return (None, None, None)\n            q_t = q"
 },
 {
  "docstring": "Returns the Gomory-Hu tree of an undirected graph G.\n\nA Gomory-Hu tree of an undirected graph with capacities is a\nweighted tree that represents the minimum s-t cuts for all s-t\npairs in the graph.\n\nIt only requires `n-1` minimum cut computations instead of the\nobvious `n(n-1)/2`. The tree represents all s-t cuts as the\nminimum cut value among any pair of nodes is the minimum edge\nweight in the shortest path between the two nodes in the\nGomory-Hu tree.\n\nThe Gomory-Hu tree also has the property that removing the\nedge with the minimum weight in the shortest path between\nany two nodes leaves two connected components that form\na partition of the nodes in G that defines the minimum s-t\ncut.\n\nSee Examples section below for details.\n\nParameters\n----------\nG : NetworkX graph\n    Undirected graph\n\ncapacity : string\n    Edges of the graph G are expected to have an attribute capacity\n    that indicates how much flow the edge can support. If this\n    attribute is not present, the edge is considered to have\n    infinite capacity. Default value: 'capacity'.\n\nflow_func : function\n    Function to perform the underlying flow computations. Default value\n    :func:`edmonds_karp`. This function performs better in sparse graphs\n    with right tailed degree distributions.\n    :func:`shortest_augmenting_path` will perform better in denser\n    graphs.\n\nReturns\n-------\nTree : NetworkX graph\n    A NetworkX graph representing the Gomory-Hu tree of the input graph.\n\nRaises\n------\nNetworkXNotImplemented\n    Raised if the input graph is directed.\n\nNetworkXError\n    Raised if the input graph is an empty Graph.\n\nExamples\n--------\n>>> G = nx.karate_club_graph()\n>>> nx.set_edge_attributes(G, 1, \"capacity\")\n>>> T = nx.gomory_hu_tree(G)\n>>> # The value of the minimum cut between any pair\n... # of nodes in G is the minimum edge weight in the\n... # shortest path between the two nodes in the\n... # Gomory-Hu tree.\n... def minimum_edge_weight_in_shortest_path(T, u, v):\n...     path = nx.shortest_path(T, u, v, weight=\"weight\")\n...     return min((T[u][v][\"weight\"], (u, v)) for (u, v) in zip(path, path[1:]))\n>>> u, v = 0, 33\n>>> cut_value, edge = minimum_edge_weight_in_shortest_path(T, u, v)\n>>> cut_value\n10\n>>> nx.minimum_cut_value(G, u, v)\n10\n>>> # The Gomory-Hu tree also has the property that removing the\n... # edge with the minimum weight in the shortest path between\n... # any two nodes leaves two connected components that form\n... # a partition of the nodes in G that defines the minimum s-t\n... # cut.\n... cut_value, edge = minimum_edge_weight_in_shortest_path(T, u, v)\n>>> T.remove_edge(*edge)\n>>> U, V = list(nx.connected_components(T))\n>>> # Thus U and V form a partition that defines a minimum cut\n... # between u and v in G. You can compute the edge cut set,\n... # that is, the set of edges that if removed from G will\n... # disconnect u from v in G, with this information:\n... cutset = set()\n>>> for x, nbrs in ((n, G[n]) for n in U):\n...     cutset.update((x, y) for y in nbrs if y in V)\n>>> # Because we have set the capacities of all edges to 1\n... # the cutset contains ten edges\n... len(cutset)\n10\n>>> # You can use any maximum flow algorithm for the underlying\n... # flow computations using the argument flow_func\n... from networkx.algorithms import flow\n>>> T = nx.gomory_hu_tree(G, flow_func=flow.boykov_kolmogorov)\n>>> cut_value, edge = minimum_edge_weight_in_shortest_path(T, u, v)\n>>> cut_value\n10\n>>> nx.minimum_cut_value(G, u, v, flow_func=flow.boykov_kolmogorov)\n10\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch(edge_attrs={'capacity': float('inf')})\ndef gomory_hu_tree(G, capacity='capacity', flow_func=None):\n    if flow_func is None:\n        flow_func = default_flow_func\n    if len(G) == 0:\n        msg = 'Empty Graph does not have a Gomory-Hu tree representation'\n        raise nx.NetworkXError(msg)\n    tree = {}\n    labels = {}\n    iter_nodes = iter(G)\n    root = next(iter_nodes)\n    for n in iter_nodes:\n        tree[n] = root\n    R = build_residual_network(G, capacity)\n    for source in tree:\n        target = tree[source]\n        cut_value, partition = nx.minimum_cut(G, source, target, capacity=capacity, flow_func=flow_func, residual=R)\n        labels[source, target] = cut_value\n        for node in partition[0]:\n            if node != source and node in tree and (tree[node] == target):\n                tree[node] = source\n                labels[node, source] = labels.get((node, target), cut_value)\n        if target != root and tree[target] in partition[0]:\n            labels[source, tree[target]] = labels[target, tree[target]]\n            labels[target, source] = cut_value\n            tree[source] = tree[target]\n            tree[target] = source\n    T = nx.Graph()\n    T.add_nodes_from(G)\n    T.add_weighted_edges_from(((u, v, labels[u, v]) for u, v in tree.items()))\n    return T"
 },
 {
  "docstring": "Find a maximum single-commodity flow.\n\nParameters\n----------\nflowG : NetworkX graph\n    Edges of the graph are expected to have an attribute called\n    'capacity'. If this attribute is not present, the edge is\n    considered to have infinite capacity.\n\n_s : node\n    Source node for the flow.\n\n_t : node\n    Sink node for the flow.\n\ncapacity : string\n    Edges of the graph G are expected to have an attribute capacity\n    that indicates how much flow the edge can support. If this\n    attribute is not present, the edge is considered to have\n    infinite capacity. Default value: 'capacity'.\n\nflow_func : function\n    A function for computing the maximum flow among a pair of nodes\n    in a capacitated graph. The function has to accept at least three\n    parameters: a Graph or Digraph, a source node, and a target node.\n    And return a residual network that follows NetworkX conventions\n    (see Notes). If flow_func is None, the default maximum\n    flow function (:meth:`preflow_push`) is used. See below for\n    alternative algorithms. The choice of the default function may change\n    from version to version and should not be relied on. Default value:\n    None.\n\nkwargs : Any other keyword parameter is passed to the function that\n    computes the maximum flow.\n\nReturns\n-------\nflow_value : integer, float\n    Value of the maximum flow, i.e., net outflow from the source.\n\nflow_dict : dict\n    A dictionary containing the value of the flow that went through\n    each edge.\n\nRaises\n------\nNetworkXError\n    The algorithm does not support MultiGraph and MultiDiGraph. If\n    the input graph is an instance of one of these two classes, a\n    NetworkXError is raised.\n\nNetworkXUnbounded\n    If the graph has a path of infinite capacity, the value of a\n    feasible flow on the graph is unbounded above and the function\n    raises a NetworkXUnbounded.\n\n",
  "code": "@nx._dispatch(graphs='flowG', edge_attrs={'capacity': float('inf')})\ndef maximum_flow(flowG, _s, _t, capacity='capacity', flow_func=None, **kwargs):\n    if flow_func is None:\n        if kwargs:\n            raise nx.NetworkXError('You have to explicitly set a flow_func if you need to pass parameters via kwargs.')\n        flow_func = default_flow_func\n    if not callable(flow_func):\n        raise nx.NetworkXError('flow_func has to be callable.')\n    R = flow_func(flowG, _s, _t, capacity=capacity, value_only=False, **kwargs)\n    flow_dict = build_flow_dict(flowG, R)\n    return (R.graph['flow_value'], flow_dict)"
 },
 {
  "docstring": "Find the value of maximum single-commodity flow.\n\nParameters\n----------\nflowG : NetworkX graph\n    Edges of the graph are expected to have an attribute called\n    'capacity'. If this attribute is not present, the edge is\n    considered to have infinite capacity.\n\n_s : node\n    Source node for the flow.\n\n_t : node\n    Sink node for the flow.\n\ncapacity : string\n    Edges of the graph G are expected to have an attribute capacity\n    that indicates how much flow the edge can support. If this\n    attribute is not present, the edge is considered to have\n    infinite capacity. Default value: 'capacity'.\n\nflow_func : function\n    A function for computing the maximum flow among a pair of nodes\n    in a capacitated graph. The function has to accept at least three\n    parameters: a Graph or Digraph, a source node, and a target node.\n    And return a residual network that follows NetworkX conventions\n    (see Notes). If flow_func is None, the default maximum\n    flow function (:meth:`preflow_push`) is used. See below for\n    alternative algorithms. The choice of the default function may change\n    from version to version and should not be relied on. Default value:\n    None.\n\nkwargs : Any other keyword parameter is passed to the function that\n    computes the maximum flow.\n\nReturns\n-------\nflow_value : integer, float\n    Value of the maximum flow, i.e., net outflow from the source.\n\nRaises\n------\nNetworkXError\n    The algorithm does not support MultiGraph and MultiDiGraph. If\n    the input graph is an instance of one of these two classes, a\n    NetworkXError is raised.\n\nNetworkXUnbounded\n    If the graph has a path of infinite capacity, the value of a\n    feasible flow on the graph is unbounded above and the function\n    raises a NetworkXUnbounded.\n\n",
  "code": "@nx._dispatch(graphs='flowG', edge_attrs={'capacity': float('inf')})\ndef maximum_flow_value(flowG, _s, _t, capacity='capacity', flow_func=None, **kwargs):\n    if flow_func is None:\n        if kwargs:\n            raise nx.NetworkXError('You have to explicitly set a flow_func if you need to pass parameters via kwargs.')\n        flow_func = default_flow_func\n    if not callable(flow_func):\n        raise nx.NetworkXError('flow_func has to be callable.')\n    R = flow_func(flowG, _s, _t, capacity=capacity, value_only=True, **kwargs)\n    return R.graph['flow_value']"
 },
 {
  "docstring": "Compute the value and the node partition of a minimum (s, t)-cut.\n\nUse the max-flow min-cut theorem, i.e., the capacity of a minimum\ncapacity cut is equal to the flow value of a maximum flow.\n\nParameters\n----------\nflowG : NetworkX graph\n    Edges of the graph are expected to have an attribute called\n    'capacity'. If this attribute is not present, the edge is\n    considered to have infinite capacity.\n\n_s : node\n    Source node for the flow.\n\n_t : node\n    Sink node for the flow.\n\ncapacity : string\n    Edges of the graph G are expected to have an attribute capacity\n    that indicates how much flow the edge can support. If this\n    attribute is not present, the edge is considered to have\n    infinite capacity. Default value: 'capacity'.\n\nflow_func : function\n    A function for computing the maximum flow among a pair of nodes\n    in a capacitated graph. The function has to accept at least three\n    parameters: a Graph or Digraph, a source node, and a target node.\n    And return a residual network that follows NetworkX conventions\n    (see Notes). If flow_func is None, the default maximum\n    flow function (:meth:`preflow_push`) is used. See below for\n    alternative algorithms. The choice of the default function may change\n    from version to version and should not be relied on. Default value:\n    None.\n\nkwargs : Any other keyword parameter is passed to the function that\n    computes the maximum flow.\n\nReturns\n-------\ncut_value : integer, float\n    Value of the minimum cut.\n\npartition : pair of node sets\n    A partitioning of the nodes that defines a minimum cut.\n\nRaises\n------\nNetworkXUnbounded\n    If the graph has a path of infinite capacity, all cuts have\n    infinite capacity and the function raises a NetworkXError.\n\n",
  "code": "@nx._dispatch(graphs='flowG', edge_attrs={'capacity': float('inf')})\ndef minimum_cut(flowG, _s, _t, capacity='capacity', flow_func=None, **kwargs):\n    if flow_func is None:\n        if kwargs:\n            raise nx.NetworkXError('You have to explicitly set a flow_func if you need to pass parameters via kwargs.')\n        flow_func = default_flow_func\n    if not callable(flow_func):\n        raise nx.NetworkXError('flow_func has to be callable.')\n    if kwargs.get('cutoff') is not None and flow_func is preflow_push:\n        raise nx.NetworkXError('cutoff should not be specified.')\n    R = flow_func(flowG, _s, _t, capacity=capacity, value_only=True, **kwargs)\n    cutset = [(u, v, d) for u, v, d in R.edges(data=True) if d['flow'] == d['capacity']]\n    R.remove_edges_from(cutset)\n    non_reachable = set(dict(nx.shortest_path_length(R, target=_t)))\n    partition = (set(flowG) - non_reachable, non_reachable)\n    if cutset is not None:\n        R.add_edges_from(cutset)\n    return (R.graph['flow_value'], partition)"
 },
 {
  "docstring": "Compute the value of a minimum (s, t)-cut.\n\nUse the max-flow min-cut theorem, i.e., the capacity of a minimum\ncapacity cut is equal to the flow value of a maximum flow.\n\nParameters\n----------\nflowG : NetworkX graph\n    Edges of the graph are expected to have an attribute called\n    'capacity'. If this attribute is not present, the edge is\n    considered to have infinite capacity.\n\n_s : node\n    Source node for the flow.\n\n_t : node\n    Sink node for the flow.\n\ncapacity : string\n    Edges of the graph G are expected to have an attribute capacity\n    that indicates how much flow the edge can support. If this\n    attribute is not present, the edge is considered to have\n    infinite capacity. Default value: 'capacity'.\n\nflow_func : function\n    A function for computing the maximum flow among a pair of nodes\n    in a capacitated graph. The function has to accept at least three\n    parameters: a Graph or Digraph, a source node, and a target node.\n    And return a residual network that follows NetworkX conventions\n    (see Notes). If flow_func is None, the default maximum\n    flow function (:meth:`preflow_push`) is used. See below for\n    alternative algorithms. The choice of the default function may change\n    from version to version and should not be relied on. Default value:\n    None.\n\nkwargs : Any other keyword parameter is passed to the function that\n    computes the maximum flow.\n\nReturns\n-------\ncut_value : integer, float\n    Value of the minimum cut.\n\nRaises\n------\nNetworkXUnbounded\n    If the graph has a path of infinite capacity, all cuts have\n    infinite capacity and the function raises a NetworkXError.\n\n",
  "code": "@nx._dispatch(graphs='flowG', edge_attrs={'capacity': float('inf')})\ndef minimum_cut_value(flowG, _s, _t, capacity='capacity', flow_func=None, **kwargs):\n    if flow_func is None:\n        if kwargs:\n            raise nx.NetworkXError('You have to explicitly set a flow_func if you need to pass parameters via kwargs.')\n        flow_func = default_flow_func\n    if not callable(flow_func):\n        raise nx.NetworkXError('flow_func has to be callable.')\n    if kwargs.get('cutoff') is not None and flow_func is preflow_push:\n        raise nx.NetworkXError('cutoff should not be specified.')\n    R = flow_func(flowG, _s, _t, capacity=capacity, value_only=True, **kwargs)\n    return R.graph['flow_value']"
 },
 {
  "docstring": "Find the cost of a minimum cost flow satisfying all demands in digraph G.\n\nG is a digraph with edge costs and capacities and in which nodes\nhave demand, i.e., they want to send or receive some amount of\nflow. A negative demand means that the node wants to send flow, a\npositive demand means that the node want to receive flow. A flow on\nthe digraph G satisfies all demand if the net flow into each node\nis equal to the demand of that node.\n\nParameters\n----------\nG : NetworkX graph\n    DiGraph on which a minimum cost flow satisfying all demands is\n    to be found.\n\ndemand : string\n    Nodes of the graph G are expected to have an attribute demand\n    that indicates how much flow a node wants to send (negative\n    demand) or receive (positive demand). Note that the sum of the\n    demands should be 0 otherwise the problem in not feasible. If\n    this attribute is not present, a node is considered to have 0\n    demand. Default value: 'demand'.\n\ncapacity : string\n    Edges of the graph G are expected to have an attribute capacity\n    that indicates how much flow the edge can support. If this\n    attribute is not present, the edge is considered to have\n    infinite capacity. Default value: 'capacity'.\n\nweight : string\n    Edges of the graph G are expected to have an attribute weight\n    that indicates the cost incurred by sending one unit of flow on\n    that edge. If not present, the weight is considered to be 0.\n    Default value: 'weight'.\n\nReturns\n-------\nflowCost : integer, float\n    Cost of a minimum cost flow satisfying all demands.\n\nRaises\n------\nNetworkXError\n    This exception is raised if the input graph is not directed or\n    not connected.\n\nNetworkXUnfeasible\n    This exception is raised in the following situations:\n\n        * The sum of the demands is not zero. Then, there is no\n          flow satisfying all demands.\n        * There is no flow satisfying all demand.\n\nNetworkXUnbounded\n    This exception is raised if the digraph G has a cycle of\n    negative cost and infinite capacity. Then, the cost of a flow\n    satisfying all demands is unbounded below.\n\n",
  "code": "@nx._dispatch(node_attrs='demand', edge_attrs={'capacity': float('inf'), 'weight': 0})\ndef min_cost_flow_cost(G, demand='demand', capacity='capacity', weight='weight'):\n    return nx.network_simplex(G, demand=demand, capacity=capacity, weight=weight)[0]"
 },
 {
  "docstring": "Returns a minimum cost flow satisfying all demands in digraph G.\n\nG is a digraph with edge costs and capacities and in which nodes\nhave demand, i.e., they want to send or receive some amount of\nflow. A negative demand means that the node wants to send flow, a\npositive demand means that the node want to receive flow. A flow on\nthe digraph G satisfies all demand if the net flow into each node\nis equal to the demand of that node.\n\nParameters\n----------\nG : NetworkX graph\n    DiGraph on which a minimum cost flow satisfying all demands is\n    to be found.\n\ndemand : string\n    Nodes of the graph G are expected to have an attribute demand\n    that indicates how much flow a node wants to send (negative\n    demand) or receive (positive demand). Note that the sum of the\n    demands should be 0 otherwise the problem in not feasible. If\n    this attribute is not present, a node is considered to have 0\n    demand. Default value: 'demand'.\n\ncapacity : string\n    Edges of the graph G are expected to have an attribute capacity\n    that indicates how much flow the edge can support. If this\n    attribute is not present, the edge is considered to have\n    infinite capacity. Default value: 'capacity'.\n\nweight : string\n    Edges of the graph G are expected to have an attribute weight\n    that indicates the cost incurred by sending one unit of flow on\n    that edge. If not present, the weight is considered to be 0.\n    Default value: 'weight'.\n\nReturns\n-------\nflowDict : dictionary\n    Dictionary of dictionaries keyed by nodes such that\n    flowDict[u][v] is the flow edge (u, v).\n\nRaises\n------\nNetworkXError\n    This exception is raised if the input graph is not directed or\n    not connected.\n\nNetworkXUnfeasible\n    This exception is raised in the following situations:\n\n        * The sum of the demands is not zero. Then, there is no\n          flow satisfying all demands.\n        * There is no flow satisfying all demand.\n\nNetworkXUnbounded\n    This exception is raised if the digraph G has a cycle of\n    negative cost and infinite capacity. Then, the cost of a flow\n    satisfying all demands is unbounded below.\n\n",
  "code": "@nx._dispatch(node_attrs='demand', edge_attrs={'capacity': float('inf'), 'weight': 0})\ndef min_cost_flow(G, demand='demand', capacity='capacity', weight='weight'):\n    return nx.network_simplex(G, demand=demand, capacity=capacity, weight=weight)[1]"
 },
 {
  "docstring": "Compute the cost of the flow given by flowDict on graph G.\n\nNote that this function does not check for the validity of the\nflow flowDict. This function will fail if the graph G and the\nflow don't have the same edge set.\n\nParameters\n----------\nG : NetworkX graph\n    DiGraph on which a minimum cost flow satisfying all demands is\n    to be found.\n\nweight : string\n    Edges of the graph G are expected to have an attribute weight\n    that indicates the cost incurred by sending one unit of flow on\n    that edge. If not present, the weight is considered to be 0.\n    Default value: 'weight'.\n\nflowDict : dictionary\n    Dictionary of dictionaries keyed by nodes such that\n    flowDict[u][v] is the flow edge (u, v).\n\nReturns\n-------\ncost : Integer, float\n    The total cost of the flow. This is given by the sum over all\n    edges of the product of the edge's flow and the edge's weight.\n\n",
  "code": "@nx._dispatch(edge_attrs={'weight': 0})\ndef cost_of_flow(G, flowDict, weight='weight'):\n    return sum((flowDict[u][v] * d.get(weight, 0) for u, v, d in G.edges(data=True)))"
 },
 {
  "docstring": "Returns a maximum (s, t)-flow of minimum cost.\n\nG is a digraph with edge costs and capacities. There is a source\nnode s and a sink node t. This function finds a maximum flow from\ns to t whose total cost is minimized.\n\nParameters\n----------\nG : NetworkX graph\n    DiGraph on which a minimum cost flow satisfying all demands is\n    to be found.\n\ns: node label\n    Source of the flow.\n\nt: node label\n    Destination of the flow.\n\ncapacity: string\n    Edges of the graph G are expected to have an attribute capacity\n    that indicates how much flow the edge can support. If this\n    attribute is not present, the edge is considered to have\n    infinite capacity. Default value: 'capacity'.\n\nweight: string\n    Edges of the graph G are expected to have an attribute weight\n    that indicates the cost incurred by sending one unit of flow on\n    that edge. If not present, the weight is considered to be 0.\n    Default value: 'weight'.\n\nReturns\n-------\nflowDict: dictionary\n    Dictionary of dictionaries keyed by nodes such that\n    flowDict[u][v] is the flow edge (u, v).\n\nRaises\n------\nNetworkXError\n    This exception is raised if the input graph is not directed or\n    not connected.\n\nNetworkXUnbounded\n    This exception is raised if there is an infinite capacity path\n    from s to t in G. In this case there is no maximum flow. This\n    exception is also raised if the digraph G has a cycle of\n    negative cost and infinite capacity. Then, the cost of a flow\n    is unbounded below.\n\n",
  "code": "@nx._dispatch(edge_attrs={'capacity': float('inf'), 'weight': 0})\ndef max_flow_min_cost(G, s, t, capacity='capacity', weight='weight'):\n    maxFlow = nx.maximum_flow_value(G, s, t, capacity=capacity)\n    H = nx.DiGraph(G)\n    H.add_node(s, demand=-maxFlow)\n    H.add_node(t, demand=maxFlow)\n    return min_cost_flow(H, capacity=capacity, weight=weight)"
 },
 {
  "docstring": "Find a minimum cost flow satisfying all demands in digraph G.\n\nThis is a primal network simplex algorithm that uses the leaving\narc rule to prevent cycling.\n\nG is a digraph with edge costs and capacities and in which nodes\nhave demand, i.e., they want to send or receive some amount of\nflow. A negative demand means that the node wants to send flow, a\npositive demand means that the node want to receive flow. A flow on\nthe digraph G satisfies all demand if the net flow into each node\nis equal to the demand of that node.\n\nParameters\n----------\nG : NetworkX graph\n    DiGraph on which a minimum cost flow satisfying all demands is\n    to be found.\n\ndemand : string\n    Nodes of the graph G are expected to have an attribute demand\n    that indicates how much flow a node wants to send (negative\n    demand) or receive (positive demand). Note that the sum of the\n    demands should be 0 otherwise the problem in not feasible. If\n    this attribute is not present, a node is considered to have 0\n    demand. Default value: 'demand'.\n\ncapacity : string\n    Edges of the graph G are expected to have an attribute capacity\n    that indicates how much flow the edge can support. If this\n    attribute is not present, the edge is considered to have\n    infinite capacity. Default value: 'capacity'.\n\nweight : string\n    Edges of the graph G are expected to have an attribute weight\n    that indicates the cost incurred by sending one unit of flow on\n    that edge. If not present, the weight is considered to be 0.\n    Default value: 'weight'.\n\nReturns\n-------\nflowCost : integer, float\n    Cost of a minimum cost flow satisfying all demands.\n\nflowDict : dictionary\n    Dictionary of dictionaries keyed by nodes such that\n    flowDict[u][v] is the flow edge (u, v).\n\nRaises\n------\nNetworkXError\n    This exception is raised if the input graph is not directed or\n    not connected.\n\nNetworkXUnfeasible\n    This exception is raised in the following situations:\n\n        * The sum of the demands is not zero. Then, there is no\n          flow satisfying all demands.\n        * There is no flow satisfying all demand.\n\nNetworkXUnbounded\n    This exception is raised if the digraph G has a cycle of\n    negative cost and infinite capacity. Then, the cost of a flow\n    satisfying all demands is unbounded below.\n\n",
  "code": "@not_implemented_for('undirected')\n@nx._dispatch(node_attrs='demand', edge_attrs={'capacity': float('inf'), 'weight': 0})\ndef network_simplex(G, demand='demand', capacity='capacity', weight='weight'):\n    if len(G) == 0:\n        raise nx.NetworkXError('graph has no nodes')\n    multigraph = G.is_multigraph()\n    DEAF = _DataEssentialsAndFunctions(G, multigraph, demand=demand, capacity=capacity, weight=weight)\n    inf = float('inf')\n    for u, d in zip(DEAF.node_list, DEAF.node_demands):\n        if abs(d) == inf:\n            raise nx.NetworkXError(f'node {u!r} has infinite demand')\n    for e, w in zip(DEAF.edge_indices, DEAF.edge_weights):\n        if abs(w) == inf:\n            raise nx.NetworkXError(f'edge {e!r} has infinite weight')\n    if not multigraph:\n        edges = nx.selfloop_edges(G, data=True)\n    else:\n        edges = nx.selfloop_edges(G, data=True, keys=True)\n    for e in edges:\n        if abs(e[-1].get(weight, 0)) == inf:\n            raise nx.NetworkXError(f'edge {e[:-1]!r} has infinite weight')\n    if sum(DEAF.node_demands) != 0:\n        raise nx.NetworkXUnfeasible('total node demand is not zero')\n    for e, c in zip(DEAF.edge_indices, DEAF.edge_capacities):\n        if c < 0:\n            raise nx.NetworkXUnfeasible(f'edge {e!r} has negative capacity')\n    if not multigraph:\n        edges = nx.selfloop_edges(G, data=True)\n    else:\n        edges = nx.selfloop_edges(G, data=True, keys=True)\n    for e in edges:\n        if e[-1].get(capacity, inf) < 0:\n            raise nx.NetworkXUnfeasible(f'edge {e[:-1]!r} has negative capacity')\n    for i, d in enumerate(DEAF.node_demands):\n        if d > 0:\n            DEAF.edge_sources.append(-1)\n            DEAF.edge_targets.append(i)\n        else:\n            DEAF.edge_sources.append(i)\n            DEAF.edge_targets.append(-1)\n    faux_inf = 3 * max(chain([sum((c for c in DEAF.edge_capacities if c < inf)), sum((abs(w) for w in DEAF.edge_weights))], (abs(d) for d in DEAF.node_demands))) or 1\n    n = len(DEAF.node_list)\n    DEAF.edge_weights.extend(repeat(faux_inf, n))\n    DEAF.edge_capacities.extend(repeat(faux_inf, n))\n    DEAF.initialize_spanning_tree(n, faux_inf)\n    for i, p, q in DEAF.find_entering_edges():\n        Wn, We = DEAF.find_cycle(i, p, q)\n        j, s, t = DEAF.find_leaving_edge(Wn, We)\n        DEAF.augment_flow(Wn, We, DEAF.residual_capacity(j, s))\n        if i != j:\n            if DEAF.parent[t] != s:\n                s, t = (t, s)\n            if We.index(i) > We.index(j):\n                p, q = (q, p)\n            DEAF.remove_edge(s, t)\n            DEAF.make_root(q)\n            DEAF.add_edge(i, p, q)\n            DEAF.update_potentials(i, p, q)\n    if any((DEAF.edge_flow[i] != 0 for i in range(-n, 0))):\n        raise nx.NetworkXUnfeasible('no flow satisfies all node demands')\n    if any((DEAF.edge_flow[i] * 2 >= faux_inf for i in range(DEAF.edge_count))) or any((e[-1].get(capacity, inf) == inf and e[-1].get(weight, 0) < 0 for e in nx.selfloop_edges(G, data=True))):\n        raise nx.NetworkXUnbounded('negative cycle with infinite capacity found')\n    del DEAF.edge_flow[DEAF.edge_count:]\n    flow_cost = sum((w * x for w, x in zip(DEAF.edge_weights, DEAF.edge_flow)))\n    flow_dict = {n: {} for n in DEAF.node_list}\n\n    def add_entry(e):\n        \"\"\"Add a flow dict entry.\"\"\"\n        d = flow_dict[e[0]]\n        for k in e[1:-2]:\n            try:\n                d = d[k]\n            except KeyError:\n                t = {}\n                d[k] = t\n                d = t\n        d[e[-2]] = e[-1]\n    DEAF.edge_sources = (DEAF.node_list[s] for s in DEAF.edge_sources)\n    DEAF.edge_targets = (DEAF.node_list[t] for t in DEAF.edge_targets)\n    if not multigraph:\n        for e in zip(DEAF.edge_sources, DEAF.edge_targets, DEAF.edge_flow):\n            add_entry(e)\n        edges = G.edges(data=True)\n    else:\n        for e in zip(DEAF.edge_sources, DEAF.edge_targets, DEAF.edge_keys, DEAF.edge_flow):\n            add_entry(e)\n        edges = G.edges(data=True, keys=True)\n    for e in edges:\n        if e[0] != e[1]:\n            if e[-1].get(capacity, inf) == 0:\n                add_entry(e[:-1] + (0,))\n        else:\n            w = e[-1].get(weight, 0)\n            if w >= 0:\n                add_entry(e[:-1] + (0,))\n            else:\n                c = e[-1][capacity]\n                flow_cost += w * c\n                add_entry(e[:-1] + (c,))\n    return (flow_cost, flow_dict)"
 },
 {
  "docstring": "Find the lowest common ancestor of nodes p and q in the spanning tree.",
  "code": "def find_apex(self, p, q):\n    size_p = self.subtree_size[p]\n    size_q = self.subtree_size[q]\n    while True:\n        while size_p < size_q:\n            p = self.parent[p]\n            size_p = self.subtree_size[p]\n        while size_p > size_q:\n            q = self.parent[q]\n            size_q = self.subtree_size[q]\n        if size_p == size_q:\n            if p != q:\n                p = self.parent[p]\n                size_p = self.subtree_size[p]\n                q = self.parent[q]\n                size_q = self.subtree_size[q]\n            else:\n                return p"
 },
 {
  "docstring": "Returns the nodes and edges on the path from node p to its ancestor w.",
  "code": "def trace_path(self, p, w):\n    Wn = [p]\n    We = []\n    while p != w:\n        We.append(self.parent_edge[p])\n        p = self.parent[p]\n        Wn.append(p)\n    return (Wn, We)"
 },
 {
  "docstring": "Returns the nodes and edges on the cycle containing edge i == (p, q)\nwhen the latter is added to the spanning tree.\n\nThe cycle is oriented in the direction from p to q.",
  "code": "def find_cycle(self, i, p, q):\n    w = self.find_apex(p, q)\n    Wn, We = self.trace_path(p, w)\n    Wn.reverse()\n    We.reverse()\n    if We != [i]:\n        We.append(i)\n    WnR, WeR = self.trace_path(q, w)\n    del WnR[-1]\n    Wn += WnR\n    We += WeR\n    return (Wn, We)"
 },
 {
  "docstring": "Augment f units of flow along a cycle represented by Wn and We.",
  "code": "def augment_flow(self, Wn, We, f):\n    for i, p in zip(We, Wn):\n        if self.edge_sources[i] == p:\n            self.edge_flow[i] += f\n        else:\n            self.edge_flow[i] -= f"
 },
 {
  "docstring": "Yield the nodes in the subtree rooted at a node p.",
  "code": "def trace_subtree(self, p):\n    yield p\n    l = self.last_descendent_dft[p]\n    while p != l:\n        p = self.next_node_dft[p]\n        yield p"
 },
 {
  "docstring": "Remove an edge (s, t) where parent[t] == s from the spanning tree.",
  "code": "def remove_edge(self, s, t):\n    size_t = self.subtree_size[t]\n    prev_t = self.prev_node_dft[t]\n    last_t = self.last_descendent_dft[t]\n    next_last_t = self.next_node_dft[last_t]\n    self.parent[t] = None\n    self.parent_edge[t] = None\n    self.next_node_dft[prev_t] = next_last_t\n    self.prev_node_dft[next_last_t] = prev_t\n    self.next_node_dft[last_t] = t\n    self.prev_node_dft[t] = last_t\n    while s is not None:\n        self.subtree_size[s] -= size_t\n        if self.last_descendent_dft[s] == last_t:\n            self.last_descendent_dft[s] = prev_t\n        s = self.parent[s]"
 },
 {
  "docstring": "Make a node q the root of its containing subtree.",
  "code": "def make_root(self, q):\n    ancestors = []\n    while q is not None:\n        ancestors.append(q)\n        q = self.parent[q]\n    ancestors.reverse()\n    for p, q in zip(ancestors, islice(ancestors, 1, None)):\n        size_p = self.subtree_size[p]\n        last_p = self.last_descendent_dft[p]\n        prev_q = self.prev_node_dft[q]\n        last_q = self.last_descendent_dft[q]\n        next_last_q = self.next_node_dft[last_q]\n        self.parent[p] = q\n        self.parent[q] = None\n        self.parent_edge[p] = self.parent_edge[q]\n        self.parent_edge[q] = None\n        self.subtree_size[p] = size_p - self.subtree_size[q]\n        self.subtree_size[q] = size_p\n        self.next_node_dft[prev_q] = next_last_q\n        self.prev_node_dft[next_last_q] = prev_q\n        self.next_node_dft[last_q] = q\n        self.prev_node_dft[q] = last_q\n        if last_p == last_q:\n            self.last_descendent_dft[p] = prev_q\n            last_p = prev_q\n        self.prev_node_dft[p] = last_q\n        self.next_node_dft[last_q] = p\n        self.next_node_dft[last_p] = q\n        self.prev_node_dft[q] = last_p\n        self.last_descendent_dft[q] = last_p"
 },
 {
  "docstring": "Add an edge (p, q) to the spanning tree where q is the root of a subtree.",
  "code": "def add_edge(self, i, p, q):\n    last_p = self.last_descendent_dft[p]\n    next_last_p = self.next_node_dft[last_p]\n    size_q = self.subtree_size[q]\n    last_q = self.last_descendent_dft[q]\n    self.parent[q] = p\n    self.parent_edge[q] = i\n    self.next_node_dft[last_p] = q\n    self.prev_node_dft[q] = last_p\n    self.prev_node_dft[next_last_p] = last_q\n    self.next_node_dft[last_q] = next_last_p\n    while p is not None:\n        self.subtree_size[p] += size_q\n        if self.last_descendent_dft[p] == last_p:\n            self.last_descendent_dft[p] = last_q\n        p = self.parent[p]"
 },
 {
  "docstring": "Update the potentials of the nodes in the subtree rooted at a node\nq connected to its parent p by an edge i.",
  "code": "def update_potentials(self, i, p, q):\n    if q == self.edge_targets[i]:\n        d = self.node_potentials[p] - self.edge_weights[i] - self.node_potentials[q]\n    else:\n        d = self.node_potentials[p] + self.edge_weights[i] - self.node_potentials[q]\n    for q in self.trace_subtree(q):\n        self.node_potentials[q] += d"
 },
 {
  "docstring": "Returns the reduced cost of an edge i.",
  "code": "def reduced_cost(self, i):\n    c = self.edge_weights[i] - self.node_potentials[self.edge_sources[i]] + self.node_potentials[self.edge_targets[i]]\n    return c if self.edge_flow[i] == 0 else -c"
 },
 {
  "docstring": "Yield entering edges until none can be found.",
  "code": "def find_entering_edges(self):\n    if self.edge_count == 0:\n        return\n    B = int(ceil(sqrt(self.edge_count)))\n    M = (self.edge_count + B - 1) // B\n    m = 0\n    f = 0\n    while m < M:\n        l = f + B\n        if l <= self.edge_count:\n            edges = range(f, l)\n        else:\n            l -= self.edge_count\n            edges = chain(range(f, self.edge_count), range(l))\n        f = l\n        i = min(edges, key=self.reduced_cost)\n        c = self.reduced_cost(i)\n        if c >= 0:\n            m += 1\n        else:\n            if self.edge_flow[i] == 0:\n                p = self.edge_sources[i]\n                q = self.edge_targets[i]\n            else:\n                p = self.edge_targets[i]\n                q = self.edge_sources[i]\n            yield (i, p, q)\n            m = 0"
 },
 {
  "docstring": "Returns the residual capacity of an edge i in the direction away\nfrom its endpoint p.",
  "code": "def residual_capacity(self, i, p):\n    return self.edge_capacities[i] - self.edge_flow[i] if self.edge_sources[i] == p else self.edge_flow[i]"
 },
 {
  "docstring": "Returns the leaving edge in a cycle represented by Wn and We.",
  "code": "def find_leaving_edge(self, Wn, We):\n    j, s = min(zip(reversed(We), reversed(Wn)), key=lambda i_p: self.residual_capacity(*i_p))\n    t = self.edge_targets[j] if self.edge_sources[j] == s else self.edge_sources[j]\n    return (j, s, t)"
 },
 {
  "docstring": "Add a flow dict entry.",
  "code": "def add_entry(e):\n    d = flow_dict[e[0]]\n    for k in e[1:-2]:\n        try:\n            d = d[k]\n        except KeyError:\n            t = {}\n            d[k] = t\n            d = t\n    d[e[-2]] = e[-1]"
 },
 {
  "docstring": "Implementation of the highest-label preflow-push algorithm.",
  "code": "def preflow_push_impl(G, s, t, capacity, residual, global_relabel_freq, value_only):\n    if s not in G:\n        raise nx.NetworkXError(f'node {str(s)} not in graph')\n    if t not in G:\n        raise nx.NetworkXError(f'node {str(t)} not in graph')\n    if s == t:\n        raise nx.NetworkXError('source and sink are the same node')\n    if global_relabel_freq is None:\n        global_relabel_freq = 0\n    if global_relabel_freq < 0:\n        raise nx.NetworkXError('global_relabel_freq must be nonnegative.')\n    if residual is None:\n        R = build_residual_network(G, capacity)\n    else:\n        R = residual\n    detect_unboundedness(R, s, t)\n    R_nodes = R.nodes\n    R_pred = R.pred\n    R_succ = R.succ\n    for u in R:\n        R_nodes[u]['excess'] = 0\n        for e in R_succ[u].values():\n            e['flow'] = 0\n\n    def reverse_bfs(src):\n        \"\"\"Perform a reverse breadth-first search from src in the residual\n        network.\n        \"\"\"\n        heights = {src: 0}\n        q = deque([(src, 0)])\n        while q:\n            u, height = q.popleft()\n            height += 1\n            for v, attr in R_pred[u].items():\n                if v not in heights and attr['flow'] < attr['capacity']:\n                    heights[v] = height\n                    q.append((v, height))\n        return heights\n    heights = reverse_bfs(t)\n    if s not in heights:\n        R.graph['flow_value'] = 0\n        return R\n    n = len(R)\n    max_height = max((heights[u] for u in heights if u != s))\n    heights[s] = n\n    grt = GlobalRelabelThreshold(n, R.size(), global_relabel_freq)\n    for u in R:\n        R_nodes[u]['height'] = heights[u] if u in heights else n + 1\n        R_nodes[u]['curr_edge'] = CurrentEdge(R_succ[u])\n\n    def push(u, v, flow):\n        \"\"\"Push flow units of flow from u to v.\"\"\"\n        R_succ[u][v]['flow'] += flow\n        R_succ[v][u]['flow'] -= flow\n        R_nodes[u]['excess'] -= flow\n        R_nodes[v]['excess'] += flow\n    for u, attr in R_succ[s].items():\n        flow = attr['capacity']\n        if flow > 0:\n            push(s, u, flow)\n    levels = [Level() for i in range(2 * n)]\n    for u in R:\n        if u != s and u != t:\n            level = levels[R_nodes[u]['height']]\n            if R_nodes[u]['excess'] > 0:\n                level.active.add(u)\n            else:\n                level.inactive.add(u)\n\n    def activate(v):\n        \"\"\"Move a node from the inactive set to the active set of its level.\"\"\"\n        if v != s and v != t:\n            level = levels[R_nodes[v]['height']]\n            if v in level.inactive:\n                level.inactive.remove(v)\n                level.active.add(v)\n\n    def relabel(u):\n        \"\"\"Relabel a node to create an admissible edge.\"\"\"\n        grt.add_work(len(R_succ[u]))\n        return min((R_nodes[v]['height'] for v, attr in R_succ[u].items() if attr['flow'] < attr['capacity'])) + 1\n\n    def discharge(u, is_phase1):\n        \"\"\"Discharge a node until it becomes inactive or, during phase 1 (see\n        below), its height reaches at least n. The node is known to have the\n        largest height among active nodes.\n        \"\"\"\n        height = R_nodes[u]['height']\n        curr_edge = R_nodes[u]['curr_edge']\n        next_height = height\n        levels[height].active.remove(u)\n        while True:\n            v, attr = curr_edge.get()\n            if height == R_nodes[v]['height'] + 1 and attr['flow'] < attr['capacity']:\n                flow = min(R_nodes[u]['excess'], attr['capacity'] - attr['flow'])\n                push(u, v, flow)\n                activate(v)\n                if R_nodes[u]['excess'] == 0:\n                    levels[height].inactive.add(u)\n                    break\n            try:\n                curr_edge.move_to_next()\n            except StopIteration:\n                height = relabel(u)\n                if is_phase1 and height >= n - 1:\n                    levels[height].active.add(u)\n                    break\n                next_height = height\n        R_nodes[u]['height'] = height\n        return next_height\n\n    def gap_heuristic(height):\n        \"\"\"Apply the gap heuristic.\"\"\"\n        for level in islice(levels, height + 1, max_height + 1):\n            for u in level.active:\n                R_nodes[u]['height'] = n + 1\n            for u in level.inactive:\n                R_nodes[u]['height'] = n + 1\n            levels[n + 1].active.update(level.active)\n            level.active.clear()\n            levels[n + 1].inactive.update(level.inactive)\n            level.inactive.clear()\n\n    def global_relabel(from_sink):\n        \"\"\"Apply the global relabeling heuristic.\"\"\"\n        src = t if from_sink else s\n        heights = reverse_bfs(src)\n        if not from_sink:\n            del heights[t]\n        max_height = max(heights.values())\n        if from_sink:\n            for u in R:\n                if u not in heights and R_nodes[u]['height'] < n:\n                    heights[u] = n + 1\n        else:\n            for u in heights:\n                heights[u] += n\n            max_height += n\n        del heights[src]\n        for u, new_height in heights.items():\n            old_height = R_nodes[u]['height']\n            if new_height != old_height:\n                if u in levels[old_height].active:\n                    levels[old_height].active.remove(u)\n                    levels[new_height].active.add(u)\n                else:\n                    levels[old_height].inactive.remove(u)\n                    levels[new_height].inactive.add(u)\n                R_nodes[u]['height'] = new_height\n        return max_height\n    height = max_height\n    while height > 0:\n        while True:\n            level = levels[height]\n            if not level.active:\n                height -= 1\n                break\n            old_height = height\n            old_level = level\n            u = arbitrary_element(level.active)\n            height = discharge(u, True)\n            if grt.is_reached():\n                height = global_relabel(True)\n                max_height = height\n                grt.clear_work()\n            elif not old_level.active and (not old_level.inactive):\n                gap_heuristic(old_height)\n                height = old_height - 1\n                max_height = height\n            else:\n                max_height = max(max_height, height)\n    if value_only:\n        R.graph['flow_value'] = R_nodes[t]['excess']\n        return R\n    height = global_relabel(False)\n    grt.clear_work()\n    while height > n:\n        while True:\n            level = levels[height]\n            if not level.active:\n                height -= 1\n                break\n            u = arbitrary_element(level.active)\n            height = discharge(u, False)\n            if grt.is_reached():\n                height = global_relabel(False)\n                grt.clear_work()\n    R.graph['flow_value'] = R_nodes[t]['excess']\n    return R"
 },
 {
  "docstring": "Find a maximum single-commodity flow using the highest-label\npreflow-push algorithm.\n\nThis function returns the residual network resulting after computing\nthe maximum flow. See below for details about the conventions\nNetworkX uses for defining residual networks.\n\nThis algorithm has a running time of $O(n^2 \\sqrt{m})$ for $n$ nodes and\n$m$ edges.\n\n\nParameters\n----------\nG : NetworkX graph\n    Edges of the graph are expected to have an attribute called\n    'capacity'. If this attribute is not present, the edge is\n    considered to have infinite capacity.\n\ns : node\n    Source node for the flow.\n\nt : node\n    Sink node for the flow.\n\ncapacity : string\n    Edges of the graph G are expected to have an attribute capacity\n    that indicates how much flow the edge can support. If this\n    attribute is not present, the edge is considered to have\n    infinite capacity. Default value: 'capacity'.\n\nresidual : NetworkX graph\n    Residual network on which the algorithm is to be executed. If None, a\n    new residual network is created. Default value: None.\n\nglobal_relabel_freq : integer, float\n    Relative frequency of applying the global relabeling heuristic to speed\n    up the algorithm. If it is None, the heuristic is disabled. Default\n    value: 1.\n\nvalue_only : bool\n    If False, compute a maximum flow; otherwise, compute a maximum preflow\n    which is enough for computing the maximum flow value. Default value:\n    False.\n\nReturns\n-------\nR : NetworkX DiGraph\n    Residual network after computing the maximum flow.\n\nRaises\n------\nNetworkXError\n    The algorithm does not support MultiGraph and MultiDiGraph. If\n    the input graph is an instance of one of these two classes, a\n    NetworkXError is raised.\n\nNetworkXUnbounded\n    If the graph has a path of infinite capacity, the value of a\n    feasible flow on the graph is unbounded above and the function\n    raises a NetworkXUnbounded.\n\n",
  "code": "@nx._dispatch(graphs={'G': 0, 'residual?': 4}, edge_attrs={'capacity': float('inf')}, preserve_edge_attrs={'residual': {'capacity': float('inf')}}, preserve_graph_attrs={'residual'})\ndef preflow_push(G, s, t, capacity='capacity', residual=None, global_relabel_freq=1, value_only=False):\n    R = preflow_push_impl(G, s, t, capacity, residual, global_relabel_freq, value_only)\n    R.graph['algorithm'] = 'preflow_push'\n    return R"
 },
 {
  "docstring": "Perform a reverse breadth-first search from src in the residual\nnetwork.",
  "code": "def reverse_bfs(src):\n    heights = {src: 0}\n    q = deque([(src, 0)])\n    while q:\n        u, height = q.popleft()\n        height += 1\n        for v, attr in R_pred[u].items():\n            if v not in heights and attr['flow'] < attr['capacity']:\n                heights[v] = height\n                q.append((v, height))\n    return heights"
 },
 {
  "docstring": "Push flow units of flow from u to v.",
  "code": "def push(u, v, flow):\n    R_succ[u][v]['flow'] += flow\n    R_succ[v][u]['flow'] -= flow\n    R_nodes[u]['excess'] -= flow\n    R_nodes[v]['excess'] += flow"
 },
 {
  "docstring": "Move a node from the inactive set to the active set of its level.",
  "code": "def activate(v):\n    if v != s and v != t:\n        level = levels[R_nodes[v]['height']]\n        if v in level.inactive:\n            level.inactive.remove(v)\n            level.active.add(v)"
 },
 {
  "docstring": "Relabel a node to create an admissible edge.",
  "code": "def relabel(u):\n    grt.add_work(len(R_succ[u]))\n    return min((R_nodes[v]['height'] for v, attr in R_succ[u].items() if attr['flow'] < attr['capacity'])) + 1"
 },
 {
  "docstring": "Discharge a node until it becomes inactive or, during phase 1 (see\nbelow), its height reaches at least n. The node is known to have the\nlargest height among active nodes.",
  "code": "def discharge(u, is_phase1):\n    height = R_nodes[u]['height']\n    curr_edge = R_nodes[u]['curr_edge']\n    next_height = height\n    levels[height].active.remove(u)\n    while True:\n        v, attr = curr_edge.get()\n        if height == R_nodes[v]['height'] + 1 and attr['flow'] < attr['capacity']:\n            flow = min(R_nodes[u]['excess'], attr['capacity'] - attr['flow'])\n            push(u, v, flow)\n            activate(v)\n            if R_nodes[u]['excess'] == 0:\n                levels[height].inactive.add(u)\n                break\n        try:\n            curr_edge.move_to_next()\n        except StopIteration:\n            height = relabel(u)\n            if is_phase1 and height >= n - 1:\n                levels[height].active.add(u)\n                break\n            next_height = height\n    R_nodes[u]['height'] = height\n    return next_height"
 },
 {
  "docstring": "Apply the gap heuristic.",
  "code": "def gap_heuristic(height):\n    for level in islice(levels, height + 1, max_height + 1):\n        for u in level.active:\n            R_nodes[u]['height'] = n + 1\n        for u in level.inactive:\n            R_nodes[u]['height'] = n + 1\n        levels[n + 1].active.update(level.active)\n        level.active.clear()\n        levels[n + 1].inactive.update(level.inactive)\n        level.inactive.clear()"
 },
 {
  "docstring": "Apply the global relabeling heuristic.",
  "code": "def global_relabel(from_sink):\n    src = t if from_sink else s\n    heights = reverse_bfs(src)\n    if not from_sink:\n        del heights[t]\n    max_height = max(heights.values())\n    if from_sink:\n        for u in R:\n            if u not in heights and R_nodes[u]['height'] < n:\n                heights[u] = n + 1\n    else:\n        for u in heights:\n            heights[u] += n\n        max_height += n\n    del heights[src]\n    for u, new_height in heights.items():\n        old_height = R_nodes[u]['height']\n        if new_height != old_height:\n            if u in levels[old_height].active:\n                levels[old_height].active.remove(u)\n                levels[new_height].active.add(u)\n            else:\n                levels[old_height].inactive.remove(u)\n                levels[new_height].inactive.add(u)\n            R_nodes[u]['height'] = new_height\n    return max_height"
 },
 {
  "docstring": "Implementation of the shortest augmenting path algorithm.",
  "code": "def shortest_augmenting_path_impl(G, s, t, capacity, residual, two_phase, cutoff):\n    if s not in G:\n        raise nx.NetworkXError(f'node {str(s)} not in graph')\n    if t not in G:\n        raise nx.NetworkXError(f'node {str(t)} not in graph')\n    if s == t:\n        raise nx.NetworkXError('source and sink are the same node')\n    if residual is None:\n        R = build_residual_network(G, capacity)\n    else:\n        R = residual\n    R_nodes = R.nodes\n    R_pred = R.pred\n    R_succ = R.succ\n    for u in R:\n        for e in R_succ[u].values():\n            e['flow'] = 0\n    heights = {t: 0}\n    q = deque([(t, 0)])\n    while q:\n        u, height = q.popleft()\n        height += 1\n        for v, attr in R_pred[u].items():\n            if v not in heights and attr['flow'] < attr['capacity']:\n                heights[v] = height\n                q.append((v, height))\n    if s not in heights:\n        R.graph['flow_value'] = 0\n        return R\n    n = len(G)\n    m = R.size() / 2\n    for u in R:\n        R_nodes[u]['height'] = heights[u] if u in heights else n\n        R_nodes[u]['curr_edge'] = CurrentEdge(R_succ[u])\n    counts = [0] * (2 * n - 1)\n    for u in R:\n        counts[R_nodes[u]['height']] += 1\n    inf = R.graph['inf']\n\n    def augment(path):\n        \"\"\"Augment flow along a path from s to t.\"\"\"\n        flow = inf\n        it = iter(path)\n        u = next(it)\n        for v in it:\n            attr = R_succ[u][v]\n            flow = min(flow, attr['capacity'] - attr['flow'])\n            u = v\n        if flow * 2 > inf:\n            raise nx.NetworkXUnbounded('Infinite capacity path, flow unbounded above.')\n        it = iter(path)\n        u = next(it)\n        for v in it:\n            R_succ[u][v]['flow'] += flow\n            R_succ[v][u]['flow'] -= flow\n            u = v\n        return flow\n\n    def relabel(u):\n        \"\"\"Relabel a node to create an admissible edge.\"\"\"\n        height = n - 1\n        for v, attr in R_succ[u].items():\n            if attr['flow'] < attr['capacity']:\n                height = min(height, R_nodes[v]['height'])\n        return height + 1\n    if cutoff is None:\n        cutoff = float('inf')\n    flow_value = 0\n    path = [s]\n    u = s\n    d = n if not two_phase else int(min(m ** 0.5, 2 * n ** (2.0 / 3)))\n    done = R_nodes[s]['height'] >= d\n    while not done:\n        height = R_nodes[u]['height']\n        curr_edge = R_nodes[u]['curr_edge']\n        while True:\n            v, attr = curr_edge.get()\n            if height == R_nodes[v]['height'] + 1 and attr['flow'] < attr['capacity']:\n                path.append(v)\n                u = v\n                break\n            try:\n                curr_edge.move_to_next()\n            except StopIteration:\n                counts[height] -= 1\n                if counts[height] == 0:\n                    R.graph['flow_value'] = flow_value\n                    return R\n                height = relabel(u)\n                if u == s and height >= d:\n                    if not two_phase:\n                        R.graph['flow_value'] = flow_value\n                        return R\n                    else:\n                        done = True\n                        break\n                counts[height] += 1\n                R_nodes[u]['height'] = height\n                if u != s:\n                    path.pop()\n                    u = path[-1]\n                    break\n        if u == t:\n            flow_value += augment(path)\n            if flow_value >= cutoff:\n                R.graph['flow_value'] = flow_value\n                return R\n            path = [s]\n            u = s\n    flow_value += edmonds_karp_core(R, s, t, cutoff - flow_value)\n    R.graph['flow_value'] = flow_value\n    return R"
 },
 {
  "docstring": "Find a maximum single-commodity flow using the shortest augmenting path\nalgorithm.\n\nThis function returns the residual network resulting after computing\nthe maximum flow. See below for details about the conventions\nNetworkX uses for defining residual networks.\n\nThis algorithm has a running time of $O(n^2 m)$ for $n$ nodes and $m$\nedges.\n\n\nParameters\n----------\nG : NetworkX graph\n    Edges of the graph are expected to have an attribute called\n    'capacity'. If this attribute is not present, the edge is\n    considered to have infinite capacity.\n\ns : node\n    Source node for the flow.\n\nt : node\n    Sink node for the flow.\n\ncapacity : string\n    Edges of the graph G are expected to have an attribute capacity\n    that indicates how much flow the edge can support. If this\n    attribute is not present, the edge is considered to have\n    infinite capacity. Default value: 'capacity'.\n\nresidual : NetworkX graph\n    Residual network on which the algorithm is to be executed. If None, a\n    new residual network is created. Default value: None.\n\nvalue_only : bool\n    If True compute only the value of the maximum flow. This parameter\n    will be ignored by this algorithm because it is not applicable.\n\ntwo_phase : bool\n    If True, a two-phase variant is used. The two-phase variant improves\n    the running time on unit-capacity networks from $O(nm)$ to\n    $O(\\min(n^{2/3}, m^{1/2}) m)$. Default value: False.\n\ncutoff : integer, float\n    If specified, the algorithm will terminate when the flow value reaches\n    or exceeds the cutoff. In this case, it may be unable to immediately\n    determine a minimum cut. Default value: None.\n\nReturns\n-------\nR : NetworkX DiGraph\n    Residual network after computing the maximum flow.\n\nRaises\n------\nNetworkXError\n    The algorithm does not support MultiGraph and MultiDiGraph. If\n    the input graph is an instance of one of these two classes, a\n    NetworkXError is raised.\n\nNetworkXUnbounded\n    If the graph has a path of infinite capacity, the value of a\n    feasible flow on the graph is unbounded above and the function\n    raises a NetworkXUnbounded.\n\n",
  "code": "@nx._dispatch(graphs={'G': 0, 'residual?': 4}, edge_attrs={'capacity': float('inf')}, preserve_edge_attrs={'residual': {'capacity': float('inf')}}, preserve_graph_attrs={'residual'})\ndef shortest_augmenting_path(G, s, t, capacity='capacity', residual=None, value_only=False, two_phase=False, cutoff=None):\n    R = shortest_augmenting_path_impl(G, s, t, capacity, residual, two_phase, cutoff)\n    R.graph['algorithm'] = 'shortest_augmenting_path'\n    return R"
 },
 {
  "docstring": "Augment flow along a path from s to t.",
  "code": "def augment(path):\n    flow = inf\n    it = iter(path)\n    u = next(it)\n    for v in it:\n        attr = R_succ[u][v]\n        flow = min(flow, attr['capacity'] - attr['flow'])\n        u = v\n    if flow * 2 > inf:\n        raise nx.NetworkXUnbounded('Infinite capacity path, flow unbounded above.')\n    it = iter(path)\n    u = next(it)\n    for v in it:\n        R_succ[u][v]['flow'] += flow\n        R_succ[v][u]['flow'] -= flow\n        u = v\n    return flow"
 },
 {
  "docstring": "Relabel a node to create an admissible edge.",
  "code": "def relabel(u):\n    height = n - 1\n    for v, attr in R_succ[u].items():\n        if attr['flow'] < attr['capacity']:\n            height = min(height, R_nodes[v]['height'])\n    return height + 1"
 },
 {
  "docstring": "Build a residual network and initialize a zero flow.\n\nThe residual network :samp:`R` from an input graph :samp:`G` has the\nsame nodes as :samp:`G`. :samp:`R` is a DiGraph that contains a pair\nof edges :samp:`(u, v)` and :samp:`(v, u)` iff :samp:`(u, v)` is not a\nself-loop, and at least one of :samp:`(u, v)` and :samp:`(v, u)` exists\nin :samp:`G`.\n\nFor each edge :samp:`(u, v)` in :samp:`R`, :samp:`R[u][v]['capacity']`\nis equal to the capacity of :samp:`(u, v)` in :samp:`G` if it exists\nin :samp:`G` or zero otherwise. If the capacity is infinite,\n:samp:`R[u][v]['capacity']` will have a high arbitrary finite value\nthat does not affect the solution of the problem. This value is stored in\n:samp:`R.graph['inf']`. For each edge :samp:`(u, v)` in :samp:`R`,\n:samp:`R[u][v]['flow']` represents the flow function of :samp:`(u, v)` and\nsatisfies :samp:`R[u][v]['flow'] == -R[v][u]['flow']`.\n\nThe flow value, defined as the total flow into :samp:`t`, the sink, is\nstored in :samp:`R.graph['flow_value']`. If :samp:`cutoff` is not\nspecified, reachability to :samp:`t` using only edges :samp:`(u, v)` such\nthat :samp:`R[u][v]['flow'] < R[u][v]['capacity']` induces a minimum\n:samp:`s`-:samp:`t` cut.",
  "code": "@nx._dispatch(edge_attrs={'capacity': float('inf')})\ndef build_residual_network(G, capacity):\n    if G.is_multigraph():\n        raise nx.NetworkXError('MultiGraph and MultiDiGraph not supported (yet).')\n    R = nx.DiGraph()\n    R.add_nodes_from(G)\n    inf = float('inf')\n    edge_list = [(u, v, attr) for u, v, attr in G.edges(data=True) if u != v and attr.get(capacity, inf) > 0]\n    inf = 3 * sum((attr[capacity] for u, v, attr in edge_list if capacity in attr and attr[capacity] != inf)) or 1\n    if G.is_directed():\n        for u, v, attr in edge_list:\n            r = min(attr.get(capacity, inf), inf)\n            if not R.has_edge(u, v):\n                R.add_edge(u, v, capacity=r)\n                R.add_edge(v, u, capacity=0)\n            else:\n                R[u][v]['capacity'] = r\n    else:\n        for u, v, attr in edge_list:\n            r = min(attr.get(capacity, inf), inf)\n            R.add_edge(u, v, capacity=r)\n            R.add_edge(v, u, capacity=r)\n    R.graph['inf'] = inf\n    return R"
 },
 {
  "docstring": "Detect an infinite-capacity s-t path in R.",
  "code": "@nx._dispatch(graphs='R', preserve_edge_attrs={'R': {'capacity': float('inf')}}, preserve_graph_attrs=True)\ndef detect_unboundedness(R, s, t):\n    q = deque([s])\n    seen = {s}\n    inf = R.graph['inf']\n    while q:\n        u = q.popleft()\n        for v, attr in R[u].items():\n            if attr['capacity'] == inf and v not in seen:\n                if v == t:\n                    raise nx.NetworkXUnbounded('Infinite capacity path, flow unbounded above.')\n                seen.add(v)\n                q.append(v)"
 },
 {
  "docstring": "Build a flow dictionary from a residual network.",
  "code": "@nx._dispatch(graphs={'G': 0, 'R': 1}, preserve_edge_attrs={'R': {'flow': None}})\ndef build_flow_dict(G, R):\n    flow_dict = {}\n    for u in G:\n        flow_dict[u] = {v: 0 for v in G[u]}\n        flow_dict[u].update(((v, attr['flow']) for v, attr in R[u].items() if attr['flow'] > 0))\n    return flow_dict"
 },
 {
  "docstring": "Combinatorial Optimization: Algorithms and Complexity,\nPapadimitriou Steiglitz at page 140 has an example, 7.1, but that\nadmits multiple solutions, so I alter it a bit. From ticket #430\nby mfrasca.",
  "code": "def test_digraph3(self):\n    G = nx.DiGraph()\n    G.add_edge('s', 'a')\n    G['s']['a'].update({0: 2, 1: 4})\n    G.add_edge('s', 'b')\n    G['s']['b'].update({0: 2, 1: 1})\n    G.add_edge('a', 'b')\n    G['a']['b'].update({0: 5, 1: 2})\n    G.add_edge('a', 't')\n    G['a']['t'].update({0: 1, 1: 5})\n    G.add_edge('b', 'a')\n    G['b']['a'].update({0: 1, 1: 3})\n    G.add_edge('b', 't')\n    G['b']['t'].update({0: 3, 1: 2})\n    'PS.ex.7.1: testing main function'\n    sol = nx.max_flow_min_cost(G, 's', 't', capacity=0, weight=1)\n    flow = sum((v for v in sol['s'].values()))\n    assert 4 == flow\n    assert 23 == nx.cost_of_flow(G, sol, weight=1)\n    assert sol['s'] == {'a': 2, 'b': 2}\n    assert sol['a'] == {'b': 1, 't': 1}\n    assert sol['b'] == {'a': 0, 't': 3}\n    assert sol['t'] == {}\n    G.add_edge('t', 's')\n    G['t']['s'].update({1: -100})\n    flowCost, sol = nx.capacity_scaling(G, capacity=0, weight=1)\n    G.remove_edge('t', 's')\n    flow = sum((v for v in sol['s'].values()))\n    assert 4 == flow\n    assert sol['t']['s'] == 4\n    assert flowCost == -377\n    del sol['t']['s']\n    assert sol['s'] == {'a': 2, 'b': 2}\n    assert sol['a'] == {'b': 1, 't': 1}\n    assert sol['b'] == {'a': 0, 't': 3}\n    assert sol['t'] == {}\n    assert nx.cost_of_flow(G, sol, weight=1) == 23"
 },
 {
  "docstring": "Address issue raised in ticket #617 by arv.",
  "code": "def test_zero_capacity_edges(self):\n    G = nx.DiGraph()\n    G.add_edges_from([(1, 2, {'capacity': 1, 'weight': 1}), (1, 5, {'capacity': 1, 'weight': 1}), (2, 3, {'capacity': 0, 'weight': 1}), (2, 5, {'capacity': 1, 'weight': 1}), (5, 3, {'capacity': 2, 'weight': 1}), (5, 4, {'capacity': 0, 'weight': 1}), (3, 4, {'capacity': 2, 'weight': 1})])\n    G.nodes[1]['demand'] = -1\n    G.nodes[2]['demand'] = -1\n    G.nodes[4]['demand'] = 2\n    flowCost, H = nx.network_simplex(G)\n    soln = {1: {2: 0, 5: 1}, 2: {3: 0, 5: 1}, 3: {4: 2}, 4: {}, 5: {3: 2, 4: 0}}\n    assert flowCost == 6\n    assert nx.min_cost_flow_cost(G) == 6\n    assert H == soln\n    assert nx.min_cost_flow(G) == soln\n    assert nx.cost_of_flow(G, H) == 6\n    flowCost, H = nx.capacity_scaling(G)\n    assert flowCost == 6\n    assert H == soln\n    assert nx.cost_of_flow(G, H) == 6"
 },
 {
  "docstring": "Check if digons are handled properly. Taken from ticket\n#618 by arv.",
  "code": "def test_digon(self):\n    nodes = [(1, {}), (2, {'demand': -4}), (3, {'demand': 4})]\n    edges = [(1, 2, {'capacity': 3, 'weight': 600000}), (2, 1, {'capacity': 2, 'weight': 0}), (2, 3, {'capacity': 5, 'weight': 714285}), (3, 2, {'capacity': 2, 'weight': 0})]\n    G = nx.DiGraph(edges)\n    G.add_nodes_from(nodes)\n    flowCost, H = nx.network_simplex(G)\n    soln = {1: {2: 0}, 2: {1: 0, 3: 4}, 3: {2: 0}}\n    assert flowCost == 2857140\n    assert nx.min_cost_flow_cost(G) == 2857140\n    assert H == soln\n    assert nx.min_cost_flow(G) == soln\n    assert nx.cost_of_flow(G, H) == 2857140\n    flowCost, H = nx.capacity_scaling(G)\n    assert flowCost == 2857140\n    assert H == soln\n    assert nx.cost_of_flow(G, H) == 2857140"
 },
 {
  "docstring": "Check if one-node cycles are handled properly. Taken from ticket\n#2906 from @sshraven.",
  "code": "def test_deadend(self):\n    G = nx.DiGraph()\n    G.add_nodes_from(range(5), demand=0)\n    G.nodes[4]['demand'] = -13\n    G.nodes[3]['demand'] = 13\n    G.add_edges_from([(0, 2), (0, 3), (2, 1)], capacity=20, weight=0.1)\n    pytest.raises(nx.NetworkXUnfeasible, nx.min_cost_flow, G)"
 },
 {
  "docstring": "An infinite capacity negative cost digon results in an unbounded\ninstance.",
  "code": "def test_infinite_capacity_neg_digon(self):\n    nodes = [(1, {}), (2, {'demand': -4}), (3, {'demand': 4})]\n    edges = [(1, 2, {'weight': -600}), (2, 1, {'weight': 0}), (2, 3, {'capacity': 5, 'weight': 714285}), (3, 2, {'capacity': 2, 'weight': 0})]\n    G = nx.DiGraph(edges)\n    G.add_nodes_from(nodes)\n    pytest.raises(nx.NetworkXUnbounded, nx.network_simplex, G)\n    pytest.raises(nx.NetworkXUnbounded, nx.capacity_scaling, G)"
 },
 {
  "docstring": "The digon should receive the maximum amount of flow it can handle.\nTaken from ticket #749 by @chuongdo.",
  "code": "def test_finite_capacity_neg_digon(self):\n    G = nx.DiGraph()\n    G.add_edge('a', 'b', capacity=1, weight=-1)\n    G.add_edge('b', 'a', capacity=1, weight=-1)\n    min_cost = -2\n    assert nx.min_cost_flow_cost(G) == min_cost\n    flowCost, H = nx.capacity_scaling(G)\n    assert flowCost == -2\n    assert H == {'a': {'b': 1}, 'b': {'a': 1}}\n    assert nx.cost_of_flow(G, H) == -2"
 },
 {
  "docstring": "Multidigraphs are acceptable.",
  "code": "def test_multidigraph(self):\n    G = nx.MultiDiGraph()\n    G.add_weighted_edges_from([(1, 2, 1), (2, 3, 2)], weight='capacity')\n    flowCost, H = nx.network_simplex(G)\n    assert flowCost == 0\n    assert H == {1: {2: {0: 0}}, 2: {3: {0: 0}}, 3: {}}\n    flowCost, H = nx.capacity_scaling(G)\n    assert flowCost == 0\n    assert H == {1: {2: {0: 0}}, 2: {3: {0: 0}}, 3: {}}"
 },
 {
  "docstring": "Negative selfloops should cause an exception if uncapacitated and\nalways be saturated otherwise.",
  "code": "def test_negative_selfloops(self):\n    G = nx.DiGraph()\n    G.add_edge(1, 1, weight=-1)\n    pytest.raises(nx.NetworkXUnbounded, nx.network_simplex, G)\n    pytest.raises(nx.NetworkXUnbounded, nx.capacity_scaling, G)\n    G[1][1]['capacity'] = 2\n    flowCost, H = nx.network_simplex(G)\n    assert flowCost == -2\n    assert H == {1: {1: 2}}\n    flowCost, H = nx.capacity_scaling(G)\n    assert flowCost == -2\n    assert H == {1: {1: 2}}\n    G = nx.MultiDiGraph()\n    G.add_edge(1, 1, 'x', weight=-1)\n    G.add_edge(1, 1, 'y', weight=1)\n    pytest.raises(nx.NetworkXUnbounded, nx.network_simplex, G)\n    pytest.raises(nx.NetworkXUnbounded, nx.capacity_scaling, G)\n    G[1][1]['x']['capacity'] = 2\n    flowCost, H = nx.network_simplex(G)\n    assert flowCost == -2\n    assert H == {1: {1: {'x': 2, 'y': 0}}}\n    flowCost, H = nx.capacity_scaling(G)\n    assert flowCost == -2\n    assert H == {1: {1: {'x': 2, 'y': 0}}}"
 },
 {
  "docstring": "Returns flow cost calculated from flow dictionary",
  "code": "def get_flowcost_from_flowdict(G, flowDict):\n    flowCost = 0\n    for u in flowDict:\n        for v in flowDict[u]:\n            flowCost += flowDict[u][v] * G[u][v]['weight']\n    return flowCost"
 },
 {
  "docstring": "https://developers.google.com/optimization/flow/mincostflow",
  "code": "def test_google_or_tools_example():\n    G = nx.DiGraph()\n    start_nodes = [0, 0, 1, 1, 1, 2, 2, 3, 4]\n    end_nodes = [1, 2, 2, 3, 4, 3, 4, 4, 2]\n    capacities = [15, 8, 20, 4, 10, 15, 4, 20, 5]\n    unit_costs = [4, 4, 2, 2, 6, 1, 3, 2, 3]\n    supplies = [20, 0, 0, -5, -15]\n    answer = 150\n    for i in range(len(supplies)):\n        G.add_node(i, demand=-1 * supplies[i])\n    for i in range(len(start_nodes)):\n        G.add_edge(start_nodes[i], end_nodes[i], weight=unit_costs[i], capacity=capacities[i])\n    flowCost, flowDict = nx.network_simplex(G)\n    assert flowCost == answer\n    assert flowCost == get_flowcost_from_flowdict(G, flowDict)"
 },
 {
  "docstring": "https://developers.google.com/optimization/flow/mincostflow",
  "code": "def test_google_or_tools_example2():\n    G = nx.DiGraph()\n    start_nodes = [0, 0, 1, 1, 1, 2, 2, 3, 4, 3]\n    end_nodes = [1, 2, 2, 3, 4, 3, 4, 4, 2, 5]\n    capacities = [15, 8, 20, 4, 10, 15, 4, 20, 5, 10]\n    unit_costs = [4, 4, 2, 2, 6, 1, 3, 2, 3, 4]\n    supplies = [23, 0, 0, -5, -15, -3]\n    answer = 183\n    for i in range(len(supplies)):\n        G.add_node(i, demand=-1 * supplies[i])\n    for i in range(len(start_nodes)):\n        G.add_edge(start_nodes[i], end_nodes[i], weight=unit_costs[i], capacity=capacities[i])\n    flowCost, flowDict = nx.network_simplex(G)\n    assert flowCost == answer\n    assert flowCost == get_flowcost_from_flowdict(G, flowDict)"
 },
 {
  "docstring": "Address issue raised in ticket #617 by arv.",
  "code": "def test_zero_capacity_edges():\n    G = nx.DiGraph()\n    G.add_edges_from([(1, 2, {'capacity': 1, 'weight': 1}), (1, 5, {'capacity': 1, 'weight': 1}), (2, 3, {'capacity': 0, 'weight': 1}), (2, 5, {'capacity': 1, 'weight': 1}), (5, 3, {'capacity': 2, 'weight': 1}), (5, 4, {'capacity': 0, 'weight': 1}), (3, 4, {'capacity': 2, 'weight': 1})])\n    G.nodes[1]['demand'] = -1\n    G.nodes[2]['demand'] = -1\n    G.nodes[4]['demand'] = 2\n    flowCost, H = nx.network_simplex(G)\n    soln = {1: {2: 0, 5: 1}, 2: {3: 0, 5: 1}, 3: {4: 2}, 4: {}, 5: {3: 2, 4: 0}}\n    assert flowCost == 6\n    assert nx.min_cost_flow_cost(G) == 6\n    assert H == soln"
 },
 {
  "docstring": "Check if digons are handled properly. Taken from ticket\n#618 by arv.",
  "code": "def test_digon():\n    nodes = [(1, {}), (2, {'demand': -4}), (3, {'demand': 4})]\n    edges = [(1, 2, {'capacity': 3, 'weight': 600000}), (2, 1, {'capacity': 2, 'weight': 0}), (2, 3, {'capacity': 5, 'weight': 714285}), (3, 2, {'capacity': 2, 'weight': 0})]\n    G = nx.DiGraph(edges)\n    G.add_nodes_from(nodes)\n    flowCost, H = nx.network_simplex(G)\n    soln = {1: {2: 0}, 2: {1: 0, 3: 4}, 3: {2: 0}}\n    assert flowCost == 2857140"
 },
 {
  "docstring": "Check if one-node cycles are handled properly. Taken from ticket\n#2906 from @sshraven.",
  "code": "def test_deadend():\n    G = nx.DiGraph()\n    G.add_nodes_from(range(5), demand=0)\n    G.nodes[4]['demand'] = -13\n    G.nodes[3]['demand'] = 13\n    G.add_edges_from([(0, 2), (0, 3), (2, 1)], capacity=20, weight=0.1)\n    pytest.raises(nx.NetworkXUnfeasible, nx.network_simplex, G)"
 },
 {
  "docstring": "An infinite capacity negative cost digon results in an unbounded\ninstance.",
  "code": "def test_infinite_capacity_neg_digon():\n    nodes = [(1, {}), (2, {'demand': -4}), (3, {'demand': 4})]\n    edges = [(1, 2, {'weight': -600}), (2, 1, {'weight': 0}), (2, 3, {'capacity': 5, 'weight': 714285}), (3, 2, {'capacity': 2, 'weight': 0})]\n    G = nx.DiGraph(edges)\n    G.add_nodes_from(nodes)\n    pytest.raises(nx.NetworkXUnbounded, nx.network_simplex, G)"
 },
 {
  "docstring": "Multidigraphs are acceptable.",
  "code": "def test_multidigraph():\n    G = nx.MultiDiGraph()\n    G.add_weighted_edges_from([(1, 2, 1), (2, 3, 2)], weight='capacity')\n    flowCost, H = nx.network_simplex(G)\n    assert flowCost == 0\n    assert H == {1: {2: {0: 0}}, 2: {3: {0: 0}}, 3: {}}"
 },
 {
  "docstring": "Negative selfloops should cause an exception if uncapacitated and\nalways be saturated otherwise.",
  "code": "def test_negative_selfloops():\n    G = nx.DiGraph()\n    G.add_edge(1, 1, weight=-1)\n    pytest.raises(nx.NetworkXUnbounded, nx.network_simplex, G)\n    G[1][1]['capacity'] = 2\n    flowCost, H = nx.network_simplex(G)\n    assert flowCost == -2\n    assert H == {1: {1: 2}}\n    G = nx.MultiDiGraph()\n    G.add_edge(1, 1, 'x', weight=-1)\n    G.add_edge(1, 1, 'y', weight=1)\n    pytest.raises(nx.NetworkXUnbounded, nx.network_simplex, G)\n    G[1][1]['x']['capacity'] = 2\n    flowCost, H = nx.network_simplex(G)\n    assert flowCost == -2\n    assert H == {1: {1: {'x': 2, 'y': 0}}}"
 },
 {
  "docstring": "Returns ``True`` if and only if all elements in `iterable` are equal; and\n``False`` otherwise.\n\nParameters\n----------\niterable: collections.abc.Iterable\n    The container whose elements will be checked.\n\nReturns\n-------\nbool\n    ``True`` iff all elements in `iterable` compare equal, ``False``\n    otherwise.",
  "code": "def are_all_equal(iterable):\n    try:\n        shape = iterable.shape\n    except AttributeError:\n        pass\n    else:\n        if len(shape) > 1:\n            message = 'The function does not works on multidimensional arrays.'\n            raise NotImplementedError(message) from None\n    iterator = iter(iterable)\n    first = next(iterator, None)\n    return all((item == first for item in iterator))"
 },
 {
  "docstring": "Partitions items into sets based on the outcome of ``test(item1, item2)``.\nPairs of items for which `test` returns `True` end up in the same set.\n\nParameters\n----------\nitems : collections.abc.Iterable[collections.abc.Hashable]\n    Items to partition\ntest : collections.abc.Callable[collections.abc.Hashable, collections.abc.Hashable]\n    A function that will be called with 2 arguments, taken from items.\n    Should return `True` if those 2 items need to end up in the same\n    partition, and `False` otherwise.\n\nReturns\n-------\nlist[set]\n    A list of sets, with each set containing part of the items in `items`,\n    such that ``all(test(*pair) for pair in  itertools.combinations(set, 2))\n    == True``\n\n",
  "code": "def make_partitions(items, test):\n    partitions = []\n    for item in items:\n        for partition in partitions:\n            p_item = next(iter(partition))\n            if test(item, p_item):\n                partition.add(item)\n                break\n        else:\n            partitions.append({item})\n    return partitions"
 },
 {
  "docstring": "Creates a dictionary that maps each item in each partition to the index of\nthe partition to which it belongs.\n\nParameters\n----------\npartitions: collections.abc.Sequence[collections.abc.Iterable]\n    As returned by :func:`make_partitions`.\n\nReturns\n-------\ndict",
  "code": "def partition_to_color(partitions):\n    colors = {}\n    for color, keys in enumerate(partitions):\n        for key in keys:\n            colors[key] = color\n    return colors"
 },
 {
  "docstring": "Given an collection of sets, returns the intersection of those sets.\n\nParameters\n----------\ncollection_of_sets: collections.abc.Collection[set]\n    A collection of sets.\n\nReturns\n-------\nset\n    An intersection of all sets in `collection_of_sets`. Will have the same\n    type as the item initially taken from `collection_of_sets`.",
  "code": "def intersect(collection_of_sets):\n    collection_of_sets = list(collection_of_sets)\n    first = collection_of_sets.pop()\n    out = reduce(set.intersection, collection_of_sets, set(first))\n    return type(first)(out)"
 },
 {
  "docstring": "Parameters\n----------\ngraph: networkx.Graph\nsubgraph: networkx.Graph\nnode_match: collections.abc.Callable or None\n    Function used to determine whether two nodes are equivalent. Its\n    signature should look like ``f(n1: dict, n2: dict) -> bool``, with\n    `n1` and `n2` node property dicts. See also\n    :func:`~networkx.algorithms.isomorphism.categorical_node_match` and\n    friends.\n    If `None`, all nodes are considered equal.\nedge_match: collections.abc.Callable or None\n    Function used to determine whether two edges are equivalent. Its\n    signature should look like ``f(e1: dict, e2: dict) -> bool``, with\n    `e1` and `e2` edge property dicts. See also\n    :func:`~networkx.algorithms.isomorphism.categorical_edge_match` and\n    friends.\n    If `None`, all edges are considered equal.\ncache: collections.abc.Mapping\n    A cache used for caching graph symmetries.",
  "code": "def __init__(self, graph, subgraph, node_match=None, edge_match=None, cache=None):\n    self.graph = graph\n    self.subgraph = subgraph\n    self._symmetry_cache = cache\n    self._sgn_partitions_ = None\n    self._sge_partitions_ = None\n    self._sgn_colors_ = None\n    self._sge_colors_ = None\n    self._gn_partitions_ = None\n    self._ge_partitions_ = None\n    self._gn_colors_ = None\n    self._ge_colors_ = None\n    self._node_compat_ = None\n    self._edge_compat_ = None\n    if node_match is None:\n        self.node_equality = self._node_match_maker(lambda n1, n2: True)\n        self._sgn_partitions_ = [set(self.subgraph.nodes)]\n        self._gn_partitions_ = [set(self.graph.nodes)]\n        self._node_compat_ = {0: 0}\n    else:\n        self.node_equality = self._node_match_maker(node_match)\n    if edge_match is None:\n        self.edge_equality = self._edge_match_maker(lambda e1, e2: True)\n        self._sge_partitions_ = [set(self.subgraph.edges)]\n        self._ge_partitions_ = [set(self.graph.edges)]\n        self._edge_compat_ = {0: 0}\n    else:\n        self.edge_equality = self._edge_match_maker(edge_match)"
 },
 {
  "docstring": "Find all subgraph isomorphisms between subgraph and graph\n\nFinds isomorphisms where :attr:`subgraph` <= :attr:`graph`.\n\nParameters\n----------\nsymmetry: bool\n    Whether symmetry should be taken into account. If False, found\n    isomorphisms may be symmetrically equivalent.\n\nYields\n------\ndict\n    The found isomorphism mappings of {graph_node: subgraph_node}.",
  "code": "def find_isomorphisms(self, symmetry=True):\n    if not self.subgraph:\n        yield {}\n        return\n    elif not self.graph:\n        return\n    elif len(self.graph) < len(self.subgraph):\n        return\n    if symmetry:\n        _, cosets = self.analyze_symmetry(self.subgraph, self._sgn_partitions, self._sge_colors)\n        constraints = self._make_constraints(cosets)\n    else:\n        constraints = []\n    candidates = self._find_nodecolor_candidates()\n    la_candidates = self._get_lookahead_candidates()\n    for sgn in self.subgraph:\n        extra_candidates = la_candidates[sgn]\n        if extra_candidates:\n            candidates[sgn] = candidates[sgn] | {frozenset(extra_candidates)}\n    if any(candidates.values()):\n        start_sgn = min(candidates, key=lambda n: min(candidates[n], key=len))\n        candidates[start_sgn] = (intersect(candidates[start_sgn]),)\n        yield from self._map_nodes(start_sgn, candidates, constraints)\n    else:\n        return"
 },
 {
  "docstring": "For `node` in `graph`, count the number of edges of a specific color\nit has to nodes of a specific color.",
  "code": "@staticmethod\ndef _find_neighbor_color_count(graph, node, node_color, edge_color):\n    counts = Counter()\n    neighbors = graph[node]\n    for neighbor in neighbors:\n        n_color = node_color[neighbor]\n        if (node, neighbor) in edge_color:\n            e_color = edge_color[node, neighbor]\n        else:\n            e_color = edge_color[neighbor, node]\n        counts[e_color, n_color] += 1\n    return counts"
 },
 {
  "docstring": "Returns a mapping of {subgraph node: collection of graph nodes} for\nwhich the graph nodes are feasible candidates for the subgraph node, as\ndetermined by looking ahead one edge.",
  "code": "def _get_lookahead_candidates(self):\n    g_counts = {}\n    for gn in self.graph:\n        g_counts[gn] = self._find_neighbor_color_count(self.graph, gn, self._gn_colors, self._ge_colors)\n    candidates = defaultdict(set)\n    for sgn in self.subgraph:\n        sg_count = self._find_neighbor_color_count(self.subgraph, sgn, self._sgn_colors, self._sge_colors)\n        new_sg_count = Counter()\n        for (sge_color, sgn_color), count in sg_count.items():\n            try:\n                ge_color = self._edge_compatibility[sge_color]\n                gn_color = self._node_compatibility[sgn_color]\n            except KeyError:\n                pass\n            else:\n                new_sg_count[ge_color, gn_color] = count\n        for gn, g_count in g_counts.items():\n            if all((new_sg_count[x] <= g_count[x] for x in new_sg_count)):\n                candidates[sgn].add(gn)\n    return candidates"
 },
 {
  "docstring": "Find the largest common induced subgraphs between :attr:`subgraph` and\n:attr:`graph`.\n\nParameters\n----------\nsymmetry: bool\n    Whether symmetry should be taken into account. If False, found\n    largest common subgraphs may be symmetrically equivalent.\n\nYields\n------\ndict\n    The found isomorphism mappings of {graph_node: subgraph_node}.",
  "code": "def largest_common_subgraph(self, symmetry=True):\n    if not self.subgraph:\n        yield {}\n        return\n    elif not self.graph:\n        return\n    if symmetry:\n        _, cosets = self.analyze_symmetry(self.subgraph, self._sgn_partitions, self._sge_colors)\n        constraints = self._make_constraints(cosets)\n    else:\n        constraints = []\n    candidates = self._find_nodecolor_candidates()\n    if any(candidates.values()):\n        yield from self._largest_common_subgraph(candidates, constraints)\n    else:\n        return"
 },
 {
  "docstring": "Find a minimal set of permutations and corresponding co-sets that\ndescribe the symmetry of `graph`, given the node and edge equalities\ngiven by `node_partitions` and `edge_colors`, respectively.\n\nParameters\n----------\ngraph : networkx.Graph\n    The graph whose symmetry should be analyzed.\nnode_partitions : list of sets\n    A list of sets containing node keys. Node keys in the same set\n    are considered equivalent. Every node key in `graph` should be in\n    exactly one of the sets. If all nodes are equivalent, this should\n    be ``[set(graph.nodes)]``.\nedge_colors : dict mapping edges to their colors\n    A dict mapping every edge in `graph` to its corresponding color.\n    Edges with the same color are considered equivalent. If all edges\n    are equivalent, this should be ``{e: 0 for e in graph.edges}``.\n\n\nReturns\n-------\nset[frozenset]\n    The found permutations. This is a set of frozensets of pairs of node\n    keys which can be exchanged without changing :attr:`subgraph`.\ndict[collections.abc.Hashable, set[collections.abc.Hashable]]\n    The found co-sets. The co-sets is a dictionary of\n    ``{node key: set of node keys}``.\n    Every key-value pair describes which ``values`` can be interchanged\n    without changing nodes less than ``key``.",
  "code": "def analyze_symmetry(self, graph, node_partitions, edge_colors):\n    if self._symmetry_cache is not None:\n        key = hash((tuple(graph.nodes), tuple(graph.edges), tuple(map(tuple, node_partitions)), tuple(edge_colors.items())))\n        if key in self._symmetry_cache:\n            return self._symmetry_cache[key]\n    node_partitions = list(self._refine_node_partitions(graph, node_partitions, edge_colors))\n    assert len(node_partitions) == 1\n    node_partitions = node_partitions[0]\n    permutations, cosets = self._process_ordered_pair_partitions(graph, node_partitions, node_partitions, edge_colors)\n    if self._symmetry_cache is not None:\n        self._symmetry_cache[key] = (permutations, cosets)\n    return (permutations, cosets)"
 },
 {
  "docstring": "Returns True if :attr:`graph` is isomorphic to :attr:`subgraph` and\nFalse otherwise.\n\nReturns\n-------\nbool",
  "code": "def is_isomorphic(self, symmetry=False):\n    return len(self.subgraph) == len(self.graph) and self.subgraph_is_isomorphic(symmetry)"
 },
 {
  "docstring": "Returns True if a subgraph of :attr:`graph` is isomorphic to\n:attr:`subgraph` and False otherwise.\n\nReturns\n-------\nbool",
  "code": "def subgraph_is_isomorphic(self, symmetry=False):\n    isom = next(self.subgraph_isomorphisms_iter(symmetry=symmetry), None)\n    return isom is not None"
 },
 {
  "docstring": "Does the same as :meth:`find_isomorphisms` if :attr:`graph` and\n:attr:`subgraph` have the same number of nodes.",
  "code": "def isomorphisms_iter(self, symmetry=True):\n    if len(self.graph) == len(self.subgraph):\n        yield from self.subgraph_isomorphisms_iter(symmetry=symmetry)"
 },
 {
  "docstring": "Alternative name for :meth:`find_isomorphisms`.",
  "code": "def subgraph_isomorphisms_iter(self, symmetry=True):\n    return self.find_isomorphisms(symmetry)"
 },
 {
  "docstring": "Per node in subgraph find all nodes in graph that have the same color.",
  "code": "def _find_nodecolor_candidates(self):\n    candidates = defaultdict(set)\n    for sgn in self.subgraph.nodes:\n        sgn_color = self._sgn_colors[sgn]\n        if sgn_color in self._node_compatibility:\n            gn_color = self._node_compatibility[sgn_color]\n            candidates[sgn].add(frozenset(self._gn_partitions[gn_color]))\n        else:\n            candidates[sgn].add(frozenset())\n    candidates = dict(candidates)\n    for sgn, options in candidates.items():\n        candidates[sgn] = frozenset(options)\n    return candidates"
 },
 {
  "docstring": "Turn cosets into constraints.",
  "code": "@staticmethod\ndef _make_constraints(cosets):\n    constraints = []\n    for node_i, node_ts in cosets.items():\n        for node_t in node_ts:\n            if node_i != node_t:\n                constraints.append((node_i, node_t))\n    return constraints"
 },
 {
  "docstring": "For every node in graph, come up with a color that combines 1) the\ncolor of the node, and 2) the number of edges of a color to each type\nof node.",
  "code": "@staticmethod\ndef _find_node_edge_color(graph, node_colors, edge_colors):\n    counts = defaultdict(lambda: defaultdict(int))\n    for node1, node2 in graph.edges:\n        if (node1, node2) in edge_colors:\n            ecolor = edge_colors[node1, node2]\n        else:\n            ecolor = edge_colors[node2, node1]\n        counts[node1][ecolor, node_colors[node2]] += 1\n        counts[node2][ecolor, node_colors[node1]] += 1\n    node_edge_colors = {}\n    for node in graph.nodes:\n        node_edge_colors[node] = (node_colors[node], set(counts[node].items()))\n    return node_edge_colors"
 },
 {
  "docstring": "Get all permutations of items, but only permute items with the same\nlength.\n\n>>> found = list(ISMAGS._get_permutations_by_length([[1], [2], [3, 4], [4, 5]]))\n>>> answer = [\n...     (([1], [2]), ([3, 4], [4, 5])),\n...     (([1], [2]), ([4, 5], [3, 4])),\n...     (([2], [1]), ([3, 4], [4, 5])),\n...     (([2], [1]), ([4, 5], [3, 4])),\n... ]\n>>> found == answer\nTrue",
  "code": "@staticmethod\ndef _get_permutations_by_length(items):\n    by_len = defaultdict(list)\n    for item in items:\n        by_len[len(item)].append(item)\n    yield from itertools.product(*(itertools.permutations(by_len[l]) for l in sorted(by_len)))"
 },
 {
  "docstring": "Given a partition of nodes in graph, make the partitions smaller such\nthat all nodes in a partition have 1) the same color, and 2) the same\nnumber of edges to specific other partitions.",
  "code": "@classmethod\ndef _refine_node_partitions(cls, graph, node_partitions, edge_colors, branch=False):\n\n    def equal_color(node1, node2):\n        return node_edge_colors[node1] == node_edge_colors[node2]\n    node_partitions = list(node_partitions)\n    node_colors = partition_to_color(node_partitions)\n    node_edge_colors = cls._find_node_edge_color(graph, node_colors, edge_colors)\n    if all((are_all_equal((node_edge_colors[node] for node in partition)) for partition in node_partitions)):\n        yield node_partitions\n        return\n    new_partitions = []\n    output = [new_partitions]\n    for partition in node_partitions:\n        if not are_all_equal((node_edge_colors[node] for node in partition)):\n            refined = make_partitions(partition, equal_color)\n            if branch and len(refined) != 1 and (len({len(r) for r in refined}) != len([len(r) for r in refined])):\n                permutations = cls._get_permutations_by_length(refined)\n                new_output = []\n                for n_p in output:\n                    for permutation in permutations:\n                        new_output.append(n_p + list(permutation[0]))\n                output = new_output\n            else:\n                for n_p in output:\n                    n_p.extend(sorted(refined, key=len))\n        else:\n            for n_p in output:\n                n_p.append(partition)\n    for n_p in output:\n        yield from cls._refine_node_partitions(graph, n_p, edge_colors, branch)"
 },
 {
  "docstring": "Returns all edges in :attr:`graph` that have the same colour as the\nedge between sgn1 and sgn2 in :attr:`subgraph`.",
  "code": "def _edges_of_same_color(self, sgn1, sgn2):\n    if (sgn1, sgn2) in self._sge_colors:\n        sge_color = self._sge_colors[sgn1, sgn2]\n    else:\n        sge_color = self._sge_colors[sgn2, sgn1]\n    if sge_color in self._edge_compatibility:\n        ge_color = self._edge_compatibility[sge_color]\n        g_edges = self._ge_partitions[ge_color]\n    else:\n        g_edges = []\n    return g_edges"
 },
 {
  "docstring": "Find all subgraph isomorphisms honoring constraints.",
  "code": "def _map_nodes(self, sgn, candidates, constraints, mapping=None, to_be_mapped=None):\n    if mapping is None:\n        mapping = {}\n    else:\n        mapping = mapping.copy()\n    if to_be_mapped is None:\n        to_be_mapped = set(self.subgraph.nodes)\n    sgn_candidates = intersect(candidates[sgn])\n    candidates[sgn] = frozenset([sgn_candidates])\n    for gn in sgn_candidates:\n        if gn in mapping.values() or sgn not in to_be_mapped:\n            continue\n        mapping[sgn] = gn\n        if to_be_mapped == set(mapping.keys()):\n            yield {v: k for k, v in mapping.items()}\n            continue\n        left_to_map = to_be_mapped - set(mapping.keys())\n        new_candidates = candidates.copy()\n        sgn_neighbours = set(self.subgraph[sgn])\n        not_gn_neighbours = set(self.graph.nodes) - set(self.graph[gn])\n        for sgn2 in left_to_map:\n            if sgn2 not in sgn_neighbours:\n                gn2_options = not_gn_neighbours\n            else:\n                g_edges = self._edges_of_same_color(sgn, sgn2)\n                gn2_options = {n for e in g_edges for n in e if gn in e}\n            new_candidates[sgn2] = new_candidates[sgn2].union([frozenset(gn2_options)])\n            if (sgn, sgn2) in constraints:\n                gn2_options = {gn2 for gn2 in self.graph if gn2 > gn}\n            elif (sgn2, sgn) in constraints:\n                gn2_options = {gn2 for gn2 in self.graph if gn2 < gn}\n            else:\n                continue\n            new_candidates[sgn2] = new_candidates[sgn2].union([frozenset(gn2_options)])\n        next_sgn = min(left_to_map, key=lambda n: min(new_candidates[n], key=len))\n        yield from self._map_nodes(next_sgn, new_candidates, constraints, mapping=mapping, to_be_mapped=to_be_mapped)"
 },
 {
  "docstring": "Find all largest common subgraphs honoring constraints.",
  "code": "def _largest_common_subgraph(self, candidates, constraints, to_be_mapped=None):\n    if to_be_mapped is None:\n        to_be_mapped = {frozenset(self.subgraph.nodes)}\n    current_size = len(next(iter(to_be_mapped), []))\n    found_iso = False\n    if current_size <= len(self.graph):\n        for nodes in sorted(to_be_mapped, key=sorted):\n            next_sgn = min(nodes, key=lambda n: min(candidates[n], key=len))\n            isomorphs = self._map_nodes(next_sgn, candidates, constraints, to_be_mapped=nodes)\n            try:\n                item = next(isomorphs)\n            except StopIteration:\n                pass\n            else:\n                yield item\n                yield from isomorphs\n                found_iso = True\n    if found_iso or current_size == 1:\n        return\n    left_to_be_mapped = set()\n    for nodes in to_be_mapped:\n        for sgn in nodes:\n            new_nodes = self._remove_node(sgn, nodes, constraints)\n            left_to_be_mapped.add(new_nodes)\n    yield from self._largest_common_subgraph(candidates, constraints, to_be_mapped=left_to_be_mapped)"
 },
 {
  "docstring": "Returns a new set where node has been removed from nodes, subject to\nsymmetry constraints. We know, that for every constraint we have\nthose subgraph nodes are equal. So whenever we would remove the\nlower part of a constraint, remove the higher instead.",
  "code": "@staticmethod\ndef _remove_node(node, nodes, constraints):\n    while True:\n        for low, high in constraints:\n            if low == node and high in nodes:\n                node = high\n                break\n        else:\n            break\n    return frozenset(nodes - {node})"
 },
 {
  "docstring": "Return the pairs of top/bottom partitions where the partitions are\ndifferent. Ensures that all partitions in both top and bottom\npartitions have size 1.",
  "code": "@staticmethod\ndef _find_permutations(top_partitions, bottom_partitions):\n    permutations = set()\n    for top, bot in zip(top_partitions, bottom_partitions):\n        if len(top) != 1 or len(bot) != 1:\n            raise IndexError(f'Not all nodes are coupled. This is impossible: {top_partitions}, {bottom_partitions}')\n        if top != bot:\n            permutations.add(frozenset((next(iter(top)), next(iter(bot)))))\n    return permutations"
 },
 {
  "docstring": "Update orbits based on permutations. Orbits is modified in place.\nFor every pair of items in permutations their respective orbits are\nmerged.",
  "code": "@staticmethod\ndef _update_orbits(orbits, permutations):\n    for permutation in permutations:\n        node, node2 = permutation\n        first = second = None\n        for idx, orbit in enumerate(orbits):\n            if first is not None and second is not None:\n                break\n            if node in orbit:\n                first = idx\n            if node2 in orbit:\n                second = idx\n        if first != second:\n            orbits[first].update(orbits[second])\n            del orbits[second]"
 },
 {
  "docstring": "Generate new partitions from top and bottom_partitions where t_node is\ncoupled to b_node. pair_idx is the index of the partitions where t_ and\nb_node can be found.",
  "code": "def _couple_nodes(self, top_partitions, bottom_partitions, pair_idx, t_node, b_node, graph, edge_colors):\n    t_partition = top_partitions[pair_idx]\n    b_partition = bottom_partitions[pair_idx]\n    assert t_node in t_partition and b_node in b_partition\n    new_top_partitions = [top.copy() for top in top_partitions]\n    new_bottom_partitions = [bot.copy() for bot in bottom_partitions]\n    new_t_groups = ({t_node}, t_partition - {t_node})\n    new_b_groups = ({b_node}, b_partition - {b_node})\n    del new_top_partitions[pair_idx]\n    del new_bottom_partitions[pair_idx]\n    new_top_partitions[pair_idx:pair_idx] = new_t_groups\n    new_bottom_partitions[pair_idx:pair_idx] = new_b_groups\n    new_top_partitions = self._refine_node_partitions(graph, new_top_partitions, edge_colors)\n    new_bottom_partitions = self._refine_node_partitions(graph, new_bottom_partitions, edge_colors, branch=True)\n    new_top_partitions = list(new_top_partitions)\n    assert len(new_top_partitions) == 1\n    new_top_partitions = new_top_partitions[0]\n    for bot in new_bottom_partitions:\n        yield (list(new_top_partitions), bot)"
 },
 {
  "docstring": "Processes ordered pair partitions as per the reference paper. Finds and\nreturns all permutations and cosets that leave the graph unchanged.",
  "code": "def _process_ordered_pair_partitions(self, graph, top_partitions, bottom_partitions, edge_colors, orbits=None, cosets=None):\n    if orbits is None:\n        orbits = [{node} for node in graph.nodes]\n    else:\n        orbits = orbits\n    if cosets is None:\n        cosets = {}\n    else:\n        cosets = cosets.copy()\n    assert all((len(t_p) == len(b_p) for t_p, b_p in zip(top_partitions, bottom_partitions)))\n    if all((len(top) == 1 for top in top_partitions)):\n        permutations = self._find_permutations(top_partitions, bottom_partitions)\n        self._update_orbits(orbits, permutations)\n        if permutations:\n            return ([permutations], cosets)\n        else:\n            return ([], cosets)\n    permutations = []\n    unmapped_nodes = {(node, idx) for idx, t_partition in enumerate(top_partitions) for node in t_partition if len(t_partition) > 1}\n    node, pair_idx = min(unmapped_nodes)\n    b_partition = bottom_partitions[pair_idx]\n    for node2 in sorted(b_partition):\n        if len(b_partition) == 1:\n            continue\n        if node != node2 and any((node in orbit and node2 in orbit for orbit in orbits)):\n            continue\n        partitions = self._couple_nodes(top_partitions, bottom_partitions, pair_idx, node, node2, graph, edge_colors)\n        for opp in partitions:\n            new_top_partitions, new_bottom_partitions = opp\n            new_perms, new_cosets = self._process_ordered_pair_partitions(graph, new_top_partitions, new_bottom_partitions, edge_colors, orbits, cosets)\n            permutations += new_perms\n            cosets.update(new_cosets)\n    mapped = {k for top, bottom in zip(top_partitions, bottom_partitions) for k in top if len(top) == 1 and top == bottom}\n    ks = {k for k in graph.nodes if k < node}\n    find_coset = ks <= mapped and node not in cosets\n    if find_coset:\n        for orbit in orbits:\n            if node in orbit:\n                cosets[node] = orbit.copy()\n    return (permutations, cosets)"
 },
 {
  "docstring": "Returns False if graphs are definitely not isomorphic.\nTrue does NOT guarantee isomorphism.\n\nParameters\n----------\nG1, G2 : graphs\n   The two graphs G1 and G2 must be the same type.\n\n",
  "code": "@nx._dispatch(graphs={'G1': 0, 'G2': 1})\ndef could_be_isomorphic(G1, G2):\n    if G1.order() != G2.order():\n        return False\n    d1 = G1.degree()\n    t1 = nx.triangles(G1)\n    clqs_1 = list(nx.find_cliques(G1))\n    c1 = {n: sum((1 for c in clqs_1 if n in c)) for n in G1}\n    props1 = [[d, t1[v], c1[v]] for v, d in d1]\n    props1.sort()\n    d2 = G2.degree()\n    t2 = nx.triangles(G2)\n    clqs_2 = list(nx.find_cliques(G2))\n    c2 = {n: sum((1 for c in clqs_2 if n in c)) for n in G2}\n    props2 = [[d, t2[v], c2[v]] for v, d in d2]\n    props2.sort()\n    if props1 != props2:\n        return False\n    return True"
 },
 {
  "docstring": "Returns False if graphs are definitely not isomorphic.\n\nTrue does NOT guarantee isomorphism.\n\nParameters\n----------\nG1, G2 : graphs\n   The two graphs G1 and G2 must be the same type.\n\n",
  "code": "@nx._dispatch(graphs={'G1': 0, 'G2': 1})\ndef fast_could_be_isomorphic(G1, G2):\n    if G1.order() != G2.order():\n        return False\n    d1 = G1.degree()\n    t1 = nx.triangles(G1)\n    props1 = [[d, t1[v]] for v, d in d1]\n    props1.sort()\n    d2 = G2.degree()\n    t2 = nx.triangles(G2)\n    props2 = [[d, t2[v]] for v, d in d2]\n    props2.sort()\n    if props1 != props2:\n        return False\n    return True"
 },
 {
  "docstring": "Returns False if graphs are definitely not isomorphic.\n\nTrue does NOT guarantee isomorphism.\n\nParameters\n----------\nG1, G2 : graphs\n   The two graphs G1 and G2 must be the same type.\n\n",
  "code": "@nx._dispatch(graphs={'G1': 0, 'G2': 1})\ndef faster_could_be_isomorphic(G1, G2):\n    if G1.order() != G2.order():\n        return False\n    d1 = sorted((d for n, d in G1.degree()))\n    d2 = sorted((d for n, d in G2.degree()))\n    if d1 != d2:\n        return False\n    return True"
 },
 {
  "docstring": "Returns True if the graphs G1 and G2 are isomorphic and False otherwise.\n\nParameters\n----------\nG1, G2: graphs\n    The two graphs G1 and G2 must be the same type.\n\nnode_match : callable\n    A function that returns True if node n1 in G1 and n2 in G2 should\n    be considered equal during the isomorphism test.\n    If node_match is not specified then node attributes are not considered.\n\n    The function will be called like\n\n       node_match(G1.nodes[n1], G2.nodes[n2]).\n\n    That is, the function will receive the node attribute dictionaries\n    for n1 and n2 as inputs.\n\nedge_match : callable\n    A function that returns True if the edge attribute dictionary\n    for the pair of nodes (u1, v1) in G1 and (u2, v2) in G2 should\n    be considered equal during the isomorphism test.  If edge_match is\n    not specified then edge attributes are not considered.\n\n    The function will be called like\n\n       edge_match(G1[u1][v1], G2[u2][v2]).\n\n    That is, the function will receive the edge attribute dictionaries\n    of the edges under consideration.\n\n",
  "code": "@nx._dispatch(graphs={'G1': 0, 'G2': 1}, preserve_edge_attrs='edge_match', preserve_node_attrs='node_match')\ndef is_isomorphic(G1, G2, node_match=None, edge_match=None):\n    if G1.is_directed() and G2.is_directed():\n        GM = nx.algorithms.isomorphism.DiGraphMatcher\n    elif not G1.is_directed() and (not G2.is_directed()):\n        GM = nx.algorithms.isomorphism.GraphMatcher\n    else:\n        raise NetworkXError('Graphs G1 and G2 are not of the same type.')\n    gm = GM(G1, G2, node_match=node_match, edge_match=edge_match)\n    return gm.is_isomorphic()"
 },
 {
  "docstring": "Initialize GraphMatcher.\n\nParameters\n----------\nG1,G2: NetworkX Graph or MultiGraph instances.\n   The two graphs to check for isomorphism or monomorphism.\n\nExamples\n--------\nTo create a GraphMatcher which checks for syntactic feasibility:\n\n>>> from networkx.algorithms import isomorphism\n>>> G1 = nx.path_graph(4)\n>>> G2 = nx.path_graph(4)\n>>> GM = isomorphism.GraphMatcher(G1, G2)",
  "code": "def __init__(self, G1, G2):\n    self.G1 = G1\n    self.G2 = G2\n    self.G1_nodes = set(G1.nodes())\n    self.G2_nodes = set(G2.nodes())\n    self.G2_node_order = {n: i for i, n in enumerate(G2)}\n    self.old_recursion_limit = sys.getrecursionlimit()\n    expected_max_recursion_level = len(self.G2)\n    if self.old_recursion_limit < 1.5 * expected_max_recursion_level:\n        sys.setrecursionlimit(int(1.5 * expected_max_recursion_level))\n    self.test = 'graph'\n    self.initialize()"
 },
 {
  "docstring": "Restores the recursion limit.",
  "code": "def reset_recursion_limit(self):\n    sys.setrecursionlimit(self.old_recursion_limit)"
 },
 {
  "docstring": "Iterator over candidate pairs of nodes in G1 and G2.",
  "code": "def candidate_pairs_iter(self):\n    G1_nodes = self.G1_nodes\n    G2_nodes = self.G2_nodes\n    min_key = self.G2_node_order.__getitem__\n    T1_inout = [node for node in self.inout_1 if node not in self.core_1]\n    T2_inout = [node for node in self.inout_2 if node not in self.core_2]\n    if T1_inout and T2_inout:\n        node_2 = min(T2_inout, key=min_key)\n        for node_1 in T1_inout:\n            yield (node_1, node_2)\n    elif 1:\n        other_node = min(G2_nodes - set(self.core_2), key=min_key)\n        for node in self.G1:\n            if node not in self.core_1:\n                yield (node, other_node)"
 },
 {
  "docstring": "Reinitializes the state of the algorithm.\n\nThis method should be redefined if using something other than GMState.\nIf only subclassing GraphMatcher, a redefinition is not necessary.",
  "code": "def initialize(self):\n    self.core_1 = {}\n    self.core_2 = {}\n    self.inout_1 = {}\n    self.inout_2 = {}\n    self.state = GMState(self)\n    self.mapping = self.core_1.copy()"
 },
 {
  "docstring": "Returns True if G1 and G2 are isomorphic graphs.",
  "code": "def is_isomorphic(self):\n    if self.G1.order() != self.G2.order():\n        return False\n    d1 = sorted((d for n, d in self.G1.degree()))\n    d2 = sorted((d for n, d in self.G2.degree()))\n    if d1 != d2:\n        return False\n    try:\n        x = next(self.isomorphisms_iter())\n        return True\n    except StopIteration:\n        return False"
 },
 {
  "docstring": "Generator over isomorphisms between G1 and G2.",
  "code": "def isomorphisms_iter(self):\n    self.test = 'graph'\n    self.initialize()\n    yield from self.match()"
 },
 {
  "docstring": "Extends the isomorphism mapping.\n\nThis function is called recursively to determine if a complete\nisomorphism can be found between G1 and G2.  It cleans up the class\nvariables after each recursive call. If an isomorphism is found,\nwe yield the mapping.",
  "code": "def match(self):\n    if len(self.core_1) == len(self.G2):\n        self.mapping = self.core_1.copy()\n        yield self.mapping\n    else:\n        for G1_node, G2_node in self.candidate_pairs_iter():\n            if self.syntactic_feasibility(G1_node, G2_node):\n                if self.semantic_feasibility(G1_node, G2_node):\n                    newstate = self.state.__class__(self, G1_node, G2_node)\n                    yield from self.match()\n                    newstate.restore()"
 },
 {
  "docstring": "Returns True if adding (G1_node, G2_node) is semantically feasible.\n\nThe semantic feasibility function should return True if it is\nacceptable to add the candidate pair (G1_node, G2_node) to the current\npartial isomorphism mapping.   The logic should focus on semantic\ninformation contained in the edge data or a formalized node class.\n\nBy acceptable, we mean that the subsequent mapping can still become a\ncomplete isomorphism mapping.  Thus, if adding the candidate pair\ndefinitely makes it so that the subsequent mapping cannot become a\ncomplete isomorphism mapping, then this function must return False.\n\nThe default semantic feasibility function always returns True. The\neffect is that semantics are not considered in the matching of G1\nand G2.\n\nThe semantic checks might differ based on the what type of test is\nbeing performed.  A keyword description of the test is stored in\nself.test.  Here is a quick description of the currently implemented\ntests::\n\n  test='graph'\n    Indicates that the graph matcher is looking for a graph-graph\n    isomorphism.\n\n  test='subgraph'\n    Indicates that the graph matcher is looking for a subgraph-graph\n    isomorphism such that a subgraph of G1 is isomorphic to G2.\n\n  test='mono'\n    Indicates that the graph matcher is looking for a subgraph-graph\n    monomorphism such that a subgraph of G1 is monomorphic to G2.\n\nAny subclass which redefines semantic_feasibility() must maintain\nthe above form to keep the match() method functional. Implementations\nshould consider multigraphs.",
  "code": "def semantic_feasibility(self, G1_node, G2_node):\n    return True"
 },
 {
  "docstring": "Returns True if a subgraph of G1 is isomorphic to G2.",
  "code": "def subgraph_is_isomorphic(self):\n    try:\n        x = next(self.subgraph_isomorphisms_iter())\n        return True\n    except StopIteration:\n        return False"
 },
 {
  "docstring": "Returns True if a subgraph of G1 is monomorphic to G2.",
  "code": "def subgraph_is_monomorphic(self):\n    try:\n        x = next(self.subgraph_monomorphisms_iter())\n        return True\n    except StopIteration:\n        return False"
 },
 {
  "docstring": "Generator over isomorphisms between a subgraph of G1 and G2.",
  "code": "def subgraph_isomorphisms_iter(self):\n    self.test = 'subgraph'\n    self.initialize()\n    yield from self.match()"
 },
 {
  "docstring": "Generator over monomorphisms between a subgraph of G1 and G2.",
  "code": "def subgraph_monomorphisms_iter(self):\n    self.test = 'mono'\n    self.initialize()\n    yield from self.match()"
 },
 {
  "docstring": "Returns True if adding (G1_node, G2_node) is syntactically feasible.\n\nThis function returns True if it is adding the candidate pair\nto the current partial isomorphism/monomorphism mapping is allowable.\nThe addition is allowable if the inclusion of the candidate pair does\nnot make it impossible for an isomorphism/monomorphism to be found.",
  "code": "def syntactic_feasibility(self, G1_node, G2_node):\n    if self.test == 'mono':\n        if self.G1.number_of_edges(G1_node, G1_node) < self.G2.number_of_edges(G2_node, G2_node):\n            return False\n    elif self.G1.number_of_edges(G1_node, G1_node) != self.G2.number_of_edges(G2_node, G2_node):\n        return False\n    if self.test != 'mono':\n        for neighbor in self.G1[G1_node]:\n            if neighbor in self.core_1:\n                if self.core_1[neighbor] not in self.G2[G2_node]:\n                    return False\n                elif self.G1.number_of_edges(neighbor, G1_node) != self.G2.number_of_edges(self.core_1[neighbor], G2_node):\n                    return False\n    for neighbor in self.G2[G2_node]:\n        if neighbor in self.core_2:\n            if self.core_2[neighbor] not in self.G1[G1_node]:\n                return False\n            elif self.test == 'mono':\n                if self.G1.number_of_edges(self.core_2[neighbor], G1_node) < self.G2.number_of_edges(neighbor, G2_node):\n                    return False\n            elif self.G1.number_of_edges(self.core_2[neighbor], G1_node) != self.G2.number_of_edges(neighbor, G2_node):\n                return False\n    if self.test != 'mono':\n        num1 = 0\n        for neighbor in self.G1[G1_node]:\n            if neighbor in self.inout_1 and neighbor not in self.core_1:\n                num1 += 1\n        num2 = 0\n        for neighbor in self.G2[G2_node]:\n            if neighbor in self.inout_2 and neighbor not in self.core_2:\n                num2 += 1\n        if self.test == 'graph':\n            if num1 != num2:\n                return False\n        elif not num1 >= num2:\n            return False\n        num1 = 0\n        for neighbor in self.G1[G1_node]:\n            if neighbor not in self.inout_1:\n                num1 += 1\n        num2 = 0\n        for neighbor in self.G2[G2_node]:\n            if neighbor not in self.inout_2:\n                num2 += 1\n        if self.test == 'graph':\n            if num1 != num2:\n                return False\n        elif not num1 >= num2:\n            return False\n    return True"
 },
 {
  "docstring": "Initialize DiGraphMatcher.\n\nG1 and G2 should be nx.Graph or nx.MultiGraph instances.\n\nExamples\n--------\nTo create a GraphMatcher which checks for syntactic feasibility:\n\n>>> from networkx.algorithms import isomorphism\n>>> G1 = nx.DiGraph(nx.path_graph(4, create_using=nx.DiGraph()))\n>>> G2 = nx.DiGraph(nx.path_graph(4, create_using=nx.DiGraph()))\n>>> DiGM = isomorphism.DiGraphMatcher(G1, G2)",
  "code": "def __init__(self, G1, G2):\n    super().__init__(G1, G2)"
 },
 {
  "docstring": "Iterator over candidate pairs of nodes in G1 and G2.",
  "code": "def candidate_pairs_iter(self):\n    G1_nodes = self.G1_nodes\n    G2_nodes = self.G2_nodes\n    min_key = self.G2_node_order.__getitem__\n    T1_out = [node for node in self.out_1 if node not in self.core_1]\n    T2_out = [node for node in self.out_2 if node not in self.core_2]\n    if T1_out and T2_out:\n        node_2 = min(T2_out, key=min_key)\n        for node_1 in T1_out:\n            yield (node_1, node_2)\n    else:\n        T1_in = [node for node in self.in_1 if node not in self.core_1]\n        T2_in = [node for node in self.in_2 if node not in self.core_2]\n        if T1_in and T2_in:\n            node_2 = min(T2_in, key=min_key)\n            for node_1 in T1_in:\n                yield (node_1, node_2)\n        else:\n            node_2 = min(G2_nodes - set(self.core_2), key=min_key)\n            for node_1 in G1_nodes:\n                if node_1 not in self.core_1:\n                    yield (node_1, node_2)"
 },
 {
  "docstring": "Reinitializes the state of the algorithm.\n\nThis method should be redefined if using something other than DiGMState.\nIf only subclassing GraphMatcher, a redefinition is not necessary.",
  "code": "def initialize(self):\n    self.core_1 = {}\n    self.core_2 = {}\n    self.in_1 = {}\n    self.in_2 = {}\n    self.out_1 = {}\n    self.out_2 = {}\n    self.state = DiGMState(self)\n    self.mapping = self.core_1.copy()"
 },
 {
  "docstring": "Returns True if adding (G1_node, G2_node) is syntactically feasible.\n\nThis function returns True if it is adding the candidate pair\nto the current partial isomorphism/monomorphism mapping is allowable.\nThe addition is allowable if the inclusion of the candidate pair does\nnot make it impossible for an isomorphism/monomorphism to be found.",
  "code": "def syntactic_feasibility(self, G1_node, G2_node):\n    if self.test == 'mono':\n        if self.G1.number_of_edges(G1_node, G1_node) < self.G2.number_of_edges(G2_node, G2_node):\n            return False\n    elif self.G1.number_of_edges(G1_node, G1_node) != self.G2.number_of_edges(G2_node, G2_node):\n        return False\n    if self.test != 'mono':\n        for predecessor in self.G1.pred[G1_node]:\n            if predecessor in self.core_1:\n                if self.core_1[predecessor] not in self.G2.pred[G2_node]:\n                    return False\n                elif self.G1.number_of_edges(predecessor, G1_node) != self.G2.number_of_edges(self.core_1[predecessor], G2_node):\n                    return False\n    for predecessor in self.G2.pred[G2_node]:\n        if predecessor in self.core_2:\n            if self.core_2[predecessor] not in self.G1.pred[G1_node]:\n                return False\n            elif self.test == 'mono':\n                if self.G1.number_of_edges(self.core_2[predecessor], G1_node) < self.G2.number_of_edges(predecessor, G2_node):\n                    return False\n            elif self.G1.number_of_edges(self.core_2[predecessor], G1_node) != self.G2.number_of_edges(predecessor, G2_node):\n                return False\n    if self.test != 'mono':\n        for successor in self.G1[G1_node]:\n            if successor in self.core_1:\n                if self.core_1[successor] not in self.G2[G2_node]:\n                    return False\n                elif self.G1.number_of_edges(G1_node, successor) != self.G2.number_of_edges(G2_node, self.core_1[successor]):\n                    return False\n    for successor in self.G2[G2_node]:\n        if successor in self.core_2:\n            if self.core_2[successor] not in self.G1[G1_node]:\n                return False\n            elif self.test == 'mono':\n                if self.G1.number_of_edges(G1_node, self.core_2[successor]) < self.G2.number_of_edges(G2_node, successor):\n                    return False\n            elif self.G1.number_of_edges(G1_node, self.core_2[successor]) != self.G2.number_of_edges(G2_node, successor):\n                return False\n    if self.test != 'mono':\n        num1 = 0\n        for predecessor in self.G1.pred[G1_node]:\n            if predecessor in self.in_1 and predecessor not in self.core_1:\n                num1 += 1\n        num2 = 0\n        for predecessor in self.G2.pred[G2_node]:\n            if predecessor in self.in_2 and predecessor not in self.core_2:\n                num2 += 1\n        if self.test == 'graph':\n            if num1 != num2:\n                return False\n        elif not num1 >= num2:\n            return False\n        num1 = 0\n        for successor in self.G1[G1_node]:\n            if successor in self.in_1 and successor not in self.core_1:\n                num1 += 1\n        num2 = 0\n        for successor in self.G2[G2_node]:\n            if successor in self.in_2 and successor not in self.core_2:\n                num2 += 1\n        if self.test == 'graph':\n            if num1 != num2:\n                return False\n        elif not num1 >= num2:\n            return False\n        num1 = 0\n        for predecessor in self.G1.pred[G1_node]:\n            if predecessor in self.out_1 and predecessor not in self.core_1:\n                num1 += 1\n        num2 = 0\n        for predecessor in self.G2.pred[G2_node]:\n            if predecessor in self.out_2 and predecessor not in self.core_2:\n                num2 += 1\n        if self.test == 'graph':\n            if num1 != num2:\n                return False\n        elif not num1 >= num2:\n            return False\n        num1 = 0\n        for successor in self.G1[G1_node]:\n            if successor in self.out_1 and successor not in self.core_1:\n                num1 += 1\n        num2 = 0\n        for successor in self.G2[G2_node]:\n            if successor in self.out_2 and successor not in self.core_2:\n                num2 += 1\n        if self.test == 'graph':\n            if num1 != num2:\n                return False\n        elif not num1 >= num2:\n            return False\n        num1 = 0\n        for predecessor in self.G1.pred[G1_node]:\n            if predecessor not in self.in_1 and predecessor not in self.out_1:\n                num1 += 1\n        num2 = 0\n        for predecessor in self.G2.pred[G2_node]:\n            if predecessor not in self.in_2 and predecessor not in self.out_2:\n                num2 += 1\n        if self.test == 'graph':\n            if num1 != num2:\n                return False\n        elif not num1 >= num2:\n            return False\n        num1 = 0\n        for successor in self.G1[G1_node]:\n            if successor not in self.in_1 and successor not in self.out_1:\n                num1 += 1\n        num2 = 0\n        for successor in self.G2[G2_node]:\n            if successor not in self.in_2 and successor not in self.out_2:\n                num2 += 1\n        if self.test == 'graph':\n            if num1 != num2:\n                return False\n        elif not num1 >= num2:\n            return False\n    return True"
 },
 {
  "docstring": "Initializes GMState object.\n\nPass in the GraphMatcher to which this GMState belongs and the\nnew node pair that will be added to the GraphMatcher's current\nisomorphism mapping.",
  "code": "def __init__(self, GM, G1_node=None, G2_node=None):\n    self.GM = GM\n    self.G1_node = None\n    self.G2_node = None\n    self.depth = len(GM.core_1)\n    if G1_node is None or G2_node is None:\n        GM.core_1 = {}\n        GM.core_2 = {}\n        GM.inout_1 = {}\n        GM.inout_2 = {}\n    if G1_node is not None and G2_node is not None:\n        GM.core_1[G1_node] = G2_node\n        GM.core_2[G2_node] = G1_node\n        self.G1_node = G1_node\n        self.G2_node = G2_node\n        self.depth = len(GM.core_1)\n        if G1_node not in GM.inout_1:\n            GM.inout_1[G1_node] = self.depth\n        if G2_node not in GM.inout_2:\n            GM.inout_2[G2_node] = self.depth\n        new_nodes = set()\n        for node in GM.core_1:\n            new_nodes.update([neighbor for neighbor in GM.G1[node] if neighbor not in GM.core_1])\n        for node in new_nodes:\n            if node not in GM.inout_1:\n                GM.inout_1[node] = self.depth\n        new_nodes = set()\n        for node in GM.core_2:\n            new_nodes.update([neighbor for neighbor in GM.G2[node] if neighbor not in GM.core_2])\n        for node in new_nodes:\n            if node not in GM.inout_2:\n                GM.inout_2[node] = self.depth"
 },
 {
  "docstring": "Deletes the GMState object and restores the class variables.",
  "code": "def restore(self):\n    if self.G1_node is not None and self.G2_node is not None:\n        del self.GM.core_1[self.G1_node]\n        del self.GM.core_2[self.G2_node]\n    for vector in (self.GM.inout_1, self.GM.inout_2):\n        for node in list(vector.keys()):\n            if vector[node] == self.depth:\n                del vector[node]"
 },
 {
  "docstring": "Initializes DiGMState object.\n\nPass in the DiGraphMatcher to which this DiGMState belongs and the\nnew node pair that will be added to the GraphMatcher's current\nisomorphism mapping.",
  "code": "def __init__(self, GM, G1_node=None, G2_node=None):\n    self.GM = GM\n    self.G1_node = None\n    self.G2_node = None\n    self.depth = len(GM.core_1)\n    if G1_node is None or G2_node is None:\n        GM.core_1 = {}\n        GM.core_2 = {}\n        GM.in_1 = {}\n        GM.in_2 = {}\n        GM.out_1 = {}\n        GM.out_2 = {}\n    if G1_node is not None and G2_node is not None:\n        GM.core_1[G1_node] = G2_node\n        GM.core_2[G2_node] = G1_node\n        self.G1_node = G1_node\n        self.G2_node = G2_node\n        self.depth = len(GM.core_1)\n        for vector in (GM.in_1, GM.out_1):\n            if G1_node not in vector:\n                vector[G1_node] = self.depth\n        for vector in (GM.in_2, GM.out_2):\n            if G2_node not in vector:\n                vector[G2_node] = self.depth\n        new_nodes = set()\n        for node in GM.core_1:\n            new_nodes.update([predecessor for predecessor in GM.G1.predecessors(node) if predecessor not in GM.core_1])\n        for node in new_nodes:\n            if node not in GM.in_1:\n                GM.in_1[node] = self.depth\n        new_nodes = set()\n        for node in GM.core_2:\n            new_nodes.update([predecessor for predecessor in GM.G2.predecessors(node) if predecessor not in GM.core_2])\n        for node in new_nodes:\n            if node not in GM.in_2:\n                GM.in_2[node] = self.depth\n        new_nodes = set()\n        for node in GM.core_1:\n            new_nodes.update([successor for successor in GM.G1.successors(node) if successor not in GM.core_1])\n        for node in new_nodes:\n            if node not in GM.out_1:\n                GM.out_1[node] = self.depth\n        new_nodes = set()\n        for node in GM.core_2:\n            new_nodes.update([successor for successor in GM.G2.successors(node) if successor not in GM.core_2])\n        for node in new_nodes:\n            if node not in GM.out_2:\n                GM.out_2[node] = self.depth"
 },
 {
  "docstring": "Deletes the DiGMState object and restores the class variables.",
  "code": "def restore(self):\n    if self.G1_node is not None and self.G2_node is not None:\n        del self.GM.core_1[self.G1_node]\n        del self.GM.core_2[self.G2_node]\n    for vector in (self.GM.in_1, self.GM.in_2, self.GM.out_1, self.GM.out_2):\n        for node in list(vector.keys()):\n            if vector[node] == self.depth:\n                del vector[node]"
 },
 {
  "docstring": "Returns a deepcopy of a function.",
  "code": "def copyfunc(f, name=None):\n    return types.FunctionType(f.__code__, f.__globals__, name or f.__name__, f.__defaults__, f.__closure__)"
 },
 {
  "docstring": "Returns True if x and y are sufficiently close, elementwise.\n\nParameters\n----------\nrtol : float\n    The relative error tolerance.\natol : float\n    The absolute error tolerance.",
  "code": "def allclose(x, y, rtol=1e-05, atol=1e-08):\n    return all((math.isclose(xi, yi, rel_tol=rtol, abs_tol=atol) for xi, yi in zip(x, y)))"
 },
 {
  "docstring": "Returns a comparison function for a generic attribute.\n\nThe value(s) of the attr(s) are compared using the specified\noperators. If all the attributes are equal, then the constructed\nfunction returns True. Potentially, the constructed edge_match\nfunction can be slow since it must verify that no isomorphism\nexists between the multiedges before it returns False.\n\nParameters\n----------\nattr : string | list\n    The edge attribute to compare, or a list of node attributes\n    to compare.\ndefault : value | list\n    The default value for the edge attribute, or a list of\n    default values for the edgeattributes.\nop : callable | list\n    The operator to use when comparing attribute values, or a list\n    of operators to use when comparing values for each attribute.\n\nReturns\n-------\nmatch : function\n    The customized, generic `edge_match` function.\n\nExamples\n--------\n>>> from operator import eq\n>>> from math import isclose\n>>> from networkx.algorithms.isomorphism import generic_node_match\n>>> nm = generic_node_match(\"weight\", 1.0, isclose)\n>>> nm = generic_node_match(\"color\", \"red\", eq)\n>>> nm = generic_node_match([\"weight\", \"color\"], [1.0, \"red\"], [isclose, eq])\n...",
  "code": "def generic_multiedge_match(attr, default, op):\n    if isinstance(attr, str):\n        attr = [attr]\n        default = [default]\n        op = [op]\n    attrs = list(zip(attr, default))\n\n    def match(datasets1, datasets2):\n        values1 = []\n        for data1 in datasets1.values():\n            x = tuple((data1.get(attr, d) for attr, d in attrs))\n            values1.append(x)\n        values2 = []\n        for data2 in datasets2.values():\n            x = tuple((data2.get(attr, d) for attr, d in attrs))\n            values2.append(x)\n        for vals2 in permutations(values2):\n            for xi, yi in zip(values1, vals2):\n                if not all(map(lambda x, y, z: z(x, y), xi, yi, op)):\n                    break\n            else:\n                return True\n        else:\n            return False\n    return match"
 },
 {
  "docstring": "Initialize TimeRespectingGraphMatcher.\n\nG1 and G2 should be nx.Graph or nx.MultiGraph instances.\n\nExamples\n--------\nTo create a TimeRespectingGraphMatcher which checks for\nsyntactic and semantic feasibility:\n\n>>> from networkx.algorithms import isomorphism\n>>> from datetime import timedelta\n>>> G1 = nx.Graph(nx.path_graph(4, create_using=nx.Graph()))\n\n>>> G2 = nx.Graph(nx.path_graph(4, create_using=nx.Graph()))\n\n>>> GM = isomorphism.TimeRespectingGraphMatcher(\n...     G1, G2, \"date\", timedelta(days=1)\n... )",
  "code": "def __init__(self, G1, G2, temporal_attribute_name, delta):\n    self.temporal_attribute_name = temporal_attribute_name\n    self.delta = delta\n    super().__init__(G1, G2)"
 },
 {
  "docstring": "Edges one hop out from a node in the mapping should be\ntime-respecting with respect to each other.",
  "code": "def one_hop(self, Gx, Gx_node, neighbors):\n    dates = []\n    for n in neighbors:\n        if isinstance(Gx, nx.Graph):\n            dates.append(Gx[Gx_node][n][self.temporal_attribute_name])\n        else:\n            for edge in Gx[Gx_node][n].values():\n                dates.append(edge[self.temporal_attribute_name])\n    if any((x is None for x in dates)):\n        raise ValueError('Datetime not supplied for at least one edge.')\n    return not dates or max(dates) - min(dates) <= self.delta"
 },
 {
  "docstring": "Paths of length 2 from Gx_node should be time-respecting.",
  "code": "def two_hop(self, Gx, core_x, Gx_node, neighbors):\n    return all((self.one_hop(Gx, v, [n for n in Gx[v] if n in core_x] + [Gx_node]) for v in neighbors))"
 },
 {
  "docstring": "Returns True if adding (G1_node, G2_node) is semantically\nfeasible.\n\nAny subclass which redefines semantic_feasibility() must\nmaintain the self.tests if needed, to keep the match() method\nfunctional. Implementations should consider multigraphs.",
  "code": "def semantic_feasibility(self, G1_node, G2_node):\n    neighbors = [n for n in self.G1[G1_node] if n in self.core_1]\n    if not self.one_hop(self.G1, G1_node, neighbors):\n        return False\n    if not self.two_hop(self.G1, self.core_1, G1_node, neighbors):\n        return False\n    return True"
 },
 {
  "docstring": "Initialize TimeRespectingDiGraphMatcher.\n\nG1 and G2 should be nx.DiGraph or nx.MultiDiGraph instances.\n\nExamples\n--------\nTo create a TimeRespectingDiGraphMatcher which checks for\nsyntactic and semantic feasibility:\n\n>>> from networkx.algorithms import isomorphism\n>>> from datetime import timedelta\n>>> G1 = nx.DiGraph(nx.path_graph(4, create_using=nx.DiGraph()))\n\n>>> G2 = nx.DiGraph(nx.path_graph(4, create_using=nx.DiGraph()))\n\n>>> GM = isomorphism.TimeRespectingDiGraphMatcher(\n...     G1, G2, \"date\", timedelta(days=1)\n... )",
  "code": "def __init__(self, G1, G2, temporal_attribute_name, delta):\n    self.temporal_attribute_name = temporal_attribute_name\n    self.delta = delta\n    super().__init__(G1, G2)"
 },
 {
  "docstring": "Get the dates of edges from predecessors.",
  "code": "def get_pred_dates(self, Gx, Gx_node, core_x, pred):\n    pred_dates = []\n    if isinstance(Gx, nx.DiGraph):\n        for n in pred:\n            pred_dates.append(Gx[n][Gx_node][self.temporal_attribute_name])\n    else:\n        for n in pred:\n            for edge in Gx[n][Gx_node].values():\n                pred_dates.append(edge[self.temporal_attribute_name])\n    return pred_dates"
 },
 {
  "docstring": "Get the dates of edges to successors.",
  "code": "def get_succ_dates(self, Gx, Gx_node, core_x, succ):\n    succ_dates = []\n    if isinstance(Gx, nx.DiGraph):\n        for n in succ:\n            succ_dates.append(Gx[Gx_node][n][self.temporal_attribute_name])\n    else:\n        for n in succ:\n            for edge in Gx[Gx_node][n].values():\n                succ_dates.append(edge[self.temporal_attribute_name])\n    return succ_dates"
 },
 {
  "docstring": "The ego node.",
  "code": "def one_hop(self, Gx, Gx_node, core_x, pred, succ):\n    pred_dates = self.get_pred_dates(Gx, Gx_node, core_x, pred)\n    succ_dates = self.get_succ_dates(Gx, Gx_node, core_x, succ)\n    return self.test_one(pred_dates, succ_dates) and self.test_two(pred_dates, succ_dates)"
 },
 {
  "docstring": "The predecessors of the ego node.",
  "code": "def two_hop_pred(self, Gx, Gx_node, core_x, pred):\n    return all((self.one_hop(Gx, p, core_x, self.preds(Gx, core_x, p), self.succs(Gx, core_x, p, Gx_node)) for p in pred))"
 },
 {
  "docstring": "The successors of the ego node.",
  "code": "def two_hop_succ(self, Gx, Gx_node, core_x, succ):\n    return all((self.one_hop(Gx, s, core_x, self.preds(Gx, core_x, s, Gx_node), self.succs(Gx, core_x, s)) for s in succ))"
 },
 {
  "docstring": "Edges one hop out from Gx_node in the mapping should be\ntime-respecting with respect to each other, regardless of\ndirection.",
  "code": "def test_one(self, pred_dates, succ_dates):\n    time_respecting = True\n    dates = pred_dates + succ_dates\n    if any((x is None for x in dates)):\n        raise ValueError('Date or datetime not supplied for at least one edge.')\n    dates.sort()\n    if 0 < len(dates) and (not dates[-1] - dates[0] <= self.delta):\n        time_respecting = False\n    return time_respecting"
 },
 {
  "docstring": "Edges from a dual Gx_node in the mapping should be ordered in\na time-respecting manner.",
  "code": "def test_two(self, pred_dates, succ_dates):\n    time_respecting = True\n    pred_dates.sort()\n    succ_dates.sort()\n    if 0 < len(succ_dates) and 0 < len(pred_dates) and (succ_dates[0] < pred_dates[-1]):\n        time_respecting = False\n    return time_respecting"
 },
 {
  "docstring": "Returns True if adding (G1_node, G2_node) is semantically\nfeasible.\n\nAny subclass which redefines semantic_feasibility() must\nmaintain the self.tests if needed, to keep the match() method\nfunctional. Implementations should consider multigraphs.",
  "code": "def semantic_feasibility(self, G1_node, G2_node):\n    pred, succ = ([n for n in self.G1.predecessors(G1_node) if n in self.core_1], [n for n in self.G1.successors(G1_node) if n in self.core_1])\n    if not self.one_hop(self.G1, G1_node, self.core_1, pred, succ):\n        return False\n    if not self.two_hop_pred(self.G1, G1_node, self.core_1, pred):\n        return False\n    if not self.two_hop_succ(self.G1, G1_node, self.core_1, succ):\n        return False\n    return True"
 },
 {
  "docstring": "Create a single digraph dT of free trees t1 and t2\n#   with roots root1 and root2 respectively\n# rename the nodes with consecutive integers\n# so that all nodes get a unique name between both trees\n\n# our new \"fake\" root node is 0\n# t1 is numbers from 1 ... n\n# t2 is numbered from n+1 to 2n",
  "code": "@nx._dispatch(graphs={'t1': 0, 't2': 2})\ndef root_trees(t1, root1, t2, root2):\n    dT = nx.DiGraph()\n    newroot1 = 1\n    newroot2 = nx.number_of_nodes(t1) + 1\n    namemap1 = {root1: newroot1}\n    namemap2 = {root2: newroot2}\n    dT.add_edge(0, namemap1[root1])\n    dT.add_edge(0, namemap2[root2])\n    for i, (v1, v2) in enumerate(nx.bfs_edges(t1, root1)):\n        namemap1[v2] = i + namemap1[root1] + 1\n        dT.add_edge(namemap1[v1], namemap1[v2])\n    for i, (v1, v2) in enumerate(nx.bfs_edges(t2, root2)):\n        namemap2[v2] = i + namemap2[root2] + 1\n        dT.add_edge(namemap2[v1], namemap2[v2])\n    namemap = {}\n    for old, new in namemap1.items():\n        namemap[new] = old\n    for old, new in namemap2.items():\n        namemap[new] = old\n    return (dT, namemap, newroot1, newroot2)"
 },
 {
  "docstring": "Given two rooted trees `t1` and `t2`,\nwith roots `root1` and `root2` respectively\nthis routine will determine if they are isomorphic.\n\nThese trees may be either directed or undirected,\nbut if they are directed, all edges should flow from the root.\n\nIt returns the isomorphism, a mapping of the nodes of `t1` onto the nodes\nof `t2`, such that two trees are then identical.\n\nNote that two trees may have more than one isomorphism, and this\nroutine just returns one valid mapping.\n\nParameters\n----------\n`t1` :  NetworkX graph\n    One of the trees being compared\n\n`root1` : a node of `t1` which is the root of the tree\n\n`t2` : undirected NetworkX graph\n    The other tree being compared\n\n`root2` : a node of `t2` which is the root of the tree\n\nThis is a subroutine used to implement `tree_isomorphism`, but will\nbe somewhat faster if you already have rooted trees.\n\nReturns\n-------\nisomorphism : list\n    A list of pairs in which the left element is a node in `t1`\n    and the right element is a node in `t2`.  The pairs are in\n    arbitrary order.  If the nodes in one tree is mapped to the names in\n    the other, then trees will be identical. Note that an isomorphism\n    will not necessarily be unique.\n\n    If `t1` and `t2` are not isomorphic, then it returns the empty list.",
  "code": "@nx._dispatch(graphs={'t1': 0, 't2': 2})\ndef rooted_tree_isomorphism(t1, root1, t2, root2):\n    assert nx.is_tree(t1)\n    assert nx.is_tree(t2)\n    dT, namemap, newroot1, newroot2 = root_trees(t1, root1, t2, root2)\n    levels = assign_levels(dT, 0)\n    h = max(levels.values())\n    L = group_by_levels(levels)\n    label = {v: 0 for v in dT}\n    ordered_labels = {v: () for v in dT}\n    ordered_children = {v: () for v in dT}\n    for i in range(h - 1, 0, -1):\n        for v in L[i]:\n            if dT.out_degree(v) > 0:\n                s = sorted(((label[u], u) for u in dT.successors(v)))\n                ordered_labels[v], ordered_children[v] = list(zip(*s))\n        forlabel = sorted(((ordered_labels[v], v) for v in L[i]))\n        current = 0\n        for i, (ol, v) in enumerate(forlabel):\n            if i != 0 and ol != forlabel[i - 1][0]:\n                current += 1\n            label[v] = current\n    isomorphism = []\n    if label[newroot1] == 0 and label[newroot2] == 0:\n        generate_isomorphism(newroot1, newroot2, isomorphism, ordered_children)\n        isomorphism = [(namemap[u], namemap[v]) for u, v in isomorphism]\n    return isomorphism"
 },
 {
  "docstring": "Given two undirected (or free) trees `t1` and `t2`,\nthis routine will determine if they are isomorphic.\nIt returns the isomorphism, a mapping of the nodes of `t1` onto the nodes\nof `t2`, such that two trees are then identical.\n\nNote that two trees may have more than one isomorphism, and this\nroutine just returns one valid mapping.\n\nParameters\n----------\nt1 : undirected NetworkX graph\n    One of the trees being compared\n\nt2 : undirected NetworkX graph\n    The other tree being compared\n\nReturns\n-------\nisomorphism : list\n    A list of pairs in which the left element is a node in `t1`\n    and the right element is a node in `t2`.  The pairs are in\n    arbitrary order.  If the nodes in one tree is mapped to the names in\n    the other, then trees will be identical. Note that an isomorphism\n    will not necessarily be unique.\n\n    If `t1` and `t2` are not isomorphic, then it returns the empty list.\n\n",
  "code": "@not_implemented_for('directed', 'multigraph')\n@nx._dispatch(graphs={'t1': 0, 't2': 1})\ndef tree_isomorphism(t1, t2):\n    assert nx.is_tree(t1)\n    assert nx.is_tree(t2)\n    if nx.number_of_nodes(t1) != nx.number_of_nodes(t2):\n        return []\n    degree_sequence1 = sorted((d for n, d in t1.degree()))\n    degree_sequence2 = sorted((d for n, d in t2.degree()))\n    if degree_sequence1 != degree_sequence2:\n        return []\n    center1 = nx.center(t1)\n    center2 = nx.center(t2)\n    if len(center1) != len(center2):\n        return []\n    if len(center1) == 1:\n        return rooted_tree_isomorphism(t1, center1[0], t2, center2[0])\n    attempts = rooted_tree_isomorphism(t1, center1[0], t2, center2[0])\n    if len(attempts) > 0:\n        return attempts\n    return rooted_tree_isomorphism(t1, center1[0], t2, center2[1])"
 },
 {
  "docstring": "Return an isomorphic mapping between `G1` and `G2` if it exists.\n\nParameters\n----------\nG1, G2 : NetworkX Graph or MultiGraph instances.\n    The two graphs to check for isomorphism.\n\nnode_label : str, optional\n    The name of the node attribute to be used when comparing nodes.\n    The default is `None`, meaning node attributes are not considered\n    in the comparison. Any node that doesn't have the `node_label`\n    attribute uses `default_label` instead.\n\ndefault_label : scalar\n    Default value to use when a node doesn't have an attribute\n    named `node_label`. Default is `None`.\n\nReturns\n-------\ndict or None\n    Node mapping if the two graphs are isomorphic. None otherwise.",
  "code": "@nx._dispatch(graphs={'G1': 0, 'G2': 1}, node_attrs={'node_label': 'default_label'})\ndef vf2pp_isomorphism(G1, G2, node_label=None, default_label=None):\n    try:\n        mapping = next(vf2pp_all_isomorphisms(G1, G2, node_label, default_label))\n        return mapping\n    except StopIteration:\n        return None"
 },
 {
  "docstring": "Examines whether G1 and G2 are isomorphic.\n\nParameters\n----------\nG1, G2 : NetworkX Graph or MultiGraph instances.\n    The two graphs to check for isomorphism.\n\nnode_label : str, optional\n    The name of the node attribute to be used when comparing nodes.\n    The default is `None`, meaning node attributes are not considered\n    in the comparison. Any node that doesn't have the `node_label`\n    attribute uses `default_label` instead.\n\ndefault_label : scalar\n    Default value to use when a node doesn't have an attribute\n    named `node_label`. Default is `None`.\n\nReturns\n-------\nbool\n    True if the two graphs are isomorphic, False otherwise.",
  "code": "@nx._dispatch(graphs={'G1': 0, 'G2': 1}, node_attrs={'node_label': 'default_label'})\ndef vf2pp_is_isomorphic(G1, G2, node_label=None, default_label=None):\n    if vf2pp_isomorphism(G1, G2, node_label, default_label) is not None:\n        return True\n    return False"
 },
 {
  "docstring": "Yields all the possible mappings between G1 and G2.\n\nParameters\n----------\nG1, G2 : NetworkX Graph or MultiGraph instances.\n    The two graphs to check for isomorphism.\n\nnode_label : str, optional\n    The name of the node attribute to be used when comparing nodes.\n    The default is `None`, meaning node attributes are not considered\n    in the comparison. Any node that doesn't have the `node_label`\n    attribute uses `default_label` instead.\n\ndefault_label : scalar\n    Default value to use when a node doesn't have an attribute\n    named `node_label`. Default is `None`.\n\nYields\n------\ndict\n    Isomorphic mapping between the nodes in `G1` and `G2`.",
  "code": "@nx._dispatch(graphs={'G1': 0, 'G2': 1}, node_attrs={'node_label': 'default_label'})\ndef vf2pp_all_isomorphisms(G1, G2, node_label=None, default_label=None):\n    if G1.number_of_nodes() == 0 or G2.number_of_nodes() == 0:\n        return False\n    if G1.is_directed():\n        G1_degree = {n: (in_degree, out_degree) for (n, in_degree), (_, out_degree) in zip(G1.in_degree, G1.out_degree)}\n        G2_degree = {n: (in_degree, out_degree) for (n, in_degree), (_, out_degree) in zip(G2.in_degree, G2.out_degree)}\n    else:\n        G1_degree = dict(G1.degree)\n        G2_degree = dict(G2.degree)\n    if not G1.is_directed():\n        find_candidates = _find_candidates\n        restore_Tinout = _restore_Tinout\n    else:\n        find_candidates = _find_candidates_Di\n        restore_Tinout = _restore_Tinout_Di\n    if G1.order() != G2.order():\n        return False\n    if sorted(G1_degree.values()) != sorted(G2_degree.values()):\n        return False\n    graph_params, state_params = _initialize_parameters(G1, G2, G2_degree, node_label, default_label)\n    if not _precheck_label_properties(graph_params):\n        return False\n    node_order = _matching_order(graph_params)\n    stack = []\n    candidates = iter(find_candidates(node_order[0], graph_params, state_params, G1_degree))\n    stack.append((node_order[0], candidates))\n    mapping = state_params.mapping\n    reverse_mapping = state_params.reverse_mapping\n    matching_node = 1\n    while stack:\n        current_node, candidate_nodes = stack[-1]\n        try:\n            candidate = next(candidate_nodes)\n        except StopIteration:\n            stack.pop()\n            matching_node -= 1\n            if stack:\n                popped_node1, _ = stack[-1]\n                popped_node2 = mapping[popped_node1]\n                mapping.pop(popped_node1)\n                reverse_mapping.pop(popped_node2)\n                restore_Tinout(popped_node1, popped_node2, graph_params, state_params)\n            continue\n        if _feasibility(current_node, candidate, graph_params, state_params):\n            if len(mapping) == G2.number_of_nodes() - 1:\n                cp_mapping = mapping.copy()\n                cp_mapping[current_node] = candidate\n                yield cp_mapping\n                continue\n            mapping[current_node] = candidate\n            reverse_mapping[candidate] = current_node\n            _update_Tinout(current_node, candidate, graph_params, state_params)\n            candidates = iter(find_candidates(node_order[matching_node], graph_params, state_params, G1_degree))\n            stack.append((node_order[matching_node], candidates))\n            matching_node += 1"
 },
 {
  "docstring": "Initializes all the necessary parameters for VF2++\n\nParameters\n----------\nG1,G2: NetworkX Graph or MultiGraph instances.\n    The two graphs to check for isomorphism or monomorphism\n\nG1_labels,G2_labels: dict\n    The label of every node in G1 and G2 respectively\n\nReturns\n-------\ngraph_params: namedtuple\n    Contains all the Graph-related parameters:\n\n    G1,G2\n    G1_labels,G2_labels: dict\n\nstate_params: namedtuple\n    Contains all the State-related parameters:\n\n    mapping: dict\n        The mapping as extended so far. Maps nodes of G1 to nodes of G2\n\n    reverse_mapping: dict\n        The reverse mapping as extended so far. Maps nodes from G2 to nodes of G1. It's basically \"mapping\" reversed\n\n    T1, T2: set\n        Ti contains uncovered neighbors of covered nodes from Gi, i.e. nodes that are not in the mapping, but are\n        neighbors of nodes that are.\n\n    T1_out, T2_out: set\n        Ti_out contains all the nodes from Gi, that are neither in the mapping nor in Ti",
  "code": "def _initialize_parameters(G1, G2, G2_degree, node_label=None, default_label=-1):\n    G1_labels = dict(G1.nodes(data=node_label, default=default_label))\n    G2_labels = dict(G2.nodes(data=node_label, default=default_label))\n    graph_params = _GraphParameters(G1, G2, G1_labels, G2_labels, nx.utils.groups(G1_labels), nx.utils.groups(G2_labels), nx.utils.groups(G2_degree))\n    T1, T1_in = (set(), set())\n    T2, T2_in = (set(), set())\n    if G1.is_directed():\n        T1_tilde, T1_tilde_in = (set(G1.nodes()), set())\n        T2_tilde, T2_tilde_in = (set(G2.nodes()), set())\n    else:\n        T1_tilde, T1_tilde_in = (set(G1.nodes()), set())\n        T2_tilde, T2_tilde_in = (set(G2.nodes()), set())\n    state_params = _StateParameters({}, {}, T1, T1_in, T1_tilde, T1_tilde_in, T2, T2_in, T2_tilde, T2_tilde_in)\n    return (graph_params, state_params)"
 },
 {
  "docstring": "The node ordering as introduced in VF2++.\n\n",
  "code": "def _matching_order(graph_params):\n    G1, G2, G1_labels, _, _, nodes_of_G2Labels, _ = graph_params\n    if not G1 and (not G2):\n        return {}\n    if G1.is_directed():\n        G1 = G1.to_undirected(as_view=True)\n    V1_unordered = set(G1.nodes())\n    label_rarity = {label: len(nodes) for label, nodes in nodes_of_G2Labels.items()}\n    used_degrees = {node: 0 for node in G1}\n    node_order = []\n    while V1_unordered:\n        max_rarity = min((label_rarity[G1_labels[x]] for x in V1_unordered))\n        rarest_nodes = [n for n in V1_unordered if label_rarity[G1_labels[n]] == max_rarity]\n        max_node = max(rarest_nodes, key=G1.degree)\n        for dlevel_nodes in nx.bfs_layers(G1, max_node):\n            nodes_to_add = dlevel_nodes.copy()\n            while nodes_to_add:\n                max_used_degree = max((used_degrees[n] for n in nodes_to_add))\n                max_used_degree_nodes = [n for n in nodes_to_add if used_degrees[n] == max_used_degree]\n                max_degree = max((G1.degree[n] for n in max_used_degree_nodes))\n                max_degree_nodes = [n for n in max_used_degree_nodes if G1.degree[n] == max_degree]\n                next_node = min(max_degree_nodes, key=lambda x: label_rarity[G1_labels[x]])\n                node_order.append(next_node)\n                for node in G1.neighbors(next_node):\n                    used_degrees[node] += 1\n                nodes_to_add.remove(next_node)\n                label_rarity[G1_labels[next_node]] -= 1\n                V1_unordered.discard(next_node)\n    return node_order"
 },
 {
  "docstring": "Given node u of G1, finds the candidates of u from G2.\n\nParameters\n----------\nu: Graph node\n    The node from G1 for which to find the candidates from G2.\n\ngraph_params: namedtuple\n    Contains all the Graph-related parameters:\n\n    G1,G2: NetworkX Graph or MultiGraph instances.\n        The two graphs to check for isomorphism or monomorphism\n\n    G1_labels,G2_labels: dict\n        The label of every node in G1 and G2 respectively\n\nstate_params: namedtuple\n    Contains all the State-related parameters:\n\n    mapping: dict\n        The mapping as extended so far. Maps nodes of G1 to nodes of G2\n\n    reverse_mapping: dict\n        The reverse mapping as extended so far. Maps nodes from G2 to nodes of G1. It's basically \"mapping\" reversed\n\n    T1, T2: set\n        Ti contains uncovered neighbors of covered nodes from Gi, i.e. nodes that are not in the mapping, but are\n        neighbors of nodes that are.\n\n    T1_tilde, T2_tilde: set\n        Ti_tilde contains all the nodes from Gi, that are neither in the mapping nor in Ti\n\nReturns\n-------\ncandidates: set\n    The nodes from G2 which are candidates for u.",
  "code": "def _find_candidates(u, graph_params, state_params, G1_degree):\n    G1, G2, G1_labels, _, _, nodes_of_G2Labels, G2_nodes_of_degree = graph_params\n    mapping, reverse_mapping, _, _, _, _, _, _, T2_tilde, _ = state_params\n    covered_neighbors = [nbr for nbr in G1[u] if nbr in mapping]\n    if not covered_neighbors:\n        candidates = set(nodes_of_G2Labels[G1_labels[u]])\n        candidates.intersection_update(G2_nodes_of_degree[G1_degree[u]])\n        candidates.intersection_update(T2_tilde)\n        candidates.difference_update(reverse_mapping)\n        if G1.is_multigraph():\n            candidates.difference_update({node for node in candidates if G1.number_of_edges(u, u) != G2.number_of_edges(node, node)})\n        return candidates\n    nbr1 = covered_neighbors[0]\n    common_nodes = set(G2[mapping[nbr1]])\n    for nbr1 in covered_neighbors[1:]:\n        common_nodes.intersection_update(G2[mapping[nbr1]])\n    common_nodes.difference_update(reverse_mapping)\n    common_nodes.intersection_update(G2_nodes_of_degree[G1_degree[u]])\n    common_nodes.intersection_update(nodes_of_G2Labels[G1_labels[u]])\n    if G1.is_multigraph():\n        common_nodes.difference_update({node for node in common_nodes if G1.number_of_edges(u, u) != G2.number_of_edges(node, node)})\n    return common_nodes"
 },
 {
  "docstring": "Given a candidate pair of nodes u and v from G1 and G2 respectively, checks if it's feasible to extend the\nmapping, i.e. if u and v can be matched.\n\n",
  "code": "def _feasibility(node1, node2, graph_params, state_params):\n    G1 = graph_params.G1\n    if _cut_PT(node1, node2, graph_params, state_params):\n        return False\n    if G1.is_multigraph():\n        if not _consistent_PT(node1, node2, graph_params, state_params):\n            return False\n    return True"
 },
 {
  "docstring": "Implements the cutting rules for the ISO problem.\n\nParameters\n----------\nu, v: Graph node\n    The two candidate nodes being examined.\n\ngraph_params: namedtuple\n    Contains all the Graph-related parameters:\n\n    G1,G2: NetworkX Graph or MultiGraph instances.\n        The two graphs to check for isomorphism or monomorphism\n\n    G1_labels,G2_labels: dict\n        The label of every node in G1 and G2 respectively\n\nstate_params: namedtuple\n    Contains all the State-related parameters:\n\n    mapping: dict\n        The mapping as extended so far. Maps nodes of G1 to nodes of G2\n\n    reverse_mapping: dict\n        The reverse mapping as extended so far. Maps nodes from G2 to nodes of G1. It's basically \"mapping\" reversed\n\n    T1, T2: set\n        Ti contains uncovered neighbors of covered nodes from Gi, i.e. nodes that are not in the mapping, but are\n        neighbors of nodes that are.\n\n    T1_tilde, T2_tilde: set\n        Ti_out contains all the nodes from Gi, that are neither in the mapping nor in Ti\n\nReturns\n-------\nTrue if we should prune this branch, i.e. the node pair failed the cutting checks. False otherwise.",
  "code": "def _cut_PT(u, v, graph_params, state_params):\n    G1, G2, G1_labels, G2_labels, _, _, _ = graph_params\n    _, _, T1, T1_in, T1_tilde, _, T2, T2_in, T2_tilde, _ = state_params\n    u_labels_predecessors, v_labels_predecessors = ({}, {})\n    if G1.is_directed():\n        u_labels_predecessors = nx.utils.groups({n1: G1_labels[n1] for n1 in G1.pred[u]})\n        v_labels_predecessors = nx.utils.groups({n2: G2_labels[n2] for n2 in G2.pred[v]})\n        if set(u_labels_predecessors.keys()) != set(v_labels_predecessors.keys()):\n            return True\n    u_labels_successors = nx.utils.groups({n1: G1_labels[n1] for n1 in G1[u]})\n    v_labels_successors = nx.utils.groups({n2: G2_labels[n2] for n2 in G2[v]})\n    if set(u_labels_successors.keys()) != set(v_labels_successors.keys()):\n        return True\n    for label, G1_nbh in u_labels_successors.items():\n        G2_nbh = v_labels_successors[label]\n        if G1.is_multigraph():\n            u_nbrs_edges = sorted((G1.number_of_edges(u, x) for x in G1_nbh))\n            v_nbrs_edges = sorted((G2.number_of_edges(v, x) for x in G2_nbh))\n            if any((u_nbr_edges != v_nbr_edges for u_nbr_edges, v_nbr_edges in zip(u_nbrs_edges, v_nbrs_edges))):\n                return True\n        if len(T1.intersection(G1_nbh)) != len(T2.intersection(G2_nbh)):\n            return True\n        if len(T1_tilde.intersection(G1_nbh)) != len(T2_tilde.intersection(G2_nbh)):\n            return True\n        if G1.is_directed() and len(T1_in.intersection(G1_nbh)) != len(T2_in.intersection(G2_nbh)):\n            return True\n    if not G1.is_directed():\n        return False\n    for label, G1_pred in u_labels_predecessors.items():\n        G2_pred = v_labels_predecessors[label]\n        if G1.is_multigraph():\n            u_pred_edges = sorted((G1.number_of_edges(u, x) for x in G1_pred))\n            v_pred_edges = sorted((G2.number_of_edges(v, x) for x in G2_pred))\n            if any((u_nbr_edges != v_nbr_edges for u_nbr_edges, v_nbr_edges in zip(u_pred_edges, v_pred_edges))):\n                return True\n        if len(T1.intersection(G1_pred)) != len(T2.intersection(G2_pred)):\n            return True\n        if len(T1_tilde.intersection(G1_pred)) != len(T2_tilde.intersection(G2_pred)):\n            return True\n        if len(T1_in.intersection(G1_pred)) != len(T2_in.intersection(G2_pred)):\n            return True\n    return False"
 },
 {
  "docstring": "Checks the consistency of extending the mapping using the current node pair.\n\nParameters\n----------\nu, v: Graph node\n    The two candidate nodes being examined.\n\ngraph_params: namedtuple\n    Contains all the Graph-related parameters:\n\n    G1,G2: NetworkX Graph or MultiGraph instances.\n        The two graphs to check for isomorphism or monomorphism\n\n    G1_labels,G2_labels: dict\n        The label of every node in G1 and G2 respectively\n\nstate_params: namedtuple\n    Contains all the State-related parameters:\n\n    mapping: dict\n        The mapping as extended so far. Maps nodes of G1 to nodes of G2\n\n    reverse_mapping: dict\n        The reverse mapping as extended so far. Maps nodes from G2 to nodes of G1. It's basically \"mapping\" reversed\n\n    T1, T2: set\n        Ti contains uncovered neighbors of covered nodes from Gi, i.e. nodes that are not in the mapping, but are\n        neighbors of nodes that are.\n\n    T1_out, T2_out: set\n        Ti_out contains all the nodes from Gi, that are neither in the mapping nor in Ti\n\nReturns\n-------\nTrue if the pair passes all the consistency checks successfully. False otherwise.",
  "code": "def _consistent_PT(u, v, graph_params, state_params):\n    G1, G2 = (graph_params.G1, graph_params.G2)\n    mapping, reverse_mapping = (state_params.mapping, state_params.reverse_mapping)\n    for neighbor in G1[u]:\n        if neighbor in mapping:\n            if G1.number_of_edges(u, neighbor) != G2.number_of_edges(v, mapping[neighbor]):\n                return False\n    for neighbor in G2[v]:\n        if neighbor in reverse_mapping:\n            if G1.number_of_edges(u, reverse_mapping[neighbor]) != G2.number_of_edges(v, neighbor):\n                return False\n    if not G1.is_directed():\n        return True\n    for predecessor in G1.pred[u]:\n        if predecessor in mapping:\n            if G1.number_of_edges(predecessor, u) != G2.number_of_edges(mapping[predecessor], v):\n                return False\n    for predecessor in G2.pred[v]:\n        if predecessor in reverse_mapping:\n            if G1.number_of_edges(reverse_mapping[predecessor], u) != G2.number_of_edges(predecessor, v):\n                return False\n    return True"
 },
 {
  "docstring": "Updates the Ti/Ti_out (i=1,2) when a new node pair u-v is added to the mapping.\n\n",
  "code": "def _update_Tinout(new_node1, new_node2, graph_params, state_params):\n    G1, G2, _, _, _, _, _ = graph_params\n    mapping, reverse_mapping, T1, T1_in, T1_tilde, T1_tilde_in, T2, T2_in, T2_tilde, T2_tilde_in = state_params\n    uncovered_successors_G1 = {succ for succ in G1[new_node1] if succ not in mapping}\n    uncovered_successors_G2 = {succ for succ in G2[new_node2] if succ not in reverse_mapping}\n    T1.update(uncovered_successors_G1)\n    T2.update(uncovered_successors_G2)\n    T1.discard(new_node1)\n    T2.discard(new_node2)\n    T1_tilde.difference_update(uncovered_successors_G1)\n    T2_tilde.difference_update(uncovered_successors_G2)\n    T1_tilde.discard(new_node1)\n    T2_tilde.discard(new_node2)\n    if not G1.is_directed():\n        return\n    uncovered_predecessors_G1 = {pred for pred in G1.pred[new_node1] if pred not in mapping}\n    uncovered_predecessors_G2 = {pred for pred in G2.pred[new_node2] if pred not in reverse_mapping}\n    T1_in.update(uncovered_predecessors_G1)\n    T2_in.update(uncovered_predecessors_G2)\n    T1_in.discard(new_node1)\n    T2_in.discard(new_node2)\n    T1_tilde.difference_update(uncovered_predecessors_G1)\n    T2_tilde.difference_update(uncovered_predecessors_G2)\n    T1_tilde.discard(new_node1)\n    T2_tilde.discard(new_node2)"
 },
 {
  "docstring": "Restores the previous version of Ti/Ti_out when a node pair is deleted from the mapping.\n\nParameters\n----------\npopped_node1, popped_node2: Graph node\n    The two nodes deleted from the mapping.\n\ngraph_params: namedtuple\n    Contains all the Graph-related parameters:\n\n    G1,G2: NetworkX Graph or MultiGraph instances.\n        The two graphs to check for isomorphism or monomorphism\n\n    G1_labels,G2_labels: dict\n        The label of every node in G1 and G2 respectively\n\nstate_params: namedtuple\n    Contains all the State-related parameters:\n\n    mapping: dict\n        The mapping as extended so far. Maps nodes of G1 to nodes of G2\n\n    reverse_mapping: dict\n        The reverse mapping as extended so far. Maps nodes from G2 to nodes of G1. It's basically \"mapping\" reversed\n\n    T1, T2: set\n        Ti contains uncovered neighbors of covered nodes from Gi, i.e. nodes that are not in the mapping, but are\n        neighbors of nodes that are.\n\n    T1_tilde, T2_tilde: set\n        Ti_out contains all the nodes from Gi, that are neither in the mapping nor in Ti",
  "code": "def _restore_Tinout(popped_node1, popped_node2, graph_params, state_params):\n    G1, G2, _, _, _, _, _ = graph_params\n    mapping, reverse_mapping, T1, T1_in, T1_tilde, T1_tilde_in, T2, T2_in, T2_tilde, T2_tilde_in = state_params\n    is_added = False\n    for neighbor in G1[popped_node1]:\n        if neighbor in mapping:\n            is_added = True\n            T1.add(popped_node1)\n        else:\n            if any((nbr in mapping for nbr in G1[neighbor])):\n                continue\n            T1.discard(neighbor)\n            T1_tilde.add(neighbor)\n    if not is_added:\n        T1_tilde.add(popped_node1)\n    is_added = False\n    for neighbor in G2[popped_node2]:\n        if neighbor in reverse_mapping:\n            is_added = True\n            T2.add(popped_node2)\n        else:\n            if any((nbr in reverse_mapping for nbr in G2[neighbor])):\n                continue\n            T2.discard(neighbor)\n            T2_tilde.add(neighbor)\n    if not is_added:\n        T2_tilde.add(popped_node2)"
 },
 {
  "docstring": "Returns True if mapping G1_node to G2_node is semantically feasible.",
  "code": "def _semantic_feasibility(self, G1_node, G2_node):\n    if self.node_match is not None:\n        nm = self.node_match(self.G1.nodes[G1_node], self.G2.nodes[G2_node])\n        if not nm:\n            return False\n    if self.edge_match is not None:\n        G1nbrs = self.G1_adj[G1_node]\n        G2nbrs = self.G2_adj[G2_node]\n        core_1 = self.core_1\n        edge_match = self.edge_match\n        for neighbor in G1nbrs:\n            if neighbor == G1_node:\n                if G2_node in G2nbrs and (not edge_match(G1nbrs[G1_node], G2nbrs[G2_node])):\n                    return False\n            elif neighbor in core_1:\n                G2_nbr = core_1[neighbor]\n                if G2_nbr in G2nbrs and (not edge_match(G1nbrs[neighbor], G2nbrs[G2_nbr])):\n                    return False\n    return True"
 },
 {
  "docstring": "Initialize graph matcher.\n\nParameters\n----------\nG1, G2: graph\n    The graphs to be tested.\n\nnode_match: callable\n    A function that returns True iff node n1 in G1 and n2 in G2\n    should be considered equal during the isomorphism test. The\n    function will be called like::\n\n       node_match(G1.nodes[n1], G2.nodes[n2])\n\n    That is, the function will receive the node attribute dictionaries\n    of the nodes under consideration. If None, then no attributes are\n    considered when testing for an isomorphism.\n\nedge_match: callable\n    A function that returns True iff the edge attribute dictionary for\n    the pair of nodes (u1, v1) in G1 and (u2, v2) in G2 should be\n    considered equal during the isomorphism test. The function will be\n    called like::\n\n       edge_match(G1[u1][v1], G2[u2][v2])\n\n    That is, the function will receive the edge attribute dictionaries\n    of the edges under consideration. If None, then no attributes are\n    considered when testing for an isomorphism.",
  "code": "def __init__(self, G1, G2, node_match=None, edge_match=None):\n    vf2.GraphMatcher.__init__(self, G1, G2)\n    self.node_match = node_match\n    self.edge_match = edge_match\n    self.G1_adj = self.G1.adj\n    self.G2_adj = self.G2.adj"
 },
 {
  "docstring": "Initialize graph matcher.\n\nParameters\n----------\nG1, G2 : graph\n    The graphs to be tested.\n\nnode_match : callable\n    A function that returns True iff node n1 in G1 and n2 in G2\n    should be considered equal during the isomorphism test. The\n    function will be called like::\n\n       node_match(G1.nodes[n1], G2.nodes[n2])\n\n    That is, the function will receive the node attribute dictionaries\n    of the nodes under consideration. If None, then no attributes are\n    considered when testing for an isomorphism.\n\nedge_match : callable\n    A function that returns True iff the edge attribute dictionary for\n    the pair of nodes (u1, v1) in G1 and (u2, v2) in G2 should be\n    considered equal during the isomorphism test. The function will be\n    called like::\n\n       edge_match(G1[u1][v1], G2[u2][v2])\n\n    That is, the function will receive the edge attribute dictionaries\n    of the edges under consideration. If None, then no attributes are\n    considered when testing for an isomorphism.",
  "code": "def __init__(self, G1, G2, node_match=None, edge_match=None):\n    vf2.DiGraphMatcher.__init__(self, G1, G2)\n    self.node_match = node_match\n    self.edge_match = edge_match\n    self.G1_adj = self.G1.adj\n    self.G2_adj = self.G2.adj"
 },
 {
  "docstring": "Returns True if mapping G1_node to G2_node is semantically feasible.",
  "code": "def semantic_feasibility(self, G1_node, G2_node):\n    feasible = _semantic_feasibility(self, G1_node, G2_node)\n    if not feasible:\n        return False\n    self.G1_adj = self.G1.pred\n    self.G2_adj = self.G2.pred\n    feasible = _semantic_feasibility(self, G1_node, G2_node)\n    self.G1_adj = self.G1.adj\n    self.G2_adj = self.G2.adj\n    return feasible"
 },
 {
  "docstring": "Helper function to facilitate comparing collections of dictionaries in\nwhich order does not matter.",
  "code": "def _matches_to_sets(matches):\n    return {frozenset(m.items()) for m in matches}"
 },
 {
  "docstring": "For some small, symmetric graphs, make sure that 1) they are isomorphic\nto themselves, and 2) that only the identity mapping is found.",
  "code": "def test_self_isomorphism(self):\n    for node_data, edge_data in self.data:\n        graph = nx.Graph()\n        graph.add_nodes_from(node_data)\n        graph.add_edges_from(edge_data)\n        ismags = iso.ISMAGS(graph, graph, node_match=iso.categorical_node_match('name', None))\n        assert ismags.is_isomorphic()\n        assert ismags.subgraph_is_isomorphic()\n        assert list(ismags.subgraph_isomorphisms_iter(symmetry=True)) == [{n: n for n in graph.nodes}]"
 },
 {
  "docstring": "This edgecase is one of the cases in which it is hard to find all\nsymmetry elements.",
  "code": "def test_edgecase_self_isomorphism(self):\n    graph = nx.Graph()\n    nx.add_path(graph, range(5))\n    graph.add_edges_from([(2, 5), (5, 6)])\n    ismags = iso.ISMAGS(graph, graph)\n    ismags_answer = list(ismags.find_isomorphisms(True))\n    assert ismags_answer == [{n: n for n in graph.nodes}]\n    graph = nx.relabel_nodes(graph, {0: 0, 1: 1, 2: 2, 3: 3, 4: 6, 5: 4, 6: 5})\n    ismags = iso.ISMAGS(graph, graph)\n    ismags_answer = list(ismags.find_isomorphisms(True))\n    assert ismags_answer == [{n: n for n in graph.nodes}]"
 },
 {
  "docstring": "For some small, directed, symmetric graphs, make sure that 1) they are\nisomorphic to themselves, and 2) that only the identity mapping is\nfound.",
  "code": "def test_directed_self_isomorphism(self):\n    for node_data, edge_data in self.data:\n        graph = nx.Graph()\n        graph.add_nodes_from(node_data)\n        graph.add_edges_from(edge_data)\n        ismags = iso.ISMAGS(graph, graph, node_match=iso.categorical_node_match('name', None))\n        assert ismags.is_isomorphic()\n        assert ismags.subgraph_is_isomorphic()\n        assert list(ismags.subgraph_isomorphisms_iter(symmetry=True)) == [{n: n for n in graph.nodes}]"
 },
 {
  "docstring": "Creates a Graph instance from the filename.",
  "code": "@staticmethod\ndef create_graph(filename):\n    fh = open(filename, mode='rb')\n    nodes = struct.unpack('<H', fh.read(2))[0]\n    graph = nx.Graph()\n    for from_node in range(nodes):\n        edges = struct.unpack('<H', fh.read(2))[0]\n        for edge in range(edges):\n            to_node = struct.unpack('<H', fh.read(2))[0]\n            graph.add_edge(from_node, to_node)\n    fh.close()\n    return graph"
 },
 {
  "docstring": "        G1                           G2\nx--------------x              x--------------x\n| \\            |              | \\            |\n|  x-------x   |              |  x-------x   |\n|  |       |   |              |  |       |   |\n|  x-------x   |              |  x-------x   |\n| /            |              |            \\ |\nx--------------x              x--------------x",
  "code": "def test_non_isomorphic_same_degree_sequence(self):\n    edges1 = [(1, 5), (1, 2), (4, 1), (3, 2), (3, 4), (4, 8), (5, 8), (6, 5), (6, 7), (7, 8)]\n    edges2 = [(1, 5), (1, 2), (4, 1), (3, 2), (4, 3), (5, 8), (6, 5), (6, 7), (3, 7), (8, 7)]\n    G1 = nx.DiGraph(edges1)\n    G2 = nx.DiGraph(edges2)\n    assert vf2pp_isomorphism(G1, G2) is None"
 },
 {
  "docstring": "Returns HITS hubs and authorities values for nodes.\n\nThe HITS algorithm computes two numbers for a node.\nAuthorities estimates the node value based on the incoming links.\nHubs estimates the node value based on outgoing links.\n\nParameters\n----------\nG : graph\n  A NetworkX graph\n\nmax_iter : integer, optional\n  Maximum number of iterations in power method.\n\ntol : float, optional\n  Error tolerance used to check convergence in power method iteration.\n\nnstart : dictionary, optional\n  Starting value of each node for power method iteration.\n\nnormalized : bool (default=True)\n   Normalize results by the sum of all of the values.\n\nReturns\n-------\n(hubs,authorities) : two-tuple of dictionaries\n   Two dictionaries keyed by node containing the hub and authority\n   values.\n\nRaises\n------\nPowerIterationFailedConvergence\n    If the algorithm fails to converge to the specified tolerance\n    within the specified number of iterations of the power iteration\n    method.\n\nExamples\n--------\n>>> G = nx.path_graph(4)\n>>> h, a = nx.hits(G)\n\n",
  "code": "@nx._dispatch(preserve_edge_attrs={'G': {'weight': 1}})\ndef hits(G, max_iter=100, tol=1e-08, nstart=None, normalized=True):\n    import numpy as np\n    import scipy as sp\n    if len(G) == 0:\n        return ({}, {})\n    A = nx.adjacency_matrix(G, nodelist=list(G), dtype=float)\n    if nstart is not None:\n        nstart = np.array(list(nstart.values()))\n    if max_iter <= 0:\n        raise nx.PowerIterationFailedConvergence(max_iter)\n    try:\n        _, _, vt = sp.sparse.linalg.svds(A, k=1, v0=nstart, maxiter=max_iter, tol=tol)\n    except sp.sparse.linalg.ArpackNoConvergence as exc:\n        raise nx.PowerIterationFailedConvergence(max_iter) from exc\n    a = vt.flatten().real\n    h = A @ a\n    if normalized:\n        h /= h.sum()\n        a /= a.sum()\n    hubs = dict(zip(G, map(float, h)))\n    authorities = dict(zip(G, map(float, a)))\n    return (hubs, authorities)"
 },
 {
  "docstring": "Returns HITS hubs and authorities values for nodes.\n\nThe HITS algorithm computes two numbers for a node.\nAuthorities estimates the node value based on the incoming links.\nHubs estimates the node value based on outgoing links.\n\nParameters\n----------\nG : graph\n  A NetworkX graph\n\nnormalized : bool (default=True)\n   Normalize results by the sum of all of the values.\n\nReturns\n-------\n(hubs,authorities) : two-tuple of dictionaries\n   Two dictionaries keyed by node containing the hub and authority\n   values.\n\nExamples\n--------\n>>> G = nx.path_graph(4)\n\nThe `hubs` and `authorities` are given by the eigenvectors corresponding to the\nmaximum eigenvalues of the hubs_matrix and the authority_matrix, respectively.\n\nThe ``hubs`` and ``authority`` matrices are computed from the adjacency\nmatrix:\n\n>>> adj_ary = nx.to_numpy_array(G)\n>>> hubs_matrix = adj_ary @ adj_ary.T\n>>> authority_matrix = adj_ary.T @ adj_ary\n\n`_hits_numpy` maps the eigenvector corresponding to the maximum eigenvalue\nof the respective matrices to the nodes in `G`:\n\n>>> from networkx.algorithms.link_analysis.hits_alg import _hits_numpy\n>>> hubs, authority = _hits_numpy(G)\n\n",
  "code": "def _hits_numpy(G, normalized=True):\n    import numpy as np\n    if len(G) == 0:\n        return ({}, {})\n    adj_ary = nx.to_numpy_array(G)\n    H = adj_ary @ adj_ary.T\n    e, ev = np.linalg.eig(H)\n    h = ev[:, np.argmax(e)]\n    A = adj_ary.T @ adj_ary\n    e, ev = np.linalg.eig(A)\n    a = ev[:, np.argmax(e)]\n    if normalized:\n        h /= h.sum()\n        a /= a.sum()\n    else:\n        h /= h.max()\n        a /= a.max()\n    hubs = dict(zip(G, map(float, h)))\n    authorities = dict(zip(G, map(float, a)))\n    return (hubs, authorities)"
 },
 {
  "docstring": "Returns HITS hubs and authorities values for nodes.\n\n\nThe HITS algorithm computes two numbers for a node.\nAuthorities estimates the node value based on the incoming links.\nHubs estimates the node value based on outgoing links.\n\nParameters\n----------\nG : graph\n  A NetworkX graph\n\nmax_iter : integer, optional\n  Maximum number of iterations in power method.\n\ntol : float, optional\n  Error tolerance used to check convergence in power method iteration.\n\nnstart : dictionary, optional\n  Starting value of each node for power method iteration.\n\nnormalized : bool (default=True)\n   Normalize results by the sum of all of the values.\n\nReturns\n-------\n(hubs,authorities) : two-tuple of dictionaries\n   Two dictionaries keyed by node containing the hub and authority\n   values.\n\nExamples\n--------\n>>> from networkx.algorithms.link_analysis.hits_alg import _hits_scipy\n>>> G = nx.path_graph(4)\n>>> h, a = _hits_scipy(G)\n\n",
  "code": "def _hits_scipy(G, max_iter=100, tol=1e-06, nstart=None, normalized=True):\n    import numpy as np\n    if len(G) == 0:\n        return ({}, {})\n    A = nx.to_scipy_sparse_array(G, nodelist=list(G))\n    n, _ = A.shape\n    ATA = A.T @ A\n    if nstart is None:\n        x = np.ones((n, 1)) / n\n    else:\n        x = np.array([nstart.get(n, 0) for n in list(G)], dtype=float)\n        x /= x.sum()\n    i = 0\n    while True:\n        xlast = x\n        x = ATA @ x\n        x /= x.max()\n        err = np.absolute(x - xlast).sum()\n        if err < tol:\n            break\n        if i > max_iter:\n            raise nx.PowerIterationFailedConvergence(max_iter)\n        i += 1\n    a = x.flatten()\n    h = A @ a\n    if normalized:\n        h /= h.sum()\n        a /= a.sum()\n    hubs = dict(zip(G, map(float, h)))\n    authorities = dict(zip(G, map(float, a)))\n    return (hubs, authorities)"
 },
 {
  "docstring": "Returns the PageRank of the nodes in the graph.\n\nPageRank computes a ranking of the nodes in the graph G based on\nthe structure of the incoming links. It was originally designed as\nan algorithm to rank web pages.\n\nParameters\n----------\nG : graph\n  A NetworkX graph.  Undirected graphs will be converted to a directed\n  graph with two directed edges for each undirected edge.\n\nalpha : float, optional\n  Damping parameter for PageRank, default=0.85.\n\npersonalization: dict, optional\n  The \"personalization vector\" consisting of a dictionary with a\n  key some subset of graph nodes and personalization value each of those.\n  At least one personalization value must be non-zero.\n  If not specified, a nodes personalization value will be zero.\n  By default, a uniform distribution is used.\n\nmax_iter : integer, optional\n  Maximum number of iterations in power method eigenvalue solver.\n\ntol : float, optional\n  Error tolerance used to check convergence in power method solver.\n  The iteration will stop after a tolerance of ``len(G) * tol`` is reached.\n\nnstart : dictionary, optional\n  Starting value of PageRank iteration for each node.\n\nweight : key, optional\n  Edge data key to use as weight.  If None weights are set to 1.\n\ndangling: dict, optional\n  The outedges to be assigned to any \"dangling\" nodes, i.e., nodes without\n  any outedges. The dict key is the node the outedge points to and the dict\n  value is the weight of that outedge. By default, dangling nodes are given\n  outedges according to the personalization vector (uniform if not\n  specified). This must be selected to result in an irreducible transition\n  matrix (see notes under google_matrix). It may be common to have the\n  dangling dict to be the same as the personalization dict.\n\n\nReturns\n-------\npagerank : dictionary\n   Dictionary of nodes with PageRank as value\n\nExamples\n--------\n>>> G = nx.DiGraph(nx.path_graph(4))\n>>> pr = nx.pagerank(G, alpha=0.9)\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef pagerank(G, alpha=0.85, personalization=None, max_iter=100, tol=1e-06, nstart=None, weight='weight', dangling=None):\n    return _pagerank_scipy(G, alpha, personalization, max_iter, tol, nstart, weight, dangling)"
 },
 {
  "docstring": "Returns the Google matrix of the graph.\n\nParameters\n----------\nG : graph\n  A NetworkX graph.  Undirected graphs will be converted to a directed\n  graph with two directed edges for each undirected edge.\n\nalpha : float\n  The damping factor.\n\npersonalization: dict, optional\n  The \"personalization vector\" consisting of a dictionary with a\n  key some subset of graph nodes and personalization value each of those.\n  At least one personalization value must be non-zero.\n  If not specified, a nodes personalization value will be zero.\n  By default, a uniform distribution is used.\n\nnodelist : list, optional\n  The rows and columns are ordered according to the nodes in nodelist.\n  If nodelist is None, then the ordering is produced by G.nodes().\n\nweight : key, optional\n  Edge data key to use as weight.  If None weights are set to 1.\n\ndangling: dict, optional\n  The outedges to be assigned to any \"dangling\" nodes, i.e., nodes without\n  any outedges. The dict key is the node the outedge points to and the dict\n  value is the weight of that outedge. By default, dangling nodes are given\n  outedges according to the personalization vector (uniform if not\n  specified) This must be selected to result in an irreducible transition\n  matrix (see notes below). It may be common to have the dangling dict to\n  be the same as the personalization dict.\n\nReturns\n-------\nA : 2D NumPy ndarray\n   Google matrix of the graph\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef google_matrix(G, alpha=0.85, personalization=None, nodelist=None, weight='weight', dangling=None):\n    import numpy as np\n    if nodelist is None:\n        nodelist = list(G)\n    A = nx.to_numpy_array(G, nodelist=nodelist, weight=weight)\n    N = len(G)\n    if N == 0:\n        return A\n    if personalization is None:\n        p = np.repeat(1.0 / N, N)\n    else:\n        p = np.array([personalization.get(n, 0) for n in nodelist], dtype=float)\n        if p.sum() == 0:\n            raise ZeroDivisionError\n        p /= p.sum()\n    if dangling is None:\n        dangling_weights = p\n    else:\n        dangling_weights = np.array([dangling.get(n, 0) for n in nodelist], dtype=float)\n        dangling_weights /= dangling_weights.sum()\n    dangling_nodes = np.where(A.sum(axis=1) == 0)[0]\n    A[dangling_nodes] = dangling_weights\n    A /= A.sum(axis=1)[:, np.newaxis]\n    return alpha * A + (1 - alpha) * p"
 },
 {
  "docstring": "Returns the PageRank of the nodes in the graph.\n\nPageRank computes a ranking of the nodes in the graph G based on\nthe structure of the incoming links. It was originally designed as\nan algorithm to rank web pages.\n\nParameters\n----------\nG : graph\n  A NetworkX graph.  Undirected graphs will be converted to a directed\n  graph with two directed edges for each undirected edge.\n\nalpha : float, optional\n  Damping parameter for PageRank, default=0.85.\n\npersonalization: dict, optional\n  The \"personalization vector\" consisting of a dictionary with a\n  key some subset of graph nodes and personalization value each of those.\n  At least one personalization value must be non-zero.\n  If not specified, a nodes personalization value will be zero.\n  By default, a uniform distribution is used.\n\nweight : key, optional\n  Edge data key to use as weight.  If None weights are set to 1.\n\ndangling: dict, optional\n  The outedges to be assigned to any \"dangling\" nodes, i.e., nodes without\n  any outedges. The dict key is the node the outedge points to and the dict\n  value is the weight of that outedge. By default, dangling nodes are given\n  outedges according to the personalization vector (uniform if not\n  specified) This must be selected to result in an irreducible transition\n  matrix (see notes under google_matrix). It may be common to have the\n  dangling dict to be the same as the personalization dict.\n\nReturns\n-------\npagerank : dictionary\n   Dictionary of nodes with PageRank as value.\n\nExamples\n--------\n>>> from networkx.algorithms.link_analysis.pagerank_alg import _pagerank_numpy\n>>> G = nx.DiGraph(nx.path_graph(4))\n>>> pr = _pagerank_numpy(G, alpha=0.9)\n\n",
  "code": "def _pagerank_numpy(G, alpha=0.85, personalization=None, weight='weight', dangling=None):\n    import numpy as np\n    if len(G) == 0:\n        return {}\n    M = google_matrix(G, alpha, personalization=personalization, weight=weight, dangling=dangling)\n    eigenvalues, eigenvectors = np.linalg.eig(M.T)\n    ind = np.argmax(eigenvalues)\n    largest = np.array(eigenvectors[:, ind]).flatten().real\n    norm = largest.sum()\n    return dict(zip(G, map(float, largest / norm)))"
 },
 {
  "docstring": "Returns the PageRank of the nodes in the graph.\n\nPageRank computes a ranking of the nodes in the graph G based on\nthe structure of the incoming links. It was originally designed as\nan algorithm to rank web pages.\n\nParameters\n----------\nG : graph\n  A NetworkX graph.  Undirected graphs will be converted to a directed\n  graph with two directed edges for each undirected edge.\n\nalpha : float, optional\n  Damping parameter for PageRank, default=0.85.\n\npersonalization: dict, optional\n  The \"personalization vector\" consisting of a dictionary with a\n  key some subset of graph nodes and personalization value each of those.\n  At least one personalization value must be non-zero.\n  If not specified, a nodes personalization value will be zero.\n  By default, a uniform distribution is used.\n\nmax_iter : integer, optional\n  Maximum number of iterations in power method eigenvalue solver.\n\ntol : float, optional\n  Error tolerance used to check convergence in power method solver.\n  The iteration will stop after a tolerance of ``len(G) * tol`` is reached.\n\nnstart : dictionary, optional\n  Starting value of PageRank iteration for each node.\n\nweight : key, optional\n  Edge data key to use as weight.  If None weights are set to 1.\n\ndangling: dict, optional\n  The outedges to be assigned to any \"dangling\" nodes, i.e., nodes without\n  any outedges. The dict key is the node the outedge points to and the dict\n  value is the weight of that outedge. By default, dangling nodes are given\n  outedges according to the personalization vector (uniform if not\n  specified) This must be selected to result in an irreducible transition\n  matrix (see notes under google_matrix). It may be common to have the\n  dangling dict to be the same as the personalization dict.\n\nReturns\n-------\npagerank : dictionary\n   Dictionary of nodes with PageRank as value\n\nExamples\n--------\n>>> from networkx.algorithms.link_analysis.pagerank_alg import _pagerank_scipy\n>>> G = nx.DiGraph(nx.path_graph(4))\n>>> pr = _pagerank_scipy(G, alpha=0.9)\n\n",
  "code": "def _pagerank_scipy(G, alpha=0.85, personalization=None, max_iter=100, tol=1e-06, nstart=None, weight='weight', dangling=None):\n    import numpy as np\n    import scipy as sp\n    N = len(G)\n    if N == 0:\n        return {}\n    nodelist = list(G)\n    A = nx.to_scipy_sparse_array(G, nodelist=nodelist, weight=weight, dtype=float)\n    S = A.sum(axis=1)\n    S[S != 0] = 1.0 / S[S != 0]\n    Q = sp.sparse.csr_array(sp.sparse.spdiags(S.T, 0, *A.shape))\n    A = Q @ A\n    if nstart is None:\n        x = np.repeat(1.0 / N, N)\n    else:\n        x = np.array([nstart.get(n, 0) for n in nodelist], dtype=float)\n        x /= x.sum()\n    if personalization is None:\n        p = np.repeat(1.0 / N, N)\n    else:\n        p = np.array([personalization.get(n, 0) for n in nodelist], dtype=float)\n        if p.sum() == 0:\n            raise ZeroDivisionError\n        p /= p.sum()\n    if dangling is None:\n        dangling_weights = p\n    else:\n        dangling_weights = np.array([dangling.get(n, 0) for n in nodelist], dtype=float)\n        dangling_weights /= dangling_weights.sum()\n    is_dangling = np.where(S == 0)[0]\n    for _ in range(max_iter):\n        xlast = x\n        x = alpha * (x @ A + sum(x[is_dangling]) * dangling_weights) + (1 - alpha) * p\n        err = np.absolute(x - xlast).sum()\n        if err < N * tol:\n            return dict(zip(nodelist, map(float, x)))\n    raise nx.PowerIterationFailedConvergence(max_iter)"
 },
 {
  "docstring": "Tests that the google_matrix doesn't change except for the dangling\nnodes.",
  "code": "def test_dangling_matrix(self):\n    G = self.G\n    dangling = self.dangling_edges\n    dangling_sum = sum(dangling.values())\n    M1 = nx.google_matrix(G, personalization=dangling)\n    M2 = nx.google_matrix(G, personalization=dangling, dangling=dangling)\n    for i in range(len(G)):\n        for j in range(len(G)):\n            if i == self.dangling_node_index and j + 1 in dangling:\n                assert M2[i, j] == pytest.approx(dangling[j + 1] / dangling_sum, abs=0.0001)\n            else:\n                assert M2[i, j] == pytest.approx(M1[i, j], abs=0.0001)"
 },
 {
  "docstring": "Returns equivalence classes of `relation` when applied to `iterable`.\n\nThe equivalence classes, or blocks, consist of objects from `iterable`\nwhich are all equivalent. They are defined to be equivalent if the\n`relation` function returns `True` when passed any two objects from that\nclass, and `False` otherwise. To define an equivalence relation the\nfunction must be reflexive, symmetric and transitive.\n\nParameters\n----------\niterable : list, tuple, or set\n    An iterable of elements/nodes.\n\nrelation : function\n    A Boolean-valued function that implements an equivalence relation\n    (reflexive, symmetric, transitive binary relation) on the elements\n    of `iterable` - it must take two elements and return `True` if\n    they are related, or `False` if not.\n\nReturns\n-------\nset of frozensets\n    A set of frozensets representing the partition induced by the equivalence\n    relation function `relation` on the elements of `iterable`. Each\n    member set in the return set represents an equivalence class, or\n    block, of the partition.\n\n    Duplicate elements will be ignored so it makes the most sense for\n    `iterable` to be a :class:`set`.\n\n",
  "code": "def equivalence_classes(iterable, relation):\n    blocks = []\n    for y in iterable:\n        for block in blocks:\n            x = arbitrary_element(block)\n            if relation(x, y):\n                block.append(y)\n                break\n        else:\n            blocks.append([y])\n    return {frozenset(block) for block in blocks}"
 },
 {
  "docstring": "Returns the quotient graph of `G` under the specified equivalence\nrelation on nodes.\n\nParameters\n----------\nG : NetworkX graph\n    The graph for which to return the quotient graph with the\n    specified node relation.\n\npartition : function, or dict or list of lists, tuples or sets\n    If a function, this function must represent an equivalence\n    relation on the nodes of `G`. It must take two arguments *u*\n    and *v* and return True exactly when *u* and *v* are in the\n    same equivalence class. The equivalence classes form the nodes\n    in the returned graph.\n\n    If a dict of lists/tuples/sets, the keys can be any meaningful\n    block labels, but the values must be the block lists/tuples/sets\n    (one list/tuple/set per block), and the blocks must form a valid\n    partition of the nodes of the graph. That is, each node must be\n    in exactly one block of the partition.\n\n    If a list of sets, the list must form a valid partition of\n    the nodes of the graph. That is, each node must be in exactly\n    one block of the partition.\n\nedge_relation : Boolean function with two arguments\n    This function must represent an edge relation on the *blocks* of\n    the `partition` of `G`. It must take two arguments, *B* and *C*,\n    each one a set of nodes, and return True exactly when there should be\n    an edge joining block *B* to block *C* in the returned graph.\n\n    If `edge_relation` is not specified, it is assumed to be the\n    following relation. Block *B* is related to block *C* if and\n    only if some node in *B* is adjacent to some node in *C*,\n    according to the edge set of `G`.\n\nnode_data : function\n    This function takes one argument, *B*, a set of nodes in `G`,\n    and must return a dictionary representing the node data\n    attributes to set on the node representing *B* in the quotient graph.\n    If None, the following node attributes will be set:\n\n    * 'graph', the subgraph of the graph `G` that this block\n      represents,\n    * 'nnodes', the number of nodes in this block,\n    * 'nedges', the number of edges within this block,\n    * 'density', the density of the subgraph of `G` that this\n      block represents.\n\nedge_data : function\n    This function takes two arguments, *B* and *C*, each one a set\n    of nodes, and must return a dictionary representing the edge\n    data attributes to set on the edge joining *B* and *C*, should\n    there be an edge joining *B* and *C* in the quotient graph (if\n    no such edge occurs in the quotient graph as determined by\n    `edge_relation`, then the output of this function is ignored).\n\n    If the quotient graph would be a multigraph, this function is\n    not applied, since the edge data from each edge in the graph\n    `G` appears in the edges of the quotient graph.\n\nweight : string or None, optional (default=\"weight\")\n    The name of an edge attribute that holds the numerical value\n    used as a weight. If None then each edge has weight 1.\n\nrelabel : bool\n    If True, relabel the nodes of the quotient graph to be\n    nonnegative integers. Otherwise, the nodes are identified with\n    :class:`frozenset` instances representing the blocks given in\n    `partition`.\n\ncreate_using : NetworkX graph constructor, optional (default=nx.Graph)\n   Graph type to create. If graph instance, then cleared before populated.\n\nReturns\n-------\nNetworkX graph\n    The quotient graph of `G` under the equivalence relation\n    specified by `partition`. If the partition were given as a\n    list of :class:`set` instances and `relabel` is False,\n    each node will be a :class:`frozenset` corresponding to the same\n    :class:`set`.\n\nRaises\n------\nNetworkXException\n    If the given partition is not a valid partition of the nodes of\n    `G`.\n\nExamples\n--------\nThe quotient graph of the complete bipartite graph under the \"same\nneighbors\" equivalence relation is `K_2`. Under this relation, two nodes\nare equivalent if they are not adjacent but have the same neighbor set.\n\n>>> G = nx.complete_bipartite_graph(2, 3)\n>>> same_neighbors = lambda u, v: (\n...     u not in G[v] and v not in G[u] and G[u] == G[v]\n... )\n>>> Q = nx.quotient_graph(G, same_neighbors)\n>>> K2 = nx.complete_graph(2)\n>>> nx.is_isomorphic(Q, K2)\nTrue\n\nThe quotient graph of a directed graph under the \"same strongly connected\ncomponent\" equivalence relation is the condensation of the graph (see\n:func:`condensation`). This example comes from the Wikipedia article\n*`Strongly connected component`_*.\n\n>>> G = nx.DiGraph()\n>>> edges = [\n...     \"ab\",\n...     \"be\",\n...     \"bf\",\n...     \"bc\",\n...     \"cg\",\n...     \"cd\",\n...     \"dc\",\n...     \"dh\",\n...     \"ea\",\n...     \"ef\",\n...     \"fg\",\n...     \"gf\",\n...     \"hd\",\n...     \"hf\",\n... ]\n>>> G.add_edges_from(tuple(x) for x in edges)\n>>> components = list(nx.strongly_connected_components(G))\n>>> sorted(sorted(component) for component in components)\n[['a', 'b', 'e'], ['c', 'd', 'h'], ['f', 'g']]\n>>>\n>>> C = nx.condensation(G, components)\n>>> component_of = C.graph[\"mapping\"]\n>>> same_component = lambda u, v: component_of[u] == component_of[v]\n>>> Q = nx.quotient_graph(G, same_component)\n>>> nx.is_isomorphic(C, Q)\nTrue\n\nNode identification can be represented as the quotient of a graph under the\nequivalence relation that places the two nodes in one block and each other\nnode in its own singleton block.\n\n>>> K24 = nx.complete_bipartite_graph(2, 4)\n>>> K34 = nx.complete_bipartite_graph(3, 4)\n>>> C = nx.contracted_nodes(K34, 1, 2)\n>>> nodes = {1, 2}\n>>> is_contracted = lambda u, v: u in nodes and v in nodes\n>>> Q = nx.quotient_graph(K34, is_contracted)\n>>> nx.is_isomorphic(Q, C)\nTrue\n>>> nx.is_isomorphic(Q, K24)\nTrue\n\nThe blockmodeling technique described in [1]_ can be implemented as a\nquotient graph.\n\n>>> G = nx.path_graph(6)\n>>> partition = [{0, 1}, {2, 3}, {4, 5}]\n>>> M = nx.quotient_graph(G, partition, relabel=True)\n>>> list(M.edges())\n[(0, 1), (1, 2)]\n\nHere is the sample example but using partition as a dict of block sets.\n\n>>> G = nx.path_graph(6)\n>>> partition = {0: {0, 1}, 2: {2, 3}, 4: {4, 5}}\n>>> M = nx.quotient_graph(G, partition, relabel=True)\n>>> list(M.edges())\n[(0, 1), (1, 2)]\n\nPartitions can be represented in various ways:\n\n0. a list/tuple/set of block lists/tuples/sets\n1. a dict with block labels as keys and blocks lists/tuples/sets as values\n2. a dict with block lists/tuples/sets as keys and block labels as values\n3. a function from nodes in the original iterable to block labels\n4. an equivalence relation function on the target iterable\n\nAs `quotient_graph` is designed to accept partitions represented as (0), (1) or\n(4) only, the `equivalence_classes` function can be used to get the partitions\nin the right form, in order to call `quotient_graph`.\n\n.. _Strongly connected component: https://en.wikipedia.org/wiki/Strongly_connected_component\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef quotient_graph(G, partition, edge_relation=None, node_data=None, edge_data=None, weight='weight', relabel=False, create_using=None):\n    if callable(partition):\n        partition = equivalence_classes(G, partition)\n        if not nx.community.is_partition(G, partition):\n            raise nx.NetworkXException('Input `partition` is not an equivalence relation for nodes of G')\n        return _quotient_graph(G, partition, edge_relation, node_data, edge_data, weight, relabel, create_using)\n    if isinstance(partition, dict):\n        partition = list(partition.values())\n    partition_nodes = set().union(*partition)\n    if len(partition_nodes) != len(G):\n        G = G.subgraph(partition_nodes)\n    if not nx.community.is_partition(G, partition):\n        raise NetworkXException('each node must be in exactly one part of `partition`')\n    return _quotient_graph(G, partition, edge_relation, node_data, edge_data, weight, relabel, create_using)"
 },
 {
  "docstring": "Construct the quotient graph assuming input has been checked",
  "code": "def _quotient_graph(G, partition, edge_relation, node_data, edge_data, weight, relabel, create_using):\n    if create_using is None:\n        H = G.__class__()\n    else:\n        H = nx.empty_graph(0, create_using)\n    if node_data is None:\n\n        def node_data(b):\n            S = G.subgraph(b)\n            return {'graph': S, 'nnodes': len(S), 'nedges': S.number_of_edges(), 'density': density(S)}\n    partition = [frozenset(b) for b in partition]\n    H.add_nodes_from(((b, node_data(b)) for b in partition))\n    if edge_relation is None:\n\n        def edge_relation(b, c):\n            return any((v in G[u] for u, v in product(b, c)))\n    if edge_data is None:\n\n        def edge_data(b, c):\n            edgedata = (d for u, v, d in G.edges(b | c, data=True) if u in b and v in c or (u in c and v in b))\n            return {'weight': sum((d.get(weight, 1) for d in edgedata))}\n    block_pairs = permutations(H, 2) if H.is_directed() else combinations(H, 2)\n    if H.is_multigraph():\n        edges = chaini((((b, c, G.get_edge_data(u, v, default={})) for u, v in product(b, c) if v in G[u]) for b, c in block_pairs if edge_relation(b, c)))\n    else:\n        edges = ((b, c, edge_data(b, c)) for b, c in block_pairs if edge_relation(b, c))\n    H.add_edges_from(edges)\n    if relabel:\n        labels = {b: i for i, b in enumerate(partition)}\n        H = nx.relabel_nodes(H, labels)\n    return H"
 },
 {
  "docstring": "Returns the graph that results from contracting `u` and `v`.\n\nNode contraction identifies the two nodes as a single node incident to any\nedge that was incident to the original two nodes.\n\nParameters\n----------\nG : NetworkX graph\n    The graph whose nodes will be contracted.\n\nu, v : nodes\n    Must be nodes in `G`.\n\nself_loops : Boolean\n    If this is True, any edges joining `u` and `v` in `G` become\n    self-loops on the new node in the returned graph.\n\ncopy : Boolean\n    If this is True (default True), make a copy of\n    `G` and return that instead of directly changing `G`.\n\n\nReturns\n-------\nNetworkx graph\n    If Copy is True,\n    A new graph object of the same type as `G` (leaving `G` unmodified)\n    with `u` and `v` identified in a single node. The right node `v`\n    will be merged into the node `u`, so only `u` will appear in the\n    returned graph.\n    If copy is False,\n    Modifies `G` with `u` and `v` identified in a single node.\n    The right node `v` will be merged into the node `u`, so\n    only `u` will appear in the returned graph.\n\n",
  "code": "@nx._dispatch(preserve_all_attrs=True)\ndef contracted_nodes(G, u, v, self_loops=True, copy=True):\n    if copy:\n        H = G.copy()\n    else:\n        H = G\n    if H.is_directed():\n        edges_to_remap = chain(G.in_edges(v, data=True), G.out_edges(v, data=True))\n    else:\n        edges_to_remap = G.edges(v, data=True)\n    if not copy:\n        edges_to_remap = list(edges_to_remap)\n    v_data = H.nodes[v]\n    H.remove_node(v)\n    for prev_w, prev_x, d in edges_to_remap:\n        w = prev_w if prev_w != v else u\n        x = prev_x if prev_x != v else u\n        if {prev_w, prev_x} == {u, v} and (not self_loops):\n            continue\n        if not H.has_edge(w, x) or G.is_multigraph():\n            H.add_edge(w, x, **d)\n        elif 'contraction' in H.edges[w, x]:\n            H.edges[w, x]['contraction'][prev_w, prev_x] = d\n        else:\n            H.edges[w, x]['contraction'] = {(prev_w, prev_x): d}\n    if 'contraction' in H.nodes[u]:\n        H.nodes[u]['contraction'][v] = v_data\n    else:\n        H.nodes[u]['contraction'] = {v: v_data}\n    return H"
 },
 {
  "docstring": "Returns the graph that results from contracting the specified edge.\n\nEdge contraction identifies the two endpoints of the edge as a single node\nincident to any edge that was incident to the original two nodes. A graph\nthat results from edge contraction is called a *minor* of the original\ngraph.\n\nParameters\n----------\nG : NetworkX graph\n   The graph whose edge will be contracted.\n\nedge : tuple\n   Must be a pair of nodes in `G`.\n\nself_loops : Boolean\n   If this is True, any edges (including `edge`) joining the\n   endpoints of `edge` in `G` become self-loops on the new node in the\n   returned graph.\n\ncopy : Boolean (default True)\n    If this is True, a the contraction will be performed on a copy of `G`,\n    otherwise the contraction will happen in place.\n\nReturns\n-------\nNetworkx graph\n   A new graph object of the same type as `G` (leaving `G` unmodified)\n   with endpoints of `edge` identified in a single node. The right node\n   of `edge` will be merged into the left one, so only the left one will\n   appear in the returned graph.\n\nRaises\n------\nValueError\n   If `edge` is not an edge in `G`.\n\nExamples\n--------\nAttempting to contract two nonadjacent nodes yields an error:\n\n>>> G = nx.cycle_graph(4)\n>>> nx.contracted_edge(G, (1, 3))\nTraceback (most recent call last):\n  ...\nValueError: Edge (1, 3) does not exist in graph G; cannot contract it\n\nContracting two adjacent nodes in the cycle graph on *n* nodes yields the\ncycle graph on *n - 1* nodes:\n\n>>> C5 = nx.cycle_graph(5)\n>>> C4 = nx.cycle_graph(4)\n>>> M = nx.contracted_edge(C5, (0, 1), self_loops=False)\n>>> nx.is_isomorphic(M, C4)\nTrue\n\n",
  "code": "@nx._dispatch(preserve_edge_attrs=True)\ndef contracted_edge(G, edge, self_loops=True, copy=True):\n    u, v = edge[:2]\n    if not G.has_edge(u, v):\n        raise ValueError(f'Edge {edge} does not exist in graph G; cannot contract it')\n    return contracted_nodes(G, u, v, self_loops=self_loops, copy=copy)"
 },
 {
  "docstring": "Tests that the quotient graph of the complete *n*-partite graph\nunder the \"same neighbors\" node relation is the complete graph on *n*\nnodes.",
  "code": "def test_quotient_graph_complete_multipartite():\n    G = nx.complete_multipartite_graph(2, 3, 4)\n\n    def same_neighbors(u, v):\n        return u not in G[v] and v not in G[u] and (G[u] == G[v])\n    expected = nx.complete_graph(3)\n    actual = nx.quotient_graph(G, same_neighbors)\n    assert nx.is_isomorphic(expected, actual)"
 },
 {
  "docstring": "Tests that the quotient graph of the complete bipartite graph under\nthe \"same neighbors\" node relation is `K_2`.",
  "code": "def test_quotient_graph_complete_bipartite():\n    G = nx.complete_bipartite_graph(2, 3)\n\n    def same_neighbors(u, v):\n        return u not in G[v] and v not in G[u] and (G[u] == G[v])\n    expected = nx.complete_graph(2)\n    actual = nx.quotient_graph(G, same_neighbors)\n    assert nx.is_isomorphic(expected, actual)"
 },
 {
  "docstring": "Tests for specifying an alternate edge relation for the quotient\ngraph.",
  "code": "def test_quotient_graph_edge_relation():\n    G = nx.path_graph(5)\n\n    def identity(u, v):\n        return u == v\n\n    def same_parity(b, c):\n        return arbitrary_element(b) % 2 == arbitrary_element(c) % 2\n    actual = nx.quotient_graph(G, identity, same_parity)\n    expected = nx.Graph()\n    expected.add_edges_from([(0, 2), (0, 4), (2, 4)])\n    expected.add_edge(1, 3)\n    assert nx.is_isomorphic(actual, expected)"
 },
 {
  "docstring": "This tests that the condensation of a graph can be viewed as the\nquotient graph under the \"in the same connected component\" equivalence\nrelation.",
  "code": "def test_condensation_as_quotient():\n    G = nx.DiGraph()\n    G.add_edges_from([(1, 2), (2, 3), (2, 11), (2, 12), (3, 4), (4, 3), (4, 5), (5, 6), (6, 5), (6, 7), (7, 8), (7, 9), (7, 10), (8, 9), (9, 7), (10, 6), (11, 2), (11, 4), (11, 6), (12, 6), (12, 11)])\n    scc = list(nx.strongly_connected_components(G))\n    C = nx.condensation(G, scc)\n    component_of = C.graph['mapping']\n\n    def same_component(u, v):\n        return component_of[u] == component_of[v]\n    Q = nx.quotient_graph(G, same_component)\n    assert nx.is_isomorphic(C, Q)"
 },
 {
  "docstring": "Tests for node contraction in an undirected graph.",
  "code": "def test_undirected_node_contraction():\n    G = nx.cycle_graph(4)\n    actual = nx.contracted_nodes(G, 0, 1)\n    expected = nx.cycle_graph(3)\n    expected.add_edge(0, 0)\n    assert nx.is_isomorphic(actual, expected)"
 },
 {
  "docstring": "Tests for node contraction in a directed graph.",
  "code": "def test_directed_node_contraction():\n    G = nx.DiGraph(nx.cycle_graph(4))\n    actual = nx.contracted_nodes(G, 0, 1)\n    expected = nx.DiGraph(nx.cycle_graph(3))\n    expected.add_edge(0, 0)\n    expected.add_edge(0, 0)\n    assert nx.is_isomorphic(actual, expected)"
 },
 {
  "docstring": "Tests for node contraction in an undirected graph\nby making changes in place.",
  "code": "def test_undirected_node_contraction_no_copy():\n    G = nx.cycle_graph(4)\n    actual = nx.contracted_nodes(G, 0, 1, copy=False)\n    expected = nx.cycle_graph(3)\n    expected.add_edge(0, 0)\n    assert nx.is_isomorphic(actual, G)\n    assert nx.is_isomorphic(actual, expected)"
 },
 {
  "docstring": "Tests for node contraction in a directed graph\nby making changes in place.",
  "code": "def test_directed_node_contraction_no_copy():\n    G = nx.DiGraph(nx.cycle_graph(4))\n    actual = nx.contracted_nodes(G, 0, 1, copy=False)\n    expected = nx.DiGraph(nx.cycle_graph(3))\n    expected.add_edge(0, 0)\n    expected.add_edge(0, 0)\n    assert nx.is_isomorphic(actual, G)\n    assert nx.is_isomorphic(actual, expected)"
 },
 {
  "docstring": "Tests that using a MultiGraph creates multiple edges.",
  "code": "def test_create_multigraph():\n    G = nx.path_graph(3, create_using=nx.MultiGraph())\n    G.add_edge(0, 1)\n    G.add_edge(0, 0)\n    G.add_edge(0, 2)\n    actual = nx.contracted_nodes(G, 0, 2)\n    expected = nx.MultiGraph()\n    expected.add_edge(0, 1)\n    expected.add_edge(0, 1)\n    expected.add_edge(0, 1)\n    expected.add_edge(0, 0)\n    expected.add_edge(0, 0)\n    assert edges_equal(actual.edges, expected.edges)"
 },
 {
  "docstring": "Tests that multiedge keys are reset in new graph.",
  "code": "def test_multigraph_keys():\n    G = nx.path_graph(3, create_using=nx.MultiGraph())\n    G.add_edge(0, 1, 5)\n    G.add_edge(0, 0, 0)\n    G.add_edge(0, 2, 5)\n    actual = nx.contracted_nodes(G, 0, 2)\n    expected = nx.MultiGraph()\n    expected.add_edge(0, 1, 0)\n    expected.add_edge(0, 1, 5)\n    expected.add_edge(0, 1, 2)\n    expected.add_edge(0, 0, 0)\n    expected.add_edge(0, 0, 1)\n    assert edges_equal(actual.edges, expected.edges)"
 },
 {
  "docstring": "Tests that node contraction preserves node attributes.",
  "code": "def test_node_attributes():\n    G = nx.cycle_graph(4)\n    G.nodes[0]['foo'] = 'bar'\n    G.nodes[1]['baz'] = 'xyzzy'\n    actual = nx.contracted_nodes(G, 0, 1)\n    expected = nx.complete_graph(3)\n    expected = nx.relabel_nodes(expected, {1: 2, 2: 3})\n    expected.add_edge(0, 0)\n    cdict = {1: {'baz': 'xyzzy'}}\n    expected.nodes[0].update({'foo': 'bar', 'contraction': cdict})\n    assert nx.is_isomorphic(actual, expected)\n    assert actual.nodes == expected.nodes"
 },
 {
  "docstring": "Tests that node contraction preserves edge attributes.",
  "code": "def test_edge_attributes():\n    G = nx.DiGraph([('src1', 'dest'), ('src2', 'dest')])\n    G['src1']['dest']['value'] = 'src1-->dest'\n    G['src2']['dest']['value'] = 'src2-->dest'\n    H = nx.MultiDiGraph(G)\n    G = nx.contracted_nodes(G, 'src1', 'src2')\n    assert G.edges['src1', 'dest']['value'] == 'src1-->dest'\n    assert G.edges['src1', 'dest']['contraction']['src2', 'dest']['value'] == 'src2-->dest'\n    H = nx.contracted_nodes(H, 'src1', 'src2')\n    assert len(H.edges(('src1', 'dest'))) == 2"
 },
 {
  "docstring": "Tests for node contraction without preserving -loops.",
  "code": "def test_without_self_loops():\n    G = nx.cycle_graph(4)\n    actual = nx.contracted_nodes(G, 0, 1, self_loops=False)\n    expected = nx.complete_graph(3)\n    assert nx.is_isomorphic(actual, expected)"
 },
 {
  "docstring": "Tests for node contraction when nodes have loops.",
  "code": "def test_contract_loop_graph():\n    G = nx.cycle_graph(4)\n    G.add_edge(0, 0)\n    actual = nx.contracted_nodes(G, 0, 1)\n    expected = nx.complete_graph([0, 2, 3])\n    expected.add_edge(0, 0)\n    expected.add_edge(0, 0)\n    assert edges_equal(actual.edges, expected.edges)\n    actual = nx.contracted_nodes(G, 1, 0)\n    expected = nx.complete_graph([1, 2, 3])\n    expected.add_edge(1, 1)\n    expected.add_edge(1, 1)\n    assert edges_equal(actual.edges, expected.edges)"
 },
 {
  "docstring": "Tests for edge contraction in an undirected graph.",
  "code": "def test_undirected_edge_contraction():\n    G = nx.cycle_graph(4)\n    actual = nx.contracted_edge(G, (0, 1))\n    expected = nx.complete_graph(3)\n    expected.add_edge(0, 0)\n    assert nx.is_isomorphic(actual, expected)"
 },
 {
  "docstring": "Tests for edge contraction in a multigraph",
  "code": "def test_multigraph_edge_contraction():\n    G = nx.cycle_graph(4)\n    actual = nx.contracted_edge(G, (0, 1, 0))\n    expected = nx.complete_graph(3)\n    expected.add_edge(0, 0)\n    assert nx.is_isomorphic(actual, expected)"
 },
 {
  "docstring": "Tests that attempting to contract a nonexistent edge raises an\nexception.",
  "code": "def test_nonexistent_edge():\n    with pytest.raises(ValueError):\n        G = nx.cycle_graph(4)\n        nx.contracted_edge(G, (0, 2))"
 },
 {
  "docstring": "Returns the union of all graphs.\n\nThe graphs must be disjoint, otherwise an exception is raised.\n\nParameters\n----------\ngraphs : iterable\n   Iterable of NetworkX graphs\n\nrename : iterable , optional\n   Node names of graphs can be changed by specifying the tuple\n   rename=('G-','H-') (for example).  Node \"u\" in G is then renamed\n   \"G-u\" and \"v\" in H is renamed \"H-v\". Infinite generators (like itertools.count)\n   are also supported.\n\nReturns\n-------\nU : a graph with the same type as the first graph in list\n\nRaises\n------\nValueError\n   If `graphs` is an empty list.\n\nNetworkXError\n    In case of mixed type graphs, like MultiGraph and Graph, or directed and undirected graphs.\n\n",
  "code": "@nx._dispatch(graphs='[graphs]', preserve_all_attrs=True)\ndef union_all(graphs, rename=()):\n    R = None\n    seen_nodes = set()\n\n    def add_prefix(graph, prefix):\n        if prefix is None:\n            return graph\n\n        def label(x):\n            return f'{prefix}{x}'\n        return nx.relabel_nodes(graph, label)\n    rename = chain(rename, repeat(None))\n    graphs = (add_prefix(G, name) for G, name in zip(graphs, rename))\n    for i, G in enumerate(graphs):\n        G_nodes_set = set(G.nodes)\n        if i == 0:\n            R = G.__class__()\n        elif G.is_directed() != R.is_directed():\n            raise nx.NetworkXError('All graphs must be directed or undirected.')\n        elif G.is_multigraph() != R.is_multigraph():\n            raise nx.NetworkXError('All graphs must be graphs or multigraphs.')\n        elif not seen_nodes.isdisjoint(G_nodes_set):\n            raise nx.NetworkXError('The node sets of the graphs are not disjoint.\\nUse `rename` to specify prefixes for the graphs or use\\ndisjoint_union(G1, G2, ..., GN).')\n        seen_nodes |= G_nodes_set\n        R.graph.update(G.graph)\n        R.add_nodes_from(G.nodes(data=True))\n        R.add_edges_from(G.edges(keys=True, data=True) if G.is_multigraph() else G.edges(data=True))\n    if R is None:\n        raise ValueError('cannot apply union_all to an empty list')\n    return R"
 },
 {
  "docstring": "Returns the disjoint union of all graphs.\n\nThis operation forces distinct integer node labels starting with 0\nfor the first graph in the list and numbering consecutively.\n\nParameters\n----------\ngraphs : iterable\n   Iterable of NetworkX graphs\n\nReturns\n-------\nU : A graph with the same type as the first graph in list\n\nRaises\n------\nValueError\n   If `graphs` is an empty list.\n\nNetworkXError\n    In case of mixed type graphs, like MultiGraph and Graph, or directed and undirected graphs.\n\nExamples\n--------\n>>> G1 = nx.Graph([(1, 2), (2, 3)])\n>>> G2 = nx.Graph([(4, 5), (5, 6)])\n>>> U = nx.disjoint_union_all([G1, G2])\n>>> list(U.nodes())\n[0, 1, 2, 3, 4, 5]\n>>> list(U.edges())\n[(0, 1), (1, 2), (3, 4), (4, 5)]\n\n",
  "code": "@nx._dispatch(graphs='[graphs]', preserve_all_attrs=True)\ndef disjoint_union_all(graphs):\n\n    def yield_relabeled(graphs):\n        first_label = 0\n        for G in graphs:\n            yield nx.convert_node_labels_to_integers(G, first_label=first_label)\n            first_label += len(G)\n    R = union_all(yield_relabeled(graphs))\n    return R"
 },
 {
  "docstring": "Returns the composition of all graphs.\n\nComposition is the simple union of the node sets and edge sets.\nThe node sets of the supplied graphs need not be disjoint.\n\nParameters\n----------\ngraphs : iterable\n   Iterable of NetworkX graphs\n\nReturns\n-------\nC : A graph with the same type as the first graph in list\n\nRaises\n------\nValueError\n   If `graphs` is an empty list.\n\nNetworkXError\n    In case of mixed type graphs, like MultiGraph and Graph, or directed and undirected graphs.\n\nExamples\n--------\n>>> G1 = nx.Graph([(1, 2), (2, 3)])\n>>> G2 = nx.Graph([(3, 4), (5, 6)])\n>>> C = nx.compose_all([G1, G2])\n>>> list(C.nodes())\n[1, 2, 3, 4, 5, 6]\n>>> list(C.edges())\n[(1, 2), (2, 3), (3, 4), (5, 6)]\n\n",
  "code": "@nx._dispatch(graphs='[graphs]', preserve_all_attrs=True)\ndef compose_all(graphs):\n    R = None\n    for i, G in enumerate(graphs):\n        if i == 0:\n            R = G.__class__()\n        elif G.is_directed() != R.is_directed():\n            raise nx.NetworkXError('All graphs must be directed or undirected.')\n        elif G.is_multigraph() != R.is_multigraph():\n            raise nx.NetworkXError('All graphs must be graphs or multigraphs.')\n        R.graph.update(G.graph)\n        R.add_nodes_from(G.nodes(data=True))\n        R.add_edges_from(G.edges(keys=True, data=True) if G.is_multigraph() else G.edges(data=True))\n    if R is None:\n        raise ValueError('cannot apply compose_all to an empty list')\n    return R"
 },
 {
  "docstring": "Returns a new graph that contains only the nodes and the edges that exist in\nall graphs.\n\nParameters\n----------\ngraphs : iterable\n   Iterable of NetworkX graphs\n\nReturns\n-------\nR : A new graph with the same type as the first graph in list\n\nRaises\n------\nValueError\n   If `graphs` is an empty list.\n\nNetworkXError\n    In case of mixed type graphs, like MultiGraph and Graph, or directed and undirected graphs.\n\n",
  "code": "@nx._dispatch(graphs='[graphs]')\ndef intersection_all(graphs):\n    R = None\n    for i, G in enumerate(graphs):\n        G_nodes_set = set(G.nodes)\n        G_edges_set = set(G.edges)\n        if not G.is_directed():\n            if G.is_multigraph():\n                G_edges_set.update(((v, u, k) for u, v, k in list(G_edges_set)))\n            else:\n                G_edges_set.update(((v, u) for u, v in list(G_edges_set)))\n        if i == 0:\n            R = G.__class__()\n            node_intersection = G_nodes_set\n            edge_intersection = G_edges_set\n        elif G.is_directed() != R.is_directed():\n            raise nx.NetworkXError('All graphs must be directed or undirected.')\n        elif G.is_multigraph() != R.is_multigraph():\n            raise nx.NetworkXError('All graphs must be graphs or multigraphs.')\n        else:\n            node_intersection &= G_nodes_set\n            edge_intersection &= G_edges_set\n    if R is None:\n        raise ValueError('cannot apply intersection_all to an empty list')\n    R.add_nodes_from(node_intersection)\n    R.add_edges_from(edge_intersection)\n    return R"
 },
 {
  "docstring": "Combine graphs G and H. The names of nodes must be unique.\n\nA name collision between the graphs will raise an exception.\n\nA renaming facility is provided to avoid name collisions.\n\n\nParameters\n----------\nG, H : graph\n   A NetworkX graph\n\nrename : iterable , optional\n   Node names of G and H can be changed by specifying the tuple\n   rename=('G-','H-') (for example).  Node \"u\" in G is then renamed\n   \"G-u\" and \"v\" in H is renamed \"H-v\".\n\nReturns\n-------\nU : A union graph with the same type as G.\n\nSee Also\n--------\ncompose\n:func:`~networkx.Graph.update`\ndisjoint_union\n\n",
  "code": "@nx._dispatch(graphs=_G_H, preserve_all_attrs=True)\ndef union(G, H, rename=()):\n    return nx.union_all([G, H], rename)"
 },
 {
  "docstring": "Combine graphs G and H. The nodes are assumed to be unique (disjoint).\n\nThis algorithm automatically relabels nodes to avoid name collisions.\n\nParameters\n----------\nG,H : graph\n   A NetworkX graph\n\nReturns\n-------\nU : A union graph with the same type as G.\n\nSee Also\n--------\nunion\ncompose\n:func:`~networkx.Graph.update`\n\n",
  "code": "@nx._dispatch(graphs=_G_H, preserve_all_attrs=True)\ndef disjoint_union(G, H):\n    return nx.disjoint_union_all([G, H])"
 },
 {
  "docstring": "Returns a new graph that contains only the nodes and the edges that exist in\nboth G and H.\n\nParameters\n----------\nG,H : graph\n   A NetworkX graph. G and H can have different node sets but must be both graphs or both multigraphs.\n\nRaises\n------\nNetworkXError\n    If one is a MultiGraph and the other one is a graph.\n\nReturns\n-------\nGH : A new graph with the same type as G.\n\n",
  "code": "@nx._dispatch(graphs=_G_H)\ndef intersection(G, H):\n    return nx.intersection_all([G, H])"
 },
 {
  "docstring": "Returns a new graph that contains the edges that exist in G but not in H.\n\nThe node sets of H and G must be the same.\n\nParameters\n----------\nG,H : graph\n   A NetworkX graph. G and H must have the same node sets.\n\nReturns\n-------\nD : A new graph with the same type as G.\n\n",
  "code": "@nx._dispatch(graphs=_G_H)\ndef difference(G, H):\n    if not G.is_multigraph() == H.is_multigraph():\n        raise nx.NetworkXError('G and H must both be graphs or multigraphs.')\n    R = nx.create_empty_copy(G, with_data=False)\n    if set(G) != set(H):\n        raise nx.NetworkXError('Node sets of graphs not equal')\n    if G.is_multigraph():\n        edges = G.edges(keys=True)\n    else:\n        edges = G.edges()\n    for e in edges:\n        if not H.has_edge(*e):\n            R.add_edge(*e)\n    return R"
 },
 {
  "docstring": "Returns new graph with edges that exist in either G or H but not both.\n\nThe node sets of H and G must be the same.\n\nParameters\n----------\nG,H : graph\n   A NetworkX graph.  G and H must have the same node sets.\n\nReturns\n-------\nD : A new graph with the same type as G.\n\n",
  "code": "@nx._dispatch(graphs=_G_H)\ndef symmetric_difference(G, H):\n    if not G.is_multigraph() == H.is_multigraph():\n        raise nx.NetworkXError('G and H must both be graphs or multigraphs.')\n    R = nx.create_empty_copy(G, with_data=False)\n    if set(G) != set(H):\n        raise nx.NetworkXError('Node sets of graphs not equal')\n    gnodes = set(G)\n    hnodes = set(H)\n    nodes = gnodes.symmetric_difference(hnodes)\n    R.add_nodes_from(nodes)\n    if G.is_multigraph():\n        edges = G.edges(keys=True)\n    else:\n        edges = G.edges()\n    for e in edges:\n        if not H.has_edge(*e):\n            R.add_edge(*e)\n    if H.is_multigraph():\n        edges = H.edges(keys=True)\n    else:\n        edges = H.edges()\n    for e in edges:\n        if not G.has_edge(*e):\n            R.add_edge(*e)\n    return R"
 },
 {
  "docstring": "Compose graph G with H by combining nodes and edges into a single graph.\n\nThe node sets and edges sets do not need to be disjoint.\n\nComposing preserves the attributes of nodes and edges.\nAttribute values from H take precedent over attribute values from G.\n\nParameters\n----------\nG, H : graph\n   A NetworkX graph\n\nReturns\n-------\nC: A new graph with the same type as G\n\nSee Also\n--------\n:func:`~networkx.Graph.update`\nunion\ndisjoint_union\n\n",
  "code": "@nx._dispatch(graphs=_G_H, preserve_all_attrs=True)\ndef compose(G, H):\n    return nx.compose_all([G, H])"
 },
 {
  "docstring": "Returns the full join of graphs G and H.\n\nFull join is the union of G and H in which all edges between\nG and H are added.\nThe node sets of G and H must be disjoint,\notherwise an exception is raised.\n\nParameters\n----------\nG, H : graph\n   A NetworkX graph\n\nrename : tuple , default=(None, None)\n   Node names of G and H can be changed by specifying the tuple\n   rename=('G-','H-') (for example).  Node \"u\" in G is then renamed\n   \"G-u\" and \"v\" in H is renamed \"H-v\".\n\nReturns\n-------\nU : The full join graph with the same type as G.\n\n",
  "code": "@nx._dispatch(graphs=_G_H, preserve_all_attrs=True)\ndef full_join(G, H, rename=(None, None)):\n    R = union(G, H, rename)\n\n    def add_prefix(graph, prefix):\n        if prefix is None:\n            return graph\n\n        def label(x):\n            return f'{prefix}{x}'\n        return nx.relabel_nodes(graph, label)\n    G = add_prefix(G, rename[0])\n    H = add_prefix(H, rename[1])\n    for i in G:\n        for j in H:\n            R.add_edge(i, j)\n    if R.is_directed():\n        for i in H:\n            for j in G:\n                R.add_edge(i, j)\n    return R"
 },
 {
  "docstring": "Returns the tensor product of G and H.\n\nThe tensor product $P$ of the graphs $G$ and $H$ has a node set that\nis the tensor product of the node sets, $V(P)=V(G) \\times V(H)$.\n$P$ has an edge $((u,v), (x,y))$ if and only if $(u,x)$ is an edge in $G$\nand $(v,y)$ is an edge in $H$.\n\nTensor product is sometimes also referred to as the categorical product,\ndirect product, cardinal product or conjunction.\n\n\nParameters\n----------\nG, H: graphs\n Networkx graphs.\n\nReturns\n-------\nP: NetworkX graph\n The tensor product of G and H. P will be a multi-graph if either G\n or H is a multi-graph, will be a directed if G and H are directed,\n and undirected if G and H are undirected.\n\nRaises\n------\nNetworkXError\n If G and H are not both directed or both undirected.\n\n",
  "code": "@nx._dispatch(graphs=_G_H, preserve_node_attrs=True)\ndef tensor_product(G, H):\n    GH = _init_product_graph(G, H)\n    GH.add_nodes_from(_node_product(G, H))\n    GH.add_edges_from(_directed_edges_cross_edges(G, H))\n    if not GH.is_directed():\n        GH.add_edges_from(_undirected_edges_cross_edges(G, H))\n    return GH"
 },
 {
  "docstring": "Returns the Cartesian product of G and H.\n\nThe Cartesian product $P$ of the graphs $G$ and $H$ has a node set that\nis the Cartesian product of the node sets, $V(P)=V(G) \\times V(H)$.\n$P$ has an edge $((u,v),(x,y))$ if and only if either $u$ is equal to $x$\nand both $v$ and $y$ are adjacent in $H$ or if $v$ is equal to $y$ and\nboth $u$ and $x$ are adjacent in $G$.\n\nParameters\n----------\nG, H: graphs\n Networkx graphs.\n\nReturns\n-------\nP: NetworkX graph\n The Cartesian product of G and H. P will be a multi-graph if either G\n or H is a multi-graph. Will be a directed if G and H are directed,\n and undirected if G and H are undirected.\n\nRaises\n------\nNetworkXError\n If G and H are not both directed or both undirected.\n\n",
  "code": "@nx._dispatch(graphs=_G_H, preserve_node_attrs=True)\ndef cartesian_product(G, H):\n    GH = _init_product_graph(G, H)\n    GH.add_nodes_from(_node_product(G, H))\n    GH.add_edges_from(_edges_cross_nodes(G, H))\n    GH.add_edges_from(_nodes_cross_edges(G, H))\n    return GH"
 },
 {
  "docstring": "Returns the lexicographic product of G and H.\n\nThe lexicographical product $P$ of the graphs $G$ and $H$ has a node set\nthat is the Cartesian product of the node sets, $V(P)=V(G) \\times V(H)$.\n$P$ has an edge $((u,v), (x,y))$ if and only if $(u,v)$ is an edge in $G$\nor $u==v$ and $(x,y)$ is an edge in $H$.\n\nParameters\n----------\nG, H: graphs\n Networkx graphs.\n\nReturns\n-------\nP: NetworkX graph\n The Cartesian product of G and H. P will be a multi-graph if either G\n or H is a multi-graph. Will be a directed if G and H are directed,\n and undirected if G and H are undirected.\n\nRaises\n------\nNetworkXError\n If G and H are not both directed or both undirected.\n\n",
  "code": "@nx._dispatch(graphs=_G_H, preserve_node_attrs=True)\ndef lexicographic_product(G, H):\n    GH = _init_product_graph(G, H)\n    GH.add_nodes_from(_node_product(G, H))\n    GH.add_edges_from(_edges_cross_nodes_and_nodes(G, H))\n    GH.add_edges_from(_nodes_cross_edges(G, H))\n    return GH"
 },
 {
  "docstring": "Returns the strong product of G and H.\n\nThe strong product $P$ of the graphs $G$ and $H$ has a node set that\nis the Cartesian product of the node sets, $V(P)=V(G) \\times V(H)$.\n$P$ has an edge $((u,v), (x,y))$ if and only if\n$u==v$ and $(x,y)$ is an edge in $H$, or\n$x==y$ and $(u,v)$ is an edge in $G$, or\n$(u,v)$ is an edge in $G$ and $(x,y)$ is an edge in $H$.\n\nParameters\n----------\nG, H: graphs\n Networkx graphs.\n\nReturns\n-------\nP: NetworkX graph\n The Cartesian product of G and H. P will be a multi-graph if either G\n or H is a multi-graph. Will be a directed if G and H are directed,\n and undirected if G and H are undirected.\n\nRaises\n------\nNetworkXError\n If G and H are not both directed or both undirected.\n\n",
  "code": "@nx._dispatch(graphs=_G_H, preserve_node_attrs=True)\ndef strong_product(G, H):\n    GH = _init_product_graph(G, H)\n    GH.add_nodes_from(_node_product(G, H))\n    GH.add_edges_from(_nodes_cross_edges(G, H))\n    GH.add_edges_from(_edges_cross_nodes(G, H))\n    GH.add_edges_from(_directed_edges_cross_edges(G, H))\n    if not GH.is_directed():\n        GH.add_edges_from(_undirected_edges_cross_edges(G, H))\n    return GH"
 },
 {
  "docstring": "Returns the specified power of a graph.\n\nThe $k$th power of a simple graph $G$, denoted $G^k$, is a\ngraph on the same set of nodes in which two distinct nodes $u$ and\n$v$ are adjacent in $G^k$ if and only if the shortest path\ndistance between $u$ and $v$ in $G$ is at most $k$.\n\nParameters\n----------\nG : graph\n    A NetworkX simple graph object.\n\nk : positive integer\n    The power to which to raise the graph `G`.\n\nReturns\n-------\nNetworkX simple graph\n    `G` to the power `k`.\n\nRaises\n------\nValueError\n    If the exponent `k` is not positive.\n\nNetworkXNotImplemented\n    If `G` is not a simple graph.\n\nExamples\n--------\nThe number of edges will never decrease when taking successive\npowers:\n\n>>> G = nx.path_graph(4)\n>>> list(nx.power(G, 2).edges)\n[(0, 1), (0, 2), (1, 2), (1, 3), (2, 3)]\n>>> list(nx.power(G, 3).edges)\n[(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]\n\nThe `k` th power of a cycle graph on *n* nodes is the complete graph\non *n* nodes, if `k` is at least ``n // 2``:\n\n>>> G = nx.cycle_graph(5)\n>>> H = nx.complete_graph(5)\n>>> nx.is_isomorphic(nx.power(G, 2), H)\nTrue\n>>> G = nx.cycle_graph(8)\n>>> H = nx.complete_graph(8)\n>>> nx.is_isomorphic(nx.power(G, 4), H)\nTrue\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch\ndef power(G, k):\n    if k <= 0:\n        raise ValueError('k must be a positive integer')\n    H = nx.Graph()\n    H.add_nodes_from(G)\n    for n in G:\n        seen = {}\n        level = 1\n        nextlevel = G[n]\n        while nextlevel:\n            thislevel = nextlevel\n            nextlevel = {}\n            for v in thislevel:\n                if v == n:\n                    continue\n                if v not in seen:\n                    seen[v] = level\n                    nextlevel.update(G[v])\n            if k <= level:\n                break\n            level += 1\n        H.add_edges_from(((n, nbr) for nbr in seen))\n    return H"
 },
 {
  "docstring": "Return the rooted product of graphs G and H rooted at root in H.\n\nA new graph is constructed representing the rooted product of\nthe inputted graphs, G and H, with a root in H.\nA rooted product duplicates H for each nodes in G with the root\nof H corresponding to the node in G. Nodes are renamed as the direct\nproduct of G and H. The result is a subgraph of the cartesian product.\n\nParameters\n----------\nG,H : graph\n   A NetworkX graph\nroot : node\n   A node in H\n\nReturns\n-------\nR : The rooted product of G and H with a specified root in H\n\n",
  "code": "@not_implemented_for('multigraph')\n@nx._dispatch(graphs=_G_H)\ndef rooted_product(G, H, root):\n    if root not in H:\n        raise nx.NetworkXError('root must be a vertex in H')\n    R = nx.Graph()\n    R.add_nodes_from(product(G, H))\n    R.add_edges_from((((e[0], root), (e[1], root)) for e in G.edges()))\n    R.add_edges_from((((g, e[0]), (g, e[1])) for g in G for e in H.edges()))\n    return R"
 },
 {
  "docstring": "Returns the Corona product of G and H.\n\nThe corona product of $G$ and $H$ is the graph $C = G \\circ H$ obtained by\ntaking one copy of $G$, called the center graph, $|V(G)|$ copies of $H$,\ncalled the outer graph, and making the $i$-th vertex of $G$ adjacent to\nevery vertex of the $i$-th copy of $H$, where $1 \u2264 i \u2264 |V(G)|$.\n\nParameters\n----------\nG, H: NetworkX graphs\n    The graphs to take the carona product of.\n    `G` is the center graph and `H` is the outer graph\n\nReturns\n-------\nC: NetworkX graph\n    The Corona product of G and H.\n\nRaises\n------\nNetworkXError\n    If G and H are not both directed or both undirected.\n\nExamples\n--------\n>>> G = nx.cycle_graph(4)\n>>> H = nx.path_graph(2)\n>>> C = nx.corona_product(G, H)\n>>> list(C)\n[0, 1, 2, 3, (0, 0), (0, 1), (1, 0), (1, 1), (2, 0), (2, 1), (3, 0), (3, 1)]\n>>> print(C)\nGraph with 12 nodes and 16 edges\n\n",
  "code": "@not_implemented_for('directed')\n@not_implemented_for('multigraph')\n@nx._dispatch(graphs=_G_H)\ndef corona_product(G, H):\n    GH = _init_product_graph(G, H)\n    GH.add_nodes_from(G)\n    GH.add_edges_from(G.edges)\n    for G_node in G:\n        GH.add_nodes_from(((G_node, v) for v in H))\n        GH.add_edges_from((((G_node, e0), (G_node, e1), d) for e0, e1, d in H.edges.data()))\n        GH.add_edges_from(((G_node, (G_node, H_node)) for H_node in H))\n    return GH"
 },
 {
  "docstring": "Returns the graph complement of G.\n\nParameters\n----------\nG : graph\n   A NetworkX graph\n\nReturns\n-------\nGC : A new graph.\n\n",
  "code": "@nx._dispatch\ndef complement(G):\n    R = G.__class__()\n    R.add_nodes_from(G)\n    R.add_edges_from(((n, n2) for n, nbrs in G.adjacency() for n2 in G if n2 not in nbrs if n != n2))\n    return R"
 },
 {
  "docstring": "Returns the reverse directed graph of G.\n\nParameters\n----------\nG : directed graph\n    A NetworkX directed graph\ncopy : bool\n    If True, then a new graph is returned. If False, then the graph is\n    reversed in place.\n\nReturns\n-------\nH : directed graph\n    The reversed G.\n\nRaises\n------\nNetworkXError\n    If graph is undirected.\n\nExamples\n--------\n>>> G = nx.DiGraph([(1, 2), (1, 3), (2, 3), (3, 4), (3, 5)])\n>>> G_reversed = nx.reverse(G)\n>>> G_reversed.edges()\nOutEdgeView([(2, 1), (3, 1), (3, 2), (4, 3), (5, 3)])",
  "code": "@nx._dispatch\ndef reverse(G, copy=True):\n    if not G.is_directed():\n        raise nx.NetworkXError('Cannot reverse an undirected graph.')\n    else:\n        return G.reverse(copy=copy)"
 },
 {
  "docstring": "Returns a list of nodes in a shortest path between source and target\nusing the A* (\"A-star\") algorithm.\n\nThere may be more than one shortest path.  This returns only one.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node\n   Starting node for path\n\ntarget : node\n   Ending node for path\n\nheuristic : function\n   A function to evaluate the estimate of the distance\n   from the a node to the target.  The function takes\n   two nodes arguments and must return a number.\n   If the heuristic is inadmissible (if it might\n   overestimate the cost of reaching the goal from a node),\n   the result may not be a shortest path.\n   The algorithm does not support updating heuristic\n   values for the same node due to caching the first\n   heuristic calculation per node.\n\nweight : string or function\n   If this is a string, then edge weights will be accessed via the\n   edge attribute with this key (that is, the weight of the edge\n   joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\n   such edge attribute exists, the weight of the edge is assumed to\n   be one.\n   If this is a function, the weight of an edge is the value\n   returned by the function. The function must accept exactly three\n   positional arguments: the two endpoints of an edge and the\n   dictionary of edge attributes for that edge. The function must\n   return a number or None to indicate a hidden edge.\n\ncutoff : float, optional\n   If this is provided, the search will be bounded to this value. I.e. if\n   the evaluation function surpasses this value for a node n, the node will not\n   be expanded further and will be ignored. More formally, let h'(n) be the\n   heuristic function, and g(n) be the cost of reaching n from the source node. Then,\n   if g(n) + h'(n) > cutoff, the node will not be explored further.\n   Note that if the heuristic is inadmissible, it is possible that paths\n   are ignored even though they satisfy the cutoff.\n\nRaises\n------\nNetworkXNoPath\n    If no path exists between source and target.\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> print(nx.astar_path(G, 0, 4))\n[0, 1, 2, 3, 4]\n>>> G = nx.grid_graph(dim=[3, 3])  # nodes are two-tuples (x,y)\n>>> nx.set_edge_attributes(G, {e: e[1][0] * 2 for e in G.edges()}, \"cost\")\n>>> def dist(a, b):\n...     (x1, y1) = a\n...     (x2, y2) = b\n...     return ((x1 - x2) ** 2 + (y1 - y2) ** 2) ** 0.5\n>>> print(nx.astar_path(G, (0, 0), (2, 2), heuristic=dist, weight=\"cost\"))\n[(0, 0), (0, 1), (0, 2), (1, 2), (2, 2)]\n\n",
  "code": "@nx._dispatch(edge_attrs='weight', preserve_node_attrs='heuristic')\ndef astar_path(G, source, target, heuristic=None, weight='weight', *, cutoff=None):\n    if source not in G or target not in G:\n        msg = f'Either source {source} or target {target} is not in G'\n        raise nx.NodeNotFound(msg)\n    if heuristic is None:\n\n        def heuristic(u, v):\n            return 0\n    push = heappush\n    pop = heappop\n    weight = _weight_function(G, weight)\n    G_succ = G._adj\n    c = count()\n    queue = [(0, next(c), source, 0, None)]\n    enqueued = {}\n    explored = {}\n    while queue:\n        _, __, curnode, dist, parent = pop(queue)\n        if curnode == target:\n            path = [curnode]\n            node = parent\n            while node is not None:\n                path.append(node)\n                node = explored[node]\n            path.reverse()\n            return path\n        if curnode in explored:\n            if explored[curnode] is None:\n                continue\n            qcost, h = enqueued[curnode]\n            if qcost < dist:\n                continue\n        explored[curnode] = parent\n        for neighbor, w in G_succ[curnode].items():\n            cost = weight(curnode, neighbor, w)\n            if cost is None:\n                continue\n            ncost = dist + cost\n            if neighbor in enqueued:\n                qcost, h = enqueued[neighbor]\n                if qcost <= ncost:\n                    continue\n            else:\n                h = heuristic(neighbor, target)\n            if cutoff and ncost + h > cutoff:\n                continue\n            enqueued[neighbor] = (ncost, h)\n            push(queue, (ncost + h, next(c), neighbor, ncost, curnode))\n    raise nx.NetworkXNoPath(f'Node {target} not reachable from {source}')"
 },
 {
  "docstring": "Returns the length of the shortest path between source and target using\nthe A* (\"A-star\") algorithm.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node\n   Starting node for path\n\ntarget : node\n   Ending node for path\n\nheuristic : function\n   A function to evaluate the estimate of the distance\n   from the a node to the target.  The function takes\n   two nodes arguments and must return a number.\n   If the heuristic is inadmissible (if it might\n   overestimate the cost of reaching the goal from a node),\n   the result may not be a shortest path.\n   The algorithm does not support updating heuristic\n   values for the same node due to caching the first\n   heuristic calculation per node.\n\nweight : string or function\n   If this is a string, then edge weights will be accessed via the\n   edge attribute with this key (that is, the weight of the edge\n   joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\n   such edge attribute exists, the weight of the edge is assumed to\n   be one.\n   If this is a function, the weight of an edge is the value\n   returned by the function. The function must accept exactly three\n   positional arguments: the two endpoints of an edge and the\n   dictionary of edge attributes for that edge. The function must\n   return a number or None to indicate a hidden edge.\n\ncutoff : float, optional\n   If this is provided, the search will be bounded to this value. I.e. if\n   the evaluation function surpasses this value for a node n, the node will not\n   be expanded further and will be ignored. More formally, let h'(n) be the\n   heuristic function, and g(n) be the cost of reaching n from the source node. Then,\n   if g(n) + h'(n) > cutoff, the node will not be explored further.\n   Note that if the heuristic is inadmissible, it is possible that paths\n   are ignored even though they satisfy the cutoff.\n\nRaises\n------\nNetworkXNoPath\n    If no path exists between source and target.\n\nSee Also\n--------\nastar_path",
  "code": "@nx._dispatch(edge_attrs='weight', preserve_node_attrs='heuristic')\ndef astar_path_length(G, source, target, heuristic=None, weight='weight', *, cutoff=None):\n    if source not in G or target not in G:\n        msg = f'Either source {source} or target {target} is not in G'\n        raise nx.NodeNotFound(msg)\n    weight = _weight_function(G, weight)\n    path = astar_path(G, source, target, heuristic, weight, cutoff=cutoff)\n    return sum((weight(u, v, G[u][v]) for u, v in zip(path[:-1], path[1:])))"
 },
 {
  "docstring": "Find all-pairs shortest path lengths using Floyd's algorithm.\n\nThis algorithm for finding shortest paths takes advantage of\nmatrix representations of a graph and works well for dense\ngraphs where all-pairs shortest path lengths are desired.\nThe results are returned as a NumPy array, distance[i, j],\nwhere i and j are the indexes of two nodes in nodelist.\nThe entry distance[i, j] is the distance along a shortest\npath from i to j. If no path exists the distance is Inf.\n\nParameters\n----------\nG : NetworkX graph\n\nnodelist : list, optional (default=G.nodes)\n   The rows and columns are ordered by the nodes in nodelist.\n   If nodelist is None then the ordering is produced by G.nodes.\n   Nodelist should include all nodes in G.\n\nweight: string, optional (default='weight')\n   Edge data key corresponding to the edge weight.\n\nReturns\n-------\ndistance : 2D numpy.ndarray\n    A numpy array of shortest path distances between nodes.\n    If there is no path between two nodes the value is Inf.\n\nExamples\n--------\n>>> G = nx.DiGraph()\n>>> G.add_weighted_edges_from([(0, 1, 5), (1, 2, 2), (2, 3, -3), (1, 3, 10), (3, 2, 8)])\n>>> nx.floyd_warshall_numpy(G)\narray([[ 0.,  5.,  7.,  4.],\n       [inf,  0.,  2., -1.],\n       [inf, inf,  0., -3.],\n       [inf, inf,  8.,  0.]])\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef floyd_warshall_numpy(G, nodelist=None, weight='weight'):\n    import numpy as np\n    if nodelist is not None:\n        if not len(nodelist) == len(G) == len(set(nodelist)):\n            raise nx.NetworkXError('nodelist must contain every node in G with no repeats.If you wanted a subgraph of G use G.subgraph(nodelist)')\n    A = nx.to_numpy_array(G, nodelist, multigraph_weight=min, weight=weight, nonedge=np.inf)\n    n, m = A.shape\n    np.fill_diagonal(A, 0)\n    for i in range(n):\n        A = np.minimum(A, A[i, :][np.newaxis, :] + A[:, i][:, np.newaxis])\n    return A"
 },
 {
  "docstring": "Find all-pairs shortest path lengths using Floyd's algorithm.\n\nParameters\n----------\nG : NetworkX graph\n\nweight: string, optional (default= 'weight')\n   Edge data key corresponding to the edge weight.\n\nReturns\n-------\npredecessor,distance : dictionaries\n   Dictionaries, keyed by source and target, of predecessors and distances\n   in the shortest path.\n\nExamples\n--------\n>>> G = nx.DiGraph()\n>>> G.add_weighted_edges_from(\n...     [\n...         (\"s\", \"u\", 10),\n...         (\"s\", \"x\", 5),\n...         (\"u\", \"v\", 1),\n...         (\"u\", \"x\", 2),\n...         (\"v\", \"y\", 1),\n...         (\"x\", \"u\", 3),\n...         (\"x\", \"v\", 5),\n...         (\"x\", \"y\", 2),\n...         (\"y\", \"s\", 7),\n...         (\"y\", \"v\", 6),\n...     ]\n... )\n>>> predecessors, _ = nx.floyd_warshall_predecessor_and_distance(G)\n>>> print(nx.reconstruct_path(\"s\", \"v\", predecessors))\n['s', 'x', 'u', 'v']\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef floyd_warshall_predecessor_and_distance(G, weight='weight'):\n    from collections import defaultdict\n    dist = defaultdict(lambda: defaultdict(lambda: float('inf')))\n    for u in G:\n        dist[u][u] = 0\n    pred = defaultdict(dict)\n    undirected = not G.is_directed()\n    for u, v, d in G.edges(data=True):\n        e_weight = d.get(weight, 1.0)\n        dist[u][v] = min(e_weight, dist[u][v])\n        pred[u][v] = u\n        if undirected:\n            dist[v][u] = min(e_weight, dist[v][u])\n            pred[v][u] = v\n    for w in G:\n        dist_w = dist[w]\n        for u in G:\n            dist_u = dist[u]\n            for v in G:\n                d = dist_u[w] + dist_w[v]\n                if dist_u[v] > d:\n                    dist_u[v] = d\n                    pred[u][v] = pred[w][v]\n    return (dict(pred), dict(dist))"
 },
 {
  "docstring": "Reconstruct a path from source to target using the predecessors\ndict as returned by floyd_warshall_predecessor_and_distance\n\nParameters\n----------\nsource : node\n   Starting node for path\n\ntarget : node\n   Ending node for path\n\npredecessors: dictionary\n   Dictionary, keyed by source and target, of predecessors in the\n   shortest path, as returned by floyd_warshall_predecessor_and_distance\n\nReturns\n-------\npath : list\n   A list of nodes containing the shortest path from source to target\n\n   If source and target are the same, an empty list is returned\n\n",
  "code": "@nx._dispatch(graphs=None)\ndef reconstruct_path(source, target, predecessors):\n    if source == target:\n        return []\n    prev = predecessors[source]\n    curr = prev[target]\n    path = [target, curr]\n    while curr != source:\n        curr = prev[curr]\n        path.append(curr)\n    return list(reversed(path))"
 },
 {
  "docstring": "Find all-pairs shortest path lengths using Floyd's algorithm.\n\nParameters\n----------\nG : NetworkX graph\n\nweight: string, optional (default= 'weight')\n   Edge data key corresponding to the edge weight.\n\n\nReturns\n-------\ndistance : dict\n   A dictionary,  keyed by source and target, of shortest paths distances\n   between nodes.\n\nExamples\n--------\n>>> G = nx.DiGraph()\n>>> G.add_weighted_edges_from([(0, 1, 5), (1, 2, 2), (2, 3, -3), (1, 3, 10), (3, 2, 8)])\n>>> fw = nx.floyd_warshall(G, weight='weight')\n>>> results = {a: dict(b) for a, b in fw.items()}\n>>> print(results)\n{0: {0: 0, 1: 5, 2: 7, 3: 4}, 1: {1: 0, 2: 2, 3: -1, 0: inf}, 2: {2: 0, 3: -3, 0: inf, 1: inf}, 3: {3: 0, 2: 8, 0: inf, 1: inf}}\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef floyd_warshall(G, weight='weight'):\n    return floyd_warshall_predecessor_and_distance(G, weight=weight)[1]"
 },
 {
  "docstring": "Returns *True* if *G* has a path from *source* to *target*.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node\n   Starting node for path\n\ntarget : node\n   Ending node for path",
  "code": "@nx._dispatch\ndef has_path(G, source, target):\n    try:\n        nx.shortest_path(G, source, target)\n    except nx.NetworkXNoPath:\n        return False\n    return True"
 },
 {
  "docstring": "Compute shortest paths in the graph.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node, optional\n    Starting node for path. If not specified, compute shortest\n    paths for each possible starting node.\n\ntarget : node, optional\n    Ending node for path. If not specified, compute shortest\n    paths to all possible nodes.\n\nweight : None, string or function, optional (default = None)\n    If None, every edge has weight/distance/cost 1.\n    If a string, use this edge attribute as the edge weight.\n    Any edge attribute not present defaults to 1.\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly\n    three positional arguments: the two endpoints of an edge and\n    the dictionary of edge attributes for that edge.\n    The function must return a number.\n\nmethod : string, optional (default = 'dijkstra')\n    The algorithm to use to compute the path.\n    Supported options: 'dijkstra', 'bellman-ford'.\n    Other inputs produce a ValueError.\n    If `weight` is None, unweighted graph methods are used, and this\n    suggestion is ignored.\n\nReturns\n-------\npath: list or dictionary\n    All returned paths include both the source and target in the path.\n\n    If the source and target are both specified, return a single list\n    of nodes in a shortest path from the source to the target.\n\n    If only the source is specified, return a dictionary keyed by\n    targets with a list of nodes in a shortest path from the source\n    to one of the targets.\n\n    If only the target is specified, return a dictionary keyed by\n    sources with a list of nodes in a shortest path from one of the\n    sources to the target.\n\n    If neither the source nor target are specified return a dictionary\n    of dictionaries with path[source][target]=[list of nodes in path].\n\nRaises\n------\nNodeNotFound\n    If `source` is not in `G`.\n\nValueError\n    If `method` is not among the supported options.\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> print(nx.shortest_path(G, source=0, target=4))\n[0, 1, 2, 3, 4]\n>>> p = nx.shortest_path(G, source=0)  # target not specified\n>>> p[3] # shortest path from source=0 to target=3\n[0, 1, 2, 3]\n>>> p = nx.shortest_path(G, target=4)  # source not specified\n>>> p[1] # shortest path from source=1 to target=4\n[1, 2, 3, 4]\n>>> p = nx.shortest_path(G)  # source, target not specified\n>>> p[2][4] # shortest path from source=2 to target=4\n[2, 3, 4]\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef shortest_path(G, source=None, target=None, weight=None, method='dijkstra'):\n    if method not in ('dijkstra', 'bellman-ford'):\n        raise ValueError(f'method not supported: {method}')\n    method = 'unweighted' if weight is None else method\n    if source is None:\n        if target is None:\n            msg = 'shortest_path for all_pairs will return an iterator in v3.3'\n            warnings.warn(msg, DeprecationWarning)\n            if method == 'unweighted':\n                paths = dict(nx.all_pairs_shortest_path(G))\n            elif method == 'dijkstra':\n                paths = dict(nx.all_pairs_dijkstra_path(G, weight=weight))\n            else:\n                paths = dict(nx.all_pairs_bellman_ford_path(G, weight=weight))\n        else:\n            if G.is_directed():\n                G = G.reverse(copy=False)\n            if method == 'unweighted':\n                paths = nx.single_source_shortest_path(G, target)\n            elif method == 'dijkstra':\n                paths = nx.single_source_dijkstra_path(G, target, weight=weight)\n            else:\n                paths = nx.single_source_bellman_ford_path(G, target, weight=weight)\n            for target in paths:\n                paths[target] = list(reversed(paths[target]))\n    elif target is None:\n        if method == 'unweighted':\n            paths = nx.single_source_shortest_path(G, source)\n        elif method == 'dijkstra':\n            paths = nx.single_source_dijkstra_path(G, source, weight=weight)\n        else:\n            paths = nx.single_source_bellman_ford_path(G, source, weight=weight)\n    elif method == 'unweighted':\n        paths = nx.bidirectional_shortest_path(G, source, target)\n    elif method == 'dijkstra':\n        _, paths = nx.bidirectional_dijkstra(G, source, target, weight)\n    else:\n        paths = nx.bellman_ford_path(G, source, target, weight)\n    return paths"
 },
 {
  "docstring": "Compute shortest path lengths in the graph.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node, optional\n    Starting node for path.\n    If not specified, compute shortest path lengths using all nodes as\n    source nodes.\n\ntarget : node, optional\n    Ending node for path.\n    If not specified, compute shortest path lengths using all nodes as\n    target nodes.\n\nweight : None, string or function, optional (default = None)\n    If None, every edge has weight/distance/cost 1.\n    If a string, use this edge attribute as the edge weight.\n    Any edge attribute not present defaults to 1.\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly\n    three positional arguments: the two endpoints of an edge and\n    the dictionary of edge attributes for that edge.\n    The function must return a number.\n\nmethod : string, optional (default = 'dijkstra')\n    The algorithm to use to compute the path length.\n    Supported options: 'dijkstra', 'bellman-ford'.\n    Other inputs produce a ValueError.\n    If `weight` is None, unweighted graph methods are used, and this\n    suggestion is ignored.\n\nReturns\n-------\nlength: int or iterator\n    If the source and target are both specified, return the length of\n    the shortest path from the source to the target.\n\n    If only the source is specified, return a dict keyed by target\n    to the shortest path length from the source to that target.\n\n    If only the target is specified, return a dict keyed by source\n    to the shortest path length from that source to the target.\n\n    If neither the source nor target are specified, return an iterator\n    over (source, dictionary) where dictionary is keyed by target to\n    shortest path length from source to that target.\n\nRaises\n------\nNodeNotFound\n    If `source` is not in `G`.\n\nNetworkXNoPath\n    If no path exists between source and target.\n\nValueError\n    If `method` is not among the supported options.\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> nx.shortest_path_length(G, source=0, target=4)\n4\n>>> p = nx.shortest_path_length(G, source=0)  # target not specified\n>>> p[4]\n4\n>>> p = nx.shortest_path_length(G, target=4)  # source not specified\n>>> p[0]\n4\n>>> p = dict(nx.shortest_path_length(G))  # source,target not specified\n>>> p[0][4]\n4\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef shortest_path_length(G, source=None, target=None, weight=None, method='dijkstra'):\n    if method not in ('dijkstra', 'bellman-ford'):\n        raise ValueError(f'method not supported: {method}')\n    method = 'unweighted' if weight is None else method\n    if source is None:\n        if target is None:\n            if method == 'unweighted':\n                paths = nx.all_pairs_shortest_path_length(G)\n            elif method == 'dijkstra':\n                paths = nx.all_pairs_dijkstra_path_length(G, weight=weight)\n            else:\n                paths = nx.all_pairs_bellman_ford_path_length(G, weight=weight)\n        else:\n            if G.is_directed():\n                G = G.reverse(copy=False)\n            if method == 'unweighted':\n                path_length = nx.single_source_shortest_path_length\n                paths = path_length(G, target)\n            elif method == 'dijkstra':\n                path_length = nx.single_source_dijkstra_path_length\n                paths = path_length(G, target, weight=weight)\n            else:\n                path_length = nx.single_source_bellman_ford_path_length\n                paths = path_length(G, target, weight=weight)\n    elif target is None:\n        if method == 'unweighted':\n            paths = nx.single_source_shortest_path_length(G, source)\n        elif method == 'dijkstra':\n            path_length = nx.single_source_dijkstra_path_length\n            paths = path_length(G, source, weight=weight)\n        else:\n            path_length = nx.single_source_bellman_ford_path_length\n            paths = path_length(G, source, weight=weight)\n    elif method == 'unweighted':\n        p = nx.bidirectional_shortest_path(G, source, target)\n        paths = len(p) - 1\n    elif method == 'dijkstra':\n        paths = nx.dijkstra_path_length(G, source, target, weight)\n    else:\n        paths = nx.bellman_ford_path_length(G, source, target, weight)\n    return paths"
 },
 {
  "docstring": "Returns the average shortest path length.\n\nThe average shortest path length is\n\n.. math::\n\n   a =\\sum_{\\substack{s,t \\in V \\\\ s\\neq t}} \\frac{d(s, t)}{n(n-1)}\n\nwhere `V` is the set of nodes in `G`,\n`d(s, t)` is the shortest path from `s` to `t`,\nand `n` is the number of nodes in `G`.\n\n.. versionchanged:: 3.0\n   An exception is raised for directed graphs that are not strongly\n   connected.\n\nParameters\n----------\nG : NetworkX graph\n\nweight : None, string or function, optional (default = None)\n    If None, every edge has weight/distance/cost 1.\n    If a string, use this edge attribute as the edge weight.\n    Any edge attribute not present defaults to 1.\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly\n    three positional arguments: the two endpoints of an edge and\n    the dictionary of edge attributes for that edge.\n    The function must return a number.\n\nmethod : string, optional (default = 'unweighted' or 'dijkstra')\n    The algorithm to use to compute the path lengths.\n    Supported options are 'unweighted', 'dijkstra', 'bellman-ford',\n    'floyd-warshall' and 'floyd-warshall-numpy'.\n    Other method values produce a ValueError.\n    The default method is 'unweighted' if `weight` is None,\n    otherwise the default method is 'dijkstra'.\n\nRaises\n------\nNetworkXPointlessConcept\n    If `G` is the null graph (that is, the graph on zero nodes).\n\nNetworkXError\n    If `G` is not connected (or not strongly connected, in the case\n    of a directed graph).\n\nValueError\n    If `method` is not among the supported options.\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> nx.average_shortest_path_length(G)\n2.0\n\nFor disconnected graphs, you can compute the average shortest path\nlength for each component\n\n>>> G = nx.Graph([(1, 2), (3, 4)])\n>>> for C in (G.subgraph(c).copy() for c in nx.connected_components(G)):\n...     print(nx.average_shortest_path_length(C))\n1.0\n1.0",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef average_shortest_path_length(G, weight=None, method=None):\n    single_source_methods = ['unweighted', 'dijkstra', 'bellman-ford']\n    all_pairs_methods = ['floyd-warshall', 'floyd-warshall-numpy']\n    supported_methods = single_source_methods + all_pairs_methods\n    if method is None:\n        method = 'unweighted' if weight is None else 'dijkstra'\n    if method not in supported_methods:\n        raise ValueError(f'method not supported: {method}')\n    n = len(G)\n    if n == 0:\n        msg = 'the null graph has no paths, thus there is no average shortest path length'\n        raise nx.NetworkXPointlessConcept(msg)\n    if n == 1:\n        return 0\n    if G.is_directed() and (not nx.is_strongly_connected(G)):\n        raise nx.NetworkXError('Graph is not strongly connected.')\n    if not G.is_directed() and (not nx.is_connected(G)):\n        raise nx.NetworkXError('Graph is not connected.')\n\n    def path_length(v):\n        if method == 'unweighted':\n            return nx.single_source_shortest_path_length(G, v)\n        elif method == 'dijkstra':\n            return nx.single_source_dijkstra_path_length(G, v, weight=weight)\n        elif method == 'bellman-ford':\n            return nx.single_source_bellman_ford_path_length(G, v, weight=weight)\n    if method in single_source_methods:\n        s = sum((l for u in G for l in path_length(u).values()))\n    elif method == 'floyd-warshall':\n        all_pairs = nx.floyd_warshall(G, weight=weight)\n        s = sum((sum(t.values()) for t in all_pairs.values()))\n    elif method == 'floyd-warshall-numpy':\n        s = nx.floyd_warshall_numpy(G, weight=weight).sum()\n    return s / (n * (n - 1))"
 },
 {
  "docstring": "Compute all shortest simple paths in the graph.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node\n   Starting node for path.\n\ntarget : node\n   Ending node for path.\n\nweight : None, string or function, optional (default = None)\n    If None, every edge has weight/distance/cost 1.\n    If a string, use this edge attribute as the edge weight.\n    Any edge attribute not present defaults to 1.\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly\n    three positional arguments: the two endpoints of an edge and\n    the dictionary of edge attributes for that edge.\n    The function must return a number.\n\nmethod : string, optional (default = 'dijkstra')\n   The algorithm to use to compute the path lengths.\n   Supported options: 'dijkstra', 'bellman-ford'.\n   Other inputs produce a ValueError.\n   If `weight` is None, unweighted graph methods are used, and this\n   suggestion is ignored.\n\nReturns\n-------\npaths : generator of lists\n    A generator of all paths between source and target.\n\nRaises\n------\nValueError\n    If `method` is not among the supported options.\n\nNetworkXNoPath\n    If `target` cannot be reached from `source`.\n\nExamples\n--------\n>>> G = nx.Graph()\n>>> nx.add_path(G, [0, 1, 2])\n>>> nx.add_path(G, [0, 10, 2])\n>>> print([p for p in nx.all_shortest_paths(G, source=0, target=2)])\n[[0, 1, 2], [0, 10, 2]]\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef all_shortest_paths(G, source, target, weight=None, method='dijkstra'):\n    method = 'unweighted' if weight is None else method\n    if method == 'unweighted':\n        pred = nx.predecessor(G, source)\n    elif method == 'dijkstra':\n        pred, dist = nx.dijkstra_predecessor_and_distance(G, source, weight=weight)\n    elif method == 'bellman-ford':\n        pred, dist = nx.bellman_ford_predecessor_and_distance(G, source, weight=weight)\n    else:\n        raise ValueError(f'method not supported: {method}')\n    return _build_paths_from_predecessors({source}, target, pred)"
 },
 {
  "docstring": "Compute all shortest simple paths from the given source in the graph.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node\n   Starting node for path.\n\nweight : None, string or function, optional (default = None)\n    If None, every edge has weight/distance/cost 1.\n    If a string, use this edge attribute as the edge weight.\n    Any edge attribute not present defaults to 1.\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly\n    three positional arguments: the two endpoints of an edge and\n    the dictionary of edge attributes for that edge.\n    The function must return a number.\n\nmethod : string, optional (default = 'dijkstra')\n   The algorithm to use to compute the path lengths.\n   Supported options: 'dijkstra', 'bellman-ford'.\n   Other inputs produce a ValueError.\n   If `weight` is None, unweighted graph methods are used, and this\n   suggestion is ignored.\n\nReturns\n-------\npaths : generator of dictionary\n    A generator of all paths between source and all nodes in the graph.\n\nRaises\n------\nValueError\n    If `method` is not among the supported options.\n\nExamples\n--------\n>>> G = nx.Graph()\n>>> nx.add_path(G, [0, 1, 2, 3, 0])\n>>> dict(nx.single_source_all_shortest_paths(G, source=0))\n{0: [[0]], 1: [[0, 1]], 2: [[0, 1, 2], [0, 3, 2]], 3: [[0, 3]]}\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef single_source_all_shortest_paths(G, source, weight=None, method='dijkstra'):\n    method = 'unweighted' if weight is None else method\n    if method == 'unweighted':\n        pred = nx.predecessor(G, source)\n    elif method == 'dijkstra':\n        pred, dist = nx.dijkstra_predecessor_and_distance(G, source, weight=weight)\n    elif method == 'bellman-ford':\n        pred, dist = nx.bellman_ford_predecessor_and_distance(G, source, weight=weight)\n    else:\n        raise ValueError(f'method not supported: {method}')\n    for n in G:\n        try:\n            yield (n, list(_build_paths_from_predecessors({source}, n, pred)))\n        except nx.NetworkXNoPath:\n            pass"
 },
 {
  "docstring": "Compute all shortest paths between all nodes.\n\nParameters\n----------\nG : NetworkX graph\n\nweight : None, string or function, optional (default = None)\n    If None, every edge has weight/distance/cost 1.\n    If a string, use this edge attribute as the edge weight.\n    Any edge attribute not present defaults to 1.\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly\n    three positional arguments: the two endpoints of an edge and\n    the dictionary of edge attributes for that edge.\n    The function must return a number.\n\nmethod : string, optional (default = 'dijkstra')\n   The algorithm to use to compute the path lengths.\n   Supported options: 'dijkstra', 'bellman-ford'.\n   Other inputs produce a ValueError.\n   If `weight` is None, unweighted graph methods are used, and this\n   suggestion is ignored.\n\nReturns\n-------\npaths : generator of dictionary\n    Dictionary of arrays, keyed by source and target, of all shortest paths.\n\nRaises\n------\nValueError\n    If `method` is not among the supported options.\n\nExamples\n--------\n>>> G = nx.cycle_graph(4)\n>>> dict(nx.all_pairs_all_shortest_paths(G))[0][2]\n[[0, 1, 2], [0, 3, 2]]\n>>> dict(nx.all_pairs_all_shortest_paths(G))[0][3]\n[[0, 3]]\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef all_pairs_all_shortest_paths(G, weight=None, method='dijkstra'):\n    for n in G:\n        yield (n, dict(single_source_all_shortest_paths(G, n, weight=weight, method=method)))"
 },
 {
  "docstring": "Compute all simple paths to target, given the predecessors found in\npred, terminating when any source in sources is found.\n\nParameters\n----------\nsources : set\n   Starting nodes for path.\n\ntarget : node\n   Ending node for path.\n\npred : dict\n   A dictionary of predecessor lists, keyed by node\n\nReturns\n-------\npaths : generator of lists\n    A generator of all paths between source and target.\n\nRaises\n------\nNetworkXNoPath\n    If `target` cannot be reached from `source`.\n\n",
  "code": "def _build_paths_from_predecessors(sources, target, pred):\n    if target not in pred:\n        raise nx.NetworkXNoPath(f'Target {target} cannot be reached from given sources')\n    seen = {target}\n    stack = [[target, 0]]\n    top = 0\n    while top >= 0:\n        node, i = stack[top]\n        if node in sources:\n            yield [p for p, n in reversed(stack[:top + 1])]\n        if len(pred[node]) > i:\n            stack[top][1] = i + 1\n            next = pred[node][i]\n            if next in seen:\n                continue\n            else:\n                seen.add(next)\n            top += 1\n            if top == len(stack):\n                stack.append([next, 0])\n            else:\n                stack[top][:] = [next, 0]\n        else:\n            seen.discard(node)\n            top -= 1"
 },
 {
  "docstring": "Compute the shortest path lengths from source to all reachable nodes.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node\n   Starting node for path\n\ncutoff : integer, optional\n    Depth to stop the search. Only paths of length <= cutoff are returned.\n\nReturns\n-------\nlengths : dict\n    Dict keyed by node to shortest path length to source.\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> length = nx.single_source_shortest_path_length(G, 0)\n>>> length[4]\n4\n>>> for node in length:\n...     print(f\"{node}: {length[node]}\")\n0: 0\n1: 1\n2: 2\n3: 3\n4: 4\n\nSee Also\n--------\nshortest_path_length",
  "code": "@nx._dispatch\ndef single_source_shortest_path_length(G, source, cutoff=None):\n    if source not in G:\n        raise nx.NodeNotFound(f'Source {source} is not in G')\n    if cutoff is None:\n        cutoff = float('inf')\n    nextlevel = [source]\n    return dict(_single_shortest_path_length(G._adj, nextlevel, cutoff))"
 },
 {
  "docstring": "Yields (node, level) in a breadth first search\n\nShortest Path Length helper function\nParameters\n----------\n    adj : dict\n        Adjacency dict or view\n    firstlevel : list\n        starting nodes, e.g. [source] or [target]\n    cutoff : int or float\n        level at which we stop the process",
  "code": "def _single_shortest_path_length(adj, firstlevel, cutoff):\n    seen = set(firstlevel)\n    nextlevel = firstlevel\n    level = 0\n    n = len(adj)\n    for v in nextlevel:\n        yield (v, level)\n    while nextlevel and cutoff > level:\n        level += 1\n        thislevel = nextlevel\n        nextlevel = []\n        for v in thislevel:\n            for w in adj[v]:\n                if w not in seen:\n                    seen.add(w)\n                    nextlevel.append(w)\n                    yield (w, level)\n            if len(seen) == n:\n                return"
 },
 {
  "docstring": "Compute the shortest path lengths to target from all reachable nodes.\n\nParameters\n----------\nG : NetworkX graph\n\ntarget : node\n   Target node for path\n\ncutoff : integer, optional\n    Depth to stop the search. Only paths of length <= cutoff are returned.\n\nReturns\n-------\nlengths : iterator\n    (source, shortest path length) iterator\n\nExamples\n--------\n>>> G = nx.path_graph(5, create_using=nx.DiGraph())\n>>> length = dict(nx.single_target_shortest_path_length(G, 4))\n>>> length[0]\n4\n>>> for node in range(5):\n...     print(f\"{node}: {length[node]}\")\n0: 4\n1: 3\n2: 2\n3: 1\n4: 0\n\nSee Also\n--------\nsingle_source_shortest_path_length, shortest_path_length",
  "code": "@nx._dispatch\ndef single_target_shortest_path_length(G, target, cutoff=None):\n    if target not in G:\n        raise nx.NodeNotFound(f'Target {target} is not in G')\n    msg = 'single_target_shortest_path_length will return a dict starting in v3.3'\n    warnings.warn(msg, DeprecationWarning)\n    if cutoff is None:\n        cutoff = float('inf')\n    adj = G._pred if G.is_directed() else G._adj\n    nextlevel = [target]\n    return _single_shortest_path_length(adj, nextlevel, cutoff)"
 },
 {
  "docstring": "Computes the shortest path lengths between all nodes in `G`.\n\nParameters\n----------\nG : NetworkX graph\n\ncutoff : integer, optional\n    Depth at which to stop the search. Only paths of length at most\n    `cutoff` are returned.\n\nReturns\n-------\nlengths : iterator\n    (source, dictionary) iterator with dictionary keyed by target and\n    shortest path length as the key value.\n\n",
  "code": "@nx._dispatch\ndef all_pairs_shortest_path_length(G, cutoff=None):\n    length = single_source_shortest_path_length\n    for n in G:\n        yield (n, length(G, n, cutoff=cutoff))"
 },
 {
  "docstring": "Returns a list of nodes in a shortest path between source and target.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node label\n   starting node for path\n\ntarget : node label\n   ending node for path\n\nReturns\n-------\npath: list\n   List of nodes in a path from source to target.\n\nRaises\n------\nNetworkXNoPath\n   If no path exists between source and target.\n\nExamples\n--------\n>>> G = nx.Graph()\n>>> nx.add_path(G, [0, 1, 2, 3, 0, 4, 5, 6, 7, 4])\n>>> nx.bidirectional_shortest_path(G, 2, 6)\n[2, 1, 0, 4, 5, 6]\n\nSee Also\n--------\nshortest_path\n\n",
  "code": "@nx._dispatch\ndef bidirectional_shortest_path(G, source, target):\n    if source not in G or target not in G:\n        msg = f'Either source {source} or target {target} is not in G'\n        raise nx.NodeNotFound(msg)\n    results = _bidirectional_pred_succ(G, source, target)\n    pred, succ, w = results\n    path = []\n    while w is not None:\n        path.append(w)\n        w = pred[w]\n    path.reverse()\n    w = succ[path[-1]]\n    while w is not None:\n        path.append(w)\n        w = succ[w]\n    return path"
 },
 {
  "docstring": "Bidirectional shortest path helper.\n\nReturns (pred, succ, w) where\npred is a dictionary of predecessors from w to the source, and\nsucc is a dictionary of successors from w to the target.",
  "code": "def _bidirectional_pred_succ(G, source, target):\n    if target == source:\n        return ({target: None}, {source: None}, source)\n    if G.is_directed():\n        Gpred = G.pred\n        Gsucc = G.succ\n    else:\n        Gpred = G.adj\n        Gsucc = G.adj\n    pred = {source: None}\n    succ = {target: None}\n    forward_fringe = [source]\n    reverse_fringe = [target]\n    while forward_fringe and reverse_fringe:\n        if len(forward_fringe) <= len(reverse_fringe):\n            this_level = forward_fringe\n            forward_fringe = []\n            for v in this_level:\n                for w in Gsucc[v]:\n                    if w not in pred:\n                        forward_fringe.append(w)\n                        pred[w] = v\n                    if w in succ:\n                        return (pred, succ, w)\n        else:\n            this_level = reverse_fringe\n            reverse_fringe = []\n            for v in this_level:\n                for w in Gpred[v]:\n                    if w not in succ:\n                        succ[w] = v\n                        reverse_fringe.append(w)\n                    if w in pred:\n                        return (pred, succ, w)\n    raise nx.NetworkXNoPath(f'No path between {source} and {target}.')"
 },
 {
  "docstring": "Compute shortest path between source\nand all other nodes reachable from source.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node label\n   Starting node for path\n\ncutoff : integer, optional\n    Depth to stop the search. Only paths of length <= cutoff are returned.\n\nReturns\n-------\npaths : dictionary\n    Dictionary, keyed by target, of shortest paths.\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> path = nx.single_source_shortest_path(G, 0)\n>>> path[4]\n[0, 1, 2, 3, 4]\n\n",
  "code": "@nx._dispatch\ndef single_source_shortest_path(G, source, cutoff=None):\n    if source not in G:\n        raise nx.NodeNotFound(f'Source {source} not in G')\n\n    def join(p1, p2):\n        return p1 + p2\n    if cutoff is None:\n        cutoff = float('inf')\n    nextlevel = {source: 1}\n    paths = {source: [source]}\n    return dict(_single_shortest_path(G.adj, nextlevel, paths, cutoff, join))"
 },
 {
  "docstring": "Returns shortest paths\n\nShortest Path helper function\nParameters\n----------\n    adj : dict\n        Adjacency dict or view\n    firstlevel : dict\n        starting nodes, e.g. {source: 1} or {target: 1}\n    paths : dict\n        paths for starting nodes, e.g. {source: [source]}\n    cutoff : int or float\n        level at which we stop the process\n    join : function\n        function to construct a path from two partial paths. Requires two\n        list inputs `p1` and `p2`, and returns a list. Usually returns\n        `p1 + p2` (forward from source) or `p2 + p1` (backward from target)",
  "code": "def _single_shortest_path(adj, firstlevel, paths, cutoff, join):\n    level = 0\n    nextlevel = firstlevel\n    while nextlevel and cutoff > level:\n        thislevel = nextlevel\n        nextlevel = {}\n        for v in thislevel:\n            for w in adj[v]:\n                if w not in paths:\n                    paths[w] = join(paths[v], [w])\n                    nextlevel[w] = 1\n        level += 1\n    return paths"
 },
 {
  "docstring": "Compute shortest path to target from all nodes that reach target.\n\nParameters\n----------\nG : NetworkX graph\n\ntarget : node label\n   Target node for path\n\ncutoff : integer, optional\n    Depth to stop the search. Only paths of length <= cutoff are returned.\n\nReturns\n-------\npaths : dictionary\n    Dictionary, keyed by target, of shortest paths.\n\nExamples\n--------\n>>> G = nx.path_graph(5, create_using=nx.DiGraph())\n>>> path = nx.single_target_shortest_path(G, 4)\n>>> path[0]\n[0, 1, 2, 3, 4]\n\n",
  "code": "@nx._dispatch\ndef single_target_shortest_path(G, target, cutoff=None):\n    if target not in G:\n        raise nx.NodeNotFound(f'Target {target} not in G')\n\n    def join(p1, p2):\n        return p2 + p1\n    adj = G.pred if G.is_directed() else G.adj\n    if cutoff is None:\n        cutoff = float('inf')\n    nextlevel = {target: 1}\n    paths = {target: [target]}\n    return dict(_single_shortest_path(adj, nextlevel, paths, cutoff, join))"
 },
 {
  "docstring": "Compute shortest paths between all nodes.\n\nParameters\n----------\nG : NetworkX graph\n\ncutoff : integer, optional\n    Depth at which to stop the search. Only paths of length at most\n    `cutoff` are returned.\n\nReturns\n-------\npaths : iterator\n    Dictionary, keyed by source and target, of shortest paths.\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> path = dict(nx.all_pairs_shortest_path(G))\n>>> print(path[0][4])\n[0, 1, 2, 3, 4]\n\n",
  "code": "@nx._dispatch\ndef all_pairs_shortest_path(G, cutoff=None):\n    for n in G:\n        yield (n, single_source_shortest_path(G, n, cutoff=cutoff))"
 },
 {
  "docstring": "Returns dict of predecessors for the path from source to all nodes in G.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node label\n   Starting node for path\n\ntarget : node label, optional\n   Ending node for path. If provided only predecessors between\n   source and target are returned\n\ncutoff : integer, optional\n    Depth to stop the search. Only paths of length <= cutoff are returned.\n\nreturn_seen : bool, optional (default=None)\n    Whether to return a dictionary, keyed by node, of the level (number of\n    hops) to reach the node (as seen during breadth-first-search).\n\nReturns\n-------\npred : dictionary\n    Dictionary, keyed by node, of predecessors in the shortest path.\n\n\n(pred, seen): tuple of dictionaries\n    If `return_seen` argument is set to `True`, then a tuple of dictionaries\n    is returned. The first element is the dictionary, keyed by node, of\n    predecessors in the shortest path. The second element is the dictionary,\n    keyed by node, of the level (number of hops) to reach the node (as seen\n    during breadth-first-search).\n\nExamples\n--------\n>>> G = nx.path_graph(4)\n>>> list(G)\n[0, 1, 2, 3]\n>>> nx.predecessor(G, 0)\n{0: [], 1: [0], 2: [1], 3: [2]}\n>>> nx.predecessor(G, 0, return_seen=True)\n({0: [], 1: [0], 2: [1], 3: [2]}, {0: 0, 1: 1, 2: 2, 3: 3})",
  "code": "@nx._dispatch\ndef predecessor(G, source, target=None, cutoff=None, return_seen=None):\n    if source not in G:\n        raise nx.NodeNotFound(f'Source {source} not in G')\n    level = 0\n    nextlevel = [source]\n    seen = {source: level}\n    pred = {source: []}\n    while nextlevel:\n        level = level + 1\n        thislevel = nextlevel\n        nextlevel = []\n        for v in thislevel:\n            for w in G[v]:\n                if w not in seen:\n                    pred[w] = [v]\n                    seen[w] = level\n                    nextlevel.append(w)\n                elif seen[w] == level:\n                    pred[w].append(v)\n        if cutoff and cutoff <= level:\n            break\n    if target is not None:\n        if return_seen:\n            if target not in pred:\n                return ([], -1)\n            return (pred[target], seen[target])\n        else:\n            if target not in pred:\n                return []\n            return pred[target]\n    elif return_seen:\n        return (pred, seen)\n    else:\n        return pred"
 },
 {
  "docstring": "Returns a function that returns the weight of an edge.\n\nThe returned function is specifically suitable for input to\nfunctions :func:`_dijkstra` and :func:`_bellman_ford_relaxation`.\n\nParameters\n----------\nG : NetworkX graph.\n\nweight : string or function\n    If it is callable, `weight` itself is returned. If it is a string,\n    it is assumed to be the name of the edge attribute that represents\n    the weight of an edge. In that case, a function is returned that\n    gets the edge weight according to the specified edge attribute.\n\nReturns\n-------\nfunction\n    This function returns a callable that accepts exactly three inputs:\n    a node, an node adjacent to the first one, and the edge attribute\n    dictionary for the eedge joining those nodes. That function returns\n    a number representing the weight of an edge.\n\nIf `G` is a multigraph, and `weight` is not callable, the\nminimum edge weight over all parallel edges is returned. If any edge\ndoes not have an attribute with key `weight`, it is assumed to\nhave weight one.",
  "code": "def _weight_function(G, weight):\n    if callable(weight):\n        return weight\n    if G.is_multigraph():\n        return lambda u, v, d: min((attr.get(weight, 1) for attr in d.values()))\n    return lambda u, v, data: data.get(weight, 1)"
 },
 {
  "docstring": "Returns the shortest weighted path from source to target in G.\n\nUses Dijkstra's Method to compute the shortest weighted path\nbetween two nodes in a graph.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node\n    Starting node\n\ntarget : node\n    Ending node\n\nweight : string or function\n    If this is a string, then edge weights will be accessed via the\n    edge attribute with this key (that is, the weight of the edge\n    joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\n    such edge attribute exists, the weight of the edge is assumed to\n    be one.\n\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly three\n    positional arguments: the two endpoints of an edge and the\n    dictionary of edge attributes for that edge. The function must\n    return a number or None to indicate a hidden edge.\n\nReturns\n-------\npath : list\n    List of nodes in a shortest path.\n\nRaises\n------\nNodeNotFound\n    If `source` is not in `G`.\n\nNetworkXNoPath\n    If no path exists between source and target.\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> print(nx.dijkstra_path(G, 0, 4))\n[0, 1, 2, 3, 4]\n\nFind edges of shortest path in Multigraph\n\n>>> G = nx.MultiDiGraph()\n>>> G.add_weighted_edges_from([(1, 2, 0.75), (1, 2, 0.5), (2, 3, 0.5), (1, 3, 1.5)])\n>>> nodes = nx.dijkstra_path(G, 1, 3)\n>>> edges = nx.utils.pairwise(nodes)\n>>> list((u, v, min(G[u][v], key=lambda k: G[u][v][k].get('weight', 1))) for u, v in edges)\n[(1, 2, 1), (2, 3, 0)]\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef dijkstra_path(G, source, target, weight='weight'):\n    length, path = single_source_dijkstra(G, source, target=target, weight=weight)\n    return path"
 },
 {
  "docstring": "Returns the shortest weighted path length in G from source to target.\n\nUses Dijkstra's Method to compute the shortest weighted path length\nbetween two nodes in a graph.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node label\n    starting node for path\n\ntarget : node label\n    ending node for path\n\nweight : string or function\n    If this is a string, then edge weights will be accessed via the\n    edge attribute with this key (that is, the weight of the edge\n    joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\n    such edge attribute exists, the weight of the edge is assumed to\n    be one.\n\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly three\n    positional arguments: the two endpoints of an edge and the\n    dictionary of edge attributes for that edge. The function must\n    return a number or None to indicate a hidden edge.\n\nReturns\n-------\nlength : number\n    Shortest path length.\n\nRaises\n------\nNodeNotFound\n    If `source` is not in `G`.\n\nNetworkXNoPath\n    If no path exists between source and target.\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> nx.dijkstra_path_length(G, 0, 4)\n4\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef dijkstra_path_length(G, source, target, weight='weight'):\n    if source not in G:\n        raise nx.NodeNotFound(f'Node {source} not found in graph')\n    if source == target:\n        return 0\n    weight = _weight_function(G, weight)\n    length = _dijkstra(G, source, weight, target=target)\n    try:\n        return length[target]\n    except KeyError as err:\n        raise nx.NetworkXNoPath(f'Node {target} not reachable from {source}') from err"
 },
 {
  "docstring": "Find shortest weighted paths in G from a source node.\n\nCompute shortest path between source and all other reachable\nnodes for a weighted graph.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node\n    Starting node for path.\n\ncutoff : integer or float, optional\n    Length (sum of edge weights) at which the search is stopped.\n    If cutoff is provided, only return paths with summed weight <= cutoff.\n\nweight : string or function\n    If this is a string, then edge weights will be accessed via the\n    edge attribute with this key (that is, the weight of the edge\n    joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\n    such edge attribute exists, the weight of the edge is assumed to\n    be one.\n\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly three\n    positional arguments: the two endpoints of an edge and the\n    dictionary of edge attributes for that edge. The function must\n    return a number or None to indicate a hidden edge.\n\nReturns\n-------\npaths : dictionary\n    Dictionary of shortest path lengths keyed by target.\n\nRaises\n------\nNodeNotFound\n    If `source` is not in `G`.\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> path = nx.single_source_dijkstra_path(G, 0)\n>>> path[4]\n[0, 1, 2, 3, 4]\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef single_source_dijkstra_path(G, source, cutoff=None, weight='weight'):\n    return multi_source_dijkstra_path(G, {source}, cutoff=cutoff, weight=weight)"
 },
 {
  "docstring": "Find shortest weighted path lengths in G from a source node.\n\nCompute the shortest path length between source and all other\nreachable nodes for a weighted graph.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node label\n    Starting node for path\n\ncutoff : integer or float, optional\n    Length (sum of edge weights) at which the search is stopped.\n    If cutoff is provided, only return paths with summed weight <= cutoff.\n\nweight : string or function\n    If this is a string, then edge weights will be accessed via the\n    edge attribute with this key (that is, the weight of the edge\n    joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\n    such edge attribute exists, the weight of the edge is assumed to\n    be one.\n\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly three\n    positional arguments: the two endpoints of an edge and the\n    dictionary of edge attributes for that edge. The function must\n    return a number or None to indicate a hidden edge.\n\nReturns\n-------\nlength : dict\n    Dict keyed by node to shortest path length from source.\n\nRaises\n------\nNodeNotFound\n    If `source` is not in `G`.\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> length = nx.single_source_dijkstra_path_length(G, 0)\n>>> length[4]\n4\n>>> for node in [0, 1, 2, 3, 4]:\n...     print(f\"{node}: {length[node]}\")\n0: 0\n1: 1\n2: 2\n3: 3\n4: 4\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef single_source_dijkstra_path_length(G, source, cutoff=None, weight='weight'):\n    return multi_source_dijkstra_path_length(G, {source}, cutoff=cutoff, weight=weight)"
 },
 {
  "docstring": "Find shortest weighted paths and lengths from a source node.\n\nCompute the shortest path length between source and all other\nreachable nodes for a weighted graph.\n\nUses Dijkstra's algorithm to compute shortest paths and lengths\nbetween a source and all other reachable nodes in a weighted graph.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node label\n    Starting node for path\n\ntarget : node label, optional\n    Ending node for path\n\ncutoff : integer or float, optional\n    Length (sum of edge weights) at which the search is stopped.\n    If cutoff is provided, only return paths with summed weight <= cutoff.\n\n\nweight : string or function\n    If this is a string, then edge weights will be accessed via the\n    edge attribute with this key (that is, the weight of the edge\n    joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\n    such edge attribute exists, the weight of the edge is assumed to\n    be one.\n\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly three\n    positional arguments: the two endpoints of an edge and the\n    dictionary of edge attributes for that edge. The function must\n    return a number or None to indicate a hidden edge.\n\nReturns\n-------\ndistance, path : pair of dictionaries, or numeric and list.\n    If target is None, paths and lengths to all nodes are computed.\n    The return value is a tuple of two dictionaries keyed by target nodes.\n    The first dictionary stores distance to each target node.\n    The second stores the path to each target node.\n    If target is not None, returns a tuple (distance, path), where\n    distance is the distance from source to target and path is a list\n    representing the path from source to target.\n\nRaises\n------\nNodeNotFound\n    If `source` is not in `G`.\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> length, path = nx.single_source_dijkstra(G, 0)\n>>> length[4]\n4\n>>> for node in [0, 1, 2, 3, 4]:\n...     print(f\"{node}: {length[node]}\")\n0: 0\n1: 1\n2: 2\n3: 3\n4: 4\n>>> path[4]\n[0, 1, 2, 3, 4]\n>>> length, path = nx.single_source_dijkstra(G, 0, 1)\n>>> length\n1\n>>> path\n[0, 1]\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef single_source_dijkstra(G, source, target=None, cutoff=None, weight='weight'):\n    return multi_source_dijkstra(G, {source}, cutoff=cutoff, target=target, weight=weight)"
 },
 {
  "docstring": "Find shortest weighted paths in G from a given set of source\nnodes.\n\nCompute shortest path between any of the source nodes and all other\nreachable nodes for a weighted graph.\n\nParameters\n----------\nG : NetworkX graph\n\nsources : non-empty set of nodes\n    Starting nodes for paths. If this is just a set containing a\n    single node, then all paths computed by this function will start\n    from that node. If there are two or more nodes in the set, the\n    computed paths may begin from any one of the start nodes.\n\ncutoff : integer or float, optional\n    Length (sum of edge weights) at which the search is stopped.\n    If cutoff is provided, only return paths with summed weight <= cutoff.\n\nweight : string or function\n    If this is a string, then edge weights will be accessed via the\n    edge attribute with this key (that is, the weight of the edge\n    joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\n    such edge attribute exists, the weight of the edge is assumed to\n    be one.\n\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly three\n    positional arguments: the two endpoints of an edge and the\n    dictionary of edge attributes for that edge. The function must\n    return a number or None to indicate a hidden edge.\n\nReturns\n-------\npaths : dictionary\n    Dictionary of shortest paths keyed by target.\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> path = nx.multi_source_dijkstra_path(G, {0, 4})\n>>> path[1]\n[0, 1]\n>>> path[3]\n[4, 3]\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef multi_source_dijkstra_path(G, sources, cutoff=None, weight='weight'):\n    length, path = multi_source_dijkstra(G, sources, cutoff=cutoff, weight=weight)\n    return path"
 },
 {
  "docstring": "Find shortest weighted path lengths in G from a given set of\nsource nodes.\n\nCompute the shortest path length between any of the source nodes and\nall other reachable nodes for a weighted graph.\n\nParameters\n----------\nG : NetworkX graph\n\nsources : non-empty set of nodes\n    Starting nodes for paths. If this is just a set containing a\n    single node, then all paths computed by this function will start\n    from that node. If there are two or more nodes in the set, the\n    computed paths may begin from any one of the start nodes.\n\ncutoff : integer or float, optional\n    Length (sum of edge weights) at which the search is stopped.\n    If cutoff is provided, only return paths with summed weight <= cutoff.\n\nweight : string or function\n    If this is a string, then edge weights will be accessed via the\n    edge attribute with this key (that is, the weight of the edge\n    joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\n    such edge attribute exists, the weight of the edge is assumed to\n    be one.\n\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly three\n    positional arguments: the two endpoints of an edge and the\n    dictionary of edge attributes for that edge. The function must\n    return a number or None to indicate a hidden edge.\n\nReturns\n-------\nlength : dict\n    Dict keyed by node to shortest path length to nearest source.\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> length = nx.multi_source_dijkstra_path_length(G, {0, 4})\n>>> for node in [0, 1, 2, 3, 4]:\n...     print(f\"{node}: {length[node]}\")\n0: 0\n1: 1\n2: 2\n3: 1\n4: 0\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef multi_source_dijkstra_path_length(G, sources, cutoff=None, weight='weight'):\n    if not sources:\n        raise ValueError('sources must not be empty')\n    for s in sources:\n        if s not in G:\n            raise nx.NodeNotFound(f'Node {s} not found in graph')\n    weight = _weight_function(G, weight)\n    return _dijkstra_multisource(G, sources, weight, cutoff=cutoff)"
 },
 {
  "docstring": "Find shortest weighted paths and lengths from a given set of\nsource nodes.\n\nUses Dijkstra's algorithm to compute the shortest paths and lengths\nbetween one of the source nodes and the given `target`, or all other\nreachable nodes if not specified, for a weighted graph.\n\nParameters\n----------\nG : NetworkX graph\n\nsources : non-empty set of nodes\n    Starting nodes for paths. If this is just a set containing a\n    single node, then all paths computed by this function will start\n    from that node. If there are two or more nodes in the set, the\n    computed paths may begin from any one of the start nodes.\n\ntarget : node label, optional\n    Ending node for path\n\ncutoff : integer or float, optional\n    Length (sum of edge weights) at which the search is stopped.\n    If cutoff is provided, only return paths with summed weight <= cutoff.\n\nweight : string or function\n    If this is a string, then edge weights will be accessed via the\n    edge attribute with this key (that is, the weight of the edge\n    joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\n    such edge attribute exists, the weight of the edge is assumed to\n    be one.\n\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly three\n    positional arguments: the two endpoints of an edge and the\n    dictionary of edge attributes for that edge. The function must\n    return a number or None to indicate a hidden edge.\n\nReturns\n-------\ndistance, path : pair of dictionaries, or numeric and list\n    If target is None, returns a tuple of two dictionaries keyed by node.\n    The first dictionary stores distance from one of the source nodes.\n    The second stores the path from one of the sources to that node.\n    If target is not None, returns a tuple of (distance, path) where\n    distance is the distance from source to target and path is a list\n    representing the path from source to target.\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> length, path = nx.multi_source_dijkstra(G, {0, 4})\n>>> for node in [0, 1, 2, 3, 4]:\n...     print(f\"{node}: {length[node]}\")\n0: 0\n1: 1\n2: 2\n3: 1\n4: 0\n>>> path[1]\n[0, 1]\n>>> path[3]\n[4, 3]\n\n>>> length, path = nx.multi_source_dijkstra(G, {0, 4}, 1)\n>>> length\n1\n>>> path\n[0, 1]\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef multi_source_dijkstra(G, sources, target=None, cutoff=None, weight='weight'):\n    if not sources:\n        raise ValueError('sources must not be empty')\n    for s in sources:\n        if s not in G:\n            raise nx.NodeNotFound(f'Node {s} not found in graph')\n    if target in sources:\n        return (0, [target])\n    weight = _weight_function(G, weight)\n    paths = {source: [source] for source in sources}\n    dist = _dijkstra_multisource(G, sources, weight, paths=paths, cutoff=cutoff, target=target)\n    if target is None:\n        return (dist, paths)\n    try:\n        return (dist[target], paths[target])\n    except KeyError as err:\n        raise nx.NetworkXNoPath(f'No path to {target}.') from err"
 },
 {
  "docstring": "Uses Dijkstra's algorithm to find shortest weighted paths from a\nsingle source.\n\nThis is a convenience function for :func:`_dijkstra_multisource`\nwith all the arguments the same, except the keyword argument\n`sources` set to ``[source]``.",
  "code": "def _dijkstra(G, source, weight, pred=None, paths=None, cutoff=None, target=None):\n    return _dijkstra_multisource(G, [source], weight, pred=pred, paths=paths, cutoff=cutoff, target=target)"
 },
 {
  "docstring": "Uses Dijkstra's algorithm to find shortest weighted paths\n\nParameters\n----------\nG : NetworkX graph\n\nsources : non-empty iterable of nodes\n    Starting nodes for paths. If this is just an iterable containing\n    a single node, then all paths computed by this function will\n    start from that node. If there are two or more nodes in this\n    iterable, the computed paths may begin from any one of the start\n    nodes.\n\nweight: function\n    Function with (u, v, data) input that returns that edge's weight\n    or None to indicate a hidden edge\n\npred: dict of lists, optional(default=None)\n    dict to store a list of predecessors keyed by that node\n    If None, predecessors are not stored.\n\npaths: dict, optional (default=None)\n    dict to store the path list from source to each node, keyed by node.\n    If None, paths are not stored.\n\ntarget : node label, optional\n    Ending node for path. Search is halted when target is found.\n\ncutoff : integer or float, optional\n    Length (sum of edge weights) at which the search is stopped.\n    If cutoff is provided, only return paths with summed weight <= cutoff.\n\nReturns\n-------\ndistance : dictionary\n    A mapping from node to shortest distance to that node from one\n    of the source nodes.\n\nRaises\n------\nNodeNotFound\n    If any of `sources` is not in `G`.\n\n",
  "code": "def _dijkstra_multisource(G, sources, weight, pred=None, paths=None, cutoff=None, target=None):\n    G_succ = G._adj\n    push = heappush\n    pop = heappop\n    dist = {}\n    seen = {}\n    c = count()\n    fringe = []\n    for source in sources:\n        seen[source] = 0\n        push(fringe, (0, next(c), source))\n    while fringe:\n        d, _, v = pop(fringe)\n        if v in dist:\n            continue\n        dist[v] = d\n        if v == target:\n            break\n        for u, e in G_succ[v].items():\n            cost = weight(v, u, e)\n            if cost is None:\n                continue\n            vu_dist = dist[v] + cost\n            if cutoff is not None:\n                if vu_dist > cutoff:\n                    continue\n            if u in dist:\n                u_dist = dist[u]\n                if vu_dist < u_dist:\n                    raise ValueError('Contradictory paths found:', 'negative weights?')\n                elif pred is not None and vu_dist == u_dist:\n                    pred[u].append(v)\n            elif u not in seen or vu_dist < seen[u]:\n                seen[u] = vu_dist\n                push(fringe, (vu_dist, next(c), u))\n                if paths is not None:\n                    paths[u] = paths[v] + [u]\n                if pred is not None:\n                    pred[u] = [v]\n            elif vu_dist == seen[u]:\n                if pred is not None:\n                    pred[u].append(v)\n    return dist"
 },
 {
  "docstring": "Compute weighted shortest path length and predecessors.\n\nUses Dijkstra's Method to obtain the shortest weighted paths\nand return dictionaries of predecessors for each node and\ndistance for each node from the `source`.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node label\n    Starting node for path\n\ncutoff : integer or float, optional\n    Length (sum of edge weights) at which the search is stopped.\n    If cutoff is provided, only return paths with summed weight <= cutoff.\n\nweight : string or function\n    If this is a string, then edge weights will be accessed via the\n    edge attribute with this key (that is, the weight of the edge\n    joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\n    such edge attribute exists, the weight of the edge is assumed to\n    be one.\n\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly three\n    positional arguments: the two endpoints of an edge and the\n    dictionary of edge attributes for that edge. The function must\n    return a number or None to indicate a hidden edge.\n\nReturns\n-------\npred, distance : dictionaries\n    Returns two dictionaries representing a list of predecessors\n    of a node and the distance to each node.\n\nRaises\n------\nNodeNotFound\n    If `source` is not in `G`.\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef dijkstra_predecessor_and_distance(G, source, cutoff=None, weight='weight'):\n    if source not in G:\n        raise nx.NodeNotFound(f'Node {source} is not found in the graph')\n    weight = _weight_function(G, weight)\n    pred = {source: []}\n    return (pred, _dijkstra(G, source, weight, pred=pred, cutoff=cutoff))"
 },
 {
  "docstring": "Find shortest weighted paths and lengths between all nodes.\n\nParameters\n----------\nG : NetworkX graph\n\ncutoff : integer or float, optional\n    Length (sum of edge weights) at which the search is stopped.\n    If cutoff is provided, only return paths with summed weight <= cutoff.\n\nweight : string or function\n    If this is a string, then edge weights will be accessed via the\n    edge attribute with this key (that is, the weight of the edge\n    joining `u` to `v` will be ``G.edge[u][v][weight]``). If no\n    such edge attribute exists, the weight of the edge is assumed to\n    be one.\n\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly three\n    positional arguments: the two endpoints of an edge and the\n    dictionary of edge attributes for that edge. The function must\n    return a number or None to indicate a hidden edge.\n\nYields\n------\n(node, (distance, path)) : (node obj, (dict, dict))\n    Each source node has two associated dicts. The first holds distance\n    keyed by target and the second holds paths keyed by target.\n    (See single_source_dijkstra for the source/target node terminology.)\n    If desired you can apply `dict()` to this function to create a dict\n    keyed by source node to the two dicts.\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> len_path = dict(nx.all_pairs_dijkstra(G))\n>>> len_path[3][0][1]\n2\n>>> for node in [0, 1, 2, 3, 4]:\n...     print(f\"3 - {node}: {len_path[3][0][node]}\")\n3 - 0: 3\n3 - 1: 2\n3 - 2: 1\n3 - 3: 0\n3 - 4: 1\n>>> len_path[3][1][1]\n[3, 2, 1]\n>>> for n, (dist, path) in nx.all_pairs_dijkstra(G):\n...     print(path[1])\n[0, 1]\n[1]\n[2, 1]\n[3, 2, 1]\n[4, 3, 2, 1]\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef all_pairs_dijkstra(G, cutoff=None, weight='weight'):\n    for n in G:\n        dist, path = single_source_dijkstra(G, n, cutoff=cutoff, weight=weight)\n        yield (n, (dist, path))"
 },
 {
  "docstring": "Compute shortest path lengths between all nodes in a weighted graph.\n\nParameters\n----------\nG : NetworkX graph\n\ncutoff : integer or float, optional\n    Length (sum of edge weights) at which the search is stopped.\n    If cutoff is provided, only return paths with summed weight <= cutoff.\n\nweight : string or function\n    If this is a string, then edge weights will be accessed via the\n    edge attribute with this key (that is, the weight of the edge\n    joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\n    such edge attribute exists, the weight of the edge is assumed to\n    be one.\n\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly three\n    positional arguments: the two endpoints of an edge and the\n    dictionary of edge attributes for that edge. The function must\n    return a number or None to indicate a hidden edge.\n\nReturns\n-------\ndistance : iterator\n    (source, dictionary) iterator with dictionary keyed by target and\n    shortest path length as the key value.\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> length = dict(nx.all_pairs_dijkstra_path_length(G))\n>>> for node in [0, 1, 2, 3, 4]:\n...     print(f\"1 - {node}: {length[1][node]}\")\n1 - 0: 1\n1 - 1: 0\n1 - 2: 1\n1 - 3: 2\n1 - 4: 3\n>>> length[3][2]\n1\n>>> length[2][2]\n0\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef all_pairs_dijkstra_path_length(G, cutoff=None, weight='weight'):\n    length = single_source_dijkstra_path_length\n    for n in G:\n        yield (n, length(G, n, cutoff=cutoff, weight=weight))"
 },
 {
  "docstring": "Compute shortest paths between all nodes in a weighted graph.\n\nParameters\n----------\nG : NetworkX graph\n\ncutoff : integer or float, optional\n    Length (sum of edge weights) at which the search is stopped.\n    If cutoff is provided, only return paths with summed weight <= cutoff.\n\nweight : string or function\n    If this is a string, then edge weights will be accessed via the\n    edge attribute with this key (that is, the weight of the edge\n    joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\n    such edge attribute exists, the weight of the edge is assumed to\n    be one.\n\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly three\n    positional arguments: the two endpoints of an edge and the\n    dictionary of edge attributes for that edge. The function must\n    return a number or None to indicate a hidden edge.\n\nReturns\n-------\npaths : iterator\n    (source, dictionary) iterator with dictionary keyed by target and\n    shortest path as the key value.\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> path = dict(nx.all_pairs_dijkstra_path(G))\n>>> path[0][4]\n[0, 1, 2, 3, 4]\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef all_pairs_dijkstra_path(G, cutoff=None, weight='weight'):\n    path = single_source_dijkstra_path\n    for n in G:\n        yield (n, path(G, n, cutoff=cutoff, weight=weight))"
 },
 {
  "docstring": "Compute shortest path lengths and predecessors on shortest paths\nin weighted graphs.\n\nThe algorithm has a running time of $O(mn)$ where $n$ is the number of\nnodes and $m$ is the number of edges.  It is slower than Dijkstra but\ncan handle negative edge weights.\n\nIf a negative cycle is detected, you can use :func:`find_negative_cycle`\nto return the cycle and examine it. Shortest paths are not defined when\na negative cycle exists because once reached, the path can cycle forever\nto build up arbitrarily low weights.\n\nParameters\n----------\nG : NetworkX graph\n    The algorithm works for all types of graphs, including directed\n    graphs and multigraphs.\n\nsource: node label\n    Starting node for path\n\ntarget : node label, optional\n    Ending node for path\n\nweight : string or function\n    If this is a string, then edge weights will be accessed via the\n    edge attribute with this key (that is, the weight of the edge\n    joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\n    such edge attribute exists, the weight of the edge is assumed to\n    be one.\n\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly three\n    positional arguments: the two endpoints of an edge and the\n    dictionary of edge attributes for that edge. The function must\n    return a number.\n\nheuristic : bool\n    Determines whether to use a heuristic to early detect negative\n    cycles at a hopefully negligible cost.\n\nReturns\n-------\npred, dist : dictionaries\n    Returns two dictionaries keyed by node to predecessor in the\n    path and to the distance from the source respectively.\n\nRaises\n------\nNodeNotFound\n    If `source` is not in `G`.\n\nNetworkXUnbounded\n    If the (di)graph contains a negative (di)cycle, the\n    algorithm raises an exception to indicate the presence of the\n    negative (di)cycle.  Note: any negative weight edge in an\n    undirected graph is a negative cycle.\n\nExamples\n--------\n>>> G = nx.path_graph(5, create_using=nx.DiGraph())\n>>> pred, dist = nx.bellman_ford_predecessor_and_distance(G, 0)\n>>> sorted(pred.items())\n[(0, []), (1, [0]), (2, [1]), (3, [2]), (4, [3])]\n>>> sorted(dist.items())\n[(0, 0), (1, 1), (2, 2), (3, 3), (4, 4)]\n\n>>> pred, dist = nx.bellman_ford_predecessor_and_distance(G, 0, 1)\n>>> sorted(pred.items())\n[(0, []), (1, [0]), (2, [1]), (3, [2]), (4, [3])]\n>>> sorted(dist.items())\n[(0, 0), (1, 1), (2, 2), (3, 3), (4, 4)]\n\n>>> G = nx.cycle_graph(5, create_using=nx.DiGraph())\n>>> G[1][2][\"weight\"] = -7\n>>> nx.bellman_ford_predecessor_and_distance(G, 0)\nTraceback (most recent call last):\n    ...\nnetworkx.exception.NetworkXUnbounded: Negative cycle detected.\n\nSee Also\n--------\nfind_negative_cycle\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef bellman_ford_predecessor_and_distance(G, source, target=None, weight='weight', heuristic=False):\n    if source not in G:\n        raise nx.NodeNotFound(f'Node {source} is not found in the graph')\n    weight = _weight_function(G, weight)\n    if G.is_multigraph():\n        if any((weight(u, v, {k: d}) < 0 for u, v, k, d in nx.selfloop_edges(G, keys=True, data=True))):\n            raise nx.NetworkXUnbounded('Negative cycle detected.')\n    elif any((weight(u, v, d) < 0 for u, v, d in nx.selfloop_edges(G, data=True))):\n        raise nx.NetworkXUnbounded('Negative cycle detected.')\n    dist = {source: 0}\n    pred = {source: []}\n    if len(G) == 1:\n        return (pred, dist)\n    weight = _weight_function(G, weight)\n    dist = _bellman_ford(G, [source], weight, pred=pred, dist=dist, target=target, heuristic=heuristic)\n    return (pred, dist)"
 },
 {
  "docstring": "Calls relaxation loop for Bellman\u2013Ford algorithm and builds paths\n\nThis is an implementation of the SPFA variant.\nSee https://en.wikipedia.org/wiki/Shortest_Path_Faster_Algorithm\n\nParameters\n----------\nG : NetworkX graph\n\nsource: list\n    List of source nodes. The shortest path from any of the source\n    nodes will be found if multiple sources are provided.\n\nweight : function\n    The weight of an edge is the value returned by the function. The\n    function must accept exactly three positional arguments: the two\n    endpoints of an edge and the dictionary of edge attributes for\n    that edge. The function must return a number.\n\npred: dict of lists, optional (default=None)\n    dict to store a list of predecessors keyed by that node\n    If None, predecessors are not stored\n\npaths: dict, optional (default=None)\n    dict to store the path list from source to each node, keyed by node\n    If None, paths are not stored\n\ndist: dict, optional (default=None)\n    dict to store distance from source to the keyed node\n    If None, returned dist dict contents default to 0 for every node in the\n    source list\n\ntarget: node label, optional\n    Ending node for path. Path lengths to other destinations may (and\n    probably will) be incorrect.\n\nheuristic : bool\n    Determines whether to use a heuristic to early detect negative\n    cycles at a hopefully negligible cost.\n\nReturns\n-------\ndist : dict\n    Returns a dict keyed by node to the distance from the source.\n    Dicts for paths and pred are in the mutated input dicts by those names.\n\nRaises\n------\nNodeNotFound\n    If any of `source` is not in `G`.\n\nNetworkXUnbounded\n    If the (di)graph contains a negative (di)cycle, the\n    algorithm raises an exception to indicate the presence of the\n    negative (di)cycle.  Note: any negative weight edge in an\n    undirected graph is a negative cycle",
  "code": "def _bellman_ford(G, source, weight, pred=None, paths=None, dist=None, target=None, heuristic=True):\n    if pred is None:\n        pred = {v: [] for v in source}\n    if dist is None:\n        dist = {v: 0 for v in source}\n    negative_cycle_found = _inner_bellman_ford(G, source, weight, pred, dist, heuristic)\n    if negative_cycle_found is not None:\n        raise nx.NetworkXUnbounded('Negative cycle detected.')\n    if paths is not None:\n        sources = set(source)\n        dsts = [target] if target is not None else pred\n        for dst in dsts:\n            gen = _build_paths_from_predecessors(sources, dst, pred)\n            paths[dst] = next(gen)\n    return dist"
 },
 {
  "docstring": "Inner Relaxation loop for Bellman\u2013Ford algorithm.\n\nThis is an implementation of the SPFA variant.\nSee https://en.wikipedia.org/wiki/Shortest_Path_Faster_Algorithm\n\nParameters\n----------\nG : NetworkX graph\n\nsource: list\n    List of source nodes. The shortest path from any of the source\n    nodes will be found if multiple sources are provided.\n\nweight : function\n    The weight of an edge is the value returned by the function. The\n    function must accept exactly three positional arguments: the two\n    endpoints of an edge and the dictionary of edge attributes for\n    that edge. The function must return a number.\n\npred: dict of lists\n    dict to store a list of predecessors keyed by that node\n\ndist: dict, optional (default=None)\n    dict to store distance from source to the keyed node\n    If None, returned dist dict contents default to 0 for every node in the\n    source list\n\nheuristic : bool\n    Determines whether to use a heuristic to early detect negative\n    cycles at a hopefully negligible cost.\n\nReturns\n-------\nnode or None\n    Return a node `v` where processing discovered a negative cycle.\n    If no negative cycle found, return None.\n\nRaises\n------\nNodeNotFound\n    If any of `source` is not in `G`.",
  "code": "def _inner_bellman_ford(G, sources, weight, pred, dist=None, heuristic=True):\n    for s in sources:\n        if s not in G:\n            raise nx.NodeNotFound(f'Source {s} not in G')\n    if pred is None:\n        pred = {v: [] for v in sources}\n    if dist is None:\n        dist = {v: 0 for v in sources}\n    nonexistent_edge = (None, None)\n    pred_edge = {v: None for v in sources}\n    recent_update = {v: nonexistent_edge for v in sources}\n    G_succ = G._adj\n    inf = float('inf')\n    n = len(G)\n    count = {}\n    q = deque(sources)\n    in_q = set(sources)\n    while q:\n        u = q.popleft()\n        in_q.remove(u)\n        if all((pred_u not in in_q for pred_u in pred[u])):\n            dist_u = dist[u]\n            for v, e in G_succ[u].items():\n                dist_v = dist_u + weight(u, v, e)\n                if dist_v < dist.get(v, inf):\n                    if heuristic:\n                        if v in recent_update[u]:\n                            pred[v].append(u)\n                            return v\n                        if v in pred_edge and pred_edge[v] == u:\n                            recent_update[v] = recent_update[u]\n                        else:\n                            recent_update[v] = (u, v)\n                    if v not in in_q:\n                        q.append(v)\n                        in_q.add(v)\n                        count_v = count.get(v, 0) + 1\n                        if count_v == n:\n                            return v\n                        count[v] = count_v\n                    dist[v] = dist_v\n                    pred[v] = [u]\n                    pred_edge[v] = u\n                elif dist.get(v) is not None and dist_v == dist.get(v):\n                    pred[v].append(u)\n    return None"
 },
 {
  "docstring": "Returns the shortest path from source to target in a weighted graph G.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node\n    Starting node\n\ntarget : node\n    Ending node\n\nweight : string or function (default=\"weight\")\n    If this is a string, then edge weights will be accessed via the\n    edge attribute with this key (that is, the weight of the edge\n    joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\n    such edge attribute exists, the weight of the edge is assumed to\n    be one.\n\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly three\n    positional arguments: the two endpoints of an edge and the\n    dictionary of edge attributes for that edge. The function must\n    return a number.\n\nReturns\n-------\npath : list\n    List of nodes in a shortest path.\n\nRaises\n------\nNodeNotFound\n    If `source` is not in `G`.\n\nNetworkXNoPath\n    If no path exists between source and target.\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> nx.bellman_ford_path(G, 0, 4)\n[0, 1, 2, 3, 4]\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef bellman_ford_path(G, source, target, weight='weight'):\n    length, path = single_source_bellman_ford(G, source, target=target, weight=weight)\n    return path"
 },
 {
  "docstring": "Returns the shortest path length from source to target\nin a weighted graph.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node label\n    starting node for path\n\ntarget : node label\n    ending node for path\n\nweight : string or function (default=\"weight\")\n    If this is a string, then edge weights will be accessed via the\n    edge attribute with this key (that is, the weight of the edge\n    joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\n    such edge attribute exists, the weight of the edge is assumed to\n    be one.\n\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly three\n    positional arguments: the two endpoints of an edge and the\n    dictionary of edge attributes for that edge. The function must\n    return a number.\n\nReturns\n-------\nlength : number\n    Shortest path length.\n\nRaises\n------\nNodeNotFound\n    If `source` is not in `G`.\n\nNetworkXNoPath\n    If no path exists between source and target.\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> nx.bellman_ford_path_length(G, 0, 4)\n4\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef bellman_ford_path_length(G, source, target, weight='weight'):\n    if source == target:\n        if source not in G:\n            raise nx.NodeNotFound(f'Node {source} not found in graph')\n        return 0\n    weight = _weight_function(G, weight)\n    length = _bellman_ford(G, [source], weight, target=target)\n    try:\n        return length[target]\n    except KeyError as err:\n        raise nx.NetworkXNoPath(f'node {target} not reachable from {source}') from err"
 },
 {
  "docstring": "Compute shortest path between source and all other reachable\nnodes for a weighted graph.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node\n    Starting node for path.\n\nweight : string or function (default=\"weight\")\n    If this is a string, then edge weights will be accessed via the\n    edge attribute with this key (that is, the weight of the edge\n    joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\n    such edge attribute exists, the weight of the edge is assumed to\n    be one.\n\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly three\n    positional arguments: the two endpoints of an edge and the\n    dictionary of edge attributes for that edge. The function must\n    return a number.\n\nReturns\n-------\npaths : dictionary\n    Dictionary of shortest path lengths keyed by target.\n\nRaises\n------\nNodeNotFound\n    If `source` is not in `G`.\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> path = nx.single_source_bellman_ford_path(G, 0)\n>>> path[4]\n[0, 1, 2, 3, 4]\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef single_source_bellman_ford_path(G, source, weight='weight'):\n    length, path = single_source_bellman_ford(G, source, weight=weight)\n    return path"
 },
 {
  "docstring": "Compute the shortest path length between source and all other\nreachable nodes for a weighted graph.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node label\n    Starting node for path\n\nweight : string or function (default=\"weight\")\n    If this is a string, then edge weights will be accessed via the\n    edge attribute with this key (that is, the weight of the edge\n    joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\n    such edge attribute exists, the weight of the edge is assumed to\n    be one.\n\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly three\n    positional arguments: the two endpoints of an edge and the\n    dictionary of edge attributes for that edge. The function must\n    return a number.\n\nReturns\n-------\nlength : dictionary\n    Dictionary of shortest path length keyed by target\n\nRaises\n------\nNodeNotFound\n    If `source` is not in `G`.\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> length = nx.single_source_bellman_ford_path_length(G, 0)\n>>> length[4]\n4\n>>> for node in [0, 1, 2, 3, 4]:\n...     print(f\"{node}: {length[node]}\")\n0: 0\n1: 1\n2: 2\n3: 3\n4: 4\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef single_source_bellman_ford_path_length(G, source, weight='weight'):\n    weight = _weight_function(G, weight)\n    return _bellman_ford(G, [source], weight)"
 },
 {
  "docstring": "Compute shortest paths and lengths in a weighted graph G.\n\nUses Bellman-Ford algorithm for shortest paths.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node label\n    Starting node for path\n\ntarget : node label, optional\n    Ending node for path\n\nweight : string or function\n    If this is a string, then edge weights will be accessed via the\n    edge attribute with this key (that is, the weight of the edge\n    joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\n    such edge attribute exists, the weight of the edge is assumed to\n    be one.\n\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly three\n    positional arguments: the two endpoints of an edge and the\n    dictionary of edge attributes for that edge. The function must\n    return a number.\n\nReturns\n-------\ndistance, path : pair of dictionaries, or numeric and list\n    If target is None, returns a tuple of two dictionaries keyed by node.\n    The first dictionary stores distance from one of the source nodes.\n    The second stores the path from one of the sources to that node.\n    If target is not None, returns a tuple of (distance, path) where\n    distance is the distance from source to target and path is a list\n    representing the path from source to target.\n\nRaises\n------\nNodeNotFound\n    If `source` is not in `G`.\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> length, path = nx.single_source_bellman_ford(G, 0)\n>>> length[4]\n4\n>>> for node in [0, 1, 2, 3, 4]:\n...     print(f\"{node}: {length[node]}\")\n0: 0\n1: 1\n2: 2\n3: 3\n4: 4\n>>> path[4]\n[0, 1, 2, 3, 4]\n>>> length, path = nx.single_source_bellman_ford(G, 0, 1)\n>>> length\n1\n>>> path\n[0, 1]\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef single_source_bellman_ford(G, source, target=None, weight='weight'):\n    if source == target:\n        if source not in G:\n            raise nx.NodeNotFound(f'Node {source} is not found in the graph')\n        return (0, [source])\n    weight = _weight_function(G, weight)\n    paths = {source: [source]}\n    dist = _bellman_ford(G, [source], weight, paths=paths, target=target)\n    if target is None:\n        return (dist, paths)\n    try:\n        return (dist[target], paths[target])\n    except KeyError as err:\n        msg = f'Node {target} not reachable from {source}'\n        raise nx.NetworkXNoPath(msg) from err"
 },
 {
  "docstring": "Compute shortest path lengths between all nodes in a weighted graph.\n\nParameters\n----------\nG : NetworkX graph\n\nweight : string or function (default=\"weight\")\n    If this is a string, then edge weights will be accessed via the\n    edge attribute with this key (that is, the weight of the edge\n    joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\n    such edge attribute exists, the weight of the edge is assumed to\n    be one.\n\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly three\n    positional arguments: the two endpoints of an edge and the\n    dictionary of edge attributes for that edge. The function must\n    return a number.\n\nReturns\n-------\ndistance : iterator\n    (source, dictionary) iterator with dictionary keyed by target and\n    shortest path length as the key value.\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> length = dict(nx.all_pairs_bellman_ford_path_length(G))\n>>> for node in [0, 1, 2, 3, 4]:\n...     print(f\"1 - {node}: {length[1][node]}\")\n1 - 0: 1\n1 - 1: 0\n1 - 2: 1\n1 - 3: 2\n1 - 4: 3\n>>> length[3][2]\n1\n>>> length[2][2]\n0\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef all_pairs_bellman_ford_path_length(G, weight='weight'):\n    length = single_source_bellman_ford_path_length\n    for n in G:\n        yield (n, dict(length(G, n, weight=weight)))"
 },
 {
  "docstring": "Compute shortest paths between all nodes in a weighted graph.\n\nParameters\n----------\nG : NetworkX graph\n\nweight : string or function (default=\"weight\")\n    If this is a string, then edge weights will be accessed via the\n    edge attribute with this key (that is, the weight of the edge\n    joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\n    such edge attribute exists, the weight of the edge is assumed to\n    be one.\n\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly three\n    positional arguments: the two endpoints of an edge and the\n    dictionary of edge attributes for that edge. The function must\n    return a number.\n\nReturns\n-------\npaths : iterator\n    (source, dictionary) iterator with dictionary keyed by target and\n    shortest path as the key value.\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> path = dict(nx.all_pairs_bellman_ford_path(G))\n>>> path[0][4]\n[0, 1, 2, 3, 4]\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef all_pairs_bellman_ford_path(G, weight='weight'):\n    path = single_source_bellman_ford_path\n    for n in G:\n        yield (n, path(G, n, weight=weight))"
 },
 {
  "docstring": "Compute shortest path lengths and predecessors on shortest paths\nin weighted graphs.\n\nThe algorithm has a running time of $O(mn)$ where $n$ is the number of\nnodes and $m$ is the number of edges.  It is slower than Dijkstra but\ncan handle negative edge weights.\n\nParameters\n----------\nG : NetworkX graph\n    The algorithm works for all types of graphs, including directed\n    graphs and multigraphs.\n\nsource: node label\n    Starting node for path\n\nweight : string or function\n    If this is a string, then edge weights will be accessed via the\n    edge attribute with this key (that is, the weight of the edge\n    joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\n    such edge attribute exists, the weight of the edge is assumed to\n    be one.\n\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly three\n    positional arguments: the two endpoints of an edge and the\n    dictionary of edge attributes for that edge. The function must\n    return a number.\n\nReturns\n-------\npred, dist : dictionaries\n    Returns two dictionaries keyed by node to predecessor in the\n    path and to the distance from the source respectively.\n\nRaises\n------\nNodeNotFound\n    If `source` is not in `G`.\n\nNetworkXUnbounded\n    If the (di)graph contains a negative (di)cycle, the\n    algorithm raises an exception to indicate the presence of the\n    negative (di)cycle.  Note: any negative weight edge in an\n    undirected graph is a negative cycle.\n\n    As of NetworkX v3.2, a zero weight cycle is no longer\n    incorrectly reported as a negative weight cycle.\n\n\nExamples\n--------\n>>> G = nx.path_graph(5, create_using=nx.DiGraph())\n>>> pred, dist = nx.goldberg_radzik(G, 0)\n>>> sorted(pred.items())\n[(0, None), (1, 0), (2, 1), (3, 2), (4, 3)]\n>>> sorted(dist.items())\n[(0, 0), (1, 1), (2, 2), (3, 3), (4, 4)]\n\n>>> G = nx.cycle_graph(5, create_using=nx.DiGraph())\n>>> G[1][2][\"weight\"] = -7\n>>> nx.goldberg_radzik(G, 0)\nTraceback (most recent call last):\n    ...\nnetworkx.exception.NetworkXUnbounded: Negative cycle detected.\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef goldberg_radzik(G, source, weight='weight'):\n    if source not in G:\n        raise nx.NodeNotFound(f'Node {source} is not found in the graph')\n    weight = _weight_function(G, weight)\n    if G.is_multigraph():\n        if any((weight(u, v, {k: d}) < 0 for u, v, k, d in nx.selfloop_edges(G, keys=True, data=True))):\n            raise nx.NetworkXUnbounded('Negative cycle detected.')\n    elif any((weight(u, v, d) < 0 for u, v, d in nx.selfloop_edges(G, data=True))):\n        raise nx.NetworkXUnbounded('Negative cycle detected.')\n    if len(G) == 1:\n        return ({source: None}, {source: 0})\n    G_succ = G._adj\n    inf = float('inf')\n    d = {u: inf for u in G}\n    d[source] = 0\n    pred = {source: None}\n\n    def topo_sort(relabeled):\n        \"\"\"Topologically sort nodes relabeled in the previous round and detect\n        negative cycles.\n        \"\"\"\n        to_scan = []\n        neg_count = {}\n        for u in relabeled:\n            if u in neg_count:\n                continue\n            d_u = d[u]\n            if all((d_u + weight(u, v, e) >= d[v] for v, e in G_succ[u].items())):\n                continue\n            stack = [(u, iter(G_succ[u].items()))]\n            in_stack = {u}\n            neg_count[u] = 0\n            while stack:\n                u, it = stack[-1]\n                try:\n                    v, e = next(it)\n                except StopIteration:\n                    to_scan.append(u)\n                    stack.pop()\n                    in_stack.remove(u)\n                    continue\n                t = d[u] + weight(u, v, e)\n                d_v = d[v]\n                if t < d_v:\n                    is_neg = t < d_v\n                    d[v] = t\n                    pred[v] = u\n                    if v not in neg_count:\n                        neg_count[v] = neg_count[u] + int(is_neg)\n                        stack.append((v, iter(G_succ[v].items())))\n                        in_stack.add(v)\n                    elif v in in_stack and neg_count[u] + int(is_neg) > neg_count[v]:\n                        raise nx.NetworkXUnbounded('Negative cycle detected.')\n        to_scan.reverse()\n        return to_scan\n\n    def relax(to_scan):\n        \"\"\"Relax out-edges of relabeled nodes.\"\"\"\n        relabeled = set()\n        for u in to_scan:\n            d_u = d[u]\n            for v, e in G_succ[u].items():\n                w_e = weight(u, v, e)\n                if d_u + w_e < d[v]:\n                    d[v] = d_u + w_e\n                    pred[v] = u\n                    relabeled.add(v)\n        return relabeled\n    relabeled = {source}\n    while relabeled:\n        to_scan = topo_sort(relabeled)\n        relabeled = relax(to_scan)\n    d = {u: d[u] for u in pred}\n    return (pred, d)"
 },
 {
  "docstring": "Returns True if there exists a negative edge cycle anywhere in G.\n\nParameters\n----------\nG : NetworkX graph\n\nweight : string or function\n    If this is a string, then edge weights will be accessed via the\n    edge attribute with this key (that is, the weight of the edge\n    joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\n    such edge attribute exists, the weight of the edge is assumed to\n    be one.\n\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly three\n    positional arguments: the two endpoints of an edge and the\n    dictionary of edge attributes for that edge. The function must\n    return a number.\n\nheuristic : bool\n    Determines whether to use a heuristic to early detect negative\n    cycles at a negligible cost. In case of graphs with a negative cycle,\n    the performance of detection increases by at least an order of magnitude.\n\nReturns\n-------\nnegative_cycle : bool\n    True if a negative edge cycle exists, otherwise False.\n\nExamples\n--------\n>>> G = nx.cycle_graph(5, create_using=nx.DiGraph())\n>>> print(nx.negative_edge_cycle(G))\nFalse\n>>> G[1][2][\"weight\"] = -7\n>>> print(nx.negative_edge_cycle(G))\nTrue\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef negative_edge_cycle(G, weight='weight', heuristic=True):\n    if G.size() == 0:\n        return False\n    newnode = -1\n    while newnode in G:\n        newnode -= 1\n    G.add_edges_from([(newnode, n) for n in G])\n    try:\n        bellman_ford_predecessor_and_distance(G, newnode, weight=weight, heuristic=heuristic)\n    except nx.NetworkXUnbounded:\n        return True\n    finally:\n        G.remove_node(newnode)\n    return False"
 },
 {
  "docstring": "Returns a cycle with negative total weight if it exists.\n\nBellman-Ford is used to find shortest_paths. That algorithm\nstops if there exists a negative cycle. This algorithm\npicks up from there and returns the found negative cycle.\n\nThe cycle consists of a list of nodes in the cycle order. The last\nnode equals the first to make it a cycle.\nYou can look up the edge weights in the original graph. In the case\nof multigraphs the relevant edge is the minimal weight edge between\nthe nodes in the 2-tuple.\n\nIf the graph has no negative cycle, a NetworkXError is raised.\n\nParameters\n----------\nG : NetworkX graph\n\nsource: node label\n    The search for the negative cycle will start from this node.\n\nweight : string or function\n    If this is a string, then edge weights will be accessed via the\n    edge attribute with this key (that is, the weight of the edge\n    joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\n    such edge attribute exists, the weight of the edge is assumed to\n    be one.\n\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly three\n    positional arguments: the two endpoints of an edge and the\n    dictionary of edge attributes for that edge. The function must\n    return a number.\n\nExamples\n--------\n>>> G = nx.DiGraph()\n>>> G.add_weighted_edges_from([(0, 1, 2), (1, 2, 2), (2, 0, 1), (1, 4, 2), (4, 0, -5)])\n>>> nx.find_negative_cycle(G, 0)\n[4, 0, 1, 4]\n\nReturns\n-------\ncycle : list\n    A list of nodes in the order of the cycle found. The last node\n    equals the first to indicate a cycle.\n\nRaises\n------\nNetworkXError\n    If no negative cycle is found.",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef find_negative_cycle(G, source, weight='weight'):\n    weight = _weight_function(G, weight)\n    pred = {source: []}\n    v = _inner_bellman_ford(G, [source], weight, pred=pred)\n    if v is None:\n        raise nx.NetworkXError('No negative cycles detected.')\n    neg_cycle = []\n    stack = [(v, list(pred[v]))]\n    seen = {v}\n    while stack:\n        node, preds = stack[-1]\n        if v in preds:\n            neg_cycle.extend([node, v])\n            neg_cycle = list(reversed(neg_cycle))\n            return neg_cycle\n        if preds:\n            nbr = preds.pop()\n            if nbr not in seen:\n                stack.append((nbr, list(pred[nbr])))\n                neg_cycle.append(node)\n                seen.add(nbr)\n        else:\n            stack.pop()\n            if neg_cycle:\n                neg_cycle.pop()\n            else:\n                if v in G[v] and weight(G, v, v) < 0:\n                    return [v, v]\n                raise nx.NetworkXError('Negative cycle is detected but not found')\n    msg = 'negative cycle detected but not identified'\n    raise nx.NetworkXUnbounded(msg)"
 },
 {
  "docstring": "Dijkstra's algorithm for shortest paths using bidirectional search.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node\n    Starting node.\n\ntarget : node\n    Ending node.\n\nweight : string or function\n    If this is a string, then edge weights will be accessed via the\n    edge attribute with this key (that is, the weight of the edge\n    joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\n    such edge attribute exists, the weight of the edge is assumed to\n    be one.\n\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly three\n    positional arguments: the two endpoints of an edge and the\n    dictionary of edge attributes for that edge. The function must\n    return a number or None to indicate a hidden edge.\n\nReturns\n-------\nlength, path : number and list\n    length is the distance from source to target.\n    path is a list of nodes on a path from source to target.\n\nRaises\n------\nNodeNotFound\n    If either `source` or `target` is not in `G`.\n\nNetworkXNoPath\n    If no path exists between source and target.\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> length, path = nx.bidirectional_dijkstra(G, 0, 4)\n>>> print(length)\n4\n>>> print(path)\n[0, 1, 2, 3, 4]\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef bidirectional_dijkstra(G, source, target, weight='weight'):\n    if source not in G or target not in G:\n        msg = f'Either source {source} or target {target} is not in G'\n        raise nx.NodeNotFound(msg)\n    if source == target:\n        return (0, [source])\n    weight = _weight_function(G, weight)\n    push = heappush\n    pop = heappop\n    dists = [{}, {}]\n    paths = [{source: [source]}, {target: [target]}]\n    fringe = [[], []]\n    seen = [{source: 0}, {target: 0}]\n    c = count()\n    push(fringe[0], (0, next(c), source))\n    push(fringe[1], (0, next(c), target))\n    if G.is_directed():\n        neighs = [G._succ, G._pred]\n    else:\n        neighs = [G._adj, G._adj]\n    finalpath = []\n    dir = 1\n    while fringe[0] and fringe[1]:\n        dir = 1 - dir\n        dist, _, v = pop(fringe[dir])\n        if v in dists[dir]:\n            continue\n        dists[dir][v] = dist\n        if v in dists[1 - dir]:\n            return (finaldist, finalpath)\n        for w, d in neighs[dir][v].items():\n            cost = weight(v, w, d) if dir == 0 else weight(w, v, d)\n            if cost is None:\n                continue\n            vwLength = dists[dir][v] + cost\n            if w in dists[dir]:\n                if vwLength < dists[dir][w]:\n                    raise ValueError('Contradictory paths found: negative weights?')\n            elif w not in seen[dir] or vwLength < seen[dir][w]:\n                seen[dir][w] = vwLength\n                push(fringe[dir], (vwLength, next(c), w))\n                paths[dir][w] = paths[dir][v] + [w]\n                if w in seen[0] and w in seen[1]:\n                    totaldist = seen[0][w] + seen[1][w]\n                    if finalpath == [] or finaldist > totaldist:\n                        finaldist = totaldist\n                        revpath = paths[1][w][:]\n                        revpath.reverse()\n                        finalpath = paths[0][w] + revpath[1:]\n    raise nx.NetworkXNoPath(f'No path between {source} and {target}.')"
 },
 {
  "docstring": "Uses Johnson's Algorithm to compute shortest paths.\n\nJohnson's Algorithm finds a shortest path between each pair of\nnodes in a weighted graph even if negative weights are present.\n\nParameters\n----------\nG : NetworkX graph\n\nweight : string or function\n    If this is a string, then edge weights will be accessed via the\n    edge attribute with this key (that is, the weight of the edge\n    joining `u` to `v` will be ``G.edges[u, v][weight]``). If no\n    such edge attribute exists, the weight of the edge is assumed to\n    be one.\n\n    If this is a function, the weight of an edge is the value\n    returned by the function. The function must accept exactly three\n    positional arguments: the two endpoints of an edge and the\n    dictionary of edge attributes for that edge. The function must\n    return a number.\n\nReturns\n-------\ndistance : dictionary\n    Dictionary, keyed by source and target, of shortest paths.\n\nExamples\n--------\n>>> graph = nx.DiGraph()\n>>> graph.add_weighted_edges_from(\n...     [(\"0\", \"3\", 3), (\"0\", \"1\", -5), (\"0\", \"2\", 2), (\"1\", \"2\", 4), (\"2\", \"3\", 1)]\n... )\n>>> paths = nx.johnson(graph, weight=\"weight\")\n>>> paths[\"0\"][\"2\"]\n['0', '1', '2']\n\n",
  "code": "@nx._dispatch(edge_attrs='weight')\ndef johnson(G, weight='weight'):\n    dist = {v: 0 for v in G}\n    pred = {v: [] for v in G}\n    weight = _weight_function(G, weight)\n    dist_bellman = _bellman_ford(G, list(G), weight, pred=pred, dist=dist)\n\n    def new_weight(u, v, d):\n        return weight(u, v, d) + dist_bellman[u] - dist_bellman[v]\n\n    def dist_path(v):\n        paths = {v: [v]}\n        _dijkstra(G, v, new_weight, paths=paths)\n        return paths\n    return {v: dist_path(v) for v in G}"
 },
 {
  "docstring": "Topologically sort nodes relabeled in the previous round and detect\nnegative cycles.",
  "code": "def topo_sort(relabeled):\n    to_scan = []\n    neg_count = {}\n    for u in relabeled:\n        if u in neg_count:\n            continue\n        d_u = d[u]\n        if all((d_u + weight(u, v, e) >= d[v] for v, e in G_succ[u].items())):\n            continue\n        stack = [(u, iter(G_succ[u].items()))]\n        in_stack = {u}\n        neg_count[u] = 0\n        while stack:\n            u, it = stack[-1]\n            try:\n                v, e = next(it)\n            except StopIteration:\n                to_scan.append(u)\n                stack.pop()\n                in_stack.remove(u)\n                continue\n            t = d[u] + weight(u, v, e)\n            d_v = d[v]\n            if t < d_v:\n                is_neg = t < d_v\n                d[v] = t\n                pred[v] = u\n                if v not in neg_count:\n                    neg_count[v] = neg_count[u] + int(is_neg)\n                    stack.append((v, iter(G_succ[v].items())))\n                    in_stack.add(v)\n                elif v in in_stack and neg_count[u] + int(is_neg) > neg_count[v]:\n                    raise nx.NetworkXUnbounded('Negative cycle detected.')\n    to_scan.reverse()\n    return to_scan"
 },
 {
  "docstring": "Relax out-edges of relabeled nodes.",
  "code": "def relax(to_scan):\n    relabeled = set()\n    for u in to_scan:\n        d_u = d[u]\n        for v, e in G_succ[u].items():\n            w_e = weight(u, v, e)\n            if d_u + w_e < d[v]:\n                d[v] = d_u + w_e\n                pred[v] = u\n                relabeled.add(v)\n    return relabeled"
 },
 {
  "docstring": "Tests that A* algorithm finds any of multiple optimal paths",
  "code": "def test_multiple_optimal_paths(self):\n    heuristic_values = {'a': 1.35, 'b': 1.18, 'c': 0.67, 'd': 0}\n\n    def h(u, v):\n        return heuristic_values[u]\n    graph = nx.Graph()\n    points = ['a', 'b', 'c', 'd']\n    edges = [('a', 'b', 0.18), ('a', 'c', 0.68), ('b', 'c', 0.5), ('c', 'd', 0.67)]\n    graph.add_nodes_from(points)\n    graph.add_weighted_edges_from(edges)\n    path1 = ['a', 'c', 'd']\n    path2 = ['a', 'b', 'c', 'd']\n    assert nx.astar_path(graph, 'a', 'd', h) in (path1, path2)"
 },
 {
  "docstring": "Tests that A* accommodates nodes that are not orderable.\n\nFor more information, see issue #554.",
  "code": "def test_unorderable_nodes(self):\n    nodes = [object() for n in range(4)]\n    G = nx.Graph()\n    G.add_edges_from(pairwise(nodes, cyclic=True))\n    path = nx.astar_path(G, nodes[0], nodes[2])\n    assert len(path) == 3"
 },
 {
  "docstring": "Tests that exception is raised when there exists no\npath between source and target",
  "code": "def test_astar_NetworkXNoPath(self):\n    G = nx.gnp_random_graph(10, 0.2, seed=10)\n    with pytest.raises(nx.NetworkXNoPath):\n        nx.astar_path(G, 4, 9)"
 },
 {
  "docstring": "Tests that exception is raised when either\nsource or target is not in graph",
  "code": "def test_astar_NodeNotFound(self):\n    G = nx.gnp_random_graph(10, 0.2, seed=10)\n    with pytest.raises(nx.NodeNotFound):\n        nx.astar_path_length(G, 11, 9)"
 },
 {
  "docstring": "Tests that the trivial graph has average path length zero,\nsince there is exactly one path of length zero in the trivial\ngraph.\n\nFor more information, see issue #1960.",
  "code": "def test_trivial_graph(self):\n    G = nx.trivial_graph()\n    assert nx.average_shortest_path_length(G) == 0"
 },
 {
  "docstring": "Creates some graphs for use in the unit tests.",
  "code": "def setup_method(self):\n    cnlti = nx.convert_node_labels_to_integers\n    self.grid = cnlti(nx.grid_2d_graph(4, 4), first_label=1, ordering='sorted')\n    self.cycle = nx.cycle_graph(7)\n    self.directed_cycle = nx.cycle_graph(7, create_using=nx.DiGraph())\n    self.XG = nx.DiGraph()\n    self.XG.add_weighted_edges_from([('s', 'u', 10), ('s', 'x', 5), ('u', 'v', 1), ('u', 'x', 2), ('v', 'y', 1), ('x', 'u', 3), ('x', 'v', 5), ('x', 'y', 2), ('y', 's', 7), ('y', 'v', 6)])\n    self.MXG = nx.MultiDiGraph(self.XG)\n    self.MXG.add_edge('s', 'u', weight=15)\n    self.XG2 = nx.DiGraph()\n    self.XG2.add_weighted_edges_from([[1, 4, 1], [4, 5, 1], [5, 6, 1], [6, 3, 1], [1, 3, 50], [1, 2, 100], [2, 3, 100]])\n    self.XG3 = nx.Graph()\n    self.XG3.add_weighted_edges_from([[0, 1, 2], [1, 2, 12], [2, 3, 1], [3, 4, 5], [4, 5, 1], [5, 0, 10]])\n    self.XG4 = nx.Graph()\n    self.XG4.add_weighted_edges_from([[0, 1, 2], [1, 2, 2], [2, 3, 1], [3, 4, 1], [4, 5, 1], [5, 6, 1], [6, 7, 1], [7, 0, 1]])\n    self.MXG4 = nx.MultiGraph(self.XG4)\n    self.MXG4.add_edge(0, 1, weight=3)\n    self.G = nx.DiGraph()\n    self.G.add_edges_from([('s', 'u'), ('s', 'x'), ('u', 'v'), ('u', 'x'), ('v', 'y'), ('x', 'u'), ('x', 'v'), ('x', 'y'), ('y', 's'), ('y', 'v')])"
 },
 {
  "docstring": "Tests that a callable weight is interpreted as a weight\nfunction instead of an edge attribute.",
  "code": "def test_weight_function(self):\n    G = nx.complete_graph(3)\n    G.adj[0][2]['weight'] = 10\n    G.adj[0][1]['weight'] = 1\n    G.adj[1][2]['weight'] = 1\n\n    def weight(u, v, d):\n        return 1 / d['weight']\n    distance, path = nx.single_source_dijkstra(G, 0, 2)\n    assert distance == 2\n    assert path == [0, 1, 2]\n    distance, path = nx.single_source_dijkstra(G, 0, 2, weight=weight)\n    assert distance == 1 / 10\n    assert path == [0, 2]"
 },
 {
  "docstring": "Tests for computing the length of the shortest path using\nDijkstra's algorithm with a user-defined weight function.",
  "code": "def test_weight_function(self):\n    G = nx.complete_graph(3)\n    G.adj[0][2]['weight'] = 10\n    G.adj[0][1]['weight'] = 1\n    G.adj[1][2]['weight'] = 1\n\n    def weight(u, v, d):\n        return 1 / d['weight']\n    length = nx.dijkstra_path_length(G, 0, 2, weight=weight)\n    assert length == 1 / 10"
 },
 {
  "docstring": "In complete graphs each node is a dominating set.\nThus the dominating set has to be of cardinality 1.",
  "code": "def test_complete():\n    K4 = nx.complete_graph(4)\n    assert len(nx.dominating_set(K4)) == 1\n    K5 = nx.complete_graph(5)\n    assert len(nx.dominating_set(K5)) == 1"
 },
 {
  "docstring": "Example from https://en.wikipedia.org/wiki/Dominating_set",
  "code": "def test_wikipedia_is_dominating_set():\n    G = nx.cycle_graph(4)\n    G.add_edges_from([(0, 4), (1, 4), (2, 5)])\n    assert nx.is_dominating_set(G, {4, 3, 5})\n    assert nx.is_dominating_set(G, {0, 2})\n    assert nx.is_dominating_set(G, {1, 2})"
 },
 {
  "docstring": "Raises an exception if the combinatorial embedding is not correct\n\nParameters\n----------\nG : NetworkX graph\nembedding : a dict mapping nodes to a list of edges\n    This specifies the ordering of the outgoing edges from a node for\n    a combinatorial embedding\n\n",
  "code": "def check_embedding(G, embedding):\n    if not isinstance(embedding, nx.PlanarEmbedding):\n        raise nx.NetworkXException('Bad embedding. Not of type nx.PlanarEmbedding')\n    embedding.check_structure()\n    assert set(G.nodes) == set(embedding.nodes), \"Bad embedding. Nodes don't match the original graph.\"\n    g_edges = set()\n    for edge in G.edges:\n        if edge[0] != edge[1]:\n            g_edges.add((edge[0], edge[1]))\n            g_edges.add((edge[1], edge[0]))\n    assert g_edges == set(embedding.edges), \"Bad embedding. Edges don't match the original graph.\""
 },
 {
  "docstring": "Raises an exception if the counterexample is wrong.\n\nParameters\n----------\nG : NetworkX graph\nsubdivision_nodes : set\n    A set of nodes inducing a subgraph as a counterexample",
  "code": "def check_counterexample(G, sub_graph):\n    sub_graph = nx.Graph(sub_graph)\n    for u in sub_graph:\n        if sub_graph.has_edge(u, u):\n            sub_graph.remove_edge(u, u)\n    contract = list(sub_graph)\n    while len(contract) > 0:\n        contract_node = contract.pop()\n        if contract_node not in sub_graph:\n            continue\n        degree = sub_graph.degree[contract_node]\n        if degree == 2:\n            neighbors = iter(sub_graph[contract_node])\n            u = next(neighbors)\n            v = next(neighbors)\n            contract.append(u)\n            contract.append(v)\n            sub_graph.remove_node(contract_node)\n            sub_graph.add_edge(u, v)\n    if len(sub_graph) == 5:\n        if not nx.is_isomorphic(nx.complete_graph(5), sub_graph):\n            raise nx.NetworkXException('Bad counter example.')\n    elif len(sub_graph) == 6:\n        if not nx.is_isomorphic(nx.complete_bipartite_graph(3, 3), sub_graph):\n            raise nx.NetworkXException('Bad counter example.')\n    else:\n        raise nx.NetworkXException('Bad counter example.')"
 },
 {
  "docstring": "Raises an exception if the lr_planarity check returns a wrong result\n\nParameters\n----------\nG : NetworkX graph\nis_planar : bool\n    The expected result of the planarity check.\n    If set to None only counter example or embedding are verified.",
  "code": "@staticmethod\ndef check_graph(G, is_planar=None):\n    is_planar_lr, result = nx.check_planarity(G, True)\n    is_planar_lr_rec, result_rec = check_planarity_recursive(G, True)\n    if is_planar is not None:\n        if is_planar:\n            msg = 'Wrong planarity check result. Should be planar.'\n        else:\n            msg = 'Wrong planarity check result. Should be non-planar.'\n        assert is_planar == is_planar_lr, msg\n        assert is_planar == is_planar_lr_rec, msg\n    if is_planar_lr:\n        check_embedding(G, result)\n        check_embedding(G, result_rec)\n    else:\n        check_counterexample(G, result)\n        check_counterexample(G, result_rec)"
 },
 {
  "docstring": "Tests that the null graph has empty node boundaries.",
  "code": "def test_null_graph(self):\n    null = nx.null_graph()\n    assert nx.node_boundary(null, []) == set()\n    assert nx.node_boundary(null, [], []) == set()\n    assert nx.node_boundary(null, [1, 2, 3]) == set()\n    assert nx.node_boundary(null, [1, 2, 3], [4, 5, 6]) == set()\n    assert nx.node_boundary(null, [1, 2, 3], [3, 4, 5]) == set()"
 },
 {
  "docstring": "Check boundaries in the petersen graph\n\ncheeger(G,k)=min(|bdy(S)|/|S| for |S|=k, 0<k<=|V(G)|/2)",
  "code": "def test_petersen(self):\n\n    def cheeger(G, k):\n        return min((len(nx.node_boundary(G, nn)) / k for nn in combinations(G, k)))\n    P = nx.petersen_graph()\n    assert cheeger(P, 1) == pytest.approx(3.0, abs=0.01)\n    assert cheeger(P, 2) == pytest.approx(2.0, abs=0.01)\n    assert cheeger(P, 3) == pytest.approx(1.67, abs=0.01)\n    assert cheeger(P, 4) == pytest.approx(1.0, abs=0.01)\n    assert cheeger(P, 5) == pytest.approx(0.8, abs=0.01)"
 },
 {
  "docstring": "Tests the node boundary of a directed graph.",
  "code": "def test_directed(self):\n    G = nx.DiGraph([(0, 1), (1, 2), (2, 3), (3, 4), (4, 0)])\n    S = {0, 1}\n    boundary = nx.node_boundary(G, S)\n    expected = {2}\n    assert boundary == expected"
 },
 {
  "docstring": "Tests the node boundary of a multigraph.",
  "code": "def test_multigraph(self):\n    G = nx.MultiGraph(list(nx.cycle_graph(5).edges()) * 2)\n    S = {0, 1}\n    boundary = nx.node_boundary(G, S)\n    expected = {2, 4}\n    assert boundary == expected"
 },
 {
  "docstring": "Tests the edge boundary of a multidigraph.",
  "code": "def test_multidigraph(self):\n    edges = [(0, 1), (1, 2), (2, 3), (3, 4), (4, 0)]\n    G = nx.MultiDiGraph(edges * 2)\n    S = {0, 1}\n    boundary = nx.node_boundary(G, S)\n    expected = {2}\n    assert boundary == expected"
 },
 {
  "docstring": "Tests the edge boundary of a directed graph.",
  "code": "def test_directed(self):\n    G = nx.DiGraph([(0, 1), (1, 2), (2, 3), (3, 4), (4, 0)])\n    S = {0, 1}\n    boundary = list(nx.edge_boundary(G, S))\n    expected = [(1, 2)]\n    assert boundary == expected"
 },
 {
  "docstring": "Tests the edge boundary of a multigraph.",
  "code": "def test_multigraph(self):\n    G = nx.MultiGraph(list(nx.cycle_graph(5).edges()) * 2)\n    S = {0, 1}\n    boundary = list(nx.edge_boundary(G, S))\n    expected = [(0, 4), (0, 4), (1, 2), (1, 2)]\n    assert boundary == expected"
 },
 {
  "docstring": "Tests the edge boundary of a multidigraph.",
  "code": "def test_multidigraph(self):\n    edges = [(0, 1), (1, 2), (2, 3), (3, 4), (4, 0)]\n    G = nx.MultiDiGraph(edges * 2)\n    S = {0, 1}\n    boundary = list(nx.edge_boundary(G, S))\n    expected = [(1, 2), (1, 2)]\n    assert boundary == expected"
 },
 {
  "docstring": "Yields cyclic permutations of the given sequence.\n\nFor example::\n\n    >>> list(cycles(\"abc\"))\n    [('a', 'b', 'c'), ('b', 'c', 'a'), ('c', 'a', 'b')]",
  "code": "def cycles(seq):\n    n = len(seq)\n    cycled_seq = cycle(seq)\n    for x in seq:\n        yield tuple(islice(cycled_seq, n))\n        next(cycled_seq)"
 },
 {
  "docstring": "Decide whether two sequences are equal up to cyclic permutations.\n\nFor example::\n\n    >>> cyclic_equals(\"xyz\", \"zxy\")\n    True\n    >>> cyclic_equals(\"xyz\", \"zyx\")\n    False",
  "code": "def cyclic_equals(seq1, seq2):\n    seq2 = tuple(seq2)\n    return any((x == tuple(seq2) for x in cycles(seq1)))"
 },
 {
  "docstring": "Test for a graph with multiple connected components.",
  "code": "def test_disconnected_graph(self):\n    G = nx.barbell_graph(3, 0)\n    H = nx.barbell_graph(3, 0)\n    mapping = dict(zip(range(6), 'abcdef'))\n    nx.relabel_nodes(H, mapping, copy=False)\n    G = nx.union(G, H)\n    chains = list(nx.chain_decomposition(G))\n    expected = [[(0, 1), (1, 2), (2, 0)], [(3, 4), (4, 5), (5, 3)], [('a', 'b'), ('b', 'c'), ('c', 'a')], [('d', 'e'), ('e', 'f'), ('f', 'd')]]\n    assert len(chains) == len(expected)\n    for chain in chains:\n        self.assertContainsChain(chain, expected)"
 },
 {
  "docstring": "Test for a single component of a disconnected graph.",
  "code": "def test_disconnected_graph_root_node(self):\n    G = nx.barbell_graph(3, 0)\n    H = nx.barbell_graph(3, 0)\n    mapping = dict(zip(range(6), 'abcdef'))\n    nx.relabel_nodes(H, mapping, copy=False)\n    G = nx.union(G, H)\n    chains = list(nx.chain_decomposition(G, root='a'))\n    expected = [[('a', 'b'), ('b', 'c'), ('c', 'a')], [('d', 'e'), ('e', 'f'), ('f', 'd')]]\n    assert len(chains) == len(expected)\n    for chain in chains:\n        self.assertContainsChain(chain, expected)"
 },
 {
  "docstring": "Test chain decomposition when root is not in graph",
  "code": "def test_chain_decomposition_root_not_in_G(self):\n    G = nx.Graph()\n    G.add_nodes_from([1, 2, 3])\n    with pytest.raises(nx.NodeNotFound):\n        nx.has_bridges(G, root=6)"
 },
 {
  "docstring": "Tests that the maximal clique graph is the same as the bipartite\nclique graph after being projected onto the nodes representing the\ncliques.",
  "code": "def test_make_max_clique_graph(self):\n    G = self.G\n    B = nx.make_clique_bipartite(G)\n    H1 = nx.projected_graph(B, range(-5, 0))\n    H1 = nx.relabel_nodes(H1, {-v: v - 1 for v in range(1, 6)})\n    H2 = nx.make_max_clique_graph(G)\n    assert H1.adj == H2.adj"
 },
 {
  "docstring": "Test C4 for figure 1 Lind et al (2005)",
  "code": "def test_lind_square_clustering(self):\n    G = nx.Graph([(1, 2), (1, 3), (1, 6), (1, 7), (2, 4), (2, 5), (3, 4), (3, 5), (6, 7), (7, 8), (6, 8), (7, 9), (7, 10), (6, 11), (6, 12), (2, 13), (2, 14), (3, 15), (3, 16)])\n    G1 = G.subgraph([1, 2, 3, 4, 5, 13, 14, 15, 16])\n    G2 = G.subgraph([1, 6, 7, 8, 9, 10, 11, 12])\n    assert nx.square_clustering(G, [1])[1] == 3 / 43\n    assert nx.square_clustering(G1, [1])[1] == 2 / 6\n    assert nx.square_clustering(G2, [1])[1] == 1 / 5"
 },
 {
  "docstring": "Test eq2 for figure 1 Peng et al (2008)",
  "code": "def test_peng_square_clustering(self):\n    G = nx.Graph([(1, 2), (1, 3), (2, 4), (3, 4), (3, 5), (3, 6)])\n    assert nx.square_clustering(G, [1])[1] == 1 / 3"
 },
 {
  "docstring": "Empty graph",
  "code": "def test_trivial(self):\n    G = nx.Graph()\n    assert nx.core_number(G) == {}"
 },
 {
  "docstring": "core number had a bug for directed graphs found in issue #1959",
  "code": "def test_directed_core_number(self):\n    G = nx.DiGraph()\n    edges = [(1, 2), (2, 1), (2, 3), (2, 4), (3, 4), (4, 3)]\n    G.add_edges_from(edges)\n    assert nx.core_number(G) == {1: 2, 2: 2, 3: 2, 4: 2}\n    more_edges = [(1, 5), (3, 5), (4, 5), (3, 6), (4, 6), (5, 6)]\n    G.add_edges_from(more_edges)\n    assert nx.core_number(G) == {1: 3, 2: 3, 3: 3, 4: 3, 5: 3, 6: 3}"
 },
 {
  "docstring": "Tests that the cut size is symmetric.",
  "code": "def test_symmetric(self):\n    G = nx.barbell_graph(3, 0)\n    S = {0, 1, 4}\n    T = {2, 3, 5}\n    assert nx.cut_size(G, S, T) == 4\n    assert nx.cut_size(G, T, S) == 4"
 },
 {
  "docstring": "Tests for a cut of a single edge.",
  "code": "def test_single_edge(self):\n    G = nx.barbell_graph(3, 0)\n    S = {0, 1, 2}\n    T = {3, 4, 5}\n    assert nx.cut_size(G, S, T) == 1\n    assert nx.cut_size(G, T, S) == 1"
 },
 {
  "docstring": "Tests that each directed edge is counted once in the cut.",
  "code": "def test_directed(self):\n    G = nx.barbell_graph(3, 0).to_directed()\n    S = {0, 1, 2}\n    T = {3, 4, 5}\n    assert nx.cut_size(G, S, T) == 2\n    assert nx.cut_size(G, T, S) == 2"
 },
 {
  "docstring": "Tests that a cut in a directed graph is symmetric.",
  "code": "def test_directed_symmetric(self):\n    G = nx.barbell_graph(3, 0).to_directed()\n    S = {0, 1, 4}\n    T = {2, 3, 5}\n    assert nx.cut_size(G, S, T) == 8\n    assert nx.cut_size(G, T, S) == 8"
 },
 {
  "docstring": "Tests that parallel edges are each counted for a cut.",
  "code": "def test_multigraph(self):\n    G = nx.MultiGraph(['ab', 'ab'])\n    assert nx.cut_size(G, {'a'}, {'b'}) == 2"
 },
 {
  "docstring": "Tests the function for graphs with self loops",
  "code": "def test_cycle_basis_self_loop(self):\n    G = nx.Graph()\n    nx.add_cycle(G, [0, 1, 2, 3])\n    nx.add_cycle(G, [0, 0, 6, 2])\n    cy = nx.cycle_basis(G)\n    sort_cy = sorted((sorted(c) for c in cy))\n    assert sort_cy == [[0], [0, 1, 2], [0, 2, 3], [0, 2, 6]]"
 },
 {
  "docstring": "Consume the iterator entirely.",
  "code": "def _consume(iterator):\n    deque(iterator, maxlen=0)"
 },
 {
  "docstring": "Regression test to ensure ancestors and descendants work as expected on\nundirected graphs.",
  "code": "def test_ancestors_descendants_undirected():\n    G = nx.path_graph(5)\n    nx.ancestors(G, 2) == nx.descendants(G, 2) == {0, 1, 3, 4}"
 },
 {
  "docstring": "Tests that computing the longest path does not depend on\nnodes being orderable.\n\nFor more information, see issue #1989.",
  "code": "def test_unorderable_nodes(self):\n    nodes = [object() for n in range(4)]\n    G = nx.DiGraph()\n    G.add_edge(nodes[0], nodes[1])\n    G.add_edge(nodes[0], nodes[2])\n    G.add_edge(nodes[2], nodes[3])\n    G.add_edge(nodes[1], nodes[3])\n    nx.dag_longest_path(G)"
 },
 {
  "docstring": "Check the case of two or more nodes with same key value.\nWant to avoid exception raised due to comparing nodes directly.\nSee Issue #3493",
  "code": "def test_lexicographical_topological_sort2(self):\n\n    class Test_Node:\n\n        def __init__(self, n):\n            self.label = n\n            self.priority = 1\n\n        def __repr__(self):\n            return f'Node({self.label})'\n\n    def sorting_key(node):\n        return node.priority\n    test_nodes = [Test_Node(n) for n in range(4)]\n    G = nx.DiGraph()\n    edges = [(0, 1), (0, 2), (0, 3), (2, 3)]\n    G.add_edges_from(((test_nodes[a], test_nodes[b]) for a, b in edges))\n    sorting = list(nx.lexicographical_topological_sort(G, key=sorting_key))\n    assert sorting == test_nodes"
 },
 {
  "docstring": "Tests that a directed acyclic graph with a single degree\nzero node produces an arborescence.",
  "code": "def test_single_root(self):\n    G = nx.DiGraph([(0, 1), (0, 2), (1, 3), (2, 3)])\n    B = nx.dag_to_branching(G)\n    expected = nx.DiGraph([(0, 1), (1, 3), (0, 2), (2, 4)])\n    assert nx.is_arborescence(B)\n    assert nx.is_isomorphic(B, expected)"
 },
 {
  "docstring": "Tests that a directed acyclic graph with multiple degree zero\nnodes creates an arborescence with multiple (weakly) connected\ncomponents.",
  "code": "def test_multiple_roots(self):\n    G = nx.DiGraph([(0, 1), (0, 2), (1, 3), (2, 3), (5, 2)])\n    B = nx.dag_to_branching(G)\n    expected = nx.DiGraph([(0, 1), (1, 3), (0, 2), (2, 4), (5, 6), (6, 7)])\n    assert nx.is_branching(B)\n    assert not nx.is_arborescence(B)\n    assert nx.is_isomorphic(B, expected)"
 },
 {
  "docstring": "Tests that a directed acyclic graph that is already an\narborescence produces an isomorphic arborescence as output.",
  "code": "def test_already_arborescence(self):\n    A = nx.balanced_tree(2, 2, create_using=nx.DiGraph())\n    B = nx.dag_to_branching(A)\n    assert nx.is_isomorphic(A, B)"
 },
 {
  "docstring": "Tests that a directed acyclic graph that is already a\nbranching produces an isomorphic branching as output.",
  "code": "def test_already_branching(self):\n    T1 = nx.balanced_tree(2, 2, create_using=nx.DiGraph())\n    T2 = nx.balanced_tree(2, 2, create_using=nx.DiGraph())\n    G = nx.disjoint_union(T1, T2)\n    B = nx.dag_to_branching(G)\n    assert nx.is_isomorphic(G, B)"
 },
 {
  "docstring": "Tests that a non-acyclic graph causes an exception.",
  "code": "def test_not_acyclic(self):\n    with pytest.raises(nx.HasACycle):\n        G = nx.DiGraph(pairwise('abc', cyclic=True))\n        nx.dag_to_branching(G)"
 },
 {
  "docstring": "Return the subgraph induced on the barycenter of g",
  "code": "def barycenter_as_subgraph(self, g, **kwargs):\n    b = nx.barycenter(g, **kwargs)\n    assert isinstance(b, list)\n    assert set(b) <= set(g)\n    return g.subgraph(b)"
 },
 {
  "docstring": "The barycenter of a tree is a single vertex or an edge.\n\nSee [West01]_, p. 78.",
  "code": "def test_trees(self):\n    prng = Random(3735928559)\n    for i in range(50):\n        RT = nx.random_labeled_tree(prng.randint(1, 75), seed=prng)\n        b = self.barycenter_as_subgraph(RT)\n        if len(b) == 2:\n            assert b.size() == 1\n        else:\n            assert len(b) == 1\n            assert b.size() == 0"
 },
 {
  "docstring": "Test the tree pictured at the bottom of [West01]_, p. 78.",
  "code": "def test_this_one_specific_tree(self):\n    g = nx.Graph({'a': ['b'], 'b': ['a', 'x'], 'x': ['b', 'y'], 'y': ['x', 'z'], 'z': ['y', 0, 1, 2, 3, 4], 0: ['z'], 1: ['z'], 2: ['z'], 3: ['z'], 4: ['z']})\n    b = self.barycenter_as_subgraph(g, attr='barycentricity')\n    assert list(b) == ['z']\n    assert not b.edges\n    expected_barycentricity = {0: 23, 1: 23, 2: 23, 3: 23, 4: 23, 'a': 35, 'b': 27, 'x': 21, 'y': 17, 'z': 15}\n    for node, barycentricity in expected_barycentricity.items():\n        assert g.nodes[node]['barycentricity'] == barycentricity\n    for edge in g.edges:\n        g.edges[edge]['weight'] = 2\n    b = self.barycenter_as_subgraph(g, weight='weight', attr='barycentricity2')\n    assert list(b) == ['z']\n    assert not b.edges\n    for node, barycentricity in expected_barycentricity.items():\n        assert g.nodes[node]['barycentricity2'] == barycentricity * 2"
 },
 {
  "docstring": "Tests that the cycle graph on five vertices is strongly\nregular.",
  "code": "def test_cycle_graph(self):\n    G = nx.cycle_graph(5)\n    assert is_strongly_regular(G)"
 },
 {
  "docstring": "Tests that the Petersen graph is strongly regular.",
  "code": "def test_petersen_graph(self):\n    G = nx.petersen_graph()\n    assert is_strongly_regular(G)"
 },
 {
  "docstring": "Tests that the path graph is not strongly regular.",
  "code": "def test_path_graph(self):\n    G = nx.path_graph(4)\n    assert not is_strongly_regular(G)"
 },
 {
  "docstring": "Return a path graph of length three.",
  "code": "def path_graph():\n    G = nx.path_graph(3, create_using=nx.DiGraph)\n    G.graph['name'] = 'path'\n    nx.freeze(G)\n    return G"
 },
 {
  "docstring": "Return a three node fork graph.",
  "code": "def fork_graph():\n    G = nx.DiGraph(name='fork')\n    G.add_edges_from([(0, 1), (0, 2)])\n    nx.freeze(G)\n    return G"
 },
 {
  "docstring": "Return a collider/v-structure graph with three nodes.",
  "code": "def collider_graph():\n    G = nx.DiGraph(name='collider')\n    G.add_edges_from([(0, 2), (1, 2)])\n    nx.freeze(G)\n    return G"
 },
 {
  "docstring": "Return a simply Naive Bayes PGM graph.",
  "code": "def naive_bayes_graph():\n    G = nx.DiGraph(name='naive_bayes')\n    G.add_edges_from([(0, 1), (0, 2), (0, 3), (0, 4)])\n    nx.freeze(G)\n    return G"
 },
 {
  "docstring": "Return the 'Asia' PGM graph.",
  "code": "def asia_graph():\n    G = nx.DiGraph(name='asia')\n    G.add_edges_from([('asia', 'tuberculosis'), ('smoking', 'cancer'), ('smoking', 'bronchitis'), ('tuberculosis', 'either'), ('cancer', 'either'), ('either', 'xray'), ('either', 'dyspnea'), ('bronchitis', 'dyspnea')])\n    nx.freeze(G)\n    return G"
 },
 {
  "docstring": "Test that the Markov condition holds for each PGM graph.",
  "code": "@pytest.mark.parametrize('graph', [path_graph(), fork_graph(), collider_graph(), naive_bayes_graph(), asia_graph()])\ndef test_markov_condition(graph):\n    for node in graph.nodes:\n        parents = set(graph.predecessors(node))\n        non_descendants = graph.nodes - nx.descendants(graph, node) - {node} - parents\n        assert nx.d_separated(graph, {node}, non_descendants, parents)"
 },
 {
  "docstring": "Example-based test of d-separation for path_graph.",
  "code": "def test_path_graph_dsep(path_graph):\n    assert nx.d_separated(path_graph, {0}, {2}, {1})\n    assert not nx.d_separated(path_graph, {0}, {2}, {})"
 },
 {
  "docstring": "Example-based test of d-separation for fork_graph.",
  "code": "def test_fork_graph_dsep(fork_graph):\n    assert nx.d_separated(fork_graph, {1}, {2}, {0})\n    assert not nx.d_separated(fork_graph, {1}, {2}, {})"
 },
 {
  "docstring": "Example-based test of d-separation for collider_graph.",
  "code": "def test_collider_graph_dsep(collider_graph):\n    assert nx.d_separated(collider_graph, {0}, {1}, {})\n    assert not nx.d_separated(collider_graph, {0}, {1}, {2})"
 },
 {
  "docstring": "Example-based test of d-separation for naive_bayes_graph.",
  "code": "def test_naive_bayes_dsep(naive_bayes_graph):\n    for u, v in combinations(range(1, 5), 2):\n        assert nx.d_separated(naive_bayes_graph, {u}, {v}, {0})\n        assert not nx.d_separated(naive_bayes_graph, {u}, {v}, {})"
 },
 {
  "docstring": "Example-based test of d-separation for asia_graph.",
  "code": "def test_asia_graph_dsep(asia_graph):\n    assert nx.d_separated(asia_graph, {'asia', 'smoking'}, {'dyspnea', 'xray'}, {'bronchitis', 'either'})\n    assert nx.d_separated(asia_graph, {'tuberculosis', 'cancer'}, {'bronchitis'}, {'smoking', 'xray'})"
 },
 {
  "docstring": "Test that undirected graphs are not supported.\n\nd-separation and its related algorithms do not apply in\nthe case of undirected graphs.",
  "code": "def test_undirected_graphs_are_not_supported():\n    g = nx.path_graph(3, nx.Graph)\n    with pytest.raises(nx.NetworkXNotImplemented):\n        nx.d_separated(g, {0}, {1}, {2})\n    with pytest.raises(nx.NetworkXNotImplemented):\n        nx.is_minimal_d_separator(g, {0}, {1}, {2})\n    with pytest.raises(nx.NetworkXNotImplemented):\n        nx.minimal_d_separator(g, {0}, {1})"
 },
 {
  "docstring": "Test that cycle graphs should cause erroring.\n\nThis is because PGMs assume a directed acyclic graph.",
  "code": "def test_cyclic_graphs_raise_error():\n    g = nx.cycle_graph(3, nx.DiGraph)\n    with pytest.raises(nx.NetworkXError):\n        nx.d_separated(g, {0}, {1}, {2})\n    with pytest.raises(nx.NetworkXError):\n        nx.minimal_d_separator(g, 0, 1)\n    with pytest.raises(nx.NetworkXError):\n        nx.is_minimal_d_separator(g, 0, 1, {2})"
 },
 {
  "docstring": "Test that graphs that have invalid nodes passed in raise errors.",
  "code": "def test_invalid_nodes_raise_error(asia_graph):\n    with pytest.raises(nx.NodeNotFound):\n        nx.d_separated(asia_graph, {0}, {1}, {2})\n    with pytest.raises(nx.NodeNotFound):\n        nx.is_minimal_d_separator(asia_graph, 0, 1, {2})\n    with pytest.raises(nx.NodeNotFound):\n        nx.minimal_d_separator(asia_graph, 0, 1)"
 },
 {
  "docstring": "Test that is_minimal_d_separator checks for d-separation as well.",
  "code": "def test_minimal_d_separator_checks_dsep():\n    g = nx.DiGraph()\n    g.add_edges_from([('A', 'B'), ('A', 'E'), ('B', 'C'), ('B', 'D'), ('D', 'C'), ('D', 'F'), ('E', 'D'), ('E', 'F')])\n    assert not nx.d_separated(g, {'C'}, {'F'}, {'D'})\n    assert not nx.is_minimal_d_separator(g, 'C', 'F', {'D'})\n    assert not nx.is_minimal_d_separator(g, 'C', 'F', {})"
 },
 {
  "docstring": "When nodes are disconnected, efficiency is 0",
  "code": "def test_efficiency_disconnected_nodes(self):\n    assert nx.efficiency(self.G1, 1, 2) == 0"
 },
 {
  "docstring": "In a disconnected graph the efficiency is 0",
  "code": "def test_local_efficiency_disconnected_graph(self):\n    assert nx.local_efficiency(self.G1) == 0"
 },
 {
  "docstring": "Tests that the average global efficiency of the complete graph is one.",
  "code": "def test_global_efficiency_complete_graph(self):\n    for n in range(2, 10):\n        G = nx.complete_graph(n)\n        assert nx.global_efficiency(G) == 1"
 },
 {
  "docstring": "Test that the local efficiency for a complete graph with at least 3\nnodes should be one. For a graph with only 2 nodes, the induced\nsubgraph has no edges.",
  "code": "def test_local_efficiency_complete_graph(self):\n    for n in range(3, 10):\n        G = nx.complete_graph(n)\n        assert nx.local_efficiency(G) == 1"
 },
 {
  "docstring": "Test that the ego graph is used when computing local efficiency.\nFor more information, see GitHub issue #2710.",
  "code": "def test_using_ego_graph(self):\n    assert nx.local_efficiency(self.G3) == 7 / 12"
 },
 {
  "docstring": "empty graphs should give hashes regardless of other params",
  "code": "def test_empty_graph_hash():\n    G1 = nx.empty_graph()\n    G2 = nx.empty_graph()\n    h1 = nx.weisfeiler_lehman_graph_hash(G1)\n    h2 = nx.weisfeiler_lehman_graph_hash(G2)\n    h3 = nx.weisfeiler_lehman_graph_hash(G2, edge_attr='edge_attr1')\n    h4 = nx.weisfeiler_lehman_graph_hash(G2, node_attr='node_attr1')\n    h5 = nx.weisfeiler_lehman_graph_hash(G2, edge_attr='edge_attr1', node_attr='node_attr1')\n    h6 = nx.weisfeiler_lehman_graph_hash(G2, iterations=10)\n    assert h1 == h2\n    assert h1 == h3\n    assert h1 == h4\n    assert h1 == h5\n    assert h1 == h6"
 },
 {
  "docstring": "A directed graph with no bi-directional edges should yield different a graph hash\nto the same graph taken as undirected if there are no hash collisions.",
  "code": "def test_directed():\n    r = 10\n    for i in range(r):\n        G_directed = nx.gn_graph(10 + r, seed=100 + i)\n        G_undirected = nx.to_undirected(G_directed)\n        h_directed = nx.weisfeiler_lehman_graph_hash(G_directed)\n        h_undirected = nx.weisfeiler_lehman_graph_hash(G_undirected)\n        assert h_directed != h_undirected"
 },
 {
  "docstring": "A directed graph with no bi-directional edges should yield different a graph hash\nto the same graph taken with edge directions reversed if there are no hash collisions.\nHere we test a cycle graph which is the minimal counterexample",
  "code": "def test_reversed():\n    G = nx.cycle_graph(5, create_using=nx.DiGraph)\n    nx.set_node_attributes(G, {n: str(n) for n in G.nodes()}, name='label')\n    G_reversed = G.reverse()\n    h = nx.weisfeiler_lehman_graph_hash(G, node_attr='label')\n    h_reversed = nx.weisfeiler_lehman_graph_hash(G_reversed, node_attr='label')\n    assert h != h_reversed"
 },
 {
  "docstring": "graph hashes should be invariant to node-relabeling (when the output is reindexed\nby the same mapping)",
  "code": "def test_isomorphic():\n    n, r = (100, 10)\n    p = 1.0 / r\n    for i in range(1, r + 1):\n        G1 = nx.erdos_renyi_graph(n, p * i, seed=200 + i)\n        G2 = nx.relabel_nodes(G1, {u: -1 * u for u in G1.nodes()})\n        g1_hash = nx.weisfeiler_lehman_graph_hash(G1)\n        g2_hash = nx.weisfeiler_lehman_graph_hash(G2)\n        assert g1_hash == g2_hash"
 },
 {
  "docstring": "Isomorphic graphs with differing edge attributes should yield different graph\nhashes if the 'edge_attr' argument is supplied and populated in the graph,\nand there are no hash collisions.\nThe output should still be invariant to node-relabeling",
  "code": "def test_isomorphic_edge_attr():\n    n, r = (100, 10)\n    p = 1.0 / r\n    for i in range(1, r + 1):\n        G1 = nx.erdos_renyi_graph(n, p * i, seed=300 + i)\n        for a, b in G1.edges:\n            G1[a][b]['edge_attr1'] = f'{a}-{b}-1'\n            G1[a][b]['edge_attr2'] = f'{a}-{b}-2'\n        g1_hash_with_edge_attr1 = nx.weisfeiler_lehman_graph_hash(G1, edge_attr='edge_attr1')\n        g1_hash_with_edge_attr2 = nx.weisfeiler_lehman_graph_hash(G1, edge_attr='edge_attr2')\n        g1_hash_no_edge_attr = nx.weisfeiler_lehman_graph_hash(G1, edge_attr=None)\n        assert g1_hash_with_edge_attr1 != g1_hash_no_edge_attr\n        assert g1_hash_with_edge_attr2 != g1_hash_no_edge_attr\n        assert g1_hash_with_edge_attr1 != g1_hash_with_edge_attr2\n        G2 = nx.relabel_nodes(G1, {u: -1 * u for u in G1.nodes()})\n        g2_hash_with_edge_attr1 = nx.weisfeiler_lehman_graph_hash(G2, edge_attr='edge_attr1')\n        g2_hash_with_edge_attr2 = nx.weisfeiler_lehman_graph_hash(G2, edge_attr='edge_attr2')\n        assert g1_hash_with_edge_attr1 == g2_hash_with_edge_attr1\n        assert g1_hash_with_edge_attr2 == g2_hash_with_edge_attr2"
 },
 {
  "docstring": "If the 'edge_attr' argument is supplied but is missing from an edge in the graph,\nwe should raise a KeyError",
  "code": "def test_missing_edge_attr():\n    G = nx.Graph()\n    G.add_edges_from([(1, 2, {'edge_attr1': 'a'}), (1, 3, {})])\n    pytest.raises(KeyError, nx.weisfeiler_lehman_graph_hash, G, edge_attr='edge_attr1')"
 },
 {
  "docstring": "Isomorphic graphs with differing node attributes should yield different graph\nhashes if the 'node_attr' argument is supplied and populated in the graph, and\nthere are no hash collisions.\nThe output should still be invariant to node-relabeling",
  "code": "def test_isomorphic_node_attr():\n    n, r = (100, 10)\n    p = 1.0 / r\n    for i in range(1, r + 1):\n        G1 = nx.erdos_renyi_graph(n, p * i, seed=400 + i)\n        for u in G1.nodes():\n            G1.nodes[u]['node_attr1'] = f'{u}-1'\n            G1.nodes[u]['node_attr2'] = f'{u}-2'\n        g1_hash_with_node_attr1 = nx.weisfeiler_lehman_graph_hash(G1, node_attr='node_attr1')\n        g1_hash_with_node_attr2 = nx.weisfeiler_lehman_graph_hash(G1, node_attr='node_attr2')\n        g1_hash_no_node_attr = nx.weisfeiler_lehman_graph_hash(G1, node_attr=None)\n        assert g1_hash_with_node_attr1 != g1_hash_no_node_attr\n        assert g1_hash_with_node_attr2 != g1_hash_no_node_attr\n        assert g1_hash_with_node_attr1 != g1_hash_with_node_attr2\n        G2 = nx.relabel_nodes(G1, {u: -1 * u for u in G1.nodes()})\n        g2_hash_with_node_attr1 = nx.weisfeiler_lehman_graph_hash(G2, node_attr='node_attr1')\n        g2_hash_with_node_attr2 = nx.weisfeiler_lehman_graph_hash(G2, node_attr='node_attr2')\n        assert g1_hash_with_node_attr1 == g2_hash_with_node_attr1\n        assert g1_hash_with_node_attr2 == g2_hash_with_node_attr2"
 },
 {
  "docstring": "If the 'node_attr' argument is supplied but is missing from a node in the graph,\nwe should raise a KeyError",
  "code": "def test_missing_node_attr():\n    G = nx.Graph()\n    G.add_nodes_from([(1, {'node_attr1': 'a'}), (2, {})])\n    G.add_edges_from([(1, 2), (2, 3), (3, 1), (1, 4)])\n    pytest.raises(KeyError, nx.weisfeiler_lehman_graph_hash, G, node_attr='node_attr1')"
 },
 {
  "docstring": "Isomorphic graphs with differing node attributes should yield different graph\nhashes if the 'node_attr' and 'edge_attr' argument is supplied and populated in\nthe graph, and there are no hash collisions.\nThe output should still be invariant to node-relabeling",
  "code": "def test_isomorphic_edge_attr_and_node_attr():\n    n, r = (100, 10)\n    p = 1.0 / r\n    for i in range(1, r + 1):\n        G1 = nx.erdos_renyi_graph(n, p * i, seed=500 + i)\n        for u in G1.nodes():\n            G1.nodes[u]['node_attr1'] = f'{u}-1'\n            G1.nodes[u]['node_attr2'] = f'{u}-2'\n        for a, b in G1.edges:\n            G1[a][b]['edge_attr1'] = f'{a}-{b}-1'\n            G1[a][b]['edge_attr2'] = f'{a}-{b}-2'\n        g1_hash_edge1_node1 = nx.weisfeiler_lehman_graph_hash(G1, edge_attr='edge_attr1', node_attr='node_attr1')\n        g1_hash_edge2_node2 = nx.weisfeiler_lehman_graph_hash(G1, edge_attr='edge_attr2', node_attr='node_attr2')\n        g1_hash_edge1_node2 = nx.weisfeiler_lehman_graph_hash(G1, edge_attr='edge_attr1', node_attr='node_attr2')\n        g1_hash_no_attr = nx.weisfeiler_lehman_graph_hash(G1)\n        assert g1_hash_edge1_node1 != g1_hash_no_attr\n        assert g1_hash_edge2_node2 != g1_hash_no_attr\n        assert g1_hash_edge1_node1 != g1_hash_edge2_node2\n        assert g1_hash_edge1_node2 != g1_hash_edge2_node2\n        assert g1_hash_edge1_node2 != g1_hash_edge1_node1\n        G2 = nx.relabel_nodes(G1, {u: -1 * u for u in G1.nodes()})\n        g2_hash_edge1_node1 = nx.weisfeiler_lehman_graph_hash(G2, edge_attr='edge_attr1', node_attr='node_attr1')\n        g2_hash_edge2_node2 = nx.weisfeiler_lehman_graph_hash(G2, edge_attr='edge_attr2', node_attr='node_attr2')\n        assert g1_hash_edge1_node1 == g2_hash_edge1_node1\n        assert g1_hash_edge2_node2 == g2_hash_edge2_node2"
 },
 {
  "docstring": "The hash string lengths should be as expected for a variety of graphs and\ndigest sizes",
  "code": "def test_digest_size():\n    n, r = (100, 10)\n    p = 1.0 / r\n    for i in range(1, r + 1):\n        G = nx.erdos_renyi_graph(n, p * i, seed=1000 + i)\n        h16 = nx.weisfeiler_lehman_graph_hash(G)\n        h32 = nx.weisfeiler_lehman_graph_hash(G, digest_size=32)\n        assert h16 != h32\n        assert len(h16) == 16 * 2\n        assert len(h32) == 32 * 2"
 },
 {
  "docstring": "returns True if that each hash sequence in 'a' is a prefix for\nthe corresponding sequence indexed by the same node in 'b'.",
  "code": "def is_subiteration(a, b):\n    return all((b[node][:len(hashes)] == hashes for node, hashes in a.items()))"
 },
 {
  "docstring": "returns True if all hex digest sizes are the expected length in a node:subgraph-hashes\ndictionary. Hex digest string length == 2 * bytes digest length since each pair of hex\ndigits encodes 1 byte (https://docs.python.org/3/library/hashlib.html)",
  "code": "def hexdigest_sizes_correct(a, digest_size):\n    hexdigest_size = digest_size * 2\n    list_digest_sizes_correct = lambda l: all((len(x) == hexdigest_size for x in l))\n    return all((list_digest_sizes_correct(hashes) for hashes in a.values()))"
 },
 {
  "docstring": "\"\nempty graphs should give empty dict subgraph hashes regardless of other params",
  "code": "def test_empty_graph_subgraph_hash():\n    G = nx.empty_graph()\n    subgraph_hashes1 = nx.weisfeiler_lehman_subgraph_hashes(G)\n    subgraph_hashes2 = nx.weisfeiler_lehman_subgraph_hashes(G, edge_attr='edge_attr')\n    subgraph_hashes3 = nx.weisfeiler_lehman_subgraph_hashes(G, node_attr='edge_attr')\n    subgraph_hashes4 = nx.weisfeiler_lehman_subgraph_hashes(G, iterations=2)\n    subgraph_hashes5 = nx.weisfeiler_lehman_subgraph_hashes(G, digest_size=64)\n    assert subgraph_hashes1 == {}\n    assert subgraph_hashes2 == {}\n    assert subgraph_hashes3 == {}\n    assert subgraph_hashes4 == {}\n    assert subgraph_hashes5 == {}"
 },
 {
  "docstring": "A directed graph with no bi-directional edges should yield different subgraph hashes\nto the same graph taken as undirected, if all hashes don't collide.",
  "code": "def test_directed_subgraph_hash():\n    r = 10\n    for i in range(r):\n        G_directed = nx.gn_graph(10 + r, seed=100 + i)\n        G_undirected = nx.to_undirected(G_directed)\n        directed_subgraph_hashes = nx.weisfeiler_lehman_subgraph_hashes(G_directed)\n        undirected_subgraph_hashes = nx.weisfeiler_lehman_subgraph_hashes(G_undirected)\n        assert directed_subgraph_hashes != undirected_subgraph_hashes"
 },
 {
  "docstring": "A directed graph with no bi-directional edges should yield different subgraph hashes\nto the same graph taken with edge directions reversed if there are no hash collisions.\nHere we test a cycle graph which is the minimal counterexample",
  "code": "def test_reversed_subgraph_hash():\n    G = nx.cycle_graph(5, create_using=nx.DiGraph)\n    nx.set_node_attributes(G, {n: str(n) for n in G.nodes()}, name='label')\n    G_reversed = G.reverse()\n    h = nx.weisfeiler_lehman_subgraph_hashes(G, node_attr='label')\n    h_reversed = nx.weisfeiler_lehman_subgraph_hashes(G_reversed, node_attr='label')\n    assert h != h_reversed"
 },
 {
  "docstring": "the subgraph hashes should be invariant to node-relabeling when the output is reindexed\nby the same mapping and all hashes don't collide.",
  "code": "def test_isomorphic_subgraph_hash():\n    n, r = (100, 10)\n    p = 1.0 / r\n    for i in range(1, r + 1):\n        G1 = nx.erdos_renyi_graph(n, p * i, seed=200 + i)\n        G2 = nx.relabel_nodes(G1, {u: -1 * u for u in G1.nodes()})\n        g1_subgraph_hashes = nx.weisfeiler_lehman_subgraph_hashes(G1)\n        g2_subgraph_hashes = nx.weisfeiler_lehman_subgraph_hashes(G2)\n        assert g1_subgraph_hashes == {-1 * k: v for k, v in g2_subgraph_hashes.items()}"
 },
 {
  "docstring": "Isomorphic graphs with differing edge attributes should yield different subgraph\nhashes if the 'edge_attr' argument is supplied and populated in the graph, and\nall hashes don't collide.\nThe output should still be invariant to node-relabeling",
  "code": "def test_isomorphic_edge_attr_subgraph_hash():\n    n, r = (100, 10)\n    p = 1.0 / r\n    for i in range(1, r + 1):\n        G1 = nx.erdos_renyi_graph(n, p * i, seed=300 + i)\n        for a, b in G1.edges:\n            G1[a][b]['edge_attr1'] = f'{a}-{b}-1'\n            G1[a][b]['edge_attr2'] = f'{a}-{b}-2'\n        g1_hash_with_edge_attr1 = nx.weisfeiler_lehman_subgraph_hashes(G1, edge_attr='edge_attr1')\n        g1_hash_with_edge_attr2 = nx.weisfeiler_lehman_subgraph_hashes(G1, edge_attr='edge_attr2')\n        g1_hash_no_edge_attr = nx.weisfeiler_lehman_subgraph_hashes(G1, edge_attr=None)\n        assert g1_hash_with_edge_attr1 != g1_hash_no_edge_attr\n        assert g1_hash_with_edge_attr2 != g1_hash_no_edge_attr\n        assert g1_hash_with_edge_attr1 != g1_hash_with_edge_attr2\n        G2 = nx.relabel_nodes(G1, {u: -1 * u for u in G1.nodes()})\n        g2_hash_with_edge_attr1 = nx.weisfeiler_lehman_subgraph_hashes(G2, edge_attr='edge_attr1')\n        g2_hash_with_edge_attr2 = nx.weisfeiler_lehman_subgraph_hashes(G2, edge_attr='edge_attr2')\n        assert g1_hash_with_edge_attr1 == {-1 * k: v for k, v in g2_hash_with_edge_attr1.items()}\n        assert g1_hash_with_edge_attr2 == {-1 * k: v for k, v in g2_hash_with_edge_attr2.items()}"
 },
 {
  "docstring": "If the 'edge_attr' argument is supplied but is missing from an edge in the graph,\nwe should raise a KeyError",
  "code": "def test_missing_edge_attr_subgraph_hash():\n    G = nx.Graph()\n    G.add_edges_from([(1, 2, {'edge_attr1': 'a'}), (1, 3, {})])\n    pytest.raises(KeyError, nx.weisfeiler_lehman_subgraph_hashes, G, edge_attr='edge_attr1')"
 },
 {
  "docstring": "Isomorphic graphs with differing node attributes should yield different subgraph\nhashes if the 'node_attr' argument is supplied and populated in the graph, and\nall hashes don't collide.\nThe output should still be invariant to node-relabeling",
  "code": "def test_isomorphic_node_attr_subgraph_hash():\n    n, r = (100, 10)\n    p = 1.0 / r\n    for i in range(1, r + 1):\n        G1 = nx.erdos_renyi_graph(n, p * i, seed=400 + i)\n        for u in G1.nodes():\n            G1.nodes[u]['node_attr1'] = f'{u}-1'\n            G1.nodes[u]['node_attr2'] = f'{u}-2'\n        g1_hash_with_node_attr1 = nx.weisfeiler_lehman_subgraph_hashes(G1, node_attr='node_attr1')\n        g1_hash_with_node_attr2 = nx.weisfeiler_lehman_subgraph_hashes(G1, node_attr='node_attr2')\n        g1_hash_no_node_attr = nx.weisfeiler_lehman_subgraph_hashes(G1, node_attr=None)\n        assert g1_hash_with_node_attr1 != g1_hash_no_node_attr\n        assert g1_hash_with_node_attr2 != g1_hash_no_node_attr\n        assert g1_hash_with_node_attr1 != g1_hash_with_node_attr2\n        G2 = nx.relabel_nodes(G1, {u: -1 * u for u in G1.nodes()})\n        g2_hash_with_node_attr1 = nx.weisfeiler_lehman_subgraph_hashes(G2, node_attr='node_attr1')\n        g2_hash_with_node_attr2 = nx.weisfeiler_lehman_subgraph_hashes(G2, node_attr='node_attr2')\n        assert g1_hash_with_node_attr1 == {-1 * k: v for k, v in g2_hash_with_node_attr1.items()}\n        assert g1_hash_with_node_attr2 == {-1 * k: v for k, v in g2_hash_with_node_attr2.items()}"
 },
 {
  "docstring": "If the 'node_attr' argument is supplied but is missing from a node in the graph,\nwe should raise a KeyError",
  "code": "def test_missing_node_attr_subgraph_hash():\n    G = nx.Graph()\n    G.add_nodes_from([(1, {'node_attr1': 'a'}), (2, {})])\n    G.add_edges_from([(1, 2), (2, 3), (3, 1), (1, 4)])\n    pytest.raises(KeyError, nx.weisfeiler_lehman_subgraph_hashes, G, node_attr='node_attr1')"
 },
 {
  "docstring": "Isomorphic graphs with differing node attributes should yield different subgraph\nhashes if the 'node_attr' and 'edge_attr' argument is supplied and populated in\nthe graph, and all hashes don't collide\nThe output should still be invariant to node-relabeling",
  "code": "def test_isomorphic_edge_attr_and_node_attr_subgraph_hash():\n    n, r = (100, 10)\n    p = 1.0 / r\n    for i in range(1, r + 1):\n        G1 = nx.erdos_renyi_graph(n, p * i, seed=500 + i)\n        for u in G1.nodes():\n            G1.nodes[u]['node_attr1'] = f'{u}-1'\n            G1.nodes[u]['node_attr2'] = f'{u}-2'\n        for a, b in G1.edges:\n            G1[a][b]['edge_attr1'] = f'{a}-{b}-1'\n            G1[a][b]['edge_attr2'] = f'{a}-{b}-2'\n        g1_hash_edge1_node1 = nx.weisfeiler_lehman_subgraph_hashes(G1, edge_attr='edge_attr1', node_attr='node_attr1')\n        g1_hash_edge2_node2 = nx.weisfeiler_lehman_subgraph_hashes(G1, edge_attr='edge_attr2', node_attr='node_attr2')\n        g1_hash_edge1_node2 = nx.weisfeiler_lehman_subgraph_hashes(G1, edge_attr='edge_attr1', node_attr='node_attr2')\n        g1_hash_no_attr = nx.weisfeiler_lehman_subgraph_hashes(G1)\n        assert g1_hash_edge1_node1 != g1_hash_no_attr\n        assert g1_hash_edge2_node2 != g1_hash_no_attr\n        assert g1_hash_edge1_node1 != g1_hash_edge2_node2\n        assert g1_hash_edge1_node2 != g1_hash_edge2_node2\n        assert g1_hash_edge1_node2 != g1_hash_edge1_node1\n        G2 = nx.relabel_nodes(G1, {u: -1 * u for u in G1.nodes()})\n        g2_hash_edge1_node1 = nx.weisfeiler_lehman_subgraph_hashes(G2, edge_attr='edge_attr1', node_attr='node_attr1')\n        g2_hash_edge2_node2 = nx.weisfeiler_lehman_subgraph_hashes(G2, edge_attr='edge_attr2', node_attr='node_attr2')\n        assert g1_hash_edge1_node1 == {-1 * k: v for k, v in g2_hash_edge1_node1.items()}\n        assert g1_hash_edge2_node2 == {-1 * k: v for k, v in g2_hash_edge2_node2.items()}"
 },
 {
  "docstring": "All nodes should have the correct number of subgraph hashes in the output when\nusing degree as initial node labels\nSubsequent iteration depths for the same graph should be additive for each node",
  "code": "def test_iteration_depth():\n    n, r = (100, 10)\n    p = 1.0 / r\n    for i in range(1, r + 1):\n        G = nx.erdos_renyi_graph(n, p * i, seed=600 + i)\n        depth3 = nx.weisfeiler_lehman_subgraph_hashes(G, iterations=3)\n        depth4 = nx.weisfeiler_lehman_subgraph_hashes(G, iterations=4)\n        depth5 = nx.weisfeiler_lehman_subgraph_hashes(G, iterations=5)\n        assert all((len(hashes) == 3 for hashes in depth3.values()))\n        assert all((len(hashes) == 4 for hashes in depth4.values()))\n        assert all((len(hashes) == 5 for hashes in depth5.values()))\n        assert is_subiteration(depth3, depth4)\n        assert is_subiteration(depth4, depth5)\n        assert is_subiteration(depth3, depth5)"
 },
 {
  "docstring": "All nodes should have the correct number of subgraph hashes in the output when\nsetting initial node labels empty and using an edge attribute when aggregating\nneighborhoods.\nSubsequent iteration depths for the same graph should be additive for each node",
  "code": "def test_iteration_depth_edge_attr():\n    n, r = (100, 10)\n    p = 1.0 / r\n    for i in range(1, r + 1):\n        G = nx.erdos_renyi_graph(n, p * i, seed=700 + i)\n        for a, b in G.edges:\n            G[a][b]['edge_attr1'] = f'{a}-{b}-1'\n        depth3 = nx.weisfeiler_lehman_subgraph_hashes(G, edge_attr='edge_attr1', iterations=3)\n        depth4 = nx.weisfeiler_lehman_subgraph_hashes(G, edge_attr='edge_attr1', iterations=4)\n        depth5 = nx.weisfeiler_lehman_subgraph_hashes(G, edge_attr='edge_attr1', iterations=5)\n        assert all((len(hashes) == 3 for hashes in depth3.values()))\n        assert all((len(hashes) == 4 for hashes in depth4.values()))\n        assert all((len(hashes) == 5 for hashes in depth5.values()))\n        assert is_subiteration(depth3, depth4)\n        assert is_subiteration(depth4, depth5)\n        assert is_subiteration(depth3, depth5)"
 },
 {
  "docstring": "All nodes should have the correct number of subgraph hashes in the output when\nsetting initial node labels to an attribute.\nSubsequent iteration depths for the same graph should be additive for each node",
  "code": "def test_iteration_depth_node_attr():\n    n, r = (100, 10)\n    p = 1.0 / r\n    for i in range(1, r + 1):\n        G = nx.erdos_renyi_graph(n, p * i, seed=800 + i)\n        for u in G.nodes():\n            G.nodes[u]['node_attr1'] = f'{u}-1'\n        depth3 = nx.weisfeiler_lehman_subgraph_hashes(G, node_attr='node_attr1', iterations=3)\n        depth4 = nx.weisfeiler_lehman_subgraph_hashes(G, node_attr='node_attr1', iterations=4)\n        depth5 = nx.weisfeiler_lehman_subgraph_hashes(G, node_attr='node_attr1', iterations=5)\n        assert all((len(hashes) == 3 for hashes in depth3.values()))\n        assert all((len(hashes) == 4 for hashes in depth4.values()))\n        assert all((len(hashes) == 5 for hashes in depth5.values()))\n        assert is_subiteration(depth3, depth4)\n        assert is_subiteration(depth4, depth5)\n        assert is_subiteration(depth3, depth5)"
 },
 {
  "docstring": "All nodes should have the correct number of subgraph hashes in the output when\nsetting initial node labels to an attribute and also using an edge attribute when\naggregating neighborhoods.\nSubsequent iteration depths for the same graph should be additive for each node",
  "code": "def test_iteration_depth_node_edge_attr():\n    n, r = (100, 10)\n    p = 1.0 / r\n    for i in range(1, r + 1):\n        G = nx.erdos_renyi_graph(n, p * i, seed=900 + i)\n        for u in G.nodes():\n            G.nodes[u]['node_attr1'] = f'{u}-1'\n        for a, b in G.edges:\n            G[a][b]['edge_attr1'] = f'{a}-{b}-1'\n        depth3 = nx.weisfeiler_lehman_subgraph_hashes(G, edge_attr='edge_attr1', node_attr='node_attr1', iterations=3)\n        depth4 = nx.weisfeiler_lehman_subgraph_hashes(G, edge_attr='edge_attr1', node_attr='node_attr1', iterations=4)\n        depth5 = nx.weisfeiler_lehman_subgraph_hashes(G, edge_attr='edge_attr1', node_attr='node_attr1', iterations=5)\n        assert all((len(hashes) == 3 for hashes in depth3.values()))\n        assert all((len(hashes) == 4 for hashes in depth4.values()))\n        assert all((len(hashes) == 5 for hashes in depth5.values()))\n        assert is_subiteration(depth3, depth4)\n        assert is_subiteration(depth4, depth5)\n        assert is_subiteration(depth3, depth5)"
 },
 {
  "docstring": "The hash string lengths should be as expected for a variety of graphs and\ndigest sizes",
  "code": "def test_digest_size_subgraph_hash():\n    n, r = (100, 10)\n    p = 1.0 / r\n    for i in range(1, r + 1):\n        G = nx.erdos_renyi_graph(n, p * i, seed=1000 + i)\n        digest_size16_hashes = nx.weisfeiler_lehman_subgraph_hashes(G)\n        digest_size32_hashes = nx.weisfeiler_lehman_subgraph_hashes(G, digest_size=32)\n        assert digest_size16_hashes != digest_size32_hashes\n        assert hexdigest_sizes_correct(digest_size16_hashes, 16)\n        assert hexdigest_sizes_correct(digest_size32_hashes, 32)"
 },
 {
  "docstring": "Self-ancestors should always be the node itself, i.e. lca of (0, 0) is 0.\nSee gh-4458.",
  "code": "def test_all_pairs_lca_self_ancestors():\n    G = nx.DiGraph()\n    G.add_nodes_from(range(5))\n    G.add_edges_from([(1, 0), (2, 0), (3, 2), (4, 1), (4, 3)])\n    ap_lca = nx.all_pairs_lowest_common_ancestor\n    assert all((u == v == a for (u, v), a in ap_lca(G) if u == v))\n    MG = nx.MultiDiGraph(G)\n    assert all((u == v == a for (u, v), a in ap_lca(MG) if u == v))\n    MG.add_edges_from([(1, 0), (2, 0)])\n    assert all((u == v == a for (u, v), a in ap_lca(MG) if u == v))"
 },
 {
  "docstring": "Checks if d1 and d2 contain the same pairs and\nhave a node at the same distance from root for each.\nIf G is None use self.DG.",
  "code": "def assert_lca_dicts_same(self, d1, d2, G=None):\n    if G is None:\n        G = self.DG\n        root_distance = self.root_distance\n    else:\n        roots = [n for n, deg in G.in_degree if deg == 0]\n        assert len(roots) == 1\n        root_distance = nx.shortest_path_length(G, source=roots[0])\n    for a, b in ((min(pair), max(pair)) for pair in chain(d1, d2)):\n        assert root_distance[get_pair(d1, a, b)] == root_distance[get_pair(d2, a, b)]"
 },
 {
  "docstring": "Empty graph",
  "code": "def test_trivial1(self):\n    G = nx.Graph()\n    assert nx.max_weight_matching(G) == set()\n    assert nx.min_weight_matching(G) == set()"
 },
 {
  "docstring": "Create S-blossom and use it for augmentation:",
  "code": "def test_s_blossom(self):\n    G = nx.Graph()\n    G.add_weighted_edges_from([(1, 2, 8), (1, 3, 9), (2, 3, 10), (3, 4, 7)])\n    answer = matching_dict_to_set({1: 2, 2: 1, 3: 4, 4: 3})\n    assert edges_equal(nx.max_weight_matching(G), answer)\n    assert edges_equal(nx.min_weight_matching(G), answer)\n    G.add_weighted_edges_from([(1, 6, 5), (4, 5, 6)])\n    answer = matching_dict_to_set({1: 6, 2: 3, 3: 2, 4: 5, 5: 4, 6: 1})\n    assert edges_equal(nx.max_weight_matching(G), answer)\n    assert edges_equal(nx.min_weight_matching(G), answer)"
 },
 {
  "docstring": "Create S-blossom, relabel as T-blossom, use for augmentation:",
  "code": "def test_s_t_blossom(self):\n    G = nx.Graph()\n    G.add_weighted_edges_from([(1, 2, 9), (1, 3, 8), (2, 3, 10), (1, 4, 5), (4, 5, 4), (1, 6, 3)])\n    answer = matching_dict_to_set({1: 6, 2: 3, 3: 2, 4: 5, 5: 4, 6: 1})\n    assert edges_equal(nx.max_weight_matching(G), answer)\n    assert edges_equal(nx.min_weight_matching(G), answer)\n    G.add_edge(4, 5, weight=3)\n    G.add_edge(1, 6, weight=4)\n    assert edges_equal(nx.max_weight_matching(G), answer)\n    assert edges_equal(nx.min_weight_matching(G), answer)\n    G.remove_edge(1, 6)\n    G.add_edge(3, 6, weight=4)\n    answer = matching_dict_to_set({1: 2, 2: 1, 3: 6, 4: 5, 5: 4, 6: 3})\n    assert edges_equal(nx.max_weight_matching(G), answer)\n    assert edges_equal(nx.min_weight_matching(G), answer)"
 },
 {
  "docstring": "Create nested S-blossom, use for augmentation:",
  "code": "def test_nested_s_blossom(self):\n    G = nx.Graph()\n    G.add_weighted_edges_from([(1, 2, 9), (1, 3, 9), (2, 3, 10), (2, 4, 8), (3, 5, 8), (4, 5, 10), (5, 6, 6)])\n    dict_format = {1: 3, 2: 4, 3: 1, 4: 2, 5: 6, 6: 5}\n    expected = {frozenset(e) for e in matching_dict_to_set(dict_format)}\n    answer = {frozenset(e) for e in nx.max_weight_matching(G)}\n    assert answer == expected\n    answer = {frozenset(e) for e in nx.min_weight_matching(G)}\n    assert answer == expected"
 },
 {
  "docstring": "Create S-blossom, relabel as S, include in nested S-blossom:",
  "code": "def test_nested_s_blossom_relabel(self):\n    G = nx.Graph()\n    G.add_weighted_edges_from([(1, 2, 10), (1, 7, 10), (2, 3, 12), (3, 4, 20), (3, 5, 20), (4, 5, 25), (5, 6, 10), (6, 7, 10), (7, 8, 8)])\n    answer = matching_dict_to_set({1: 2, 2: 1, 3: 4, 4: 3, 5: 6, 6: 5, 7: 8, 8: 7})\n    assert edges_equal(nx.max_weight_matching(G), answer)\n    assert edges_equal(nx.min_weight_matching(G), answer)"
 },
 {
  "docstring": "Create nested S-blossom, augment, expand recursively:",
  "code": "def test_nested_s_blossom_expand(self):\n    G = nx.Graph()\n    G.add_weighted_edges_from([(1, 2, 8), (1, 3, 8), (2, 3, 10), (2, 4, 12), (3, 5, 12), (4, 5, 14), (4, 6, 12), (5, 7, 12), (6, 7, 14), (7, 8, 12)])\n    answer = matching_dict_to_set({1: 2, 2: 1, 3: 5, 4: 6, 5: 3, 6: 4, 7: 8, 8: 7})\n    assert edges_equal(nx.max_weight_matching(G), answer)\n    assert edges_equal(nx.min_weight_matching(G), answer)"
 },
 {
  "docstring": "Create S-blossom, relabel as T, expand:",
  "code": "def test_s_blossom_relabel_expand(self):\n    G = nx.Graph()\n    G.add_weighted_edges_from([(1, 2, 23), (1, 5, 22), (1, 6, 15), (2, 3, 25), (3, 4, 22), (4, 5, 25), (4, 8, 14), (5, 7, 13)])\n    answer = matching_dict_to_set({1: 6, 2: 3, 3: 2, 4: 8, 5: 7, 6: 1, 7: 5, 8: 4})\n    assert edges_equal(nx.max_weight_matching(G), answer)\n    assert edges_equal(nx.min_weight_matching(G), answer)"
 },
 {
  "docstring": "Create nested S-blossom, relabel as T, expand:",
  "code": "def test_nested_s_blossom_relabel_expand(self):\n    G = nx.Graph()\n    G.add_weighted_edges_from([(1, 2, 19), (1, 3, 20), (1, 8, 8), (2, 3, 25), (2, 4, 18), (3, 5, 18), (4, 5, 13), (4, 7, 7), (5, 6, 7)])\n    answer = matching_dict_to_set({1: 8, 2: 3, 3: 2, 4: 7, 5: 6, 6: 5, 7: 4, 8: 1})\n    assert edges_equal(nx.max_weight_matching(G), answer)\n    assert edges_equal(nx.min_weight_matching(G), answer)"
 },
 {
  "docstring": "Create blossom, relabel as T in more than one way, expand,\naugment:",
  "code": "def test_nasty_blossom1(self):\n    G = nx.Graph()\n    G.add_weighted_edges_from([(1, 2, 45), (1, 5, 45), (2, 3, 50), (3, 4, 45), (4, 5, 50), (1, 6, 30), (3, 9, 35), (4, 8, 35), (5, 7, 26), (9, 10, 5)])\n    ansdict = {1: 6, 2: 3, 3: 2, 4: 8, 5: 7, 6: 1, 7: 5, 8: 4, 9: 10, 10: 9}\n    answer = matching_dict_to_set(ansdict)\n    assert edges_equal(nx.max_weight_matching(G), answer)\n    assert edges_equal(nx.min_weight_matching(G), answer)"
 },
 {
  "docstring": "Again but slightly different:",
  "code": "def test_nasty_blossom2(self):\n    G = nx.Graph()\n    G.add_weighted_edges_from([(1, 2, 45), (1, 5, 45), (2, 3, 50), (3, 4, 45), (4, 5, 50), (1, 6, 30), (3, 9, 35), (4, 8, 26), (5, 7, 40), (9, 10, 5)])\n    ans = {1: 6, 2: 3, 3: 2, 4: 8, 5: 7, 6: 1, 7: 5, 8: 4, 9: 10, 10: 9}\n    answer = matching_dict_to_set(ans)\n    assert edges_equal(nx.max_weight_matching(G), answer)\n    assert edges_equal(nx.min_weight_matching(G), answer)"
 },
 {
  "docstring": "Create blossom, relabel as T, expand such that a new\nleast-slack S-to-free dge is produced, augment:",
  "code": "def test_nasty_blossom_least_slack(self):\n    G = nx.Graph()\n    G.add_weighted_edges_from([(1, 2, 45), (1, 5, 45), (2, 3, 50), (3, 4, 45), (4, 5, 50), (1, 6, 30), (3, 9, 35), (4, 8, 28), (5, 7, 26), (9, 10, 5)])\n    ans = {1: 6, 2: 3, 3: 2, 4: 8, 5: 7, 6: 1, 7: 5, 8: 4, 9: 10, 10: 9}\n    answer = matching_dict_to_set(ans)\n    assert edges_equal(nx.max_weight_matching(G), answer)\n    assert edges_equal(nx.min_weight_matching(G), answer)"
 },
 {
  "docstring": "Create nested blossom, relabel as T in more than one way",
  "code": "def test_nasty_blossom_augmenting(self):\n    G = nx.Graph()\n    G.add_weighted_edges_from([(1, 2, 45), (1, 7, 45), (2, 3, 50), (3, 4, 45), (4, 5, 95), (4, 6, 94), (5, 6, 94), (6, 7, 50), (1, 8, 30), (3, 11, 35), (5, 9, 36), (7, 10, 26), (11, 12, 5)])\n    ans = {1: 8, 2: 3, 3: 2, 4: 6, 5: 9, 6: 4, 7: 10, 8: 1, 9: 5, 10: 7, 11: 12, 12: 11}\n    answer = matching_dict_to_set(ans)\n    assert edges_equal(nx.max_weight_matching(G), answer)\n    assert edges_equal(nx.min_weight_matching(G), answer)"
 },
 {
  "docstring": "Create nested S-blossom, relabel as S, expand recursively:",
  "code": "def test_nasty_blossom_expand_recursively(self):\n    G = nx.Graph()\n    G.add_weighted_edges_from([(1, 2, 40), (1, 3, 40), (2, 3, 60), (2, 4, 55), (3, 5, 55), (4, 5, 50), (1, 8, 15), (5, 7, 30), (7, 6, 10), (8, 10, 10), (4, 9, 30)])\n    ans = {1: 2, 2: 1, 3: 5, 4: 9, 5: 3, 6: 7, 7: 6, 8: 10, 9: 4, 10: 8}\n    answer = matching_dict_to_set(ans)\n    assert edges_equal(nx.max_weight_matching(G), answer)\n    assert edges_equal(nx.min_weight_matching(G), answer)"
 },
 {
  "docstring": "Tests that a maximal matching is computed correctly\nregardless of the order in which nodes are added to the graph.",
  "code": "def test_ordering(self):\n    for nodes in permutations(range(3)):\n        G = nx.Graph()\n        G.add_nodes_from(nodes)\n        G.add_edges_from([(0, 1), (0, 2)])\n        matching = nx.maximal_matching(G)\n        assert len(matching) == 1\n        assert nx.is_maximal_matching(G, matching)"
 },
 {
  "docstring": "Maximal independent set for complete graphs",
  "code": "@pytest.mark.parametrize('graph', [nx.complete_graph(5), nx.complete_graph(55)])\ndef test_K5(graph):\n    assert all((nx.maximal_independent_set(graph, [n]) == [n] for n in graph))"
 },
 {
  "docstring": "Bad input should raise exception.",
  "code": "def test_exceptions():\n    G = nx.florentine_families_graph()\n    pytest.raises(nx.NetworkXUnfeasible, nx.maximal_independent_set, G, ['Smith'])\n    pytest.raises(nx.NetworkXUnfeasible, nx.maximal_independent_set, G, ['Salviati', 'Pazzi'])\n    pytest.raises(nx.NetworkXNotImplemented, nx.maximal_independent_set, nx.DiGraph(G))"
 },
 {
  "docstring": "Generate 5 random graphs of different types and sizes and\nmake sure that all sets are independent and maximal.",
  "code": "def test_random_graphs():\n    for i in range(0, 50, 10):\n        G = nx.erdos_renyi_graph(i * 10 + 1, random.random())\n        IS = nx.maximal_independent_set(G)\n        assert G.subgraph(IS).number_of_edges() == 0\n        neighbors_of_MIS = set.union(*(set(G.neighbors(v)) for v in IS))\n        assert all((v in neighbors_of_MIS for v in set(G.nodes()).difference(IS)))"
 },
 {
  "docstring": "Checks that the planar embedding of the input is correct",
  "code": "def check_embedding_data(embedding_data):\n    embedding = nx.PlanarEmbedding()\n    embedding.set_data(embedding_data)\n    pos_fully = nx.combinatorial_embedding_to_pos(embedding, False)\n    msg = 'Planar drawing does not conform to the embedding (fully triangulation)'\n    assert planar_drawing_conforms_to_embedding(embedding, pos_fully), msg\n    check_edge_intersections(embedding, pos_fully)\n    pos_internally = nx.combinatorial_embedding_to_pos(embedding, True)\n    msg = 'Planar drawing does not conform to the embedding (internal triangulation)'\n    assert planar_drawing_conforms_to_embedding(embedding, pos_internally), msg\n    check_edge_intersections(embedding, pos_internally)"
 },
 {
  "docstring": "Check all edges in G for intersections.\n\nRaises an exception if an intersection is found.\n\nParameters\n----------\nG : NetworkX graph\npos : dict\n    Maps every node to a tuple (x, y) representing its position",
  "code": "def check_edge_intersections(G, pos):\n    for a, b in G.edges():\n        for c, d in G.edges():\n            if a != c and b != d and (b != c) and (a != d):\n                x1, y1 = pos[a]\n                x2, y2 = pos[b]\n                x3, y3 = pos[c]\n                x4, y4 = pos[d]\n                determinant = (x1 - x2) * (y3 - y4) - (y1 - y2) * (x3 - x4)\n                if determinant != 0:\n                    px = (x1 * y2 - y1 * x2) * (x3 - x4) - (x1 - x2) * (x3 * y4 - y3 * x4) / determinant\n                    py = (x1 * y2 - y1 * x2) * (y3 - y4) - (y1 - y2) * (x3 * y4 - y3 * x4) / determinant\n                    if point_in_between(pos[a], pos[b], (px, py)) and point_in_between(pos[c], pos[d], (px, py)):\n                        msg = f'There is an intersection at {px},{py}'\n                        raise nx.NetworkXException(msg)\n                msg = 'A node lies on a edge connecting two other nodes'\n                if point_in_between(pos[a], pos[b], pos[c]) or point_in_between(pos[a], pos[b], pos[d]) or point_in_between(pos[c], pos[d], pos[a]) or point_in_between(pos[c], pos[d], pos[b]):\n                    raise nx.NetworkXException(msg)"
 },
 {
  "docstring": "Checks if pos conforms to the planar embedding\n\nReturns true iff the neighbors are actually oriented in the orientation\nspecified of the embedding",
  "code": "def planar_drawing_conforms_to_embedding(embedding, pos):\n    for v in embedding:\n        nbr_vectors = []\n        v_pos = pos[v]\n        for nbr in embedding[v]:\n            new_vector = Vector(pos[nbr][0] - v_pos[0], pos[nbr][1] - v_pos[1], nbr)\n            nbr_vectors.append(new_vector)\n        nbr_vectors.sort()\n        for idx, nbr_vector in enumerate(nbr_vectors):\n            cw_vector = nbr_vectors[(idx + 1) % len(nbr_vectors)]\n            ccw_vector = nbr_vectors[idx - 1]\n            if embedding[v][nbr_vector.node]['cw'] != cw_vector.node or embedding[v][nbr_vector.node]['ccw'] != ccw_vector.node:\n                return False\n            if cw_vector.node != nbr_vector.node and cw_vector == nbr_vector:\n                return False\n            if ccw_vector.node != nbr_vector.node and ccw_vector == nbr_vector:\n                return False\n    return True"
 },
 {
  "docstring": "Tutte polynomial factors into the Tutte polynomials of its components.\nVerify this property with the disjoint union of two copies of the input graph.",
  "code": "@pytest.mark.parametrize('G', _test_tutte_graphs.keys())\ndef test_tutte_polynomial_disjoint(G):\n    t_g = nx.tutte_polynomial(G)\n    H = nx.disjoint_union(G, G)\n    t_h = nx.tutte_polynomial(H)\n    assert sympy.simplify(t_g * t_g).equals(t_h)"
 },
 {
  "docstring": "Chromatic polynomial factors into the Chromatic polynomials of its\ncomponents. Verify this property with the disjoint union of two copies of\nthe input graph.",
  "code": "@pytest.mark.parametrize('G', _test_chromatic_graphs.keys())\ndef test_chromatic_polynomial_disjoint(G):\n    x_g = nx.chromatic_polynomial(G)\n    H = nx.disjoint_union(G, G)\n    x_h = nx.chromatic_polynomial(H)\n    assert sympy.simplify(x_g * x_g).equals(x_h)"
 },
 {
  "docstring": "you may need to draw this graph to make sure it is reasonable",
  "code": "def test_all_simple_paths_on_non_trivial_graph():\n    G = nx.path_graph(5, create_using=nx.DiGraph())\n    G.add_edges_from([(0, 5), (1, 5), (1, 3), (5, 4), (4, 2), (4, 3)])\n    paths = nx.all_simple_paths(G, 1, [2, 3])\n    assert {tuple(p) for p in paths} == {(1, 2), (1, 3, 4, 2), (1, 5, 4, 2), (1, 3), (1, 2, 3), (1, 5, 4, 3), (1, 5, 4, 2, 3)}\n    paths = nx.all_simple_paths(G, 1, [2, 3], cutoff=3)\n    assert {tuple(p) for p in paths} == {(1, 2), (1, 3, 4, 2), (1, 5, 4, 2), (1, 3), (1, 2, 3), (1, 5, 4, 3)}\n    paths = nx.all_simple_paths(G, 1, [2, 3], cutoff=2)\n    assert {tuple(p) for p in paths} == {(1, 2), (1, 3), (1, 2, 3)}"
 },
 {
  "docstring": "you may need to draw this graph to make sure it is reasonable",
  "code": "def test_all_simple_edge_paths_on_non_trivial_graph():\n    G = nx.path_graph(5, create_using=nx.DiGraph())\n    G.add_edges_from([(0, 5), (1, 5), (1, 3), (5, 4), (4, 2), (4, 3)])\n    paths = nx.all_simple_edge_paths(G, 1, [2, 3])\n    assert {tuple(p) for p in paths} == {((1, 2),), ((1, 3), (3, 4), (4, 2)), ((1, 5), (5, 4), (4, 2)), ((1, 3),), ((1, 2), (2, 3)), ((1, 5), (5, 4), (4, 3)), ((1, 5), (5, 4), (4, 2), (2, 3))}\n    paths = nx.all_simple_edge_paths(G, 1, [2, 3], cutoff=3)\n    assert {tuple(p) for p in paths} == {((1, 2),), ((1, 3), (3, 4), (4, 2)), ((1, 5), (5, 4), (4, 2)), ((1, 3),), ((1, 2), (2, 3)), ((1, 5), (5, 4), (4, 3))}\n    paths = nx.all_simple_edge_paths(G, 1, [2, 3], cutoff=2)\n    assert {tuple(p) for p in paths} == {((1, 2),), ((1, 3),), ((1, 2), (2, 3))}"
 },
 {
  "docstring": "Tests that the empty list is not a valid path, since there\nshould be a one-to-one correspondence between paths as lists of\nnodes and paths as lists of edges.",
  "code": "def test_empty_list(self):\n    G = nx.trivial_graph()\n    assert not nx.is_simple_path(G, [])"
 },
 {
  "docstring": "Tests that the trivial path, a path of length one, is\nconsidered a simple path in a graph.",
  "code": "def test_trivial_path(self):\n    G = nx.trivial_graph()\n    assert nx.is_simple_path(G, [0])"
 },
 {
  "docstring": "Tests that a list whose sole element is an object not in the\ngraph is not considered a simple path.",
  "code": "def test_trivial_nonpath(self):\n    G = nx.trivial_graph()\n    assert not nx.is_simple_path(G, ['not a node'])"
 },
 {
  "docstring": "Test that a deprecation warning is raised when s_metric is called with\na `normalized` kwarg.",
  "code": "def test_normalized_deprecation_warning():\n    G = nx.cycle_graph(7)\n    with warnings.catch_warnings():\n        warnings.simplefilter('error')\n        assert nx.s_metric(G) == 28\n    with pytest.deprecated_call():\n        nx.s_metric(G, normalized=True)\n    with pytest.raises(TypeError):\n        nx.s_metric(G, normalize=True)"
 },
 {
  "docstring": "Test whether a spanner is valid.\n\nThis function tests whether the given spanner is a subgraph of the\ngiven graph G with the same node set. It also tests for all shortest\npaths whether they adhere to the given stretch.\n\nParameters\n----------\nG : NetworkX graph\n    The original graph for which the spanner was constructed.\n\nspanner : NetworkX graph\n    The spanner to be tested.\n\nstretch : float\n    The proclaimed stretch of the spanner.\n\nweight : object\n    The edge attribute to use as distance.",
  "code": "def _test_spanner(G, spanner, stretch, weight=None):\n    assert set(G.nodes()) == set(spanner.nodes())\n    for u, v in spanner.edges():\n        assert G.has_edge(u, v)\n        if weight:\n            assert spanner[u][v][weight] == G[u][v][weight]\n    original_length = dict(nx.shortest_path_length(G, weight=weight))\n    spanner_length = dict(nx.shortest_path_length(spanner, weight=weight))\n    for u in G.nodes():\n        for v in G.nodes():\n            if u in original_length and v in original_length[u]:\n                assert spanner_length[u][v] <= stretch * original_length[u][v]"
 },
 {
  "docstring": "Assigns random weights to the edges of a graph.\n\nParameters\n----------\n\nG : NetworkX graph\n    The original graph for which the spanner was constructed.\n\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.",
  "code": "@py_random_state(1)\ndef _assign_random_weights(G, seed=None):\n    for u, v in G.edges():\n        G[u][v]['weight'] = seed.random()"
 },
 {
  "docstring": "Test a trivial spanner with stretch 1.",
  "code": "def test_spanner_trivial():\n    G = nx.complete_graph(20)\n    spanner = nx.spanner(G, 1, seed=_seed)\n    for u, v in G.edges:\n        assert spanner.has_edge(u, v)"
 },
 {
  "docstring": "Test spanner construction on a complete unweighted graph.",
  "code": "def test_spanner_unweighted_complete_graph():\n    G = nx.complete_graph(20)\n    spanner = nx.spanner(G, 4, seed=_seed)\n    _test_spanner(G, spanner, 4)\n    spanner = nx.spanner(G, 10, seed=_seed)\n    _test_spanner(G, spanner, 10)"
 },
 {
  "docstring": "Test spanner construction on a complete weighted graph.",
  "code": "def test_spanner_weighted_complete_graph():\n    G = nx.complete_graph(20)\n    _assign_random_weights(G, seed=_seed)\n    spanner = nx.spanner(G, 4, weight='weight', seed=_seed)\n    _test_spanner(G, spanner, 4, weight='weight')\n    spanner = nx.spanner(G, 10, weight='weight', seed=_seed)\n    _test_spanner(G, spanner, 10, weight='weight')"
 },
 {
  "docstring": "Test spanner construction on an unweighted gnp graph.",
  "code": "def test_spanner_unweighted_gnp_graph():\n    G = nx.gnp_random_graph(20, 0.4, seed=_seed)\n    spanner = nx.spanner(G, 4, seed=_seed)\n    _test_spanner(G, spanner, 4)\n    spanner = nx.spanner(G, 10, seed=_seed)\n    _test_spanner(G, spanner, 10)"
 },
 {
  "docstring": "Test spanner construction on an weighted gnp graph.",
  "code": "def test_spanner_weighted_gnp_graph():\n    G = nx.gnp_random_graph(20, 0.4, seed=_seed)\n    _assign_random_weights(G, seed=_seed)\n    spanner = nx.spanner(G, 4, weight='weight', seed=_seed)\n    _test_spanner(G, spanner, 4, weight='weight')\n    spanner = nx.spanner(G, 10, weight='weight', seed=_seed)\n    _test_spanner(G, spanner, 10, weight='weight')"
 },
 {
  "docstring": "Test spanner construction on a disconnected graph.",
  "code": "def test_spanner_unweighted_disconnected_graph():\n    G = nx.disjoint_union(nx.complete_graph(10), nx.complete_graph(10))\n    spanner = nx.spanner(G, 4, seed=_seed)\n    _test_spanner(G, spanner, 4)\n    spanner = nx.spanner(G, 10, seed=_seed)\n    _test_spanner(G, spanner, 10)"
 },
 {
  "docstring": "Check whether an invalid stretch is caught.",
  "code": "def test_spanner_invalid_stretch():\n    with pytest.raises(ValueError):\n        G = nx.empty_graph()\n        nx.spanner(G, 0)"
 },
 {
  "docstring": "Verify that an empty directed graph results in no compressor nodes",
  "code": "def test_empty(self):\n    G = nx.DiGraph()\n    compressed_graph, c_nodes = nx.dedensify(G, threshold=2)\n    assert c_nodes == set()"
 },
 {
  "docstring": "Reconstructs the original graph from a dedensified, directed graph\n\nParameters\n----------\nG: dedensified graph\n   A networkx graph\ncompressor_nodes: iterable\n   Iterable of compressor nodes in the dedensified graph\ninplace: bool, optional (default: False)\n   Indicates if densification should be done inplace\n\nReturns\n-------\nG: graph\n   A densified networkx graph",
  "code": "@staticmethod\ndef densify(G, compressor_nodes, copy=True):\n    if copy:\n        G = G.copy()\n    for compressor_node in compressor_nodes:\n        all_neighbors = set(nx.all_neighbors(G, compressor_node))\n        out_neighbors = set(G.neighbors(compressor_node))\n        for out_neighbor in out_neighbors:\n            G.remove_edge(compressor_node, out_neighbor)\n        in_neighbors = all_neighbors - out_neighbors\n        for in_neighbor in in_neighbors:\n            G.remove_edge(in_neighbor, compressor_node)\n            for out_neighbor in out_neighbors:\n                G.add_edge(in_neighbor, out_neighbor)\n        G.remove_node(compressor_node)\n    return G"
 },
 {
  "docstring": "Verifies that dedensify produced the correct edges to/from compressor\nnodes in a directed graph",
  "code": "def test_dedensify_edges(self):\n    G = self.build_original_graph()\n    compressed_G = self.build_compressed_graph()\n    compressed_graph, c_nodes = nx.dedensify(G, threshold=2)\n    for s, t in compressed_graph.edges():\n        o_s = ''.join(sorted(s))\n        o_t = ''.join(sorted(t))\n        compressed_graph_exists = compressed_graph.has_edge(s, t)\n        verified_compressed_exists = compressed_G.has_edge(o_s, o_t)\n        assert compressed_graph_exists == verified_compressed_exists\n    assert len(c_nodes) == len(self.c_nodes)"
 },
 {
  "docstring": "Verifies that dedensify produced the correct number of compressor nodes\nin a directed graph",
  "code": "def test_dedensify_edge_count(self):\n    G = self.build_original_graph()\n    original_edge_count = len(G.edges())\n    c_G, c_nodes = nx.dedensify(G, threshold=2)\n    compressed_edge_count = len(c_G.edges())\n    assert compressed_edge_count <= original_edge_count\n    compressed_G = self.build_compressed_graph()\n    assert compressed_edge_count == len(compressed_G.edges())"
 },
 {
  "docstring": "Verifies that densification produces the correct edges from the\noriginal directed graph",
  "code": "def test_densify_edges(self):\n    compressed_G = self.build_compressed_graph()\n    original_graph = self.densify(compressed_G, self.c_nodes, copy=True)\n    G = self.build_original_graph()\n    for s, t in G.edges():\n        assert G.has_edge(s, t) == original_graph.has_edge(s, t)"
 },
 {
  "docstring": "Verifies that densification produces the correct number of edges in the\noriginal directed graph",
  "code": "def test_densify_edge_count(self):\n    compressed_G = self.build_compressed_graph()\n    compressed_edge_count = len(compressed_G.edges())\n    original_graph = self.densify(compressed_G, self.c_nodes)\n    original_edge_count = len(original_graph.edges())\n    assert compressed_edge_count <= original_edge_count\n    G = self.build_original_graph()\n    assert original_edge_count == len(G.edges())"
 },
 {
  "docstring": "Builds graph shown in the original research paper",
  "code": "def build_original_graph(self):\n    original_matrix = [('1', 'CB'), ('2', 'ABC'), ('3', ['A', 'B', '6']), ('4', 'ABC'), ('5', 'AB'), ('6', ['5']), ('A', ['6'])]\n    graph = nx.Graph()\n    for source, targets in original_matrix:\n        for target in targets:\n            graph.add_edge(source, target)\n    return graph"
 },
 {
  "docstring": "Verify that an empty undirected graph results in no compressor nodes",
  "code": "def test_empty(self):\n    G = nx.Graph()\n    compressed_G, c_nodes = nx.dedensify(G, threshold=2)\n    assert c_nodes == set()"
 },
 {
  "docstring": "Verifies that dedensify produced correct compressor nodes and the\ncorrect edges to/from the compressor nodes in an undirected graph",
  "code": "def test_dedensify_edges(self):\n    G = self.build_original_graph()\n    c_G, c_nodes = nx.dedensify(G, threshold=2)\n    v_compressed_G = self.build_compressed_graph()\n    for s, t in c_G.edges():\n        o_s = ''.join(sorted(s))\n        o_t = ''.join(sorted(t))\n        has_compressed_edge = c_G.has_edge(s, t)\n        verified_has_compressed_edge = v_compressed_G.has_edge(o_s, o_t)\n        assert has_compressed_edge == verified_has_compressed_edge\n    assert len(c_nodes) == len(self.c_nodes)"
 },
 {
  "docstring": "Verifies that dedensify produced the correct number of edges in an\nundirected graph",
  "code": "def test_dedensify_edge_count(self):\n    G = self.build_original_graph()\n    c_G, c_nodes = nx.dedensify(G, threshold=2, copy=True)\n    compressed_edge_count = len(c_G.edges())\n    verified_original_edge_count = len(G.edges())\n    assert compressed_edge_count <= verified_original_edge_count\n    verified_compressed_G = self.build_compressed_graph()\n    verified_compressed_edge_count = len(verified_compressed_G.edges())\n    assert compressed_edge_count == verified_compressed_edge_count"
 },
 {
  "docstring": "A tournament must have no self-loops.",
  "code": "def test_self_loops():\n    G = DiGraph()\n    G.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 0), (1, 3), (0, 2)])\n    G.add_edge(0, 0)\n    assert not is_tournament(G)"
 },
 {
  "docstring": "A tournament must not have any pair of nodes without at least\none edge joining the pair.",
  "code": "def test_missing_edges():\n    G = DiGraph()\n    G.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 0), (1, 3)])\n    assert not is_tournament(G)"
 },
 {
  "docstring": "A tournament must not have any pair of nodes with greater\nthan one edge joining the pair.",
  "code": "def test_bidirectional_edges():\n    G = DiGraph()\n    G.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 0), (1, 3), (0, 2)])\n    G.add_edge(1, 0)\n    assert not is_tournament(G)"
 },
 {
  "docstring": "Tests that :func:`networkx.tournament.hamiltonian_path`\nreturns a Hamiltonian cycle when provided a strongly connected\ntournament.",
  "code": "def test_hamiltonian_cycle():\n    G = DiGraph()\n    G.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 0), (1, 3), (0, 2)])\n    path = hamiltonian_path(G)\n    assert len(path) == 4\n    assert all((v in G[u] for u, v in zip(path, path[1:])))\n    assert path[0] in G[path[-1]]"
 },
 {
  "docstring": "Tests for a reachable pair of nodes.",
  "code": "def test_reachable_pair():\n    G = DiGraph([(0, 1), (1, 2), (2, 0)])\n    assert is_reachable(G, 0, 2)"
 },
 {
  "docstring": "Tests that a node is always reachable from it.",
  "code": "def test_same_node_is_reachable():\n    G = DiGraph((sorted(p) for p in combinations(range(10), 2)))\n    assert all((is_reachable(G, v, v) for v in G))"
 },
 {
  "docstring": "Tests for an unreachable pair of nodes.",
  "code": "def test_unreachable_pair():\n    G = DiGraph([(0, 1), (0, 2), (1, 2)])\n    assert not is_reachable(G, 1, 0)"
 },
 {
  "docstring": "Tests for a strongly connected tournament.",
  "code": "def test_is_strongly_connected():\n    G = DiGraph([(0, 1), (1, 2), (2, 0)])\n    assert is_strongly_connected(G)"
 },
 {
  "docstring": "Tests for a tournament that is not strongly connected.",
  "code": "def test_not_strongly_connected():\n    G = DiGraph([(0, 1), (0, 2), (1, 2)])\n    assert not is_strongly_connected(G)"
 },
 {
  "docstring": "Tests the triadic_census function.",
  "code": "def test_triadic_census():\n    G = nx.DiGraph()\n    G.add_edges_from(['01', '02', '03', '04', '05', '12', '16', '51', '56', '65'])\n    expected = {'030T': 2, '120C': 1, '210': 0, '120U': 0, '012': 9, '102': 3, '021U': 0, '111U': 0, '003': 8, '030C': 0, '021D': 9, '201': 0, '111D': 1, '300': 0, '120D': 0, '021C': 2}\n    actual = nx.triadic_census(G)\n    assert expected == actual"
 },
 {
  "docstring": "Tests the is_triad function",
  "code": "def test_is_triad():\n    G = nx.karate_club_graph()\n    G = G.to_directed()\n    for i in range(100):\n        nodes = sample(sorted(G.nodes()), 3)\n        G2 = G.subgraph(nodes)\n        assert nx.is_triad(G2)"
 },
 {
  "docstring": "Tests the all_triplets function.",
  "code": "def test_all_triplets():\n    G = nx.DiGraph()\n    G.add_edges_from(['01', '02', '03', '04', '05', '12', '16', '51', '56', '65'])\n    expected = [f'{i},{j},{k}' for i in range(7) for j in range(i + 1, 7) for k in range(j + 1, 7)]\n    expected = [set(x.split(',')) for x in expected]\n    actual = [set(x) for x in nx.all_triplets(G)]\n    assert all((any((s1 == s2 for s1 in expected)) for s2 in actual))"
 },
 {
  "docstring": "Tests the all_triplets function.",
  "code": "def test_all_triads():\n    G = nx.DiGraph()\n    G.add_edges_from(['01', '02', '03', '04', '05', '12', '16', '51', '56', '65'])\n    expected = [f'{i},{j},{k}' for i in range(7) for j in range(i + 1, 7) for k in range(j + 1, 7)]\n    expected = [G.subgraph(x.split(',')) for x in expected]\n    actual = list(nx.all_triads(G))\n    assert all((any((nx.is_isomorphic(G1, G2) for G1 in expected)) for G2 in actual))"
 },
 {
  "docstring": "Tests the triad_type function.",
  "code": "def test_triad_type():\n    G = nx.DiGraph({0: [], 1: [], 2: []})\n    assert nx.triad_type(G) == '003'\n    G = nx.DiGraph({0: [1], 1: [], 2: []})\n    assert nx.triad_type(G) == '012'\n    G = nx.DiGraph([(0, 1), (0, 2)])\n    assert nx.triad_type(G) == '021D'\n    G = nx.DiGraph({0: [1], 1: [0], 2: []})\n    assert nx.triad_type(G) == '102'\n    G = nx.DiGraph([(0, 1), (2, 1)])\n    assert nx.triad_type(G) == '021U'\n    G = nx.DiGraph([(0, 1), (1, 2)])\n    assert nx.triad_type(G) == '021C'\n    G = nx.DiGraph([(0, 1), (1, 0), (2, 1)])\n    assert nx.triad_type(G) == '111D'\n    G = nx.DiGraph([(0, 1), (1, 0), (1, 2)])\n    assert nx.triad_type(G) == '111U'\n    G = nx.DiGraph([(0, 1), (1, 2), (0, 2)])\n    assert nx.triad_type(G) == '030T'\n    G = nx.DiGraph([(0, 1), (1, 2), (2, 0)])\n    assert nx.triad_type(G) == '030C'\n    G = nx.DiGraph([(0, 1), (1, 0), (2, 0), (0, 2)])\n    assert nx.triad_type(G) == '201'\n    G = nx.DiGraph([(0, 1), (1, 0), (2, 0), (2, 1)])\n    assert nx.triad_type(G) == '120D'\n    G = nx.DiGraph([(0, 1), (1, 0), (0, 2), (1, 2)])\n    assert nx.triad_type(G) == '120U'\n    G = nx.DiGraph([(0, 1), (1, 0), (0, 2), (2, 1)])\n    assert nx.triad_type(G) == '120C'\n    G = nx.DiGraph([(0, 1), (1, 0), (2, 1), (1, 2), (0, 2)])\n    assert nx.triad_type(G) == '210'\n    G = nx.DiGraph([(0, 1), (1, 0), (1, 2), (2, 1), (0, 2), (2, 0)])\n    assert nx.triad_type(G) == '300'"
 },
 {
  "docstring": "Tests the all_triplets function.",
  "code": "def test_triads_by_type():\n    G = nx.DiGraph()\n    G.add_edges_from(['01', '02', '03', '04', '05', '12', '16', '51', '56', '65'])\n    all_triads = nx.all_triads(G)\n    expected = defaultdict(list)\n    for triad in all_triads:\n        name = nx.triad_type(triad)\n        expected[name].append(triad)\n    actual = nx.triads_by_type(G)\n    assert set(actual.keys()) == set(expected.keys())\n    for tri_type, actual_Gs in actual.items():\n        expected_Gs = expected[tri_type]\n        for a in actual_Gs:\n            assert any((nx.is_isomorphic(a, e) for e in expected_Gs))"
 },
 {
  "docstring": "Tests the random_triad function",
  "code": "def test_random_triad():\n    G = nx.karate_club_graph()\n    G = G.to_directed()\n    for i in range(100):\n        assert nx.is_triad(nx.random_triad(G))\n    G = nx.DiGraph()\n    msg = 'at least 3 nodes to form a triad'\n    with pytest.raises(nx.NetworkXError, match=msg):\n        nx.random_triad(G)"
 },
 {
  "docstring": "Tests the triadic_census function.",
  "code": "def test_triadic_census_nodelist():\n    G = nx.DiGraph()\n    G.add_edges_from(['01', '02', '03', '04', '05', '12', '16', '51', '56', '65'])\n    expected = {'030T': 2, '120C': 1, '210': 0, '120U': 0, '012': 9, '102': 3, '021U': 0, '111U': 0, '003': 8, '030C': 0, '021D': 9, '201': 0, '111D': 1, '300': 0, '120D': 0, '021C': 2}\n    actual = {k: 0 for k in expected}\n    for node in G.nodes():\n        node_triad_census = nx.triadic_census(G, nodelist=[node])\n        for triad_key in expected:\n            actual[triad_key] += node_triad_census[triad_key]\n    for k, v in actual.items():\n        actual[k] //= 3\n    assert expected == actual"
 },
 {
  "docstring": "Tests that the closeness vitality of a node whose removal\ndisconnects the graph is negative infinity.",
  "code": "def test_disconnecting_graph(self):\n    G = nx.path_graph(3)\n    assert nx.closeness_vitality(G, node=1) == -float('inf')"
 },
 {
  "docstring": "Tests that a graph with isolated nodes has all isolates in\none block of the partition.",
  "code": "def test_isolates(self):\n    G = nx.empty_graph(5)\n    cells = nx.voronoi_cells(G, {0, 2, 4})\n    expected = {0: {0}, 2: {2}, 4: {4}, 'unreachable': {1, 3}}\n    assert expected == cells"
 },
 {
  "docstring": "Tests that reversing the graph gives the \"inward\" Voronoi\npartition.",
  "code": "def test_directed_inward(self):\n    G = nx.DiGraph(pairwise(range(6), cyclic=True))\n    G = G.reverse(copy=False)\n    cells = nx.voronoi_cells(G, {0, 3})\n    expected = {0: {0, 4, 5}, 3: {1, 2, 3}}\n    assert expected == cells"
 },
 {
  "docstring": "Tests that the Voronoi cells for a multigraph are the same as\nfor a simple graph.",
  "code": "def test_multigraph_unweighted(self):\n    edges = [(0, 1), (1, 2), (2, 3)]\n    G = nx.MultiGraph(2 * edges)\n    H = nx.Graph(G)\n    G_cells = nx.voronoi_cells(G, {0, 3})\n    H_cells = nx.voronoi_cells(H, {0, 3})\n    assert G_cells == H_cells"
 },
 {
  "docstring": "Tests that the Wiener index of a disconnected graph is\npositive infinity.",
  "code": "def test_disconnected_graph(self):\n    assert wiener_index(empty_graph(2)) == float('inf')"
 },
 {
  "docstring": "Tests that each pair of nodes in the directed graph is\ncounted once when computing the Wiener index.",
  "code": "def test_directed(self):\n    G = complete_graph(3)\n    H = DiGraph(G)\n    assert 2 * wiener_index(G) == wiener_index(H)"
 },
 {
  "docstring": "Tests that the Wiener index of the complete graph is simply\nthe number of edges.",
  "code": "def test_complete_graph(self):\n    n = 10\n    G = complete_graph(n)\n    assert wiener_index(G) == n * (n - 1) / 2"
 },
 {
  "docstring": "Tests that the Wiener index of the path graph is correctly\ncomputed.",
  "code": "def test_path_graph(self):\n    n = 9\n    G = path_graph(n)\n    expected = 2 * sum((i * (n - i) for i in range(1, n // 2 + 1)))\n    actual = wiener_index(G)\n    assert expected == actual"
 },
 {
  "docstring": "Iterates over edges in a beam search.\n\nThe beam search is a generalized breadth-first search in which only\nthe \"best\" *w* neighbors of the current node are enqueued, where *w*\nis the beam width and \"best\" is an application-specific\nheuristic. In general, a beam search with a small beam width might\nnot visit each node in the graph.\n\n.. note::\n\n   With the default value of ``width=None`` or `width` greater than the\n   maximum degree of the graph, this function equates to a slower\n   version of `~networkx.algorithms.traversal.breadth_first_search.bfs_edges`.\n   All nodes will be visited, though the order of the reported edges may\n   vary. In such cases, `value` has no effect - consider using `bfs_edges`\n   directly instead.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node\n    Starting node for the breadth-first search; this function\n    iterates over only those edges in the component reachable from\n    this node.\n\nvalue : function\n    A function that takes a node of the graph as input and returns a\n    real number indicating how \"good\" it is. A higher value means it\n    is more likely to be visited sooner during the search. When\n    visiting a new node, only the `width` neighbors with the highest\n    `value` are enqueued (in decreasing order of `value`).\n\nwidth : int (default = None)\n    The beam width for the search. This is the number of neighbors\n    (ordered by `value`) to enqueue when visiting each new node.\n\nYields\n------\nedge\n    Edges in the beam search starting from `source`, given as a pair\n    of nodes.\n\nExamples\n--------\nTo give nodes with, for example, a higher centrality precedence\nduring the search, set the `value` function to return the centrality\nvalue of the node:\n\n>>> G = nx.karate_club_graph()\n>>> centrality = nx.eigenvector_centrality(G)\n>>> list(nx.bfs_beam_edges(G, source=0, value=centrality.get, width=3))\n[(0, 2), (0, 1), (0, 8), (2, 32), (1, 13), (8, 33)]",
  "code": "@nx._dispatch\ndef bfs_beam_edges(G, source, value, width=None):\n    if width is None:\n        width = len(G)\n\n    def successors(v):\n        \"\"\"Returns a list of the best neighbors of a node.\n\n        `v` is a node in the graph `G`.\n\n        The \"best\" neighbors are chosen according to the `value`\n        function (higher is better). Only the `width` best neighbors of\n        `v` are returned.\n        \"\"\"\n        return iter(sorted(G.neighbors(v), key=value, reverse=True)[:width])\n    yield from nx.generic_bfs_edges(G, source, successors)"
 },
 {
  "docstring": "Returns a list of the best neighbors of a node.\n\n`v` is a node in the graph `G`.\n\nThe \"best\" neighbors are chosen according to the `value`\nfunction (higher is better). Only the `width` best neighbors of\n`v` are returned.",
  "code": "def successors(v):\n    return iter(sorted(G.neighbors(v), key=value, reverse=True)[:width])"
 },
 {
  "docstring": "Iterate over edges in a breadth-first search.\n\nThe breadth-first search begins at `source` and enqueues the\nneighbors of newly visited nodes specified by the `neighbors`\nfunction.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node\n    Starting node for the breadth-first search; this function\n    iterates over only those edges in the component reachable from\n    this node.\n\nneighbors : function\n    A function that takes a newly visited node of the graph as input\n    and returns an *iterator* (not just a list) of nodes that are\n    neighbors of that node with custom ordering. If not specified, this is\n    just the ``G.neighbors`` method, but in general it can be any function\n    that returns an iterator over some or all of the neighbors of a\n    given node, in any order.\n\ndepth_limit : int, optional(default=len(G))\n    Specify the maximum search depth.\n\nsort_neighbors : Callable\n\n    .. deprecated:: 3.2\n\n       The sort_neighbors parameter is deprecated and will be removed in\n       version 3.4. A custom (e.g. sorted) ordering of neighbors can be\n       specified with the `neighbors` parameter.\n\n    A function that takes the list of neighbors of a given node as input,\n    and returns an iterator over these neighbors but with a custom\n    ordering.\n\nYields\n------\nedge\n    Edges in the breadth-first search starting from `source`.\n\nExamples\n--------\n>>> G = nx.path_graph(7)\n>>> list(nx.generic_bfs_edges(G, source=0))\n[(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6)]\n>>> list(nx.generic_bfs_edges(G, source=2))\n[(2, 1), (2, 3), (1, 0), (3, 4), (4, 5), (5, 6)]\n>>> list(nx.generic_bfs_edges(G, source=2, depth_limit=2))\n[(2, 1), (2, 3), (1, 0), (3, 4)]\n\nThe `neighbors` param can be used to specify the visitation order of each\nnode's neighbors generically. In the following example, we modify the default\nneighbor to return *odd* nodes first:\n\n>>> def odd_first(n):\n...     return sorted(G.neighbors(n), key=lambda x: x % 2, reverse=True)\n\n>>> G = nx.star_graph(5)\n>>> list(nx.generic_bfs_edges(G, source=0))  # Default neighbor ordering\n[(0, 1), (0, 2), (0, 3), (0, 4), (0, 5)]\n>>> list(nx.generic_bfs_edges(G, source=0, neighbors=odd_first))\n[(0, 1), (0, 3), (0, 5), (0, 2), (0, 4)]\n\n",
  "code": "@nx._dispatch\ndef generic_bfs_edges(G, source, neighbors=None, depth_limit=None, sort_neighbors=None):\n    if neighbors is None:\n        neighbors = G.neighbors\n    if sort_neighbors is not None:\n        import warnings\n        warnings.warn('The sort_neighbors parameter is deprecated and will be removed\\nin NetworkX 3.4, use the neighbors parameter instead.', DeprecationWarning, stacklevel=2)\n        _neighbors = neighbors\n        neighbors = lambda node: iter(sort_neighbors(_neighbors(node)))\n    if depth_limit is None:\n        depth_limit = len(G)\n    seen = {source}\n    n = len(G)\n    depth = 0\n    next_parents_children = [(source, neighbors(source))]\n    while next_parents_children and depth < depth_limit:\n        this_parents_children = next_parents_children\n        next_parents_children = []\n        for parent, children in this_parents_children:\n            for child in children:\n                if child not in seen:\n                    seen.add(child)\n                    next_parents_children.append((child, neighbors(child)))\n                    yield (parent, child)\n            if len(seen) == n:\n                return\n        depth += 1"
 },
 {
  "docstring": "Iterate over edges in a breadth-first-search starting at source.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node\n   Specify starting node for breadth-first search; this function\n   iterates over only those edges in the component reachable from\n   this node.\n\nreverse : bool, optional\n   If True traverse a directed graph in the reverse direction\n\ndepth_limit : int, optional(default=len(G))\n    Specify the maximum search depth\n\nsort_neighbors : function\n    A function that takes the list of neighbors of given node as input, and\n    returns an *iterator* over these neighbors but with custom ordering.\n\nYields\n------\nedge: 2-tuple of nodes\n   Yields edges resulting from the breadth-first search.\n\nExamples\n--------\nTo get the edges in a breadth-first search::\n\n    >>> G = nx.path_graph(3)\n    >>> list(nx.bfs_edges(G, 0))\n    [(0, 1), (1, 2)]\n    >>> list(nx.bfs_edges(G, source=0, depth_limit=1))\n    [(0, 1)]\n\nTo get the nodes in a breadth-first search order::\n\n    >>> G = nx.path_graph(3)\n    >>> root = 2\n    >>> edges = nx.bfs_edges(G, root)\n    >>> nodes = [root] + [v for u, v in edges]\n    >>> nodes\n    [2, 1, 0]\n\n",
  "code": "@nx._dispatch\ndef bfs_edges(G, source, reverse=False, depth_limit=None, sort_neighbors=None):\n    if reverse and G.is_directed():\n        successors = G.predecessors\n    else:\n        successors = G.neighbors\n    if callable(sort_neighbors):\n        yield from generic_bfs_edges(G, source, lambda node: iter(sort_neighbors(successors(node))), depth_limit)\n    else:\n        yield from generic_bfs_edges(G, source, successors, depth_limit)"
 },
 {
  "docstring": "Returns an oriented tree constructed from of a breadth-first-search\nstarting at source.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node\n   Specify starting node for breadth-first search\n\nreverse : bool, optional\n   If True traverse a directed graph in the reverse direction\n\ndepth_limit : int, optional(default=len(G))\n    Specify the maximum search depth\n\nsort_neighbors : function\n    A function that takes the list of neighbors of given node as input, and\n    returns an *iterator* over these neighbors but with custom ordering.\n\nReturns\n-------\nT: NetworkX DiGraph\n   An oriented tree\n\nExamples\n--------\n>>> G = nx.path_graph(3)\n>>> list(nx.bfs_tree(G, 1).edges())\n[(1, 0), (1, 2)]\n>>> H = nx.Graph()\n>>> nx.add_path(H, [0, 1, 2, 3, 4, 5, 6])\n>>> nx.add_path(H, [2, 7, 8, 9, 10])\n>>> sorted(list(nx.bfs_tree(H, source=3, depth_limit=3).edges()))\n[(1, 0), (2, 1), (2, 7), (3, 2), (3, 4), (4, 5), (5, 6), (7, 8)]\n\n\n",
  "code": "@nx._dispatch\ndef bfs_tree(G, source, reverse=False, depth_limit=None, sort_neighbors=None):\n    T = nx.DiGraph()\n    T.add_node(source)\n    edges_gen = bfs_edges(G, source, reverse=reverse, depth_limit=depth_limit, sort_neighbors=sort_neighbors)\n    T.add_edges_from(edges_gen)\n    return T"
 },
 {
  "docstring": "Returns an iterator of predecessors in breadth-first-search from source.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node\n   Specify starting node for breadth-first search\n\ndepth_limit : int, optional(default=len(G))\n    Specify the maximum search depth\n\nsort_neighbors : function\n    A function that takes the list of neighbors of given node as input, and\n    returns an *iterator* over these neighbors but with custom ordering.\n\nReturns\n-------\npred: iterator\n    (node, predecessor) iterator where `predecessor` is the predecessor of\n    `node` in a breadth first search starting from `source`.\n\nExamples\n--------\n>>> G = nx.path_graph(3)\n>>> dict(nx.bfs_predecessors(G, 0))\n{1: 0, 2: 1}\n>>> H = nx.Graph()\n>>> H.add_edges_from([(0, 1), (0, 2), (1, 3), (1, 4), (2, 5), (2, 6)])\n>>> dict(nx.bfs_predecessors(H, 0))\n{1: 0, 2: 0, 3: 1, 4: 1, 5: 2, 6: 2}\n>>> M = nx.Graph()\n>>> nx.add_path(M, [0, 1, 2, 3, 4, 5, 6])\n>>> nx.add_path(M, [2, 7, 8, 9, 10])\n>>> sorted(nx.bfs_predecessors(M, source=1, depth_limit=3))\n[(0, 1), (2, 1), (3, 2), (4, 3), (7, 2), (8, 7)]\n>>> N = nx.DiGraph()\n>>> nx.add_path(N, [0, 1, 2, 3, 4, 7])\n>>> nx.add_path(N, [3, 5, 6, 7])\n>>> sorted(nx.bfs_predecessors(N, source=2))\n[(3, 2), (4, 3), (5, 3), (6, 5), (7, 4)]\n\n",
  "code": "@nx._dispatch\ndef bfs_predecessors(G, source, depth_limit=None, sort_neighbors=None):\n    for s, t in bfs_edges(G, source, depth_limit=depth_limit, sort_neighbors=sort_neighbors):\n        yield (t, s)"
 },
 {
  "docstring": "Returns an iterator of successors in breadth-first-search from source.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node\n   Specify starting node for breadth-first search\n\ndepth_limit : int, optional(default=len(G))\n    Specify the maximum search depth\n\nsort_neighbors : function\n    A function that takes the list of neighbors of given node as input, and\n    returns an *iterator* over these neighbors but with custom ordering.\n\nReturns\n-------\nsucc: iterator\n   (node, successors) iterator where `successors` is the non-empty list of\n   successors of `node` in a breadth first search from `source`.\n   To appear in the iterator, `node` must have successors.\n\nExamples\n--------\n>>> G = nx.path_graph(3)\n>>> dict(nx.bfs_successors(G, 0))\n{0: [1], 1: [2]}\n>>> H = nx.Graph()\n>>> H.add_edges_from([(0, 1), (0, 2), (1, 3), (1, 4), (2, 5), (2, 6)])\n>>> dict(nx.bfs_successors(H, 0))\n{0: [1, 2], 1: [3, 4], 2: [5, 6]}\n>>> G = nx.Graph()\n>>> nx.add_path(G, [0, 1, 2, 3, 4, 5, 6])\n>>> nx.add_path(G, [2, 7, 8, 9, 10])\n>>> dict(nx.bfs_successors(G, source=1, depth_limit=3))\n{1: [0, 2], 2: [3, 7], 3: [4], 7: [8]}\n>>> G = nx.DiGraph()\n>>> nx.add_path(G, [0, 1, 2, 3, 4, 5])\n>>> dict(nx.bfs_successors(G, source=3))\n{3: [4], 4: [5]}\n\n",
  "code": "@nx._dispatch\ndef bfs_successors(G, source, depth_limit=None, sort_neighbors=None):\n    parent = source\n    children = []\n    for p, c in bfs_edges(G, source, depth_limit=depth_limit, sort_neighbors=sort_neighbors):\n        if p == parent:\n            children.append(c)\n            continue\n        yield (parent, children)\n        children = [c]\n        parent = p\n    yield (parent, children)"
 },
 {
  "docstring": "Returns an iterator of all the layers in breadth-first search traversal.\n\nParameters\n----------\nG : NetworkX graph\n    A graph over which to find the layers using breadth-first search.\n\nsources : node in `G` or list of nodes in `G`\n    Specify starting nodes for single source or multiple sources breadth-first search\n\nYields\n------\nlayer: list of nodes\n    Yields list of nodes at the same distance from sources\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> dict(enumerate(nx.bfs_layers(G, [0, 4])))\n{0: [0, 4], 1: [1, 3], 2: [2]}\n>>> H = nx.Graph()\n>>> H.add_edges_from([(0, 1), (0, 2), (1, 3), (1, 4), (2, 5), (2, 6)])\n>>> dict(enumerate(nx.bfs_layers(H, [1])))\n{0: [1], 1: [0, 3, 4], 2: [2], 3: [5, 6]}\n>>> dict(enumerate(nx.bfs_layers(H, [1, 6])))\n{0: [1, 6], 1: [0, 3, 4, 2], 2: [5]}",
  "code": "@nx._dispatch\ndef bfs_layers(G, sources):\n    if sources in G:\n        sources = [sources]\n    current_layer = list(sources)\n    visited = set(sources)\n    for source in current_layer:\n        if source not in G:\n            raise nx.NetworkXError(f'The node {source} is not in the graph.')\n    while current_layer:\n        yield current_layer\n        next_layer = []\n        for node in current_layer:\n            for child in G[node]:\n                if child not in visited:\n                    visited.add(child)\n                    next_layer.append(child)\n        current_layer = next_layer"
 },
 {
  "docstring": "Iterate over edges in a breadth-first search (BFS) labeled by type.\n\nWe generate triple of the form (*u*, *v*, *d*), where (*u*, *v*) is the\nedge being explored in the breadth-first search and *d* is one of the\nstrings 'tree', 'forward', 'level', or 'reverse'.  A 'tree' edge is one in\nwhich *v* is first discovered and placed into the layer below *u*.  A\n'forward' edge is one in which *u* is on the layer above *v* and *v* has\nalready been discovered.  A 'level' edge is one in which both *u* and *v*\noccur on the same layer.  A 'reverse' edge is one in which *u* is on a layer\nbelow *v*.\n\nWe emit each edge exactly once.  In an undirected graph, 'reverse' edges do\nnot occur, because each is discovered either as a 'tree' or 'forward' edge.\n\nParameters\n----------\nG : NetworkX graph\n    A graph over which to find the layers using breadth-first search.\n\nsources : node in `G` or list of nodes in `G`\n    Starting nodes for single source or multiple sources breadth-first search\n\nYields\n------\nedges: generator\n   A generator of triples (*u*, *v*, *d*) where (*u*, *v*) is the edge being\n   explored and *d* is described above.\n\nExamples\n--------\n>>> G = nx.cycle_graph(4, create_using = nx.DiGraph)\n>>> list(nx.bfs_labeled_edges(G, 0))\n[(0, 1, 'tree'), (1, 2, 'tree'), (2, 3, 'tree'), (3, 0, 'reverse')]\n>>> G = nx.complete_graph(3)\n>>> list(nx.bfs_labeled_edges(G, 0))\n[(0, 1, 'tree'), (0, 2, 'tree'), (1, 2, 'level')]\n>>> list(nx.bfs_labeled_edges(G, [0, 1]))\n[(0, 1, 'level'), (0, 2, 'tree'), (1, 2, 'forward')]",
  "code": "@nx._dispatch\ndef bfs_labeled_edges(G, sources):\n    if sources in G:\n        sources = [sources]\n    neighbors = G._adj\n    directed = G.is_directed()\n    visited = set()\n    visit = visited.discard if directed else visited.add\n    depth = {s: 0 for s in sources}\n    queue = deque(depth.items())\n    push = queue.append\n    pop = queue.popleft\n    while queue:\n        u, du = pop()\n        for v in neighbors[u]:\n            if v not in depth:\n                depth[v] = dv = du + 1\n                push((v, dv))\n                yield (u, v, TREE_EDGE)\n            else:\n                dv = depth[v]\n                if du == dv:\n                    if v not in visited:\n                        yield (u, v, LEVEL_EDGE)\n                elif du < dv:\n                    yield (u, v, FORWARD_EDGE)\n                elif directed:\n                    yield (u, v, REVERSE_EDGE)\n        visit(u)"
 },
 {
  "docstring": "Returns all nodes at a fixed `distance` from `source` in `G`.\n\nParameters\n----------\nG : NetworkX graph\n    A graph\nsource : node in `G`\ndistance : the distance of the wanted nodes from `source`\n\nReturns\n-------\nset()\n    The descendants of `source` in `G` at the given `distance` from `source`\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> nx.descendants_at_distance(G, 2, 2)\n{0, 4}\n>>> H = nx.DiGraph()\n>>> H.add_edges_from([(0, 1), (0, 2), (1, 3), (1, 4), (2, 5), (2, 6)])\n>>> nx.descendants_at_distance(H, 0, 2)\n{3, 4, 5, 6}\n>>> nx.descendants_at_distance(H, 5, 0)\n{5}\n>>> nx.descendants_at_distance(H, 5, 1)\nset()",
  "code": "@nx._dispatch\ndef descendants_at_distance(G, source, distance):\n    if source not in G:\n        raise nx.NetworkXError(f'The node {source} is not in the graph.')\n    bfs_generator = nx.bfs_layers(G, source)\n    for i, layer in enumerate(bfs_generator):\n        if i == distance:\n            return set(layer)\n    return set()"
 },
 {
  "docstring": "Iterate over edges in a depth-first-search (DFS).\n\nPerform a depth-first-search over the nodes of `G` and yield\nthe edges in order. This may not generate all edges in `G`\n(see `~networkx.algorithms.traversal.edgedfs.edge_dfs`).\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node, optional\n   Specify starting node for depth-first search and yield edges in\n   the component reachable from source.\n\ndepth_limit : int, optional (default=len(G))\n   Specify the maximum search depth.\n\nYields\n------\nedge: 2-tuple of nodes\n   Yields edges resulting from the depth-first-search.\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> list(nx.dfs_edges(G, source=0))\n[(0, 1), (1, 2), (2, 3), (3, 4)]\n>>> list(nx.dfs_edges(G, source=0, depth_limit=2))\n[(0, 1), (1, 2)]\n\n",
  "code": "@nx._dispatch\ndef dfs_edges(G, source=None, depth_limit=None):\n    if source is None:\n        nodes = G\n    else:\n        nodes = [source]\n    if depth_limit is None:\n        depth_limit = len(G)\n    visited = set()\n    for start in nodes:\n        if start in visited:\n            continue\n        visited.add(start)\n        stack = [(start, iter(G[start]))]\n        depth_now = 1\n        while stack:\n            parent, children = stack[-1]\n            for child in children:\n                if child not in visited:\n                    yield (parent, child)\n                    visited.add(child)\n                    if depth_now < depth_limit:\n                        stack.append((child, iter(G[child])))\n                        depth_now += 1\n                        break\n            else:\n                stack.pop()\n                depth_now -= 1"
 },
 {
  "docstring": "Returns oriented tree constructed from a depth-first-search from source.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node, optional\n   Specify starting node for depth-first search.\n\ndepth_limit : int, optional (default=len(G))\n   Specify the maximum search depth.\n\nReturns\n-------\nT : NetworkX DiGraph\n   An oriented tree\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> T = nx.dfs_tree(G, source=0, depth_limit=2)\n>>> list(T.edges())\n[(0, 1), (1, 2)]\n>>> T = nx.dfs_tree(G, source=0)\n>>> list(T.edges())\n[(0, 1), (1, 2), (2, 3), (3, 4)]\n\nSee Also\n--------\ndfs_preorder_nodes\ndfs_postorder_nodes\ndfs_labeled_edges\nedge_dfs\nbfs_tree",
  "code": "@nx._dispatch\ndef dfs_tree(G, source=None, depth_limit=None):\n    T = nx.DiGraph()\n    if source is None:\n        T.add_nodes_from(G)\n    else:\n        T.add_node(source)\n    T.add_edges_from(dfs_edges(G, source, depth_limit))\n    return T"
 },
 {
  "docstring": "Returns dictionary of predecessors in depth-first-search from source.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node, optional\n   Specify starting node for depth-first search.\n   Note that you will get predecessors for all nodes in the\n   component containing `source`. This input only specifies\n   where the DFS starts.\n\ndepth_limit : int, optional (default=len(G))\n   Specify the maximum search depth.\n\nReturns\n-------\npred: dict\n   A dictionary with nodes as keys and predecessor nodes as values.\n\nExamples\n--------\n>>> G = nx.path_graph(4)\n>>> nx.dfs_predecessors(G, source=0)\n{1: 0, 2: 1, 3: 2}\n>>> nx.dfs_predecessors(G, source=0, depth_limit=2)\n{1: 0, 2: 1}\n\n",
  "code": "@nx._dispatch\ndef dfs_predecessors(G, source=None, depth_limit=None):\n    return {t: s for s, t in dfs_edges(G, source, depth_limit)}"
 },
 {
  "docstring": "Returns dictionary of successors in depth-first-search from source.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node, optional\n   Specify starting node for depth-first search.\n   Note that you will get successors for all nodes in the\n   component containing `source`. This input only specifies\n   where the DFS starts.\n\ndepth_limit : int, optional (default=len(G))\n   Specify the maximum search depth.\n\nReturns\n-------\nsucc: dict\n   A dictionary with nodes as keys and list of successor nodes as values.\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> nx.dfs_successors(G, source=0)\n{0: [1], 1: [2], 2: [3], 3: [4]}\n>>> nx.dfs_successors(G, source=0, depth_limit=2)\n{0: [1], 1: [2]}\n\n",
  "code": "@nx._dispatch\ndef dfs_successors(G, source=None, depth_limit=None):\n    d = defaultdict(list)\n    for s, t in dfs_edges(G, source=source, depth_limit=depth_limit):\n        d[s].append(t)\n    return dict(d)"
 },
 {
  "docstring": "Generate nodes in a depth-first-search post-ordering starting at source.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node, optional\n   Specify starting node for depth-first search.\n\ndepth_limit : int, optional (default=len(G))\n   Specify the maximum search depth.\n\nReturns\n-------\nnodes: generator\n   A generator of nodes in a depth-first-search post-ordering.\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> list(nx.dfs_postorder_nodes(G, source=0))\n[4, 3, 2, 1, 0]\n>>> list(nx.dfs_postorder_nodes(G, source=0, depth_limit=2))\n[1, 0]\n\n",
  "code": "@nx._dispatch\ndef dfs_postorder_nodes(G, source=None, depth_limit=None):\n    edges = nx.dfs_labeled_edges(G, source=source, depth_limit=depth_limit)\n    return (v for u, v, d in edges if d == 'reverse')"
 },
 {
  "docstring": "Generate nodes in a depth-first-search pre-ordering starting at source.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node, optional\n   Specify starting node for depth-first search and return nodes in\n   the component reachable from source.\n\ndepth_limit : int, optional (default=len(G))\n   Specify the maximum search depth.\n\nReturns\n-------\nnodes: generator\n   A generator of nodes in a depth-first-search pre-ordering.\n\nExamples\n--------\n>>> G = nx.path_graph(5)\n>>> list(nx.dfs_preorder_nodes(G, source=0))\n[0, 1, 2, 3, 4]\n>>> list(nx.dfs_preorder_nodes(G, source=0, depth_limit=2))\n[0, 1, 2]\n\n",
  "code": "@nx._dispatch\ndef dfs_preorder_nodes(G, source=None, depth_limit=None):\n    edges = nx.dfs_labeled_edges(G, source=source, depth_limit=depth_limit)\n    return (v for u, v, d in edges if d == 'forward')"
 },
 {
  "docstring": "Iterate over edges in a depth-first-search (DFS) labeled by type.\n\nParameters\n----------\nG : NetworkX graph\n\nsource : node, optional\n   Specify starting node for depth-first search and return edges in\n   the component reachable from source.\n\ndepth_limit : int, optional (default=len(G))\n   Specify the maximum search depth.\n\nReturns\n-------\nedges: generator\n   A generator of triples of the form (*u*, *v*, *d*), where (*u*,\n   *v*) is the edge being explored in the depth-first search and *d*\n   is one of the strings 'forward', 'nontree', 'reverse', or 'reverse-depth_limit'.\n   A 'forward' edge is one in which *u* has been visited but *v* has\n   not. A 'nontree' edge is one in which both *u* and *v* have been\n   visited but the edge is not in the DFS tree. A 'reverse' edge is\n   one in which both *u* and *v* have been visited and the edge is in\n   the DFS tree. When the `depth_limit` is reached via a 'forward' edge,\n   a 'reverse' edge is immediately generated rather than the subtree\n   being explored. To indicate this flavor of 'reverse' edge, the string\n   yielded is 'reverse-depth_limit'.\n\nExamples\n--------\n\nThe labels reveal the complete transcript of the depth-first search\nalgorithm in more detail than, for example, :func:`dfs_edges`::\n\n    >>> from pprint import pprint\n    >>>\n    >>> G = nx.DiGraph([(0, 1), (1, 2), (2, 1)])\n    >>> pprint(list(nx.dfs_labeled_edges(G, source=0)))\n    [(0, 0, 'forward'),\n     (0, 1, 'forward'),\n     (1, 2, 'forward'),\n     (2, 1, 'nontree'),\n     (1, 2, 'reverse'),\n     (0, 1, 'reverse'),\n     (0, 0, 'reverse')]\n\n",
  "code": "@nx._dispatch\ndef dfs_labeled_edges(G, source=None, depth_limit=None):\n    if source is None:\n        nodes = G\n    else:\n        nodes = [source]\n    if depth_limit is None:\n        depth_limit = len(G)\n    visited = set()\n    for start in nodes:\n        if start in visited:\n            continue\n        yield (start, start, 'forward')\n        visited.add(start)\n        stack = [(start, iter(G[start]))]\n        depth_now = 1\n        while stack:\n            parent, children = stack[-1]\n            for child in children:\n                if child in visited:\n                    yield (parent, child, 'nontree')\n                else:\n                    yield (parent, child, 'forward')\n                    visited.add(child)\n                    if depth_now < depth_limit:\n                        stack.append((child, iter(G[child])))\n                        depth_now += 1\n                        break\n                    else:\n                        yield (parent, child, 'reverse-depth_limit')\n            else:\n                stack.pop()\n                depth_now -= 1\n                if stack:\n                    yield (stack[-1][0], parent, 'reverse')\n        yield (start, start, 'reverse')"
 },
 {
  "docstring": "A directed, breadth-first-search of edges in `G`, beginning at `source`.\n\nYield the edges of G in a breadth-first-search order continuing until\nall edges are generated.\n\nParameters\n----------\nG : graph\n    A directed/undirected graph/multigraph.\n\nsource : node, list of nodes\n    The node from which the traversal begins. If None, then a source\n    is chosen arbitrarily and repeatedly until all edges from each node in\n    the graph are searched.\n\norientation : None | 'original' | 'reverse' | 'ignore' (default: None)\n    For directed graphs and directed multigraphs, edge traversals need not\n    respect the original orientation of the edges.\n    When set to 'reverse' every edge is traversed in the reverse direction.\n    When set to 'ignore', every edge is treated as undirected.\n    When set to 'original', every edge is treated as directed.\n    In all three cases, the yielded edge tuples add a last entry to\n    indicate the direction in which that edge was traversed.\n    If orientation is None, the yielded edge has no direction indicated.\n    The direction is respected, but not reported.\n\nYields\n------\nedge : directed edge\n    A directed edge indicating the path taken by the breadth-first-search.\n    For graphs, `edge` is of the form `(u, v)` where `u` and `v`\n    are the tail and head of the edge as determined by the traversal.\n    For multigraphs, `edge` is of the form `(u, v, key)`, where `key` is\n    the key of the edge. When the graph is directed, then `u` and `v`\n    are always in the order of the actual directed edge.\n    If orientation is not None then the edge tuple is extended to include\n    the direction of traversal ('forward' or 'reverse') on that edge.\n\nExamples\n--------\n>>> nodes = [0, 1, 2, 3]\n>>> edges = [(0, 1), (1, 0), (1, 0), (2, 0), (2, 1), (3, 1)]\n\n>>> list(nx.edge_bfs(nx.Graph(edges), nodes))\n[(0, 1), (0, 2), (1, 2), (1, 3)]\n\n>>> list(nx.edge_bfs(nx.DiGraph(edges), nodes))\n[(0, 1), (1, 0), (2, 0), (2, 1), (3, 1)]\n\n>>> list(nx.edge_bfs(nx.MultiGraph(edges), nodes))\n[(0, 1, 0), (0, 1, 1), (0, 1, 2), (0, 2, 0), (1, 2, 0), (1, 3, 0)]\n\n>>> list(nx.edge_bfs(nx.MultiDiGraph(edges), nodes))\n[(0, 1, 0), (1, 0, 0), (1, 0, 1), (2, 0, 0), (2, 1, 0), (3, 1, 0)]\n\n>>> list(nx.edge_bfs(nx.DiGraph(edges), nodes, orientation=\"ignore\"))\n[(0, 1, 'forward'), (1, 0, 'reverse'), (2, 0, 'reverse'), (2, 1, 'reverse'), (3, 1, 'reverse')]\n\n>>> list(nx.edge_bfs(nx.MultiDiGraph(edges), nodes, orientation=\"ignore\"))\n[(0, 1, 0, 'forward'), (1, 0, 0, 'reverse'), (1, 0, 1, 'reverse'), (2, 0, 0, 'reverse'), (2, 1, 0, 'reverse'), (3, 1, 0, 'reverse')]\n\n",
  "code": "@nx._dispatch\ndef edge_bfs(G, source=None, orientation=None):\n    nodes = list(G.nbunch_iter(source))\n    if not nodes:\n        return\n    directed = G.is_directed()\n    kwds = {'data': False}\n    if G.is_multigraph() is True:\n        kwds['keys'] = True\n    if orientation is None:\n\n        def edges_from(node):\n            return iter(G.edges(node, **kwds))\n    elif not directed or orientation == 'original':\n\n        def edges_from(node):\n            for e in G.edges(node, **kwds):\n                yield (e + (FORWARD,))\n    elif orientation == 'reverse':\n\n        def edges_from(node):\n            for e in G.in_edges(node, **kwds):\n                yield (e + (REVERSE,))\n    elif orientation == 'ignore':\n\n        def edges_from(node):\n            for e in G.edges(node, **kwds):\n                yield (e + (FORWARD,))\n            for e in G.in_edges(node, **kwds):\n                yield (e + (REVERSE,))\n    else:\n        raise nx.NetworkXError('invalid orientation argument.')\n    if directed:\n        neighbors = G.successors\n\n        def edge_id(edge):\n            return edge[:-1] if orientation is not None else edge\n    else:\n        neighbors = G.neighbors\n\n        def edge_id(edge):\n            return (frozenset(edge[:2]),) + edge[2:]\n    check_reverse = directed and orientation in ('reverse', 'ignore')\n    visited_nodes = set(nodes)\n    visited_edges = set()\n    queue = deque([(n, edges_from(n)) for n in nodes])\n    while queue:\n        parent, children_edges = queue.popleft()\n        for edge in children_edges:\n            if check_reverse and edge[-1] == REVERSE:\n                child = edge[0]\n            else:\n                child = edge[1]\n            if child not in visited_nodes:\n                visited_nodes.add(child)\n                queue.append((child, edges_from(child)))\n            edgeid = edge_id(edge)\n            if edgeid not in visited_edges:\n                visited_edges.add(edgeid)\n                yield edge"
 },
 {
  "docstring": "A directed, depth-first-search of edges in `G`, beginning at `source`.\n\nYield the edges of G in a depth-first-search order continuing until\nall edges are generated.\n\nParameters\n----------\nG : graph\n    A directed/undirected graph/multigraph.\n\nsource : node, list of nodes\n    The node from which the traversal begins. If None, then a source\n    is chosen arbitrarily and repeatedly until all edges from each node in\n    the graph are searched.\n\norientation : None | 'original' | 'reverse' | 'ignore' (default: None)\n    For directed graphs and directed multigraphs, edge traversals need not\n    respect the original orientation of the edges.\n    When set to 'reverse' every edge is traversed in the reverse direction.\n    When set to 'ignore', every edge is treated as undirected.\n    When set to 'original', every edge is treated as directed.\n    In all three cases, the yielded edge tuples add a last entry to\n    indicate the direction in which that edge was traversed.\n    If orientation is None, the yielded edge has no direction indicated.\n    The direction is respected, but not reported.\n\nYields\n------\nedge : directed edge\n    A directed edge indicating the path taken by the depth-first traversal.\n    For graphs, `edge` is of the form `(u, v)` where `u` and `v`\n    are the tail and head of the edge as determined by the traversal.\n    For multigraphs, `edge` is of the form `(u, v, key)`, where `key` is\n    the key of the edge. When the graph is directed, then `u` and `v`\n    are always in the order of the actual directed edge.\n    If orientation is not None then the edge tuple is extended to include\n    the direction of traversal ('forward' or 'reverse') on that edge.\n\nExamples\n--------\n>>> nodes = [0, 1, 2, 3]\n>>> edges = [(0, 1), (1, 0), (1, 0), (2, 1), (3, 1)]\n\n>>> list(nx.edge_dfs(nx.Graph(edges), nodes))\n[(0, 1), (1, 2), (1, 3)]\n\n>>> list(nx.edge_dfs(nx.DiGraph(edges), nodes))\n[(0, 1), (1, 0), (2, 1), (3, 1)]\n\n>>> list(nx.edge_dfs(nx.MultiGraph(edges), nodes))\n[(0, 1, 0), (1, 0, 1), (0, 1, 2), (1, 2, 0), (1, 3, 0)]\n\n>>> list(nx.edge_dfs(nx.MultiDiGraph(edges), nodes))\n[(0, 1, 0), (1, 0, 0), (1, 0, 1), (2, 1, 0), (3, 1, 0)]\n\n>>> list(nx.edge_dfs(nx.DiGraph(edges), nodes, orientation=\"ignore\"))\n[(0, 1, 'forward'), (1, 0, 'forward'), (2, 1, 'reverse'), (3, 1, 'reverse')]\n\n>>> list(nx.edge_dfs(nx.MultiDiGraph(edges), nodes, orientation=\"ignore\"))\n[(0, 1, 0, 'forward'), (1, 0, 0, 'forward'), (1, 0, 1, 'reverse'), (2, 1, 0, 'reverse'), (3, 1, 0, 'reverse')]\n\n",
  "code": "@nx._dispatch\ndef edge_dfs(G, source=None, orientation=None):\n    nodes = list(G.nbunch_iter(source))\n    if not nodes:\n        return\n    directed = G.is_directed()\n    kwds = {'data': False}\n    if G.is_multigraph() is True:\n        kwds['keys'] = True\n    if orientation is None:\n\n        def edges_from(node):\n            return iter(G.edges(node, **kwds))\n    elif not directed or orientation == 'original':\n\n        def edges_from(node):\n            for e in G.edges(node, **kwds):\n                yield (e + (FORWARD,))\n    elif orientation == 'reverse':\n\n        def edges_from(node):\n            for e in G.in_edges(node, **kwds):\n                yield (e + (REVERSE,))\n    elif orientation == 'ignore':\n\n        def edges_from(node):\n            for e in G.edges(node, **kwds):\n                yield (e + (FORWARD,))\n            for e in G.in_edges(node, **kwds):\n                yield (e + (REVERSE,))\n    else:\n        raise nx.NetworkXError('invalid orientation argument.')\n    if directed:\n\n        def edge_id(edge):\n            return edge[:-1] if orientation is not None else edge\n    else:\n\n        def edge_id(edge):\n            return (frozenset(edge[:2]),) + edge[2:]\n    check_reverse = directed and orientation in ('reverse', 'ignore')\n    visited_edges = set()\n    visited_nodes = set()\n    edges = {}\n    for start_node in nodes:\n        stack = [start_node]\n        while stack:\n            current_node = stack[-1]\n            if current_node not in visited_nodes:\n                edges[current_node] = edges_from(current_node)\n                visited_nodes.add(current_node)\n            try:\n                edge = next(edges[current_node])\n            except StopIteration:\n                stack.pop()\n            else:\n                edgeid = edge_id(edge)\n                if edgeid not in visited_edges:\n                    visited_edges.add(edgeid)\n                    if check_reverse and edge[-1] == REVERSE:\n                        stack.append(edge[0])\n                    else:\n                        stack.append(edge[1])\n                    yield edge"
 },
 {
  "docstring": "Tests that a narrow beam width may cause an incomplete search.",
  "code": "def test_narrow():\n    G = nx.cycle_graph(4)\n    edges = nx.bfs_beam_edges(G, source=0, value=lambda n: n, width=1)\n    assert list(edges) == [(0, 3), (3, 2)]"
 },
 {
  "docstring": "All nodes are searched when `width` is None or >= max degree",
  "code": "@pytest.mark.parametrize('width', (2, None))\ndef test_wide(width):\n    G = nx.cycle_graph(4)\n    edges = nx.bfs_beam_edges(G, source=0, value=lambda n: n, width=width)\n    assert list(edges) == [(0, 3), (0, 1), (3, 2)]"
 },
 {
  "docstring": "Returns the total weight of a branching.\n\nYou must access this function through the networkx.algorithms.tree module.\n\nParameters\n----------\nG : DiGraph\n    The directed graph.\nattr : str\n    The attribute to use as weights. If None, then each edge will be\n    treated equally with a weight of 1.\ndefault : float\n    When `attr` is not None, then if an edge does not have that attribute,\n    `default` specifies what value it should take.\n\nReturns\n-------\nweight: int or float\n    The total weight of the branching.\n\nExamples\n--------\n>>> G = nx.DiGraph()\n>>> G.add_weighted_edges_from([(0, 1, 2), (1, 2, 4), (2, 3, 3), (3, 4, 2)])\n>>> nx.tree.branching_weight(G)\n11",
  "code": "@nx._dispatch(edge_attrs={'attr': 'default'})\ndef branching_weight(G, attr='weight', default=1):\n    return sum((edge[2].get(attr, default) for edge in G.edges(data=True)))"
 },
 {
  "docstring": "Returns a branching obtained through a greedy algorithm.\n\nThis algorithm is wrong, and cannot give a proper optimal branching.\nHowever, we include it for pedagogical reasons, as it can be helpful to\nsee what its outputs are.\n\nThe output is a branching, and possibly, a spanning arborescence. However,\nit is not guaranteed to be optimal in either case.\n\nParameters\n----------\nG : DiGraph\n    The directed graph to scan.\nattr : str\n    The attribute to use as weights. If None, then each edge will be\n    treated equally with a weight of 1.\ndefault : float\n    When `attr` is not None, then if an edge does not have that attribute,\n    `default` specifies what value it should take.\nkind : str\n    The type of optimum to search for: 'min' or 'max' greedy branching.\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\nReturns\n-------\nB : directed graph\n    The greedily obtained branching.",
  "code": "@py_random_state(4)\n@nx._dispatch(edge_attrs={'attr': 'default'})\ndef greedy_branching(G, attr='weight', default=1, kind='max', seed=None):\n    if kind not in KINDS:\n        raise nx.NetworkXException('Unknown value for `kind`.')\n    if kind == 'min':\n        reverse = False\n    else:\n        reverse = True\n    if attr is None:\n        attr = random_string(seed=seed)\n    edges = [(u, v, data.get(attr, default)) for u, v, data in G.edges(data=True)]\n    try:\n        edges.sort(key=itemgetter(2, 0, 1), reverse=reverse)\n    except TypeError:\n        edges.sort(key=itemgetter(2), reverse=reverse)\n    B = nx.DiGraph()\n    B.add_nodes_from(G)\n    uf = nx.utils.UnionFind()\n    for i, (u, v, w) in enumerate(edges):\n        if uf[u] == uf[v]:\n            continue\n        elif B.in_degree(v) == 1:\n            continue\n        else:\n            data = {}\n            if attr is not None:\n                data[attr] = w\n            B.add_edge(u, v, **data)\n            uf.union(u, v)\n    return B"
 },
 {
  "docstring": "Returns the edge keys of the unique path between u and v.\n\nThis is not a generic function. G must be a branching and an instance of\nMultiDiGraph_EdgeKey.",
  "code": "def get_path(G, u, v):\n    nodes = nx.shortest_path(G, u, v)\n\n    def first_key(i, vv):\n        keys = G[nodes[i]][vv].keys()\n        keys = list(keys)\n        return keys[0]\n    edges = [first_key(i, vv) for i, vv in enumerate(nodes[1:])]\n    return (nodes, edges)"
 },
 {
  "docstring": "Returns a minimal branching from `G`.\n\nA minimal branching is a branching similar to a minimal arborescence but\nwithout the requirement that the result is actually a spanning arborescence.\nThis allows minimal branchinges to be computed over graphs which may not\nhave arborescence (such as multiple components).\n\nParameters\n----------\nG : (multi)digraph-like\n    The graph to be searched.\nattr : str\n    The edge attribute used in determining optimality.\ndefault : float\n    The value of the edge attribute used if an edge does not have\n    the attribute `attr`.\npreserve_attrs : bool\n    If True, preserve the other attributes of the original graph (that are not\n    passed to `attr`)\npartition : str\n    The key for the edge attribute containing the partition\n    data on the graph. Edges can be included, excluded or open using the\n    `EdgePartition` enum.\n\nReturns\n-------\nB : (multi)digraph-like\n    A minimal branching.",
  "code": "@nx._dispatch(edge_attrs={'attr': 'default', 'partition': None}, preserve_edge_attrs='preserve_attrs')\ndef minimal_branching(G, /, *, attr='weight', default=1, preserve_attrs=False, partition=None):\n    max_weight = -INF\n    min_weight = INF\n    for _, _, w in G.edges(data=attr):\n        if w > max_weight:\n            max_weight = w\n        if w < min_weight:\n            min_weight = w\n    for _, _, d in G.edges(data=True):\n        d[attr] = max_weight + 1 + (max_weight - min_weight) - d[attr]\n    B = maximum_branching(G, attr, default, preserve_attrs, partition)\n    for _, _, d in G.edges(data=True):\n        d[attr] = max_weight + 1 + (max_weight - min_weight) - d[attr]\n    for _, _, d in B.edges(data=True):\n        d[attr] = max_weight + 1 + (max_weight - min_weight) - d[attr]\n    return B"
 },
 {
  "docstring": "Key is now required.",
  "code": "def add_edge(self, u_for_edge, v_for_edge, key_for_edge, **attr):\n    u, v, key = (u_for_edge, v_for_edge, key_for_edge)\n    if key in self.edge_index:\n        uu, vv, _ = self.edge_index[key]\n        if u != uu or v != vv:\n            raise Exception(f'Key {key!r} is already in use.')\n    self._cls.add_edge(u, v, key, **attr)\n    self.edge_index[key] = (u, v, self.succ[u][v][key])"
 },
 {
  "docstring": "So we need the code in _init and find_optimum to successfully run edmonds algorithm.\nResponsibilities of the _init function:\n- Check that the kind argument is in {min, max} or raise a NetworkXException.\n- Transform the graph if we need a minimum arborescence/branching.\n  - The current method is to map weight -> -weight. This is NOT a good approach since\n    the algorithm can and does choose to ignore negative weights when creating a branching\n    since that is always optimal when maximzing the weights. I think we should set the edge\n    weights to be (max_weight + 1) - edge_weight.\n- Transform the graph into a MultiDiGraph, adding the partition information and potoentially\n  other edge attributes if we set preserve_attrs = True.\n- Setup the buckets and union find data structures required for the algorithm.",
  "code": "def _init(self, attr, default, kind, style, preserve_attrs, seed, partition):\n    if kind not in KINDS:\n        raise nx.NetworkXException('Unknown value for `kind`.')\n    self.attr = attr\n    self.default = default\n    self.kind = kind\n    self.style = style\n    if kind == 'min':\n        self.trans = trans = _min_weight\n    else:\n        self.trans = trans = _max_weight\n    if attr is None:\n        attr = random_string(seed=seed)\n    self._attr = attr\n    self.candidate_attr = 'candidate_' + random_string(seed=seed)\n    self.G = G = MultiDiGraph_EdgeKey()\n    for key, (u, v, data) in enumerate(self.G_original.edges(data=True)):\n        d = {attr: trans(data.get(attr, default))}\n        if data.get(partition) is not None:\n            d[partition] = data.get(partition)\n        if preserve_attrs:\n            for d_k, d_v in data.items():\n                if d_k != attr:\n                    d[d_k] = d_v\n        G.add_edge(u, v, key, **d)\n    self.level = 0\n    self.B = MultiDiGraph_EdgeKey()\n    self.B.edge_index = {}\n    self.graphs = []\n    self.branchings = []\n    self.uf = nx.utils.UnionFind()\n    self.circuits = []\n    self.minedge_circuit = []"
 },
 {
  "docstring": "Returns a branching from G.\n\nParameters\n----------\nattr : str\n    The edge attribute used to in determining optimality.\ndefault : float\n    The value of the edge attribute used if an edge does not have\n    the attribute `attr`.\nkind : {'min', 'max'}\n    The type of optimum to search for, either 'min' or 'max'.\nstyle : {'branching', 'arborescence'}\n    If 'branching', then an optimal branching is found. If `style` is\n    'arborescence', then a branching is found, such that if the\n    branching is also an arborescence, then the branching is an\n    optimal spanning arborescences. A given graph G need not have\n    an optimal spanning arborescence.\npreserve_attrs : bool\n    If True, preserve the other edge attributes of the original\n    graph (that are not the one passed to `attr`)\npartition : str\n    The edge attribute holding edge partition data. Used in the\n    spanning arborescence iterator.\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\nReturns\n-------\nH : (multi)digraph\n    The branching.",
  "code": "def find_optimum(self, attr='weight', default=1, kind='max', style='branching', preserve_attrs=False, partition=None, seed=None):\n    self._init(attr, default, kind, style, preserve_attrs, seed, partition)\n    uf = self.uf\n    G, B = (self.G, self.B)\n    D = set()\n    nodes = iter(list(G.nodes()))\n    attr = self._attr\n    G_pred = G.pred\n\n    def desired_edge(v):\n        \"\"\"\n            Find the edge directed toward v with maximal weight.\n\n            If an edge partition exists in this graph, return the included edge\n            if it exists and no not return any excluded edges. There can only\n            be one included edge for each vertex otherwise the edge partition is\n            empty.\n            \"\"\"\n        edge = None\n        weight = -INF\n        for u, _, key, data in G.in_edges(v, data=True, keys=True):\n            if data.get(partition) == nx.EdgePartition.EXCLUDED:\n                continue\n            new_weight = data[attr]\n            if data.get(partition) == nx.EdgePartition.INCLUDED:\n                weight = new_weight\n                edge = (u, v, key, new_weight, data)\n                return (edge, weight)\n            if new_weight > weight:\n                weight = new_weight\n                edge = (u, v, key, new_weight, data)\n        return (edge, weight)\n    while True:\n        try:\n            v = next(nodes)\n        except StopIteration:\n            assert len(G) == len(B)\n            if len(B):\n                assert is_branching(B)\n            if self.store:\n                self.graphs.append(G.copy())\n                self.branchings.append(B.copy())\n                self.circuits.append([])\n                self.minedge_circuit.append(None)\n            break\n        else:\n            if v in D:\n                continue\n        D.add(v)\n        B.add_node(v)\n        edge, weight = desired_edge(v)\n        if edge is None:\n            continue\n        else:\n            u = edge[0]\n            if uf[u] == uf[v]:\n                Q_nodes, Q_edges = get_path(B, v, u)\n                Q_edges.append(edge[2])\n            else:\n                Q_nodes, Q_edges = (None, None)\n            if self.style == 'branching' and weight <= 0:\n                acceptable = False\n            else:\n                acceptable = True\n            if acceptable:\n                dd = {attr: weight}\n                if edge[4].get(partition) is not None:\n                    dd[partition] = edge[4].get(partition)\n                B.add_edge(u, v, edge[2], **dd)\n                G[u][v][edge[2]][self.candidate_attr] = True\n                uf.union(u, v)\n                if Q_edges is not None:\n                    minweight = INF\n                    minedge = None\n                    Q_incoming_weight = {}\n                    for edge_key in Q_edges:\n                        u, v, data = B.edge_index[edge_key]\n                        w = data[attr]\n                        Q_incoming_weight[v] = w\n                        if data.get(partition) == nx.EdgePartition.INCLUDED:\n                            continue\n                        if w < minweight:\n                            minweight = w\n                            minedge = edge_key\n                    self.circuits.append(Q_edges)\n                    self.minedge_circuit.append(minedge)\n                    if self.store:\n                        self.graphs.append(G.copy())\n                    self.branchings.append(B.copy())\n                    new_node = self.template.format(self.level)\n                    G.add_node(new_node)\n                    new_edges = []\n                    for u, v, key, data in G.edges(data=True, keys=True):\n                        if u in Q_incoming_weight:\n                            if v in Q_incoming_weight:\n                                continue\n                            else:\n                                dd = data.copy()\n                                new_edges.append((new_node, v, key, dd))\n                        elif v in Q_incoming_weight:\n                            w = data[attr]\n                            w += minweight - Q_incoming_weight[v]\n                            dd = data.copy()\n                            dd[attr] = w\n                            new_edges.append((u, new_node, key, dd))\n                        else:\n                            continue\n                    G.remove_nodes_from(Q_nodes)\n                    B.remove_nodes_from(Q_nodes)\n                    D.difference_update(set(Q_nodes))\n                    for u, v, key, data in new_edges:\n                        G.add_edge(u, v, key, **data)\n                        if self.candidate_attr in data:\n                            del data[self.candidate_attr]\n                            B.add_edge(u, v, key, **data)\n                            uf.union(u, v)\n                    nodes = iter(list(G.nodes()))\n                    self.level += 1\n    H = self.G_original.__class__()\n\n    def is_root(G, u, edgekeys):\n        \"\"\"\n            Returns True if `u` is a root node in G.\n\n            Node `u` will be a root node if its in-degree, restricted to the\n            specified edges, is equal to 0.\n\n            \"\"\"\n        if u not in G:\n            raise Exception(f'{u!r} not in G')\n        for v in G.pred[u]:\n            for edgekey in G.pred[u][v]:\n                if edgekey in edgekeys:\n                    return (False, edgekey)\n        else:\n            return (True, None)\n    edges = set(self.branchings[self.level].edge_index)\n    while self.level > 0:\n        self.level -= 1\n        merged_node = self.template.format(self.level)\n        circuit = self.circuits[self.level]\n        isroot, edgekey = is_root(self.graphs[self.level + 1], merged_node, edges)\n        edges.update(circuit)\n        if isroot:\n            minedge = self.minedge_circuit[self.level]\n            if minedge is None:\n                raise Exception\n            edges.remove(minedge)\n        else:\n            G = self.graphs[self.level]\n            target = G.edge_index[edgekey][1]\n            for edgekey in circuit:\n                u, v, data = G.edge_index[edgekey]\n                if v == target:\n                    break\n            else:\n                raise Exception(\"Couldn't find edge incoming to merged node.\")\n            edges.remove(edgekey)\n    self.edges = edges\n    H.add_nodes_from(self.G_original)\n    for edgekey in edges:\n        u, v, d = self.graphs[0].edge_index[edgekey]\n        dd = {self.attr: self.trans(d[self.attr])}\n        if preserve_attrs:\n            for key, value in d.items():\n                if key not in [self.attr, self.candidate_attr]:\n                    dd[key] = value\n        H.add_edge(u, v, **dd)\n    return H"
 },
 {
  "docstring": "Adds an edge to `G` while also updating the edge index.\n\nThis algorithm requires the use of an external dictionary to track\nthe edge keys since it is possible that the source or destination\nnode of an edge will be changed and the default key-handling\ncapabilities of the MultiDiGraph class do not account for this.\n\nParameters\n----------\nG : MultiDiGraph\n    The graph to insert an edge into.\nedge_index : dict\n    A mapping from integers to the edges of the graph.\nu : node\n    The source node of the new edge.\nv : node\n    The destination node of the new edge.\nkey : int\n    The key to use from `edge_index`.\nd : keyword arguments, optional\n    Other attributes to store on the new edge.",
  "code": "def edmonds_add_edge(G, edge_index, u, v, key, **d):\n    if key in edge_index:\n        uu, vv, _ = edge_index[key]\n        if u != uu or v != vv:\n            raise Exception(f'Key {key!r} is already in use.')\n    G.add_edge(u, v, key, **d)\n    edge_index[key] = (u, v, G.succ[u][v][key])"
 },
 {
  "docstring": "Remove a node from the graph, updating the edge index to match.\n\nParameters\n----------\nG : MultiDiGraph\n    The graph to remove an edge from.\nedge_index : dict\n    A mapping from integers to the edges of the graph.\nn : node\n    The node to remove from `G`.",
  "code": "def edmonds_remove_node(G, edge_index, n):\n    keys = set()\n    for keydict in G.pred[n].values():\n        keys.update(keydict)\n    for keydict in G.succ[n].values():\n        keys.update(keydict)\n    for key in keys:\n        del edge_index[key]\n    G.remove_node(n)"
 },
 {
  "docstring": "Find the edge directed towards v with maximal weight.\n\nIf an edge partition exists in this graph, return the included\nedge if it exists and never return any excluded edge.\n\nNote: There can only be one included edge for each vertex otherwise\nthe edge partition is empty.\n\nParameters\n----------\nv : node\n    The node to search for the maximal weight incoming edge.",
  "code": "def edmonds_find_desired_edge(v):\n    edge = None\n    max_weight = -INF\n    for u, _, key, data in G.in_edges(v, data=True, keys=True):\n        if data.get(partition) == nx.EdgePartition.EXCLUDED:\n            continue\n        new_weight = data[attr]\n        if data.get(partition) == nx.EdgePartition.INCLUDED:\n            max_weight = new_weight\n            edge = (u, v, key, new_weight, data)\n            break\n        if new_weight > max_weight:\n            max_weight = new_weight\n            edge = (u, v, key, new_weight, data)\n    return (edge, max_weight)"
 },
 {
  "docstring": "Perform step I2 from Edmonds' paper\n\nFirst, check if the last step I1 created a cycle. If it did not, do nothing.\nIf it did, store the cycle for later reference and contract it.\n\nParameters\n----------\nv : node\n    The current node to consider\ndesired_edge : edge\n    The minimum desired edge to remove from the cycle.\nlevel : int\n    The current level, i.e. the number of cycles that have already been removed.",
  "code": "def edmonds_step_I2(v, desired_edge, level):\n    u = desired_edge[0]\n    Q_nodes = nx.shortest_path(B, v, u)\n    Q_edges = [list(B[Q_nodes[i]][vv].keys())[0] for i, vv in enumerate(Q_nodes[1:])]\n    Q_edges.append(desired_edge[2])\n    minweight = INF\n    minedge = None\n    Q_incoming_weight = {}\n    for edge_key in Q_edges:\n        u, v, data = B_edge_index[edge_key]\n        w = data[attr]\n        Q_incoming_weight[v] = w\n        if data.get(partition) == nx.EdgePartition.INCLUDED:\n            continue\n        if w < minweight:\n            minweight = w\n            minedge = edge_key\n    circuits.append(Q_edges)\n    minedge_circuit.append(minedge)\n    graphs.append((G.copy(), G_edge_index.copy()))\n    branchings.append((B.copy(), B_edge_index.copy()))\n    new_node = new_node_base_name + str(level)\n    G.add_node(new_node)\n    new_edges = []\n    for u, v, key, data in G.edges(data=True, keys=True):\n        if u in Q_incoming_weight:\n            if v in Q_incoming_weight:\n                continue\n            else:\n                dd = data.copy()\n                new_edges.append((new_node, v, key, dd))\n        elif v in Q_incoming_weight:\n            w = data[attr]\n            w += minweight - Q_incoming_weight[v]\n            dd = data.copy()\n            dd[attr] = w\n            new_edges.append((u, new_node, key, dd))\n        else:\n            continue\n    for node in Q_nodes:\n        edmonds_remove_node(G, G_edge_index, node)\n        edmonds_remove_node(B, B_edge_index, node)\n    selected_nodes.difference_update(set(Q_nodes))\n    for u, v, key, data in new_edges:\n        edmonds_add_edge(G, G_edge_index, u, v, key, **data)\n        if candidate_attr in data:\n            del data[candidate_attr]\n            edmonds_add_edge(B, B_edge_index, u, v, key, **data)\n            uf.union(u, v)"
 },
 {
  "docstring": "Returns True if `u` is a root node in G.\n\nNode `u` is a root node if its in-degree over the specified edges is zero.\n\nParameters\n----------\nG : Graph\n    The current graph.\nu : node\n    The node in `G` to check if it is a root.\nedgekeys : iterable of edges\n    The edges for which to check if `u` is a root of.",
  "code": "def is_root(G, u, edgekeys):\n    if u not in G:\n        raise Exception(f'{u!r} not in G')\n    for v in G.pred[u]:\n        for edgekey in G.pred[u][v]:\n            if edgekey in edgekeys:\n                return (False, edgekey)\n    else:\n        return (True, None)"
 },
 {
  "docstring": "Initialize the iterator\n\nParameters\n----------\nG : nx.DiGraph\n    The directed graph which we need to iterate trees over\n\nweight : String, default = \"weight\"\n    The edge attribute used to store the weight of the edge\n\nminimum : bool, default = True\n    Return the trees in increasing order while true and decreasing order\n    while false.\n\ninit_partition : tuple, default = None\n    In the case that certain edges have to be included or excluded from\n    the arborescences, `init_partition` should be in the form\n    `(included_edges, excluded_edges)` where each edges is a\n    `(u, v)`-tuple inside an iterable such as a list or set.",
  "code": "def __init__(self, G, weight='weight', minimum=True, init_partition=None):\n    self.G = G.copy()\n    self.weight = weight\n    self.minimum = minimum\n    self.method = minimum_spanning_arborescence if minimum else maximum_spanning_arborescence\n    self.partition_key = 'ArborescenceIterators super secret partition attribute name'\n    if init_partition is not None:\n        partition_dict = {}\n        for e in init_partition[0]:\n            partition_dict[e] = nx.EdgePartition.INCLUDED\n        for e in init_partition[1]:\n            partition_dict[e] = nx.EdgePartition.EXCLUDED\n        self.init_partition = ArborescenceIterator.Partition(0, partition_dict)\n    else:\n        self.init_partition = None"
 },
 {
  "docstring": "Returns\n-------\nArborescenceIterator\n    The iterator object for this graph",
  "code": "def __iter__(self):\n    self.partition_queue = PriorityQueue()\n    self._clear_partition(self.G)\n    if self.init_partition is not None:\n        self._write_partition(self.init_partition)\n    mst_weight = self.method(self.G, self.weight, partition=self.partition_key, preserve_attrs=True).size(weight=self.weight)\n    self.partition_queue.put(self.Partition(mst_weight if self.minimum else -mst_weight, {} if self.init_partition is None else self.init_partition.partition_dict))\n    return self"
 },
 {
  "docstring": "Returns\n-------\n(multi)Graph\n    The spanning tree of next greatest weight, which ties broken\n    arbitrarily.",
  "code": "def __next__(self):\n    if self.partition_queue.empty():\n        del self.G, self.partition_queue\n        raise StopIteration\n    partition = self.partition_queue.get()\n    self._write_partition(partition)\n    next_arborescence = self.method(self.G, self.weight, partition=self.partition_key, preserve_attrs=True)\n    self._partition(partition, next_arborescence)\n    self._clear_partition(next_arborescence)\n    return next_arborescence"
 },
 {
  "docstring": "Create new partitions based of the minimum spanning tree of the\ncurrent minimum partition.\n\nParameters\n----------\npartition : Partition\n    The Partition instance used to generate the current minimum spanning\n    tree.\npartition_arborescence : nx.Graph\n    The minimum spanning arborescence of the input partition.",
  "code": "def _partition(self, partition, partition_arborescence):\n    p1 = self.Partition(0, partition.partition_dict.copy())\n    p2 = self.Partition(0, partition.partition_dict.copy())\n    for e in partition_arborescence.edges:\n        if e not in partition.partition_dict:\n            p1.partition_dict[e] = nx.EdgePartition.EXCLUDED\n            p2.partition_dict[e] = nx.EdgePartition.INCLUDED\n            self._write_partition(p1)\n            try:\n                p1_mst = self.method(self.G, self.weight, partition=self.partition_key, preserve_attrs=True)\n                p1_mst_weight = p1_mst.size(weight=self.weight)\n                p1.mst_weight = p1_mst_weight if self.minimum else -p1_mst_weight\n                self.partition_queue.put(p1.__copy__())\n            except nx.NetworkXException:\n                pass\n            p1.partition_dict = p2.partition_dict.copy()"
 },
 {
  "docstring": "Writes the desired partition into the graph to calculate the minimum\nspanning tree. Also, if one incoming edge is included, mark all others\nas excluded so that if that vertex is merged during Edmonds' algorithm\nwe cannot still pick another of that vertex's included edges.\n\nParameters\n----------\npartition : Partition\n    A Partition dataclass describing a partition on the edges of the\n    graph.",
  "code": "def _write_partition(self, partition):\n    for u, v, d in self.G.edges(data=True):\n        if (u, v) in partition.partition_dict:\n            d[self.partition_key] = partition.partition_dict[u, v]\n        else:\n            d[self.partition_key] = nx.EdgePartition.OPEN\n    for n in self.G:\n        included_count = 0\n        excluded_count = 0\n        for u, v, d in self.G.in_edges(nbunch=n, data=True):\n            if d.get(self.partition_key) == nx.EdgePartition.INCLUDED:\n                included_count += 1\n            elif d.get(self.partition_key) == nx.EdgePartition.EXCLUDED:\n                excluded_count += 1\n        if included_count == 1 and excluded_count != self.G.in_degree(n) - 1:\n            for u, v, d in self.G.in_edges(nbunch=n, data=True):\n                if d.get(self.partition_key) != nx.EdgePartition.INCLUDED:\n                    d[self.partition_key] = nx.EdgePartition.EXCLUDED"
 },
 {
  "docstring": "Removes partition data from the graph",
  "code": "def _clear_partition(self, G):\n    for u, v, d in G.edges(data=True):\n        if self.partition_key in d:\n            del d[self.partition_key]"
 },
 {
  "docstring": "Find the edge directed toward v with maximal weight.\n\nIf an edge partition exists in this graph, return the included edge\nif it exists and no not return any excluded edges. There can only\nbe one included edge for each vertex otherwise the edge partition is\nempty.",
  "code": "def desired_edge(v):\n    edge = None\n    weight = -INF\n    for u, _, key, data in G.in_edges(v, data=True, keys=True):\n        if data.get(partition) == nx.EdgePartition.EXCLUDED:\n            continue\n        new_weight = data[attr]\n        if data.get(partition) == nx.EdgePartition.INCLUDED:\n            weight = new_weight\n            edge = (u, v, key, new_weight, data)\n            return (edge, weight)\n        if new_weight > weight:\n            weight = new_weight\n            edge = (u, v, key, new_weight, data)\n    return (edge, weight)"
 },
 {
  "docstring": "Returns True if `u` is a root node in G.\n\nNode `u` will be a root node if its in-degree, restricted to the\nspecified edges, is equal to 0.",
  "code": "def is_root(G, u, edgekeys):\n    if u not in G:\n        raise Exception(f'{u!r} not in G')\n    for v in G.pred[u]:\n        for edgekey in G.pred[u][v]:\n            if edgekey in edgekeys:\n                return (False, edgekey)\n    else:\n        return (True, None)"
 },
 {
  "docstring": "Returns a nested tuple representation of the given tree.\n\nThe nested tuple representation of a tree is defined\nrecursively. The tree with one node and no edges is represented by\nthe empty tuple, ``()``. A tree with ``k`` subtrees is represented\nby a tuple of length ``k`` in which each element is the nested tuple\nrepresentation of a subtree.\n\nParameters\n----------\nT : NetworkX graph\n    An undirected graph object representing a tree.\n\nroot : node\n    The node in ``T`` to interpret as the root of the tree.\n\ncanonical_form : bool\n    If ``True``, each tuple is sorted so that the function returns\n    a canonical form for rooted trees. This means \"lighter\" subtrees\n    will appear as nested tuples before \"heavier\" subtrees. In this\n    way, each isomorphic rooted tree has the same nested tuple\n    representation.\n\nReturns\n-------\ntuple\n    A nested tuple representation of the tree.\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch(graphs='T')\ndef to_nested_tuple(T, root, canonical_form=False):\n\n    def _make_tuple(T, root, _parent):\n        \"\"\"Recursively compute the nested tuple representation of the\n        given rooted tree.\n\n        ``_parent`` is the parent node of ``root`` in the supertree in\n        which ``T`` is a subtree, or ``None`` if ``root`` is the root of\n        the supertree. This argument is used to determine which\n        neighbors of ``root`` are children and which is the parent.\n\n        \"\"\"\n        children = set(T[root]) - {_parent}\n        if len(children) == 0:\n            return ()\n        nested = (_make_tuple(T, v, root) for v in children)\n        if canonical_form:\n            nested = sorted(nested)\n        return tuple(nested)\n    if not nx.is_tree(T):\n        raise nx.NotATree('provided graph is not a tree')\n    if root not in T:\n        raise nx.NodeNotFound(f'Graph {T} contains no node {root}')\n    return _make_tuple(T, root, None)"
 },
 {
  "docstring": "Returns the rooted tree corresponding to the given nested tuple.\n\nThe nested tuple representation of a tree is defined\nrecursively. The tree with one node and no edges is represented by\nthe empty tuple, ``()``. A tree with ``k`` subtrees is represented\nby a tuple of length ``k`` in which each element is the nested tuple\nrepresentation of a subtree.\n\nParameters\n----------\nsequence : tuple\n    A nested tuple representing a rooted tree.\n\nsensible_relabeling : bool\n    Whether to relabel the nodes of the tree so that nodes are\n    labeled in increasing order according to their breadth-first\n    search order from the root node.\n\nReturns\n-------\nNetworkX graph\n    The tree corresponding to the given nested tuple, whose root\n    node is node 0. If ``sensible_labeling`` is ``True``, nodes will\n    be labeled in breadth-first search order starting from the root\n    node.\n\n",
  "code": "@nx._dispatch(graphs=None)\ndef from_nested_tuple(sequence, sensible_relabeling=False):\n\n    def _make_tree(sequence):\n        \"\"\"Recursively creates a tree from the given sequence of nested\n        tuples.\n\n        This function employs the :func:`~networkx.tree.join` function\n        to recursively join subtrees into a larger tree.\n\n        \"\"\"\n        if len(sequence) == 0:\n            return nx.empty_graph(1)\n        return nx.tree.join_trees([(_make_tree(child), 0) for child in sequence])\n    T = _make_tree(sequence)\n    if sensible_relabeling:\n        bfs_nodes = chain([0], (v for u, v in nx.bfs_edges(T, 0)))\n        labels = {v: i for i, v in enumerate(bfs_nodes)}\n        T = nx.relabel_nodes(T, labels)\n    return T"
 },
 {
  "docstring": "Returns the Pr\u00fcfer sequence of the given tree.\n\nA *Pr\u00fcfer sequence* is a list of *n* - 2 numbers between 0 and\n*n* - 1, inclusive. The tree corresponding to a given Pr\u00fcfer\nsequence can be recovered by repeatedly joining a node in the\nsequence with a node with the smallest potential degree according to\nthe sequence.\n\nParameters\n----------\nT : NetworkX graph\n    An undirected graph object representing a tree.\n\nReturns\n-------\nlist\n    The Pr\u00fcfer sequence of the given tree.\n\nRaises\n------\nNetworkXPointlessConcept\n    If the number of nodes in `T` is less than two.\n\nNotATree\n    If `T` is not a tree.\n\nKeyError\n    If the set of nodes in `T` is not {0, \u2026, *n* - 1}.\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch(graphs='T')\ndef to_prufer_sequence(T):\n    n = len(T)\n    if n < 2:\n        msg = 'Pr\u00fcfer sequence undefined for trees with fewer than two nodes'\n        raise nx.NetworkXPointlessConcept(msg)\n    if not nx.is_tree(T):\n        raise nx.NotATree('provided graph is not a tree')\n    if set(T) != set(range(n)):\n        raise KeyError('tree must have node labels {0, ..., n - 1}')\n    degree = dict(T.degree())\n\n    def parents(u):\n        return next((v for v in T[u] if degree[v] > 1))\n    index = u = next((k for k in range(n) if degree[k] == 1))\n    result = []\n    for i in range(n - 2):\n        v = parents(u)\n        result.append(v)\n        degree[v] -= 1\n        if v < index and degree[v] == 1:\n            u = v\n        else:\n            index = u = next((k for k in range(index + 1, n) if degree[k] == 1))\n    return result"
 },
 {
  "docstring": "Returns the tree corresponding to the given Pr\u00fcfer sequence.\n\nA *Pr\u00fcfer sequence* is a list of *n* - 2 numbers between 0 and\n*n* - 1, inclusive. The tree corresponding to a given Pr\u00fcfer\nsequence can be recovered by repeatedly joining a node in the\nsequence with a node with the smallest potential degree according to\nthe sequence.\n\nParameters\n----------\nsequence : list\n    A Pr\u00fcfer sequence, which is a list of *n* - 2 integers between\n    zero and *n* - 1, inclusive.\n\nReturns\n-------\nNetworkX graph\n    The tree corresponding to the given Pr\u00fcfer sequence.\n\nRaises\n------\nNetworkXError\n    If the Pr\u00fcfer sequence is not valid.\n\n",
  "code": "@nx._dispatch(graphs=None)\ndef from_prufer_sequence(sequence):\n    n = len(sequence) + 2\n    degree = Counter(chain(sequence, range(n)))\n    T = nx.empty_graph(n)\n    not_orphaned = set()\n    index = u = next((k for k in range(n) if degree[k] == 1))\n    for v in sequence:\n        if v < 0 or v > n - 1:\n            raise nx.NetworkXError(f'Invalid Prufer sequence: Values must be between 0 and {n - 1}, got {v}')\n        T.add_edge(u, v)\n        not_orphaned.add(u)\n        degree[v] -= 1\n        if v < index and degree[v] == 1:\n            u = v\n        else:\n            index = u = next((k for k in range(index + 1, n) if degree[k] == 1))\n    orphans = set(T) - not_orphaned\n    u, v = orphans\n    T.add_edge(u, v)\n    return T"
 },
 {
  "docstring": "Recursively compute the nested tuple representation of the\ngiven rooted tree.\n\n``_parent`` is the parent node of ``root`` in the supertree in\nwhich ``T`` is a subtree, or ``None`` if ``root`` is the root of\nthe supertree. This argument is used to determine which\nneighbors of ``root`` are children and which is the parent.",
  "code": "def _make_tuple(T, root, _parent):\n    children = set(T[root]) - {_parent}\n    if len(children) == 0:\n        return ()\n    nested = (_make_tuple(T, v, root) for v in children)\n    if canonical_form:\n        nested = sorted(nested)\n    return tuple(nested)"
 },
 {
  "docstring": "Recursively creates a tree from the given sequence of nested\ntuples.\n\nThis function employs the :func:`~networkx.tree.join` function\nto recursively join subtrees into a larger tree.",
  "code": "def _make_tree(sequence):\n    if len(sequence) == 0:\n        return nx.empty_graph(1)\n    return nx.tree.join_trees([(_make_tree(child), 0) for child in sequence])"
 },
 {
  "docstring": "Returns a junction tree of a given graph.\n\nA junction tree (or clique tree) is constructed from a (un)directed graph G.\nThe tree is constructed based on a moralized and triangulated version of G.\nThe tree's nodes consist of maximal cliques and sepsets of the revised graph.\nThe sepset of two cliques is the intersection of the nodes of these cliques,\ne.g. the sepset of (A,B,C) and (A,C,E,F) is (A,C). These nodes are often called\n\"variables\" in this literature. The tree is bipartite with each sepset\nconnected to its two cliques.\n\nJunction Trees are not unique as the order of clique consideration determines\nwhich sepsets are included.\n\nThe junction tree algorithm consists of five steps [1]_:\n\n1. Moralize the graph\n2. Triangulate the graph\n3. Find maximal cliques\n4. Build the tree from cliques, connecting cliques with shared\n   nodes, set edge-weight to number of shared variables\n5. Find maximum spanning tree\n\n\nParameters\n----------\nG : networkx.Graph\n    Directed or undirected graph.\n\nReturns\n-------\njunction_tree : networkx.Graph\n    The corresponding junction tree of `G`.\n\nRaises\n------\nNetworkXNotImplemented\n    Raised if `G` is an instance of `MultiGraph` or `MultiDiGraph`.\n\n",
  "code": "@not_implemented_for('multigraph')\n@nx._dispatch\ndef junction_tree(G):\n    clique_graph = nx.Graph()\n    if G.is_directed():\n        G = moral.moral_graph(G)\n    chordal_graph, _ = complete_to_chordal_graph(G)\n    cliques = [tuple(sorted(i)) for i in chordal_graph_cliques(chordal_graph)]\n    clique_graph.add_nodes_from(cliques, type='clique')\n    for edge in combinations(cliques, 2):\n        set_edge_0 = set(edge[0])\n        set_edge_1 = set(edge[1])\n        if not set_edge_0.isdisjoint(set_edge_1):\n            sepset = tuple(sorted(set_edge_0.intersection(set_edge_1)))\n            clique_graph.add_edge(edge[0], edge[1], weight=len(sepset), sepset=sepset)\n    junction_tree = nx.maximum_spanning_tree(clique_graph)\n    for edge in list(junction_tree.edges(data=True)):\n        junction_tree.add_node(edge[2]['sepset'], type='sepset')\n        junction_tree.add_edge(edge[0], edge[2]['sepset'])\n        junction_tree.add_edge(edge[1], edge[2]['sepset'])\n        junction_tree.remove_edge(edge[0], edge[1])\n    return junction_tree"
 },
 {
  "docstring": "Iterate over edges of a Bor\u016fvka's algorithm min/max spanning tree.\n\nParameters\n----------\nG : NetworkX Graph\n    The edges of `G` must have distinct weights,\n    otherwise the edges may not form a tree.\n\nminimum : bool (default: True)\n    Find the minimum (True) or maximum (False) spanning tree.\n\nweight : string (default: 'weight')\n    The name of the edge attribute holding the edge weights.\n\nkeys : bool (default: True)\n    This argument is ignored since this function is not\n    implemented for multigraphs; it exists only for consistency\n    with the other minimum spanning tree functions.\n\ndata : bool (default: True)\n    Flag for whether to yield edge attribute dicts.\n    If True, yield edges `(u, v, d)`, where `d` is the attribute dict.\n    If False, yield edges `(u, v)`.\n\nignore_nan : bool (default: False)\n    If a NaN is found as an edge weight normally an exception is raised.\n    If `ignore_nan is True` then that edge is ignored instead.",
  "code": "@not_implemented_for('multigraph')\n@nx._dispatch(edge_attrs='weight', preserve_edge_attrs='data')\ndef boruvka_mst_edges(G, minimum=True, weight='weight', keys=False, data=True, ignore_nan=False):\n    forest = UnionFind(G)\n\n    def best_edge(component):\n        \"\"\"Returns the optimum (minimum or maximum) edge on the edge\n        boundary of the given set of nodes.\n\n        A return value of ``None`` indicates an empty boundary.\n\n        \"\"\"\n        sign = 1 if minimum else -1\n        minwt = float('inf')\n        boundary = None\n        for e in nx.edge_boundary(G, component, data=True):\n            wt = e[-1].get(weight, 1) * sign\n            if isnan(wt):\n                if ignore_nan:\n                    continue\n                msg = f'NaN found as an edge weight. Edge {e}'\n                raise ValueError(msg)\n            if wt < minwt:\n                minwt = wt\n                boundary = e\n        return boundary\n    best_edges = (best_edge(component) for component in forest.to_sets())\n    best_edges = [edge for edge in best_edges if edge is not None]\n    while best_edges:\n        best_edges = (best_edge(component) for component in forest.to_sets())\n        best_edges = [edge for edge in best_edges if edge is not None]\n        for u, v, d in best_edges:\n            if forest[u] != forest[v]:\n                if data:\n                    yield (u, v, d)\n                else:\n                    yield (u, v)\n                forest.union(u, v)"
 },
 {
  "docstring": "Iterate over edge of a Kruskal's algorithm min/max spanning tree.\n\nParameters\n----------\nG : NetworkX Graph\n    The graph holding the tree of interest.\n\nminimum : bool (default: True)\n    Find the minimum (True) or maximum (False) spanning tree.\n\nweight : string (default: 'weight')\n    The name of the edge attribute holding the edge weights.\n\nkeys : bool (default: True)\n    If `G` is a multigraph, `keys` controls whether edge keys ar yielded.\n    Otherwise `keys` is ignored.\n\ndata : bool (default: True)\n    Flag for whether to yield edge attribute dicts.\n    If True, yield edges `(u, v, d)`, where `d` is the attribute dict.\n    If False, yield edges `(u, v)`.\n\nignore_nan : bool (default: False)\n    If a NaN is found as an edge weight normally an exception is raised.\n    If `ignore_nan is True` then that edge is ignored instead.\n\npartition : string (default: None)\n    The name of the edge attribute holding the partition data, if it exists.\n    Partition data is written to the edges using the `EdgePartition` enum.\n    If a partition exists, all included edges and none of the excluded edges\n    will appear in the final tree. Open edges may or may not be used.\n\nYields\n------\nedge tuple\n    The edges as discovered by Kruskal's method. Each edge can\n    take the following forms: `(u, v)`, `(u, v, d)` or `(u, v, k, d)`\n    depending on the `key` and `data` parameters",
  "code": "@nx._dispatch(edge_attrs={'weight': None, 'partition': None}, preserve_edge_attrs='data')\ndef kruskal_mst_edges(G, minimum, weight='weight', keys=True, data=True, ignore_nan=False, partition=None):\n    subtrees = UnionFind()\n    if G.is_multigraph():\n        edges = G.edges(keys=True, data=True)\n    else:\n        edges = G.edges(data=True)\n    '\\n    Sort the edges of the graph with respect to the partition data. \\n    Edges are returned in the following order:\\n\\n    * Included edges\\n    * Open edges from smallest to largest weight\\n    * Excluded edges\\n    '\n    included_edges = []\n    open_edges = []\n    for e in edges:\n        d = e[-1]\n        wt = d.get(weight, 1)\n        if isnan(wt):\n            if ignore_nan:\n                continue\n            raise ValueError(f'NaN found as an edge weight. Edge {e}')\n        edge = (wt,) + e\n        if d.get(partition) == EdgePartition.INCLUDED:\n            included_edges.append(edge)\n        elif d.get(partition) == EdgePartition.EXCLUDED:\n            continue\n        else:\n            open_edges.append(edge)\n    if minimum:\n        sorted_open_edges = sorted(open_edges, key=itemgetter(0))\n    else:\n        sorted_open_edges = sorted(open_edges, key=itemgetter(0), reverse=True)\n    included_edges.extend(sorted_open_edges)\n    sorted_edges = included_edges\n    del open_edges, sorted_open_edges, included_edges\n    if G.is_multigraph():\n        for wt, u, v, k, d in sorted_edges:\n            if subtrees[u] != subtrees[v]:\n                if keys:\n                    if data:\n                        yield (u, v, k, d)\n                    else:\n                        yield (u, v, k)\n                elif data:\n                    yield (u, v, d)\n                else:\n                    yield (u, v)\n                subtrees.union(u, v)\n    else:\n        for wt, u, v, d in sorted_edges:\n            if subtrees[u] != subtrees[v]:\n                if data:\n                    yield (u, v, d)\n                else:\n                    yield (u, v)\n                subtrees.union(u, v)"
 },
 {
  "docstring": "Iterate over edges of Prim's algorithm min/max spanning tree.\n\nParameters\n----------\nG : NetworkX Graph\n    The graph holding the tree of interest.\n\nminimum : bool (default: True)\n    Find the minimum (True) or maximum (False) spanning tree.\n\nweight : string (default: 'weight')\n    The name of the edge attribute holding the edge weights.\n\nkeys : bool (default: True)\n    If `G` is a multigraph, `keys` controls whether edge keys ar yielded.\n    Otherwise `keys` is ignored.\n\ndata : bool (default: True)\n    Flag for whether to yield edge attribute dicts.\n    If True, yield edges `(u, v, d)`, where `d` is the attribute dict.\n    If False, yield edges `(u, v)`.\n\nignore_nan : bool (default: False)\n    If a NaN is found as an edge weight normally an exception is raised.\n    If `ignore_nan is True` then that edge is ignored instead.",
  "code": "@nx._dispatch(edge_attrs='weight', preserve_edge_attrs='data')\ndef prim_mst_edges(G, minimum, weight='weight', keys=True, data=True, ignore_nan=False):\n    is_multigraph = G.is_multigraph()\n    push = heappush\n    pop = heappop\n    nodes = set(G)\n    c = count()\n    sign = 1 if minimum else -1\n    while nodes:\n        u = nodes.pop()\n        frontier = []\n        visited = {u}\n        if is_multigraph:\n            for v, keydict in G.adj[u].items():\n                for k, d in keydict.items():\n                    wt = d.get(weight, 1) * sign\n                    if isnan(wt):\n                        if ignore_nan:\n                            continue\n                        msg = f'NaN found as an edge weight. Edge {(u, v, k, d)}'\n                        raise ValueError(msg)\n                    push(frontier, (wt, next(c), u, v, k, d))\n        else:\n            for v, d in G.adj[u].items():\n                wt = d.get(weight, 1) * sign\n                if isnan(wt):\n                    if ignore_nan:\n                        continue\n                    msg = f'NaN found as an edge weight. Edge {(u, v, d)}'\n                    raise ValueError(msg)\n                push(frontier, (wt, next(c), u, v, d))\n        while nodes and frontier:\n            if is_multigraph:\n                W, _, u, v, k, d = pop(frontier)\n            else:\n                W, _, u, v, d = pop(frontier)\n            if v in visited or v not in nodes:\n                continue\n            if is_multigraph and keys:\n                if data:\n                    yield (u, v, k, d)\n                else:\n                    yield (u, v, k)\n            elif data:\n                yield (u, v, d)\n            else:\n                yield (u, v)\n            visited.add(v)\n            nodes.discard(v)\n            if is_multigraph:\n                for w, keydict in G.adj[v].items():\n                    if w in visited:\n                        continue\n                    for k2, d2 in keydict.items():\n                        new_weight = d2.get(weight, 1) * sign\n                        if isnan(new_weight):\n                            if ignore_nan:\n                                continue\n                            msg = f'NaN found as an edge weight. Edge {(v, w, k2, d2)}'\n                            raise ValueError(msg)\n                        push(frontier, (new_weight, next(c), v, w, k2, d2))\n            else:\n                for w, d2 in G.adj[v].items():\n                    if w in visited:\n                        continue\n                    new_weight = d2.get(weight, 1) * sign\n                    if isnan(new_weight):\n                        if ignore_nan:\n                            continue\n                        msg = f'NaN found as an edge weight. Edge {(v, w, d2)}'\n                        raise ValueError(msg)\n                    push(frontier, (new_weight, next(c), v, w, d2))"
 },
 {
  "docstring": "Generate edges in a minimum spanning forest of an undirected\nweighted graph.\n\nA minimum spanning tree is a subgraph of the graph (a tree)\nwith the minimum sum of edge weights.  A spanning forest is a\nunion of the spanning trees for each connected component of the graph.\n\nParameters\n----------\nG : undirected Graph\n   An undirected graph. If `G` is connected, then the algorithm finds a\n   spanning tree. Otherwise, a spanning forest is found.\n\nalgorithm : string\n   The algorithm to use when finding a minimum spanning tree. Valid\n   choices are 'kruskal', 'prim', or 'boruvka'. The default is 'kruskal'.\n\nweight : string\n   Edge data key to use for weight (default 'weight').\n\nkeys : bool\n   Whether to yield edge key in multigraphs in addition to the edge.\n   If `G` is not a multigraph, this is ignored.\n\ndata : bool, optional\n   If True yield the edge data along with the edge.\n\nignore_nan : bool (default: False)\n    If a NaN is found as an edge weight normally an exception is raised.\n    If `ignore_nan is True` then that edge is ignored instead.\n\nReturns\n-------\nedges : iterator\n   An iterator over edges in a maximum spanning tree of `G`.\n   Edges connecting nodes `u` and `v` are represented as tuples:\n   `(u, v, k, d)` or `(u, v, k)` or `(u, v, d)` or `(u, v)`\n\n   If `G` is a multigraph, `keys` indicates whether the edge key `k` will\n   be reported in the third position in the edge tuple. `data` indicates\n   whether the edge datadict `d` will appear at the end of the edge tuple.\n\n   If `G` is not a multigraph, the tuples are `(u, v, d)` if `data` is True\n   or `(u, v)` if `data` is False.\n\nExamples\n--------\n>>> from networkx.algorithms import tree\n\nFind minimum spanning edges by Kruskal's algorithm\n\n>>> G = nx.cycle_graph(4)\n>>> G.add_edge(0, 3, weight=2)\n>>> mst = tree.minimum_spanning_edges(G, algorithm=\"kruskal\", data=False)\n>>> edgelist = list(mst)\n>>> sorted(sorted(e) for e in edgelist)\n[[0, 1], [1, 2], [2, 3]]\n\nFind minimum spanning edges by Prim's algorithm\n\n>>> G = nx.cycle_graph(4)\n>>> G.add_edge(0, 3, weight=2)\n>>> mst = tree.minimum_spanning_edges(G, algorithm=\"prim\", data=False)\n>>> edgelist = list(mst)\n>>> sorted(sorted(e) for e in edgelist)\n[[0, 1], [1, 2], [2, 3]]\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch(edge_attrs='weight', preserve_edge_attrs='data')\ndef minimum_spanning_edges(G, algorithm='kruskal', weight='weight', keys=True, data=True, ignore_nan=False):\n    try:\n        algo = ALGORITHMS[algorithm]\n    except KeyError as err:\n        msg = f'{algorithm} is not a valid choice for an algorithm.'\n        raise ValueError(msg) from err\n    return algo(G, minimum=True, weight=weight, keys=keys, data=data, ignore_nan=ignore_nan)"
 },
 {
  "docstring": "Generate edges in a maximum spanning forest of an undirected\nweighted graph.\n\nA maximum spanning tree is a subgraph of the graph (a tree)\nwith the maximum possible sum of edge weights.  A spanning forest is a\nunion of the spanning trees for each connected component of the graph.\n\nParameters\n----------\nG : undirected Graph\n   An undirected graph. If `G` is connected, then the algorithm finds a\n   spanning tree. Otherwise, a spanning forest is found.\n\nalgorithm : string\n   The algorithm to use when finding a maximum spanning tree. Valid\n   choices are 'kruskal', 'prim', or 'boruvka'. The default is 'kruskal'.\n\nweight : string\n   Edge data key to use for weight (default 'weight').\n\nkeys : bool\n   Whether to yield edge key in multigraphs in addition to the edge.\n   If `G` is not a multigraph, this is ignored.\n\ndata : bool, optional\n   If True yield the edge data along with the edge.\n\nignore_nan : bool (default: False)\n    If a NaN is found as an edge weight normally an exception is raised.\n    If `ignore_nan is True` then that edge is ignored instead.\n\nReturns\n-------\nedges : iterator\n   An iterator over edges in a maximum spanning tree of `G`.\n   Edges connecting nodes `u` and `v` are represented as tuples:\n   `(u, v, k, d)` or `(u, v, k)` or `(u, v, d)` or `(u, v)`\n\n   If `G` is a multigraph, `keys` indicates whether the edge key `k` will\n   be reported in the third position in the edge tuple. `data` indicates\n   whether the edge datadict `d` will appear at the end of the edge tuple.\n\n   If `G` is not a multigraph, the tuples are `(u, v, d)` if `data` is True\n   or `(u, v)` if `data` is False.\n\nExamples\n--------\n>>> from networkx.algorithms import tree\n\nFind maximum spanning edges by Kruskal's algorithm\n\n>>> G = nx.cycle_graph(4)\n>>> G.add_edge(0, 3, weight=2)\n>>> mst = tree.maximum_spanning_edges(G, algorithm=\"kruskal\", data=False)\n>>> edgelist = list(mst)\n>>> sorted(sorted(e) for e in edgelist)\n[[0, 1], [0, 3], [1, 2]]\n\nFind maximum spanning edges by Prim's algorithm\n\n>>> G = nx.cycle_graph(4)\n>>> G.add_edge(0, 3, weight=2)  # assign weight 2 to edge 0-3\n>>> mst = tree.maximum_spanning_edges(G, algorithm=\"prim\", data=False)\n>>> edgelist = list(mst)\n>>> sorted(sorted(e) for e in edgelist)\n[[0, 1], [0, 3], [2, 3]]\n\n",
  "code": "@not_implemented_for('directed')\n@nx._dispatch(edge_attrs='weight', preserve_edge_attrs='data')\ndef maximum_spanning_edges(G, algorithm='kruskal', weight='weight', keys=True, data=True, ignore_nan=False):\n    try:\n        algo = ALGORITHMS[algorithm]\n    except KeyError as err:\n        msg = f'{algorithm} is not a valid choice for an algorithm.'\n        raise ValueError(msg) from err\n    return algo(G, minimum=False, weight=weight, keys=keys, data=data, ignore_nan=ignore_nan)"
 },
 {
  "docstring": "Returns a minimum spanning tree or forest on an undirected graph `G`.\n\nParameters\n----------\nG : undirected graph\n    An undirected graph. If `G` is connected, then the algorithm finds a\n    spanning tree. Otherwise, a spanning forest is found.\n\nweight : str\n   Data key to use for edge weights.\n\nalgorithm : string\n   The algorithm to use when finding a minimum spanning tree. Valid\n   choices are 'kruskal', 'prim', or 'boruvka'. The default is\n   'kruskal'.\n\nignore_nan : bool (default: False)\n    If a NaN is found as an edge weight normally an exception is raised.\n    If `ignore_nan is True` then that edge is ignored instead.\n\nReturns\n-------\nG : NetworkX Graph\n   A minimum spanning tree or forest.\n\nExamples\n--------\n>>> G = nx.cycle_graph(4)\n>>> G.add_edge(0, 3, weight=2)\n>>> T = nx.minimum_spanning_tree(G)\n>>> sorted(T.edges(data=True))\n[(0, 1, {}), (1, 2, {}), (2, 3, {})]\n\n\n",
  "code": "@nx._dispatch(preserve_all_attrs=True)\ndef minimum_spanning_tree(G, weight='weight', algorithm='kruskal', ignore_nan=False):\n    edges = minimum_spanning_edges(G, algorithm, weight, keys=True, data=True, ignore_nan=ignore_nan)\n    T = G.__class__()\n    T.graph.update(G.graph)\n    T.add_nodes_from(G.nodes.items())\n    T.add_edges_from(edges)\n    return T"
 },
 {
  "docstring": "Find a spanning tree while respecting a partition of edges.\n\nEdges can be flagged as either `INCLUDED` which are required to be in the\nreturned tree, `EXCLUDED`, which cannot be in the returned tree and `OPEN`.\n\nThis is used in the SpanningTreeIterator to create new partitions following\nthe algorithm of S\u00f6rensen and Janssens [1]_.\n\nParameters\n----------\nG : undirected graph\n    An undirected graph.\n\nminimum : bool (default: True)\n    Determines whether the returned tree is the minimum spanning tree of\n    the partition of the maximum one.\n\nweight : str\n    Data key to use for edge weights.\n\npartition : str\n    The key for the edge attribute containing the partition\n    data on the graph. Edges can be included, excluded or open using the\n    `EdgePartition` enum.\n\nignore_nan : bool (default: False)\n    If a NaN is found as an edge weight normally an exception is raised.\n    If `ignore_nan is True` then that edge is ignored instead.\n\n\nReturns\n-------\nG : NetworkX Graph\n    A minimum spanning tree using all of the included edges in the graph and\n    none of the excluded edges.\n\n",
  "code": "@nx._dispatch(preserve_all_attrs=True)\ndef partition_spanning_tree(G, minimum=True, weight='weight', partition='partition', ignore_nan=False):\n    edges = kruskal_mst_edges(G, minimum, weight, keys=True, data=True, ignore_nan=ignore_nan, partition=partition)\n    T = G.__class__()\n    T.graph.update(G.graph)\n    T.add_nodes_from(G.nodes.items())\n    T.add_edges_from(edges)\n    return T"
 },
 {
  "docstring": "Returns a maximum spanning tree or forest on an undirected graph `G`.\n\nParameters\n----------\nG : undirected graph\n    An undirected graph. If `G` is connected, then the algorithm finds a\n    spanning tree. Otherwise, a spanning forest is found.\n\nweight : str\n   Data key to use for edge weights.\n\nalgorithm : string\n   The algorithm to use when finding a maximum spanning tree. Valid\n   choices are 'kruskal', 'prim', or 'boruvka'. The default is\n   'kruskal'.\n\nignore_nan : bool (default: False)\n    If a NaN is found as an edge weight normally an exception is raised.\n    If `ignore_nan is True` then that edge is ignored instead.\n\n\nReturns\n-------\nG : NetworkX Graph\n   A maximum spanning tree or forest.\n\n\nExamples\n--------\n>>> G = nx.cycle_graph(4)\n>>> G.add_edge(0, 3, weight=2)\n>>> T = nx.maximum_spanning_tree(G)\n>>> sorted(T.edges(data=True))\n[(0, 1, {}), (0, 3, {'weight': 2}), (1, 2, {})]\n\n\n",
  "code": "@nx._dispatch(preserve_all_attrs=True)\ndef maximum_spanning_tree(G, weight='weight', algorithm='kruskal', ignore_nan=False):\n    edges = maximum_spanning_edges(G, algorithm, weight, keys=True, data=True, ignore_nan=ignore_nan)\n    edges = list(edges)\n    T = G.__class__()\n    T.graph.update(G.graph)\n    T.add_nodes_from(G.nodes.items())\n    T.add_edges_from(edges)\n    return T"
 },
 {
  "docstring": "Sample a random spanning tree using the edges weights of `G`.\n\nThis function supports two different methods for determining the\nprobability of the graph. If ``multiplicative=True``, the probability\nis based on the product of edge weights, and if ``multiplicative=False``\nit is based on the sum of the edge weight. However, since it is\neasier to determine the total weight of all spanning trees for the\nmultiplicative version, that is significantly faster and should be used if\npossible. Additionally, setting `weight` to `None` will cause a spanning tree\nto be selected with uniform probability.\n\nThe function uses algorithm A8 in [1]_ .\n\nParameters\n----------\nG : nx.Graph\n    An undirected version of the original graph.\n\nweight : string\n    The edge key for the edge attribute holding edge weight.\n\nmultiplicative : bool, default=True\n    If `True`, the probability of each tree is the product of its edge weight\n    over the sum of the product of all the spanning trees in the graph. If\n    `False`, the probability is the sum of its edge weight over the sum of\n    the sum of weights for all spanning trees in the graph.\n\nseed : integer, random_state, or None (default)\n    Indicator of random number generation state.\n    See :ref:`Randomness<randomness>`.\n\nReturns\n-------\nnx.Graph\n    A spanning tree using the distribution defined by the weight of the tree.\n\n",
  "code": "@py_random_state(3)\n@nx._dispatch(preserve_edge_attrs=True)\ndef random_spanning_tree(G, weight=None, *, multiplicative=True, seed=None):\n\n    def find_node(merged_nodes, node):\n        \"\"\"\n        We can think of clusters of contracted nodes as having one\n        representative in the graph. Each node which is not in merged_nodes\n        is still its own representative. Since a representative can be later\n        contracted, we need to recursively search though the dict to find\n        the final representative, but once we know it we can use path\n        compression to speed up the access of the representative for next time.\n\n        This cannot be replaced by the standard NetworkX union_find since that\n        data structure will merge nodes with less representing nodes into the\n        one with more representing nodes but this function requires we merge\n        them using the order that contract_edges contracts using.\n\n        Parameters\n        ----------\n        merged_nodes : dict\n            The dict storing the mapping from node to representative\n        node\n            The node whose representative we seek\n\n        Returns\n        -------\n        The representative of the `node`\n        \"\"\"\n        if node not in merged_nodes:\n            return node\n        else:\n            rep = find_node(merged_nodes, merged_nodes[node])\n            merged_nodes[node] = rep\n            return rep\n\n    def prepare_graph():\n        \"\"\"\n        For the graph `G`, remove all edges not in the set `V` and then\n        contract all edges in the set `U`.\n\n        Returns\n        -------\n        A copy of `G` which has had all edges not in `V` removed and all edges\n        in `U` contracted.\n        \"\"\"\n        result = nx.MultiGraph(incoming_graph_data=G)\n        edges_to_remove = set(result.edges()).difference(V)\n        result.remove_edges_from(edges_to_remove)\n        merged_nodes = {}\n        for u, v in U:\n            u_rep = find_node(merged_nodes, u)\n            v_rep = find_node(merged_nodes, v)\n            if u_rep == v_rep:\n                continue\n            nx.contracted_nodes(result, u_rep, v_rep, self_loops=False, copy=False)\n            merged_nodes[v_rep] = u_rep\n        return (merged_nodes, result)\n\n    def spanning_tree_total_weight(G, weight):\n        \"\"\"\n        Find the sum of weights of the spanning trees of `G` using the\n        appropriate `method`.\n\n        This is easy if the chosen method is 'multiplicative', since we can\n        use Kirchhoff's Tree Matrix Theorem directly. However, with the\n        'additive' method, this process is slightly more complex and less\n        computationally efficient as we have to find the number of spanning\n        trees which contain each possible edge in the graph.\n\n        Parameters\n        ----------\n        G : NetworkX Graph\n            The graph to find the total weight of all spanning trees on.\n\n        weight : string\n            The key for the weight edge attribute of the graph.\n\n        Returns\n        -------\n        float\n            The sum of either the multiplicative or additive weight for all\n            spanning trees in the graph.\n        \"\"\"\n        if multiplicative:\n            return nx.total_spanning_tree_weight(G, weight)\n        elif G.number_of_edges() == 1:\n            return G.edges(data=weight).__iter__().__next__()[2]\n        else:\n            total = 0\n            for u, v, w in G.edges(data=weight):\n                total += w * nx.total_spanning_tree_weight(nx.contracted_edge(G, edge=(u, v), self_loops=False), None)\n            return total\n    U = set()\n    st_cached_value = 0\n    V = set(G.edges())\n    shuffled_edges = list(G.edges())\n    seed.shuffle(shuffled_edges)\n    for u, v in shuffled_edges:\n        e_weight = G[u][v][weight] if weight is not None else 1\n        node_map, prepared_G = prepare_graph()\n        G_total_tree_weight = spanning_tree_total_weight(prepared_G, weight)\n        rep_edge = (find_node(node_map, u), find_node(node_map, v))\n        if rep_edge in prepared_G.edges:\n            prepared_G_e = nx.contracted_edge(prepared_G, edge=rep_edge, self_loops=False)\n            G_e_total_tree_weight = spanning_tree_total_weight(prepared_G_e, weight)\n            if multiplicative:\n                threshold = e_weight * G_e_total_tree_weight / G_total_tree_weight\n            else:\n                numerator = (st_cached_value + e_weight) * nx.total_spanning_tree_weight(prepared_G_e) + G_e_total_tree_weight\n                denominator = st_cached_value * nx.total_spanning_tree_weight(prepared_G) + G_total_tree_weight\n                threshold = numerator / denominator\n        else:\n            threshold = 0.0\n        z = seed.uniform(0.0, 1.0)\n        if z > threshold:\n            V.remove((u, v))\n        else:\n            st_cached_value += e_weight\n            U.add((u, v))\n        if len(U) == G.number_of_nodes() - 1:\n            spanning_tree = nx.Graph()\n            spanning_tree.add_edges_from(U)\n            return spanning_tree\n    raise Exception(f'Something went wrong! Only {len(U)} edges in the spanning tree!')"
 },
 {
  "docstring": "Returns the optimum (minimum or maximum) edge on the edge\nboundary of the given set of nodes.\n\nA return value of ``None`` indicates an empty boundary.",
  "code": "def best_edge(component):\n    sign = 1 if minimum else -1\n    minwt = float('inf')\n    boundary = None\n    for e in nx.edge_boundary(G, component, data=True):\n        wt = e[-1].get(weight, 1) * sign\n        if isnan(wt):\n            if ignore_nan:\n                continue\n            msg = f'NaN found as an edge weight. Edge {e}'\n            raise ValueError(msg)\n        if wt < minwt:\n            minwt = wt\n            boundary = e\n    return boundary"
 },
 {
  "docstring": "We can think of clusters of contracted nodes as having one\nrepresentative in the graph. Each node which is not in merged_nodes\nis still its own representative. Since a representative can be later\ncontracted, we need to recursively search though the dict to find\nthe final representative, but once we know it we can use path\ncompression to speed up the access of the representative for next time.\n\nThis cannot be replaced by the standard NetworkX union_find since that\ndata structure will merge nodes with less representing nodes into the\none with more representing nodes but this function requires we merge\nthem using the order that contract_edges contracts using.\n\nParameters\n----------\nmerged_nodes : dict\n    The dict storing the mapping from node to representative\nnode\n    The node whose representative we seek\n\nReturns\n-------\nThe representative of the `node`",
  "code": "def find_node(merged_nodes, node):\n    if node not in merged_nodes:\n        return node\n    else:\n        rep = find_node(merged_nodes, merged_nodes[node])\n        merged_nodes[node] = rep\n        return rep"
 },
 {
  "docstring": "For the graph `G`, remove all edges not in the set `V` and then\ncontract all edges in the set `U`.\n\nReturns\n-------\nA copy of `G` which has had all edges not in `V` removed and all edges\nin `U` contracted.",
  "code": "def prepare_graph():\n    result = nx.MultiGraph(incoming_graph_data=G)\n    edges_to_remove = set(result.edges()).difference(V)\n    result.remove_edges_from(edges_to_remove)\n    merged_nodes = {}\n    for u, v in U:\n        u_rep = find_node(merged_nodes, u)\n        v_rep = find_node(merged_nodes, v)\n        if u_rep == v_rep:\n            continue\n        nx.contracted_nodes(result, u_rep, v_rep, self_loops=False, copy=False)\n        merged_nodes[v_rep] = u_rep\n    return (merged_nodes, result)"
 },
 {
  "docstring": "Find the sum of weights of the spanning trees of `G` using the\nappropriate `method`.\n\nThis is easy if the chosen method is 'multiplicative', since we can\nuse Kirchhoff's Tree Matrix Theorem directly. However, with the\n'additive' method, this process is slightly more complex and less\ncomputationally efficient as we have to find the number of spanning\ntrees which contain each possible edge in the graph.\n\nParameters\n----------\nG : NetworkX Graph\n    The graph to find the total weight of all spanning trees on.\n\nweight : string\n    The key for the weight edge attribute of the graph.\n\nReturns\n-------\nfloat\n    The sum of either the multiplicative or additive weight for all\n    spanning trees in the graph.",
  "code": "def spanning_tree_total_weight(G, weight):\n    if multiplicative:\n        return nx.total_spanning_tree_weight(G, weight)\n    elif G.number_of_edges() == 1:\n        return G.edges(data=weight).__iter__().__next__()[2]\n    else:\n        total = 0\n        for u, v, w in G.edges(data=weight):\n            total += w * nx.total_spanning_tree_weight(nx.contracted_edge(G, edge=(u, v), self_loops=False), None)\n        return total"
 },
 {
  "docstring": "Initialize the iterator\n\nParameters\n----------\nG : nx.Graph\n    The directed graph which we need to iterate trees over\n\nweight : String, default = \"weight\"\n    The edge attribute used to store the weight of the edge\n\nminimum : bool, default = True\n    Return the trees in increasing order while true and decreasing order\n    while false.\n\nignore_nan : bool, default = False\n    If a NaN is found as an edge weight normally an exception is raised.\n    If `ignore_nan is True` then that edge is ignored instead.",
  "code": "def __init__(self, G, weight='weight', minimum=True, ignore_nan=False):\n    self.G = G.copy()\n    self.weight = weight\n    self.minimum = minimum\n    self.ignore_nan = ignore_nan\n    self.partition_key = 'SpanningTreeIterators super secret partition attribute name'"
 },
 {
  "docstring": "Returns\n-------\nSpanningTreeIterator\n    The iterator object for this graph",
  "code": "def __iter__(self):\n    self.partition_queue = PriorityQueue()\n    self._clear_partition(self.G)\n    mst_weight = partition_spanning_tree(self.G, self.minimum, self.weight, self.partition_key, self.ignore_nan).size(weight=self.weight)\n    self.partition_queue.put(self.Partition(mst_weight if self.minimum else -mst_weight, {}))\n    return self"
 },
 {
  "docstring": "Returns\n-------\n(multi)Graph\n    The spanning tree of next greatest weight, which ties broken\n    arbitrarily.",
  "code": "def __next__(self):\n    if self.partition_queue.empty():\n        del self.G, self.partition_queue\n        raise StopIteration\n    partition = self.partition_queue.get()\n    self._write_partition(partition)\n    next_tree = partition_spanning_tree(self.G, self.minimum, self.weight, self.partition_key, self.ignore_nan)\n    self._partition(partition, next_tree)\n    self._clear_partition(next_tree)\n    return next_tree"
 },
 {
  "docstring": "Create new partitions based of the minimum spanning tree of the\ncurrent minimum partition.\n\nParameters\n----------\npartition : Partition\n    The Partition instance used to generate the current minimum spanning\n    tree.\npartition_tree : nx.Graph\n    The minimum spanning tree of the input partition.",
  "code": "def _partition(self, partition, partition_tree):\n    p1 = self.Partition(0, partition.partition_dict.copy())\n    p2 = self.Partition(0, partition.partition_dict.copy())\n    for e in partition_tree.edges:\n        if e not in partition.partition_dict:\n            p1.partition_dict[e] = EdgePartition.EXCLUDED\n            p2.partition_dict[e] = EdgePartition.INCLUDED\n            self._write_partition(p1)\n            p1_mst = partition_spanning_tree(self.G, self.minimum, self.weight, self.partition_key, self.ignore_nan)\n            p1_mst_weight = p1_mst.size(weight=self.weight)\n            if nx.is_connected(p1_mst):\n                p1.mst_weight = p1_mst_weight if self.minimum else -p1_mst_weight\n                self.partition_queue.put(p1.__copy__())\n            p1.partition_dict = p2.partition_dict.copy()"
 },
 {
  "docstring": "Writes the desired partition into the graph to calculate the minimum\nspanning tree.\n\nParameters\n----------\npartition : Partition\n    A Partition dataclass describing a partition on the edges of the\n    graph.",
  "code": "def _write_partition(self, partition):\n    for u, v, d in self.G.edges(data=True):\n        if (u, v) in partition.partition_dict:\n            d[self.partition_key] = partition.partition_dict[u, v]\n        else:\n            d[self.partition_key] = EdgePartition.OPEN"
 },
 {
  "docstring": "Removes partition data from the graph",
  "code": "def _clear_partition(self, G):\n    for u, v, d in G.edges(data=True):\n        if self.partition_key in d:\n            del d[self.partition_key]"
 },
 {
  "docstring": "A deprecated name for `join_trees`\n\nReturns a new rooted tree with a root node joined with the roots\nof each of the given rooted trees.\n\n.. deprecated:: 3.2\n\n   `join` is deprecated in NetworkX v3.2 and will be removed in v3.4.\n   It has been renamed join_trees with the same syntax/interface.",
  "code": "def join(rooted_trees, label_attribute=None):\n    import warnings\n    warnings.warn('The function `join` is deprecated and is renamed `join_trees`.\\nThe ``join`` function itself will be removed in v3.4', DeprecationWarning, stacklevel=2)\n    return join_trees(rooted_trees, label_attribute=label_attribute)"
 },
 {
  "docstring": "Returns a new rooted tree made by joining `rooted_trees`\n\nConstructs a new tree by joining each tree in `rooted_trees`.\nA new root node is added and connected to each of the roots\nof the input trees. While copying the nodes from the trees,\nrelabeling to integers occurs. If the `label_attribute` is provided,\nthe old node labels will be stored in the new tree under this attribute.\n\nParameters\n----------\nrooted_trees : list\n    A list of pairs in which each left element is a NetworkX graph\n    object representing a tree and each right element is the root\n    node of that tree. The nodes of these trees will be relabeled to\n    integers.\n\nlabel_attribute : str\n    If provided, the old node labels will be stored in the new tree\n    under this node attribute. If not provided, the original labels\n    of the nodes in the input trees are not stored.\n\nfirst_label : int, optional (default=0)\n    Specifies the label for the new root node. If provided, the root node of the joined tree\n    will have this label. If not provided, the root node will default to a label of 0.\n\nReturns\n-------\nNetworkX graph\n    The rooted tree resulting from joining the provided `rooted_trees`. The new tree has a root node\n    labeled as specified by `first_label` (defaulting to 0 if not provided). Subtrees from the input\n    `rooted_trees` are attached to this new root node. Each non-root node, if the `label_attribute`\n    is provided, has an attribute that indicates the original label of the node in the input tree.\n\n",
  "code": "@nx._dispatch(graphs=None)\ndef join_trees(rooted_trees, *, label_attribute=None, first_label=0):\n    if not rooted_trees:\n        return nx.empty_graph(1)\n    trees, roots = zip(*rooted_trees)\n    R = type(trees[0])()\n    lengths = (len(tree) for tree in trees[:-1])\n    first_labels = list(accumulate(lengths, initial=first_label + 1))\n    new_roots = []\n    for tree, root, first_node in zip(trees, roots, first_labels):\n        new_root = first_node + list(tree.nodes()).index(root)\n        new_roots.append(new_root)\n    relabel = partial(nx.convert_node_labels_to_integers, label_attribute=label_attribute)\n    new_trees = [relabel(tree, first_label=first_label) for tree, first_label in zip(trees, first_labels)]\n    for tree in new_trees:\n        R.update(tree)\n    R.add_node(first_label)\n    R.add_edges_from(((first_label, root) for root in new_roots))\n    return R"
 },
 {
  "docstring": "Returns True if `G` is an arborescence.\n\nAn arborescence is a directed tree with maximum in-degree equal to 1.\n\nParameters\n----------\nG : graph\n    The graph to test.\n\nReturns\n-------\nb : bool\n    A boolean that is True if `G` is an arborescence.\n\nExamples\n--------\n>>> G = nx.DiGraph([(0, 1), (0, 2), (2, 3), (3, 4)])\n>>> nx.is_arborescence(G)\nTrue\n>>> G.remove_edge(0, 1)\n>>> G.add_edge(1, 2)  # maximum in-degree is 2\n>>> nx.is_arborescence(G)\nFalse\n\n",
  "code": "@nx.utils.not_implemented_for('undirected')\n@nx._dispatch\ndef is_arborescence(G):\n    return is_tree(G) and max((d for n, d in G.in_degree())) <= 1"
 },
 {
  "docstring": "Returns True if `G` is a branching.\n\nA branching is a directed forest with maximum in-degree equal to 1.\n\nParameters\n----------\nG : directed graph\n    The directed graph to test.\n\nReturns\n-------\nb : bool\n    A boolean that is True if `G` is a branching.\n\nExamples\n--------\n>>> G = nx.DiGraph([(0, 1), (1, 2), (2, 3), (3, 4)])\n>>> nx.is_branching(G)\nTrue\n>>> G.remove_edge(2, 3)\n>>> G.add_edge(3, 1)  # maximum in-degree is 2\n>>> nx.is_branching(G)\nFalse\n\n",
  "code": "@nx.utils.not_implemented_for('undirected')\n@nx._dispatch\ndef is_branching(G):\n    return is_forest(G) and max((d for n, d in G.in_degree())) <= 1"
 },
 {
  "docstring": "Returns True if `G` is a forest.\n\nA forest is a graph with no undirected cycles.\n\nFor directed graphs, `G` is a forest if the underlying graph is a forest.\nThe underlying graph is obtained by treating each directed edge as a single\nundirected edge in a multigraph.\n\nParameters\n----------\nG : graph\n    The graph to test.\n\nReturns\n-------\nb : bool\n    A boolean that is True if `G` is a forest.\n\nRaises\n------\nNetworkXPointlessConcept\n    If `G` is empty.\n\nExamples\n--------\n>>> G = nx.Graph()\n>>> G.add_edges_from([(1, 2), (1, 3), (2, 4), (2, 5)])\n>>> nx.is_forest(G)\nTrue\n>>> G.add_edge(4, 1)\n>>> nx.is_forest(G)\nFalse\n\n",
  "code": "@nx._dispatch\ndef is_forest(G):\n    if len(G) == 0:\n        raise nx.exception.NetworkXPointlessConcept('G has no nodes.')\n    if G.is_directed():\n        components = (G.subgraph(c) for c in nx.weakly_connected_components(G))\n    else:\n        components = (G.subgraph(c) for c in nx.connected_components(G))\n    return all((len(c) - 1 == c.number_of_edges() for c in components))"
 },
 {
  "docstring": "Returns True if `G` is a tree.\n\nA tree is a connected graph with no undirected cycles.\n\nFor directed graphs, `G` is a tree if the underlying graph is a tree. The\nunderlying graph is obtained by treating each directed edge as a single\nundirected edge in a multigraph.\n\nParameters\n----------\nG : graph\n    The graph to test.\n\nReturns\n-------\nb : bool\n    A boolean that is True if `G` is a tree.\n\nRaises\n------\nNetworkXPointlessConcept\n    If `G` is empty.\n\nExamples\n--------\n>>> G = nx.Graph()\n>>> G.add_edges_from([(1, 2), (1, 3), (2, 4), (2, 5)])\n>>> nx.is_tree(G)  # n-1 edges\nTrue\n>>> G.add_edge(3, 4)\n>>> nx.is_tree(G)  # n edges\nFalse\n\n",
  "code": "@nx._dispatch\ndef is_tree(G):\n    if len(G) == 0:\n        raise nx.exception.NetworkXPointlessConcept('G has no nodes.')\n    if G.is_directed():\n        is_connected = nx.is_weakly_connected\n    else:\n        is_connected = nx.is_connected\n    return len(G) - 1 == G.number_of_edges() and is_connected(G)"
 },
 {
  "docstring": "Test that we can generate minimum spanning arborescences which respect the\ngiven partition.",
  "code": "def test_partition_spanning_arborescence():\n    G = nx.from_numpy_array(G_array, create_using=nx.DiGraph)\n    G[3][0]['partition'] = nx.EdgePartition.EXCLUDED\n    G[2][3]['partition'] = nx.EdgePartition.INCLUDED\n    G[7][3]['partition'] = nx.EdgePartition.EXCLUDED\n    G[0][2]['partition'] = nx.EdgePartition.EXCLUDED\n    G[6][2]['partition'] = nx.EdgePartition.INCLUDED\n    actual_edges = [(0, 4, 12), (1, 0, 4), (1, 5, 13), (2, 3, 21), (4, 7, 12), (5, 6, 14), (5, 8, 12), (6, 2, 21)]\n    B = branchings.minimum_spanning_arborescence(G, partition='partition')\n    assert_equal_branchings(build_branching(actual_edges), B)"
 },
 {
  "docstring": "Tests the arborescence iterator.\n\nA brute force method found 680 arborescences in this graph.\nThis test will not verify all of them individually, but will check two\nthings\n\n* The iterator returns 680 arborescences\n* The weight of the arborescences is non-strictly increasing\n\nfor more information please visit\nhttps://mjschwenne.github.io/2021/06/10/implementing-the-iterators.html",
  "code": "def test_arborescence_iterator_min():\n    G = nx.from_numpy_array(G_array, create_using=nx.DiGraph)\n    arborescence_count = 0\n    arborescence_weight = -math.inf\n    for B in branchings.ArborescenceIterator(G):\n        arborescence_count += 1\n        new_arborescence_weight = B.size(weight='weight')\n        assert new_arborescence_weight >= arborescence_weight\n        arborescence_weight = new_arborescence_weight\n    assert arborescence_count == 680"
 },
 {
  "docstring": "Tests the arborescence iterator.\n\nA brute force method found 680 arborescences in this graph.\nThis test will not verify all of them individually, but will check two\nthings\n\n* The iterator returns 680 arborescences\n* The weight of the arborescences is non-strictly decreasing\n\nfor more information please visit\nhttps://mjschwenne.github.io/2021/06/10/implementing-the-iterators.html",
  "code": "def test_arborescence_iterator_max():\n    G = nx.from_numpy_array(G_array, create_using=nx.DiGraph)\n    arborescence_count = 0\n    arborescence_weight = math.inf\n    for B in branchings.ArborescenceIterator(G, minimum=False):\n        arborescence_count += 1\n        new_arborescence_weight = B.size(weight='weight')\n        assert new_arborescence_weight <= arborescence_weight\n        arborescence_weight = new_arborescence_weight\n    assert arborescence_count == 680"
 },
 {
  "docstring": "Tests the arborescence iterator with three included edges and three excluded\nin the initial partition.\n\nA brute force method similar to the one used in the above tests found that\nthere are 16 arborescences which contain the included edges and not the\nexcluded edges.",
  "code": "def test_arborescence_iterator_initial_partition():\n    G = nx.from_numpy_array(G_array, create_using=nx.DiGraph)\n    included_edges = [(1, 0), (5, 6), (8, 7)]\n    excluded_edges = [(0, 2), (3, 6), (1, 5)]\n    arborescence_count = 0\n    arborescence_weight = -math.inf\n    for B in branchings.ArborescenceIterator(G, init_partition=(included_edges, excluded_edges)):\n        arborescence_count += 1\n        new_arborescence_weight = B.size(weight='weight')\n        assert new_arborescence_weight >= arborescence_weight\n        arborescence_weight = new_arborescence_weight\n        for e in included_edges:\n            assert e in B.edges\n        for e in excluded_edges:\n            assert e not in B.edges\n    assert arborescence_count == 16"
 },
 {
  "docstring": "Tests for encoding a tree as a Pr\u00fcfer sequence using the\niterative strategy.",
  "code": "def test_encoding(self):\n    tree = nx.Graph([(0, 3), (1, 3), (2, 3), (3, 4), (4, 5)])\n    sequence = nx.to_prufer_sequence(tree)\n    assert sequence == [3, 3, 3, 4]"
 },
 {
  "docstring": "Tests for decoding a tree from a Pr\u00fcfer sequence.",
  "code": "def test_decoding(self):\n    sequence = [3, 3, 3, 4]\n    tree = nx.from_prufer_sequence(sequence)\n    assert nodes_equal(list(tree), list(range(6)))\n    edges = [(0, 3), (1, 3), (2, 3), (3, 4), (4, 5)]\n    assert edges_equal(list(tree.edges()), edges)"
 },
 {
  "docstring": "Tests that the encoding and decoding functions are inverses.",
  "code": "def test_inverse(self):\n    for T in nx.nonisomorphic_trees(4):\n        T2 = nx.from_prufer_sequence(nx.to_prufer_sequence(T))\n        assert nodes_equal(list(T), list(T2))\n        assert edges_equal(list(T.edges()), list(T2.edges()))\n    for seq in product(range(4), repeat=2):\n        seq2 = nx.to_prufer_sequence(nx.from_prufer_sequence(seq))\n        assert list(seq) == seq2"
 },
 {
  "docstring": "Using a fixed seed, sample one tree for repeatability.",
  "code": "def test_random_spanning_tree_multiplicative_small():\n    from math import exp\n    pytest.importorskip('scipy')\n    gamma = {(0, 1): -0.6383, (0, 2): -0.6827, (0, 5): 0, (1, 2): -1.0781, (1, 4): 0, (2, 3): 0, (5, 3): -0.282, (5, 4): -0.3327, (4, 3): -0.9927}\n    G = nx.Graph()\n    for u, v in gamma:\n        G.add_edge(u, v, lambda_key=exp(gamma[u, v]))\n    solution_edges = [(2, 3), (3, 4), (0, 5), (5, 4), (4, 1)]\n    solution = nx.Graph()\n    solution.add_edges_from(solution_edges)\n    sampled_tree = nx.random_spanning_tree(G, 'lambda_key', seed=42)\n    assert nx.utils.edges_equal(solution.edges, sampled_tree.edges)"
 },
 {
  "docstring": "Sample many trees from the distribution created in the last test",
  "code": "@pytest.mark.slow\ndef test_random_spanning_tree_multiplicative_large():\n    from math import exp\n    from random import Random\n    pytest.importorskip('numpy')\n    stats = pytest.importorskip('scipy.stats')\n    gamma = {(0, 1): -0.6383, (0, 2): -0.6827, (0, 5): 0, (1, 2): -1.0781, (1, 4): 0, (2, 3): 0, (5, 3): -0.282, (5, 4): -0.3327, (4, 3): -0.9927}\n    G = nx.Graph()\n    for u, v in gamma:\n        G.add_edge(u, v, lambda_key=exp(gamma[u, v]))\n    total_weight = 0\n    tree_expected = {}\n    for t in nx.SpanningTreeIterator(G):\n        weight = 1\n        for u, v, d in t.edges(data='lambda_key'):\n            weight *= d\n        tree_expected[t] = weight\n        total_weight += weight\n    assert len(tree_expected) == 75\n    sample_size = 1200\n    tree_actual = {}\n    for t in tree_expected:\n        tree_expected[t] = tree_expected[t] / total_weight * sample_size\n        tree_actual[t] = 0\n    rng = Random(37)\n    for _ in range(sample_size):\n        sampled_tree = nx.random_spanning_tree(G, 'lambda_key', seed=rng)\n        assert nx.is_tree(sampled_tree)\n        for t in tree_expected:\n            if nx.utils.edges_equal(t.edges, sampled_tree.edges):\n                tree_actual[t] += 1\n                break\n    _, p = stats.chisquare(list(tree_actual.values()), list(tree_expected.values()))\n    assert not p < 0.05"
 },
 {
  "docstring": "Sample a single spanning tree from the additive method.",
  "code": "def test_random_spanning_tree_additive_small():\n    pytest.importorskip('scipy')\n    edges = {(0, 1): 1, (0, 2): 1, (0, 5): 3, (1, 2): 2, (1, 4): 3, (2, 3): 3, (5, 3): 4, (5, 4): 5, (4, 3): 4}\n    G = nx.Graph()\n    for u, v in edges:\n        G.add_edge(u, v, weight=edges[u, v])\n    solution_edges = [(0, 2), (1, 2), (2, 3), (3, 4), (3, 5)]\n    solution = nx.Graph()\n    solution.add_edges_from(solution_edges)\n    sampled_tree = nx.random_spanning_tree(G, weight='weight', multiplicative=False, seed=37)\n    assert nx.utils.edges_equal(solution.edges, sampled_tree.edges)"
 },
 {
  "docstring": "Sample many spanning trees from the additive method.",
  "code": "@pytest.mark.slow\ndef test_random_spanning_tree_additive_large():\n    from random import Random\n    pytest.importorskip('numpy')\n    stats = pytest.importorskip('scipy.stats')\n    edges = {(0, 1): 1, (0, 2): 1, (0, 5): 3, (1, 2): 2, (1, 4): 3, (2, 3): 3, (5, 3): 4, (5, 4): 5, (4, 3): 4}\n    G = nx.Graph()\n    for u, v in edges:\n        G.add_edge(u, v, weight=edges[u, v])\n    total_weight = 0\n    tree_expected = {}\n    for t in nx.SpanningTreeIterator(G):\n        weight = 0\n        for u, v, d in t.edges(data='weight'):\n            weight += d\n        tree_expected[t] = weight\n        total_weight += weight\n    assert len(tree_expected) == 75\n    sample_size = 500\n    tree_actual = {}\n    for t in tree_expected:\n        tree_expected[t] = tree_expected[t] / total_weight * sample_size\n        tree_actual[t] = 0\n    rng = Random(37)\n    for _ in range(sample_size):\n        sampled_tree = nx.random_spanning_tree(G, 'weight', multiplicative=False, seed=rng)\n        assert nx.is_tree(sampled_tree)\n        for t in tree_expected:\n            if nx.utils.edges_equal(t.edges, sampled_tree.edges):\n                tree_actual[t] += 1\n                break\n    _, p = stats.chisquare(list(tree_actual.values()), list(tree_expected.values()))\n    assert not p < 0.05"
 },
 {
  "docstring": "Creates an example graph and stores the expected minimum and\nmaximum spanning tree edges.",
  "code": "def setup_method(self, method):\n    self.algo = self.algorithm\n    edges = [(0, 1, 7), (0, 3, 5), (1, 2, 8), (1, 3, 9), (1, 4, 7), (2, 4, 5), (3, 4, 15), (3, 5, 6), (4, 5, 8), (4, 6, 9), (5, 6, 11)]\n    self.G = nx.Graph()\n    self.G.add_weighted_edges_from(edges)\n    self.minimum_spanning_edgelist = [(0, 1, {'weight': 7}), (0, 3, {'weight': 5}), (1, 4, {'weight': 7}), (2, 4, {'weight': 5}), (3, 5, {'weight': 6}), (4, 6, {'weight': 9})]\n    self.maximum_spanning_edgelist = [(0, 1, {'weight': 7}), (1, 2, {'weight': 8}), (1, 3, {'weight': 9}), (3, 4, {'weight': 15}), (4, 6, {'weight': 9}), (5, 6, {'weight': 11})]"
 },
 {
  "docstring": "Tests that using a Unicode string can correctly indicate\nBor\u016fvka's algorithm.",
  "code": "def test_unicode_name(self):\n    edges = nx.minimum_spanning_edges(self.G, algorithm='bor\u016fvka')\n    actual = sorted(((min(u, v), max(u, v), d) for u, v, d in edges))\n    assert edges_equal(actual, self.minimum_spanning_edgelist)"
 },
 {
  "docstring": "Tests that the minimum spanning edges of a multigraph\npreserves edge keys.",
  "code": "def test_multigraph_keys_min(self):\n    G = nx.MultiGraph()\n    G.add_edge(0, 1, key='a', weight=2)\n    G.add_edge(0, 1, key='b', weight=1)\n    min_edges = nx.minimum_spanning_edges\n    mst_edges = min_edges(G, algorithm=self.algo, data=False)\n    assert edges_equal([(0, 1, 'b')], list(mst_edges))"
 },
 {
  "docstring": "Tests that the maximum spanning edges of a multigraph\npreserves edge keys.",
  "code": "def test_multigraph_keys_max(self):\n    G = nx.MultiGraph()\n    G.add_edge(0, 1, key='a', weight=2)\n    G.add_edge(0, 1, key='b', weight=1)\n    max_edges = nx.maximum_spanning_edges\n    mst_edges = max_edges(G, algorithm=self.algo, data=False)\n    assert edges_equal([(0, 1, 'a')], list(mst_edges))"
 },
 {
  "docstring": "Tests that the keys and data values are included in\nMST edges based on whether keys and data parameters are\ntrue or false",
  "code": "def test_key_data_bool(self):\n    G = nx.MultiGraph()\n    G.add_edge(1, 2, key=1, weight=2)\n    G.add_edge(1, 2, key=2, weight=3)\n    G.add_edge(3, 2, key=1, weight=2)\n    G.add_edge(3, 1, key=1, weight=4)\n    mst_edges = nx.minimum_spanning_edges(G, algorithm=self.algo, keys=True, data=False)\n    assert edges_equal([(1, 2, 1), (2, 3, 1)], list(mst_edges))\n    mst_edges = nx.minimum_spanning_edges(G, algorithm=self.algo, keys=False, data=True)\n    assert edges_equal([(1, 2, {'weight': 2}), (2, 3, {'weight': 2})], list(mst_edges))\n    mst_edges = nx.minimum_spanning_edges(G, algorithm=self.algo, keys=False, data=False)\n    assert edges_equal([(1, 2), (2, 3)], list(mst_edges))\n    mst_edges = nx.minimum_spanning_edges(G, algorithm=self.algo, keys=True, data=True)\n    assert edges_equal([(1, 2, 1, {'weight': 2}), (2, 3, 1, {'weight': 2})], list(mst_edges))"
 },
 {
  "docstring": "Tests that the edges with NaN weights are ignored or\nraise an Error based on ignore_nan is true or false",
  "code": "def test_ignore_nan(self):\n    H = nx.MultiGraph()\n    H.add_edge(1, 2, key=1, weight=float('nan'))\n    H.add_edge(1, 2, key=2, weight=3)\n    H.add_edge(3, 2, key=1, weight=2)\n    H.add_edge(3, 1, key=1, weight=4)\n    mst_edges = nx.minimum_spanning_edges(H, algorithm=self.algo, ignore_nan=True)\n    assert edges_equal([(1, 2, 2, {'weight': 3}), (2, 3, 1, {'weight': 2})], list(mst_edges))\n    with pytest.raises(ValueError):\n        list(nx.minimum_spanning_edges(H, algorithm=self.algo, ignore_nan=False))"
 },
 {
  "docstring": "Tests that the spanning trees are correctly returned in increasing order",
  "code": "def test_minimum_spanning_tree_iterator(self):\n    tree_index = 0\n    for tree in nx.SpanningTreeIterator(self.G):\n        actual = sorted(tree.edges(data=True))\n        assert edges_equal(actual, self.spanning_trees[tree_index])\n        tree_index += 1"
 },
 {
  "docstring": "Tests that the spanning trees are correctly returned in decreasing order",
  "code": "def test_maximum_spanning_tree_iterator(self):\n    tree_index = 7\n    for tree in nx.SpanningTreeIterator(self.G, minimum=False):\n        actual = sorted(tree.edges(data=True))\n        assert edges_equal(actual, self.spanning_trees[tree_index])\n        tree_index -= 1"
 },
 {
  "docstring": "Joining the empty sequence results in the tree with one node.",
  "code": "def test_empty_sequence():\n    T = nx.join_trees([])\n    assert len(T) == 1\n    assert T.number_of_edges() == 0"
 },
 {
  "docstring": "Joining just one tree yields a tree with one more node.",
  "code": "def test_single():\n    T = nx.empty_graph(1)\n    trees = [(T, 0)]\n    actual_with_label = nx.join_trees(trees, label_attribute='custom_label')\n    expected = nx.path_graph(2)\n    assert nodes_equal(list(expected), list(actual_with_label))\n    assert edges_equal(list(expected.edges()), list(actual_with_label.edges()))"
 },
 {
  "docstring": "Joining multiple subtrees at a root node.",
  "code": "def test_basic():\n    trees = [(nx.full_rary_tree(2, 2 ** 2 - 1), 0) for i in range(2)]\n    expected = nx.full_rary_tree(2, 2 ** 3 - 1)\n    actual = nx.join_trees(trees, label_attribute='old_labels')\n    assert nx.is_isomorphic(actual, expected)\n    assert _check_custom_label_attribute(trees, actual, 'old_labels')\n    actual_without_label = nx.join_trees(trees)\n    assert nx.is_isomorphic(actual_without_label, expected)\n    assert all((not data for _, data in actual_without_label.nodes(data=True)))"
 },
 {
  "docstring": "Test the functionality of the first_label argument.",
  "code": "def test_first_label():\n    T1 = nx.path_graph(3)\n    T2 = nx.path_graph(2)\n    actual = nx.join_trees([(T1, 0), (T2, 0)], first_label=10)\n    expected_nodes = set(range(10, 16))\n    assert set(actual.nodes()) == expected_nodes\n    assert set(actual.neighbors(10)) == {11, 14}"
 }
]